
---
title: "What Are Large Language Models Mapping to in the Brain? A Case Against Over-Reliance on Brain Scores"
id: "2406.01538v1"
description: "LLMs' brain scores may be inflated, partly due to simple features like sentence length and position. Over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains."
author: Ebrahim Feghhi, Nima Hadidi, Bryan Song, Idan A. Blank, Jonathan C. Kao
date: "2024-06-03"
image: "https://browse.arxiv.org/html/2406.01538v1/x1.png"
categories: ['social-sciences', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.01538v1/x1.png)

**Summary:**

The study investigates the similarity between large language models (LLMs) and the human brain, focusing on the use of "brain scores" to measure this similarity. The authors question the assumption that the subset of neural activity predicted by LLMs reflects core processes of the human language system. They analyze three neural datasets, with a focus on an fMRI dataset where participants read short passages. The authors find that when using shuffled train-test splits, a trivial feature that encodes temporal autocorrelation outperforms LLMs and accounts for the majority of neural variance explained by LLMs. They caution against using shuffled train-test splits and use contiguous test splits moving forward. The study also explains the surprising result that untrained LLMs have higher-than-expected brain scores by showing they do not account for additional neural variance beyond two simple features: sentence length and sentence position. This undermines evidence used to claim that the transformer architecture biases computations to be more brain-like. The authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, and emphasize the importance of deconstructing what LLMs are mapping to in neural signals.

**Major Findings:**

1. Shuffled train-test splits are severely affected by temporal autocorrelation, leading to contradictory results compared to contiguous splits.
2. Untrained GPT2-XL neural predictivity is fully accounted for by sentence length and position.
3. Sentence length, sentence position, and static word embeddings account for the majority of trained GPT2-XL neural predictivity.

**Analysis and Critique:**

The study raises several potential problems and shortcomings in the use of brain scores to evaluate the similarity between LLMs and the human brain. The authors highlight the issue of temporal autocorrelation and the need to use contiguous test splits instead of shuffled train-test splits. They also question the assumption that the subset of neural activity predicted by LLMs reflects core processes of the human language system. The study provides a critical analysis of the use of brain scores and emphasizes the importance of deconstructing what LLMs are mapping to in neural signals. However, the study does not provide a clear alternative

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.01538v1](https://arxiv.org/abs/2406.01538v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.01538v1](https://browse.arxiv.org/html/2406.01538v1)       |
| Truncated       | False       |
| Word Count       | 10801       |