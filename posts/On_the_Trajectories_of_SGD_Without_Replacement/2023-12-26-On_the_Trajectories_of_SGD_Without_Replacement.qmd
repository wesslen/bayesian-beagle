
---
title: "On the Trajectories of SGD Without Replacement"
id: "2312.16143v1"
description: "Stochastic Gradient Descent without replacement implicitly regularizes and optimizes differently than other methods, leading to faster escape from saddles and sparser Hessian spectra."
author: ['Pierfrancesco Beneventano']
date: "2023-12-26"
image: "https://browse.arxiv.org/html/2312.16143v1/x1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.16143v1/x1.png)

### Major Takeaways

1. **Implicit Regularization of SGD**: The paper demonstrates that Stochastic Gradient Descent (SGD) without replacement has an implicit regularization effect, which biases the dynamics towards areas with lower variance in the eigendirections corresponding to smaller Hessian eigenvalues.

2. **Trajectory Differences**: The trajectory of SGD without replacement differs significantly from noise-injected GD and SGD with replacement, traveling faster and presenting a smaller variance. Additionally, it diverges from these algorithms in regions of parameter space where the loss is nearly constant, showing smaller oscillations and better accuracy.

3. **Explanation for Empirical Observations**: The study provides potential explanations for empirically observed phenomena, such as SGD converging to almost-global loss minima, shaping the spectrum of the Hessian of the loss, and producing clusters of large outlying eigenvalues in the course of training.

### Sections Summary

#### 1 Introduction
- Evaluates the implicit regularization effect of SGD without replacement and its divergence from noise-injected GD and SGD with replacement.
  
#### 2 The Problem
- Discusses the training of neural networks using the SGD without replacement variant and outlines the goals of the study.

#### 3 Implicit Bias of SGD Without Replacement
- Introduces the concept of a regularizer biases the trajectory of SGD and its implications, along with mathematical notations and technical aspects.

#### 4 Shaping the Hessian
- Explains how the regularizer shapes the Hessian of the loss, emphasizing empirical observations and the impact on variance, global minima, and flatter models.

### Critique

- While the paper provides valuable insights into the implicit regularization effect of SGD without replacement, it lacks a comprehensive discussion on the limitations and potential drawbacks of the proposed analysis. Addressing potential limitations and exploring alternative explanations would enhance the robustness of the findings.
- The heavy use of mathematical notations and technical details in the main text may hinder the accessibility of the paper to a broader audience. A clearer and more accessible presentation of the key findings and implications would improve the readability of the paper.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [http://arxiv.org/abs/2312.16143v1](http://arxiv.org/abs/2312.16143v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.16143v1](https://browse.arxiv.org/html/2312.16143v1)       |
| Truncated       | True       |
| Word Count       | 30429       |