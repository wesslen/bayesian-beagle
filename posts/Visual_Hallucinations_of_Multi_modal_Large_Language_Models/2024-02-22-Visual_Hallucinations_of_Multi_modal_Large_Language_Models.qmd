
---
title: "Visual Hallucinations of Multi-modal Large Language Models"
id: "2402.14683v1"
description: "Tool VHTest generates diverse VH instances, finds MLLM hallucinations, and improves performance through fine-tuning."
author: Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14683v1/x1.png"
categories: ['robustness', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14683v1/x1.png)

### Summary:
- The article introduces the concept of visual hallucination (VH) in multi-modal large language models (MLLMs) and proposes a tool called VHTest to generate a diverse set of VH instances.
- VHTest is used to collect a benchmark dataset with 1,200 VH instances in 8 VH modes, and it is found that existing MLLMs hallucinate for a large fraction of the instances in the benchmark.
- The article also shows that fine-tuning an MLLM using the benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks.

### Major Findings:
1. Existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in the benchmark dataset.
2. Fine-tuning an MLLM using the benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks.
3. VHTest is highly effective at generating successful VH instances that trigger VHs in MLLMs.

### Analysis and Critique:
- The article provides a comprehensive evaluation of state-of-the-art MLLMs and demonstrates the effectiveness of VHTest in generating diverse VH instances. However, the reliance on human workers to manually generate question-answer pairs for automatically generated VH images may limit the scalability of VHTest.
- The article acknowledges the need for future work to make VHTest fully automatic to generate as many VH instances as needed, addressing the potential scalability issue.
- The fine-tuning experiments provide valuable insights into mitigating VH in MLLMs, but the article could benefit from a more in-depth discussion of the limitations and potential biases associated with the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.14683v1](https://arxiv.org/abs/2402.14683v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14683v1](https://browse.arxiv.org/html/2402.14683v1)       |
| Truncated       | False       |
| Word Count       | 7988       |