
---
title: "HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization"
id: "2402.16694v1"
description: "HumanEval-XL: Multilingual code generation benchmark for evaluating multilingual LLMs. 22,080 prompts, 23 NLs, 12 PLs."
author: Qiwei Peng, Yekun Chai, Xuhong Li
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16694v1/x1.png"
categories: ['prompt-engineering', 'programming', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16694v1/x1.png)

### Summary:
- Large language models (LLMs) have made significant progress in generating codes from textual prompts.
- Existing benchmarks have mainly focused on translating English prompts to multilingual codes or have been limited to very few natural languages (NLs).
- HumanEval-XL is a massively multilingual code generation benchmark that addresses the deficiency in evaluating multilingual LLMs.
- The benchmark connects 23 NLs and 12 programming languages (PLs) and comprises 22,080 prompts with an average of 8.33 test cases.

### Major Findings:
1. HumanEval-XL establishes connections between 23 NLs and 12 PLs, offering a comprehensive evaluation platform for multilingual LLMs.
2. Extensive experiments reveal that substantial increases in model size significantly boost the proficiency of generating code in multiple languages.
3. Current LLMs struggle to capture the equivalent semantic meaning expressed in different languages in the task of code generation.

### Analysis and Critique:
- The article provides a comprehensive evaluation platform for multilingual LLMs, addressing the gap in evaluating NL generalization in the area of multilingual code generation.
- The study highlights the challenges faced by current LLMs in capturing the equivalent semantic meaning expressed in different languages, indicating the need for further research in this area.
- The article does not discuss potential biases or limitations in the construction of the HumanEval-XL benchmark, which could impact the generalizability of the findings.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.16694v1](https://arxiv.org/abs/2402.16694v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16694v1](https://browse.arxiv.org/html/2402.16694v1)       |
| Truncated       | False       |
| Word Count       | 4198       |