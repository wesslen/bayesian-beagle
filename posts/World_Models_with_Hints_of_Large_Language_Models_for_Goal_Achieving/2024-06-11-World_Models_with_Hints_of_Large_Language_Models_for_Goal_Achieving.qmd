
---
title: "World Models with Hints of Large Language Models for Goal Achieving"
id: "2406.07381v1"
description: "DLLM, a multi-modal RL approach, improves exploration in long-horizon tasks by integrating hinting subgoals from LLMs, outperforming recent methods in sparse-reward environments."
author: Zeyuan Liu, Ziyu Huan, Xiyao Wang, Jiafei Lyu, Jian Tao, Xiu Li, Furong Huang, Huazhe Xu
date: "2024-06-11"
image: "https://browse.arxiv.org/html/2406.07381v1/x1.png"
categories: ['production', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.07381v1/x1.png)

# Summary:

The paper introduces a new multi-modal model-based RL approach called Dreaming with Large Language Models (DLLM). DLLM integrates hinting subgoals from LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks. By assigning higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts, DLLM guides the agent toward meaningful and efficient exploration. Extensive experiments demonstrate that DLLM outperforms recent methods in various challenging, sparse-reward environments.

## Major Findings:

1. DLLM integrates hinting subgoals from LLMs into the model rollouts to encourage goal discovery and reaching in challenging tasks.
2. DLLM assigns higher intrinsic rewards to samples that align with the hints outlined by the language model during model rollouts.
3. DLLM outperforms recent methods in various challenging, sparse-reward environments such as HomeGrid, Crafter, and Minecraft.

## Analysis and Critique:

The paper presents an innovative approach to addressing the challenges of long-horizon tasks and sparse rewards in RL. The use of LLMs to provide hinting subgoals is a promising direction for improving exploration and goal-reaching in complex environments. However, the paper does not discuss potential limitations or biases in the LLMs used, which could impact the performance of DLLM. Additionally, the paper does not provide a detailed comparison with other methods that use intrinsic rewards or LLMs for goal-setting. Further research is needed to evaluate the robustness and generalizability of DLLM in different environments and tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.07381v1](https://arxiv.org/abs/2406.07381v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07381v1](https://browse.arxiv.org/html/2406.07381v1)       |
| Truncated       | False       |
| Word Count       | 10623       |