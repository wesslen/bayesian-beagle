
---
title: "Efficient Detection of Toxic Prompts in Large Language Models"
id: "2408.11727v1"
description: "ToxicDetector: Efficient, accurate method for toxic prompt detection in LLMs."
author: Yi Liu, Junzhe Yu, Huijia Sun, Ling Shi, Gelei Deng, Yuqi Chen, Yang Liu
date: "2024-08-21"
image: "../../img/2408.11727v1/image_1.png"
categories: ['robustness', 'security', 'programming']
format:
  html:
    code-overflow: wrap
---

![](../../img/2408.11727v1/image_1.png)

**Summary:**

The paper "Efficient Detection of Toxic Prompts in Large Language Models" presents a novel method called ToxicDetector for detecting toxic prompts in large language models (LLMs). The method leverages LLMs to create toxic concept prompts, uses embedding vectors to form feature vectors, and employs a Multi-Layer Perceptron (MLP) classifier for prompt classification. The evaluation on various versions of the LLama models, Gemma-2, and multiple datasets demonstrates that ToxicDetector achieves a high accuracy of 96.39% and a low false positive rate of 2.00%, outperforming state-of-the-art methods. Additionally, ToxicDetector's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications.

**Major Findings:**

1. ToxicDetector achieves a high accuracy of 96.39% and a low false positive rate of 2.00% in detecting toxic prompts in LLMs.
2. The method outperforms state-of-the-art methods in terms of accuracy and false positive rate.
3. ToxicDetector's processing time of 0.0780 seconds per prompt makes it highly suitable for real-time applications.

**Analysis and Critique:**

The paper presents a promising method for detecting toxic prompts in LLMs. The high accuracy and low false positive rate achieved by ToxicDetector demonstrate its effectiveness in identifying toxic prompts. The use of embedding vectors and an MLP classifier allows for efficient and scalable detection, making it suitable for real-time applications. However, the paper does not discuss potential limitations or shortcomings of the method, such as its performance on different types of toxic prompts or its generalizability to other LLMs. Additionally, the paper does not provide a detailed comparison with other state-of-the-art methods, making it difficult to fully evaluate its performance. Overall, ToxicDetector shows promise as a method for detecting toxic prompts in LLMs, but further research is needed to fully understand its strengths and limitations.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.11727v1](https://arxiv.org/abs/2408.11727v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.11727v1](https://browse.arxiv.org/html/2408.11727v1)       |
| Truncated       | False       |
| Word Count       | 21143       |