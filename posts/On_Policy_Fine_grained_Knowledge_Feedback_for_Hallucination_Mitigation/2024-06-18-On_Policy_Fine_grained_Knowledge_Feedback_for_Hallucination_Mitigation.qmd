
---
title: "On-Policy Fine-grained Knowledge Feedback for Hallucination Mitigation"
id: "2406.12221v1"
description: "RLFH is an online reinforcement learning method for hallucination mitigation in LLMs, using fine-grained feedback and an LLM-based fact assessment framework."
author: Xueru Wen, Xinyu Lu, Xinyan Guan, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun
date: "2024-06-18"
image: "https://browse.arxiv.org/html/2406.12221v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.12221v1/x1.png)

### Summary:

The paper introduces Ṟeinforcement Ḻearning f̱or H̱allucination (RLFH), a fine-grained feedback-based online reinforcement learning method for hallucination mitigation in large language models (LLMs). Unlike previous learning-based methods, RLFH enables LLMs to explore their knowledge scope and adjust their behavior based on fine-grained on-policy feedback. The approach provides fine-grained knowledge feedback based on atomic fact judgment and constructs token-level dense rewards for online reinforcement learning. Experiments on three factual benchmarks show that RLFH can significantly improve the truthfulness and informativeness of LLMs under both in-distribution and out-of-distribution settings.

### Major Findings:

1. RLFH enables LLMs to explore their knowledge scope and adjust their behavior based on fine-grained on-policy feedback.
2. The approach provides fine-grained knowledge feedback based on atomic fact judgment and constructs token-level dense rewards for online reinforcement learning.
3. Experiments on three factual benchmarks show that RLFH can significantly improve the truthfulness and informativeness of LLMs under both in-distribution and out-of-distribution settings.

### Analysis and Critique:

1. The paper does not provide a detailed comparison with other state-of-the-art methods for hallucination mitigation in LLMs.
2. The paper does not discuss the potential limitations of the proposed approach, such as the computational cost of generating fine-grained feedback and the potential for overfitting to the specific feedback used during training.
3. The paper does not provide a detailed analysis of the impact of the proposed approach on the overall performance of LLMs, such as the impact on perplexity or other language modeling metrics.
4. The paper does not discuss the potential for the proposed approach to be applied to other types of models, such as non-language models or models with different architectures.
5. The paper does not provide a detailed discussion of the potential ethical implications of the proposed approach, such as the potential for the approach to be used to generate misleading or harmful content.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.12221v1](https://arxiv.org/abs/2406.12221v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.12221v1](https://browse.arxiv.org/html/2406.12221v1)       |
| Truncated       | False       |
| Word Count       | 7146       |