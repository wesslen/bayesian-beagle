
---
title: "CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models"
id: "2407.02408v1"
description: "CEB: A Comprehensive Benchmark for Evaluating Bias in Large Language Models."
author: Song Wang, Peng Wang, Tong Zhou, Yushun Dong, Zhen Tan, Jundong Li
date: "2024-07-02"
image: "../../img/2407.02408v1/image_1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](../../img/2407.02408v1/image_1.png)

**Summary:**
The paper introduces a Compositional Evaluation Benchmark (CEB) to address the limitations of existing bias evaluation efforts for Large Language Models (LLMs). CEB consists of 11,004 samples covering different types of bias across various social groups and tasks. The curation of CEB is based on a newly proposed compositional taxonomy that characterizes each dataset from three dimensions: bias types, social groups, and tasks. The paper demonstrates that the levels of bias vary across these dimensions, providing guidance for the development of specific bias mitigation methods.

**Major Findings:**
1. The introduction of CEB, a Compositional Evaluation Benchmark, to address the limitations of existing bias evaluation efforts for LLMs.
2. The curation of CEB is based on a newly proposed compositional taxonomy that characterizes each dataset from three dimensions: bias types, social groups, and tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02408v1](https://arxiv.org/abs/2407.02408v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02408v1](https://browse.arxiv.org/html/2407.02408v1)       |
| Truncated       | True       |
| Word Count       | 32723       |