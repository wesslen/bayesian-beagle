
---
title: "IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus"
id: "2402.14710v1"
description: "Large Language Models struggle with Information Extraction; IEPile corpus improves LLM performance."
author: Honghao Gui, Hongbin Ye, Lin Yuan, Ningyu Zhang, Mengshu Sun, Lei Liang, Huajun Chen
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14710v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14710v1/x1.png)

### **Summary:**
- Large Language Models (LLMs) have shown potential in various Natural Language Processing (NLP) tasks, but they struggle with Information Extraction (IE) due to limited high-quality, large-scale data.
- IEPile is a bilingual (English and Chinese) IE instruction corpus with approximately 0.32B tokens, constructed by collecting and cleaning 33 existing IE datasets and introducing schema-based instruction generation.
- Experimental results show that using IEPile enhances the performance of LLMs for IE, especially in zero-shot generalization.

### Major Findings:
1. IEPile is a comprehensive bilingual IE instruction corpus with approximately 0.32B tokens, constructed by collecting and cleaning 33 existing IE datasets and introducing schema-based instruction generation.
2. Experimental results demonstrate that using IEPile can enhance the performance of LLMs for IE, especially in zero-shot generalization.
3. IEPile is open-sourced, providing valuable support to the NLP community.

### Analysis and Critique:
- The study focuses on schema-based IE, limiting its generalizability to human instructions that do not follow specific format requirements.
- The research evaluates only two models, Baichuan and LLaMA, and a few baselines due to computational resource limitations.
- IEPile is confined to data in English and Chinese, and the authors hope to include data in more languages in the future.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.14710v1](https://arxiv.org/abs/2402.14710v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14710v1](https://browse.arxiv.org/html/2402.14710v1)       |
| Truncated       | False       |
| Word Count       | 5256       |