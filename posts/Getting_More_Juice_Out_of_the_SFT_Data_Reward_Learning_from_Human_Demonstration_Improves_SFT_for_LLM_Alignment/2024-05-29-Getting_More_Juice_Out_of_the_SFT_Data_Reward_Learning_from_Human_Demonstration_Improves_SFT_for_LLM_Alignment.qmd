
---
title: "Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment"
id: "2405.17888v2"
description: "RLHF's SFT stage benefits from learning a reward model too, improving model quality and efficiency."
author: Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, Mingyi Hong
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.17888v2/extracted/5628136/Plots/fig12.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.17888v2/extracted/5628136/Plots/fig12.png)

### Summary:

This paper explores the benefits of incorporating reward learning from human demonstration to improve the alignment of large language models (LLMs) with human preferences. The authors propose a new approach that leverages Inverse Reinforcement Learning (IRL) techniques to build a reward model while learning the policy model. This approach leads to more efficient and effective SFT algorithms that can better distinguish between preferred and non-preferred continuations. The authors also identify a connection between their proposed IRL-based approach and a recently proposed self-play approach, showing that self-play is a special case of modeling a reward-learning agent.

#### Major Findings:

1. The proposed IRL-based SFT algorithms significantly improve the performance of LLMs in aligning with human preferences compared to existing SFT approaches.
2. The proposed methods show that it is beneficial to explicitly or implicitly leverage reward learning throughout the entire alignment process.
3. Theoretical analysis demonstrates that the proposed algorithms converge to the stationary solutions of the IRL problem.

#### Analysis and Critique:

The paper presents a novel approach to improving the alignment of LLMs with human preferences by incorporating reward learning from human demonstration. The proposed IRL-based SFT algorithms show promising results in improving the performance of LLMs in aligning with human preferences. However, the paper does not discuss the potential limitations or challenges of implementing these methods in practice, such as the computational cost of training two models simultaneously or the need for large-scale human demonstration data. Additionally, the paper does not provide a comprehensive comparison with other state-of-the-art methods for aligning LLMs with human preferences. Further research is needed to evaluate the proposed methods in more diverse and complex scenarios and to address the potential challenges and limitations of their implementation.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.17888v2](https://arxiv.org/abs/2405.17888v2)        |
| HTML     | [https://browse.arxiv.org/html/2405.17888v2](https://browse.arxiv.org/html/2405.17888v2)       |
| Truncated       | False       |
| Word Count       | 8795       |