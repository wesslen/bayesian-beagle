
---
title: "Beyond Demographics: Aligning Role-playing LLM-based Agents Using Human Belief Networks"
id: "2406.17232v1"
description: "LLMs align better with human beliefs when seeded with a single belief, improving social simulations."
author: Yun-Shiuan Chuang, Zach Studdiford, Krirk Nirunwiroj, Agam Goyal, Vincent V. Frigo, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
date: "2024-06-25"
image: "https://browse.arxiv.org/html/2406.17232v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17232v1/x1.png)

### Summary:

This study explores the alignment of human beliefs with those expressed by role-playing large language models (LLMs). The authors propose an alternative approach to aligning LLM attitudes with human groups by considering human belief networks, which show that beliefs on different topics are not distributed randomly but tend to cohere together in patterns of high-order covariation. The study tests this idea using a simple belief network constructed from a dataset measuring human beliefs across a diverse array of topics. The results suggest that attention to empirically-derived human belief networks may provide a useful strategy for human-LLM alignment, more so than demographic role-playing.

### Major Findings:

1. Role-playing based on demographic information alone does not align LLM and human opinions.
2. Seeding the agent with a single belief greatly improves alignment for topics related in the belief network, and not for topics outside the network.
3. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.

### Analysis and Critique:

The study presents an innovative approach to aligning LLM attitudes with human groups by considering human belief networks. However, the scope of topics considered is limited, and the structure of the belief network is based on two highly distinct clusters, which may not fully capture the complexity of human belief networks. Additionally, the actions of the LLM agents are limited to expressing their opinions through Likert-scale ratings, which may not fully capture the expression of opinions in real-world settings. Future research could expand the scope of topics, apply more sophisticated models to characterize belief networks, and explore more complex actions to assess the human-likeness of LLM agents in realistic applications.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17232v1](https://arxiv.org/abs/2406.17232v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17232v1](https://browse.arxiv.org/html/2406.17232v1)       |
| Truncated       | False       |
| Word Count       | 7041       |