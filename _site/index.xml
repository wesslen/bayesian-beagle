<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Bayesian beagle</title>
<link>https://bayesian-beagle.netlify.app/index.html</link>
<atom:link href="https://bayesian-beagle.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
<description>Quarto blog of LLM-generated summaries of Arxiv preprints</description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Thu, 04 Jan 2024 05:00:00 GMT</lastBuildDate>
<item>
  <title>ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers</title>
  <dc:creator>Chen Zheng, Ke Sun, Da Tang, Yukun Ma, Yuyu Zhang, Chenguang Xi, Xun Zhou</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/ICE_GRT_Instruction_Context_Enhancement_by_Generative_Reinforcement_based_Transformers/2024-01-04-ICE_GRT_Instruction_Context_Enhancement_by_Generative_Reinforcement_based_Transformers.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/ICE_GRT_Instruction_Context_Enhancement_by_Generative_Reinforcement_based_Transformers/https:/browse.arxiv.org/html/2401.02072v1/extracted/5329451/images/model_architecture.png" class="img-fluid"></p>
<section id="major-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h3>
<ul>
<li>ICE-GRT, a Large Language Model (LLM), addresses limitations in domain-specific tasks by utilizing Reinforcement Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization (PPO).</li>
<li>ICE-GRT demonstrates exceptional performance in both general and domain-specific tasks, showcasing improved ability for detailed analysis, particularly in scenarios where smaller-sized LLMs fall short.</li>
<li>The success of ICE-GRT is dependent on crucial factors such as appropriate data, reward size scaling, KL-control, and advantage normalization.</li>
</ul>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Large Language Models like ChatGPT and LLaMA face limitations in domain-specific tasks, lacking depth and accuracy.</li>
<li>ICE-GRT, leveraging RLHF based on PPO, excels in domain-specific scenarios without compromising general task performance.</li>
<li>The model displays profound understanding and reasoning abilities, going beyond Supervised Fine-Tuning (SFT) models.</li>
</ul>
</section>
<section id="related-works" class="level3">
<h3 class="anchored" data-anchor-id="related-works">Related Works</h3>
<ul>
<li>Recent advancements in Large Language Models have focused on instruction-tuning and RLHF to improve LLMs’ capabilities in specialized tasks.</li>
</ul>
</section>
<section id="model" class="level3">
<h3 class="anchored" data-anchor-id="model">Model</h3>
<ul>
<li>ICE-GRT is built upon the ICE-Instruct model and utilizes RLHF for training the reward model and the entire ICE-GRT model.</li>
<li>The model components include the Actor, Reference, Reward, and Critic models.</li>
<li>Important training strategies such as data collection, reward size scaling, KL-control, and advantage normalization contribute to ICE-GRT’s effectiveness.</li>
</ul>
</section>
<section id="experimental-details" class="level3">
<h3 class="anchored" data-anchor-id="experimental-details">Experimental Details</h3>
<ul>
<li>ICE-GRT’s training process employs a multi-node, multi-GPU strategy and utilizes data collected from diverse sources, including in-domain data and public resources.</li>
<li>Evaluations involve general task benchmarks and manual annotation-based assessments.</li>
</ul>
</section>
<section id="results-and-analysis" class="level3">
<h3 class="anchored" data-anchor-id="results-and-analysis">Results and Analysis</h3>
<ul>
<li>ICE-GRT outperforms other models in general and in-domain tasks, demonstrating its superior performance and comprehension abilities.</li>
<li>ICE-GRT’s training data significantly influences its performance, and strategies like advantage normalization contribute to its effectiveness.</li>
<li>Case studies illustrate ICE-GRT’s comprehensive understanding and creative compliance in domain-specific tasks.</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<ul>
<li>ICE-GRT represents a significant advancement in LLMs, especially in domain-specific performance, and offers insights into effective RLHF training methodologies.</li>
</ul>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<ul>
<li>The paper largely focuses on the capabilities of ICE-GRT without addressing potential limitations or challenges encountered during the development and implementation of the model.</li>
<li>The paper could benefit from a more extensive evaluation and comparison with a wider range of existing models to provide a more comprehensive understanding of ICE-GRT’s positioning in the field.</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02072v1">http://arxiv.org/abs/2401.02072v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02072v1">https://browse.arxiv.org/html/2401.02072v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>8390</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>hci</category>
  <guid>https://bayesian-beagle.netlify.app/posts/ICE_GRT_Instruction_Context_Enhancement_by_Generative_Reinforcement_based_Transformers/2024-01-04-ICE_GRT_Instruction_Context_Enhancement_by_Generative_Reinforcement_based_Transformers.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02072v1/extracted/5329451/images/model_architecture.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives</title>
  <dc:creator>Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Self_Contrast_Better_Reflection_Through_Inconsistent_Solving_Perspectives/2024-01-04-Self_Contrast_Better_Reflection_Through_Inconsistent_Solving_Perspectives.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Self_Contrast_Better_Reflection_Through_Inconsistent_Solving_Perspectives/https:/browse.arxiv.org/html/2401.02009v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p><strong>Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives</strong></p>
<p><strong>1. Major Findings</strong> - <strong>Intrinsic Reflection Deficiencies</strong>: Large Language Models (LLMs) struggle with self-correction due to ineffective intrinsic reflection. - <strong>Overconfident and Inconsistent Feedback</strong>: LLMs often exhibit overconfidence (46.7%) or inconsistency (45.7%) when self-evaluating, hindering accurate self-reflection. - <strong>Self-Contrast Approach</strong>: The Self-Contrast method, which involves creating diverse solving perspectives and contrasting the differences, significantly improves LLMs’ reflection capabilities across reasoning and translation tasks.</p>
<p><strong>2. Evaluation of Intrinsic Reflection</strong> - <strong>Limited Reflection Capability</strong>: LLMs show insignificant performance gains from reflection and struggle to correct incorrect initial responses. - <strong>Feedback Analysis</strong>: The self-evaluate process results in overconfident and inconsistent feedback, impeding effective reflection.</p>
<p><strong>3. Self-Contrast</strong> - <strong>Create Diverse Perspectives</strong>: LLMs autonomously generate multiple prompts tailored to the user’s request, fostering diverse solving perspectives. - <strong>Contrast Inter-Perspective Discrepancies</strong>: LLM contrasts the differences between responses, identifying errors and providing re-examining instructions. - <strong>Eliminate Discrepancies</strong>: Discrepancies between perspectives guide LLMs to revise inconsistent responses for more accurate reflection.</p>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<p>The paper presents a novel approach, but it may benefit from a discussion on potential limitations and challenges in implementing the Self-Contrast method. Additionally, further exploration of real-world application and scalability would enhance the paper’s practical significance. Moreover, a comparison with existing state-of-the-art reflection strategies could provide a better understanding of the method’s effectiveness.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02009v1">http://arxiv.org/abs/2401.02009v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02009v1">https://browse.arxiv.org/html/2401.02009v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>10949</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>robustness</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Self_Contrast_Better_Reflection_Through_Inconsistent_Solving_Perspectives/2024-01-04-Self_Contrast_Better_Reflection_Through_Inconsistent_Solving_Perspectives.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02009v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Are LLMs Robust for Spoken Dialogues?</title>
  <dc:creator>Seyed Mahed Mousavi, Gabriel Roccabruna, Simone Alghisi, Massimo Rizzoli, Mirco Ravanelli, Giuseppe Riccardi</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Are_LLMs_Robust_for_Spoken_Dialogues/2024-01-04-Are_LLMs_Robust_for_Spoken_Dialogues.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Are_LLMs_Robust_for_Spoken_Dialogues/https:/browse.arxiv.org/html/2401.02297v1/x1.png" class="img-fluid"></p>
<section id="are-llms-robust-for-spoken-dialogues" class="level3">
<h3 class="anchored" data-anchor-id="are-llms-robust-for-spoken-dialogues">Are LLMs Robust for Spoken Dialogues?</h3>
<section id="abstract" class="level4">
<h4 class="anchored" data-anchor-id="abstract">Abstract</h4>
<ul>
<li>Large Pre-Trained Language Models (LLMs) have shown excellent performance in task-oriented dialogues. However, their robustness to spoken interactions is unknown. This study evaluates LLMs’ performance for spoken task-oriented dialogues and suggests that fine-tuning such models on a proper dataset of spoken TODs can result in a more robust performance.</li>
</ul>
</section>
<section id="introduction" class="level4">
<h4 class="anchored" data-anchor-id="introduction">Introduction</h4>
<ul>
<li>Large Pre-Trained Language Models (LLMs) have outperformed other data-driven models in open-domain response generation and task-oriented dialogue modeling. However, their robustness to spoken dialogues is unknown due to the lack of proper datasets for spoken TODs.</li>
</ul>
</section>
<section id="literature-review" class="level4">
<h4 class="anchored" data-anchor-id="literature-review">Literature Review</h4>
<ul>
<li>Various studies have explored the application of LLMs in Dialogue State Tracking and Response Generation, highlighting the importance of fine-tuning on proper datasets for robust performance. However, there is a lack of proper spoken dialogue datasets for evaluating LLMs’ robustness to spoken interactions.</li>
</ul>
</section>
<section id="approach" class="level4">
<h4 class="anchored" data-anchor-id="approach">Approach</h4>
<ul>
<li>The study transcribed a small number of spoken TODs and studied the transcription errors to simulate the same pattern in a larger dataset. It fine-tuned T5 and GPT-2 models for Dialogue State Tracking and Response Generation using a dataset of written TODs and its noise-injected version.</li>
</ul>
</section>
<section id="evaluation" class="level4">
<h4 class="anchored" data-anchor-id="evaluation">Evaluation</h4>
<ul>
<li>The fine-tuned models’ performance was evaluated on spoken test sets, indicating that fine-tuning on noisy TODs can improve the models’ performance for spoken dialogues. The study involved both automatic evaluation and human evaluation, with mixed results that suggest the limitations and uninterpretability of automatic metrics.</li>
</ul>
</section>
</section>
<section id="major-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h3>
<ol type="1">
<li><strong>LLMs’ Robustness</strong>: LLMs are not inherently robust to spoken noise, but fine-tuning on noisy TODs can lead to improved performance.</li>
<li><strong>Dataset Importance</strong>: The lack of proper spoken dialogue datasets hinders the evaluation of LLMs’ robustness to spoken interactions.</li>
<li><strong>Evaluation Challenges</strong>: Automatic metrics and human evaluations showed mixed results, highlighting the limitations and uninterpretability of automatic metrics.</li>
</ol>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<ul>
<li>The study’s reliance on automatic transcription and simulated noise may not fully capture the complexities and variations present in actual spoken dialogues.</li>
<li>Additional human evaluations could provide deeper insights into the models’ performance beyond automatic metrics.</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02297v1">http://arxiv.org/abs/2401.02297v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02297v1">https://browse.arxiv.org/html/2401.02297v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>7839</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>social-sciences</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Are_LLMs_Robust_for_Spoken_Dialogues/2024-01-04-Are_LLMs_Robust_for_Spoken_Dialogues.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02297v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models</title>
  <dc:creator>Wendi Cui, Jiaxin Zhang, Zhuohang Li, Lopez Damien, Kamalika Das, Bradley Malin, Sricharan Kumar</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/DCR_Consistency_Divide_Conquer_Reasoning_for_Consistency_Evaluation_and_Improvement_of_Large_Language_Models/2024-01-04-DCR_Consistency_Divide_Conquer_Reasoning_for_Consistency_Evaluation_and_Improvement_of_Large_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/DCR_Consistency_Divide_Conquer_Reasoning_for_Consistency_Evaluation_and_Improvement_of_Large_Language_Models/https:/browse.arxiv.org/html/2401.02132v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p><strong>DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models</strong></p>
<ul>
<li><strong>Findings</strong>
<ul>
<li>The paper proposes a new framework, DCR, for evaluating and improving the consistency of Large Language Model (LLM)-generated texts which outperforms state-of-the-art methods by a large margin in semantic, factual, and summarization consistency tasks.</li>
<li>The framework employs three components: Divide-Conquer Evaluator (DCE), Auto-Metric Converter (AMC), and Reason-Assisted Improver (RAI) to evaluate and improve the consistency of generated responses.</li>
<li>The DCR framework demonstrates high correlations with human judgments, reduces output inconsistencies, and shows promise for effective hallucination mitigation.</li>
</ul></li>
<li><strong>Preliminaries</strong>
<ul>
<li>Conventional evaluation methods relying on token-level comparison fail to capture overall semantic meaning, leading to low correlation with human judgments.</li>
<li>The consistency of LLMs is essential for AI safety and reliability, but current methods often overlook self-consistency failures.</li>
</ul></li>
<li><strong>Divide-Conquer-Reasoning</strong>
<ul>
<li>DCE evaluates semantic consistency between reference and candidate paragraphs at a sentence level using a divide-and-conquer strategy.</li>
<li>AMC converts the evaluation reasons into a numeric score for quantitative interpretation.</li>
<li>RAI utilizes the outputs of DCE to generate new responses to mitigate inconsistencies.</li>
</ul></li>
<li><strong>Experiments</strong>
<ul>
<li>The DCR framework outperforms baseline methods in semantic, factual, and summarization consistency evaluations, showing high correlations with human judgment.</li>
<li>RAI significantly improves consistency, reducing nearly 90% of output inconsistencies.</li>
</ul></li>
</ul>
</section>
<section id="critique" class="level2">
<h2 class="anchored" data-anchor-id="critique">Critique</h2>
<p>While the DCR framework shows promise in evaluating and improving LLM-generated texts’ consistency, several limitations should be considered.</p>
<ul>
<li><strong>Not Comprehensive</strong>: The approach may not universally address all dimensions of text evaluation, such as coherence and relevance.</li>
<li><strong>Input Dependence</strong>: The accuracy of the framework is inherently limited by the correctness of the input paragraphs, potentially affecting the detection of non-factual statements.</li>
<li><strong>Manual Prompting</strong>: The requirement for hand-crafted prompts for specific tasks may limit the scalability and automation of the framework.</li>
</ul>
<p>Overall, the paper provides valuable insights into consistency evaluation and improvement for LLM-generated texts, but further research is needed to address the identified limitations.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02132v1">http://arxiv.org/abs/2401.02132v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02132v1">https://browse.arxiv.org/html/2401.02132v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>9608</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>robustness</category>
  <guid>https://bayesian-beagle.netlify.app/posts/DCR_Consistency_Divide_Conquer_Reasoning_for_Consistency_Evaluation_and_Improvement_of_Large_Language_Models/2024-01-04-DCR_Consistency_Divide_Conquer_Reasoning_for_Consistency_Evaluation_and_Improvement_of_Large_Language_Models.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02132v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Learning to Prompt with Text Only Supervision for Vision-Language Models</title>
  <dc:creator>Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, Federico Tombari</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Learning_to_Prompt_with_Text_Only_Supervision_for_Vision_Language_Models/2024-01-04-Learning_to_Prompt_with_Text_Only_Supervision_for_Vision_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Learning_to_Prompt_with_Text_Only_Supervision_for_Vision_Language_Models/https:/browse.arxiv.org/html/2401.02418v1/x1.png" class="img-fluid"></p>
<section id="learning-to-prompt-with-text-only-supervision-for-vision-language-models" class="level1">
<h1>Learning to Prompt with Text Only Supervision for Vision-Language Models</h1>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract:</h2>
<p>Vision-language models such as CLIP have shown excellent generalization abilities, but adapting these models for downstream tasks while maintaining their generalization remains a challenge. In this work, the authors propose a method, ProText, which learns prompts using only text data derived from large language models (LLMs). This approach enables zero-shot transfer of prompts to new classes and datasets, potentially reducing the LLM prompt engineering cost. Extensive evaluations show that ProText improves upon prior ensembling works and is competitive with those utilizing labeled images.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1 Introduction</h2>
<ul>
<li>Vision-language models (VLMs) like CLIP leverage contrastive pre-training on massive image-text pairs from the internet.</li>
<li>Adapting CLIP for downstream tasks while maintaining its generalization is challenging.</li>
<li>Most methods for adapting CLIP require annotated image labels, which is impractical in real-world scenarios.</li>
</ul>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">2 Related Work</h2>
<ul>
<li>Foundational Vision-Language models (VLMs) leverage joint image-text pretraining using internet-scale data in a self-supervised fashion.</li>
<li>Prompt Learning [6, 49, 50, 27, 9, 41, 40] and Training-Free Text Prompt Enhancement are effective fine-tuning strategies for VLMs.</li>
</ul>
</section>
<section id="method" class="level2">
<h2 class="anchored" data-anchor-id="method">3 Method</h2>
<section id="preliminaries" class="level3">
<h3 class="anchored" data-anchor-id="preliminaries">3.1 Preliminaries</h3>
<ul>
<li>CLIP consists of an image encoder and a text encoder which maps image and text input into visual and textual features respectively.</li>
<li>Existing prompt learning methods require visual samples with labels to optimize prompts using cross-entropy loss.</li>
</ul>
</section>
<section id="prompt-learning-with-text-only-supervision" class="level3">
<h3 class="anchored" data-anchor-id="prompt-learning-with-text-only-supervision">3.2 Prompt Learning with Text-Only Supervision</h3>
<ul>
<li>ProText employs a contextual mapping strategy that effectively learns a mapping function that embeds rich contextual knowledge from LLM data within the prompts.</li>
<li>At inference, the learned prompts are used with class-name templates for zero-shot inference.</li>
</ul>
</section>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">4 Experiments</h2>
<ul>
<li>ProText improves the generalization of CLIP across various settings and is competitive with approaches that explicitly use labeled image samples for training.</li>
<li>Achieves substantial gains over CLIP and CuPL in cross-dataset transfer settings.</li>
</ul>
<section id="ablative-analysis" class="level3">
<h3 class="anchored" data-anchor-id="ablative-analysis">4.7 Ablative Analysis</h3>
<ul>
<li>Contextual mapping loss allows learnable prompts to exploit internal knowledge of CLIP’s text encoder for generalized context from the LLM descriptions.</li>
</ul>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>ProText improves upon prior ensembling works and is competitive with approaches that utilize labeled images for training.</p>
</section>
</section>
<section id="critique" class="level1">
<h1>Critique</h1>
<p>The paper presents an innovative approach that addresses the challenges of adapting CLIP for downstream tasks. However, it could benefit from further discussion on the limitations of ProText, potential areas for improvement, and comparisons with other state-of-the-art text-only methods for vision-language models. Additionally, the paper lacks a detailed discussion on potential biases introduced by using LLM-generated text data and the implications of zero-shot transfer on task-specific performance.</p>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02418v1">http://arxiv.org/abs/2401.02418v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02418v1">https://browse.arxiv.org/html/2401.02418v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>12266</td>
</tr>
</tbody>
</table>


</section>
</section>

 ]]></description>
  <category>education</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Learning_to_Prompt_with_Text_Only_Supervision_for_Vision_Language_Models/2024-01-04-Learning_to_Prompt_with_Text_Only_Supervision_for_Vision_Language_Models.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02418v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>The Effects of Generative AI on Computing Students’ Help-Seeking Preferences</title>
  <dc:creator>Irene Hou, Sophia Metille, Zhuo Li, Owen Man, Cynthia Zastudil, Stephen MacNeil</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/The_Effects_of_Generative_AI_on_Computing_Students_Help_Seeking_Preferences/2024-01-04-The_Effects_of_Generative_AI_on_Computing_Students_Help_Seeking_Preferences.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/The_Effects_of_Generative_AI_on_Computing_Students_Help_Seeking_Preferences/https:/browse.arxiv.org/html/2401.02262v1/extracted/5330212/figs/rq-1.png" class="img-fluid"></p>
<section id="major-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h2>
<ul>
<li>Generative AI tools, such as ChatGPT, are being adopted by computing students, but have not yet fully replaced traditional help resources.</li>
<li>Students’ help-seeking preferences vary across different tasks, and they often prioritize convenience, iteration, and avoiding social pressures when using generative AI tools.</li>
<li>The quality of assistance students receive from generative AI tools is dependent on their ability to formulate effective help requests and evaluate the responses.</li>
</ul>
</section>
<section id="abstract-and-introduction" class="level2">
<h2 class="anchored" data-anchor-id="abstract-and-introduction">Abstract and Introduction</h2>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<ul>
<li>Help-seeking is essential for computing students, and the emergence of generative AI tools like ChatGPT offers a new on-demand resource.</li>
<li>This paper investigates computing students’ help-seeking preferences and experiences with generative AI tools through surveys and interviews.</li>
<li>Preliminary evidence suggests that generative AI tools have not fully replaced traditional help resources, and using these tools requires developing the skill of harnessing their capabilities.</li>
</ul>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>The introduction explores the historical and recent emergence of help-seeking resources for students, focusing on the recent availability of generative AI tools like ChatGPT.</li>
<li>It sets the context for the study by discussing the potential impact of generative AI tools on students’ help-seeking preferences in computing education classes.</li>
<li>The research questions pertaining to help-seeking resource usage, influencing factors, and comparisons with other resources are introduced.</li>
</ul>
</section>
</section>
<section id="related-work" class="level2">
<h2 class="anchored" data-anchor-id="related-work">Related Work</h2>
<section id="help-seeking-behaviors-and-challenges" class="level3">
<h3 class="anchored" data-anchor-id="help-seeking-behaviors-and-challenges">Help-Seeking Behaviors and Challenges</h3>
<ul>
<li>Effective help-seeking is vital for academic success, but students encounter socio-emotional and decision-making barriers when seeking help from peers, instructors, and online resources.</li>
<li>The barriers guide students’ decisions in choosing which help resources to utilize and when to engage with them based on quality and availability.</li>
</ul>
</section>
<section id="help-seeking-in-computing-education" class="level3">
<h3 class="anchored" data-anchor-id="help-seeking-in-computing-education">Help-Seeking in Computing Education</h3>
<ul>
<li>Undergraduate computing students face challenges related to learning programming and seeking help, with a focus on troubleshooting, self-directed exploration, and prioritizing online tools over peers and instructors.</li>
</ul>
</section>
<section id="generative-ai-in-computing-education" class="level3">
<h3 class="anchored" data-anchor-id="generative-ai-in-computing-education">Generative AI in Computing Education</h3>
<ul>
<li>The potential for using generative AI in computing classrooms is discussed, including its capabilities in providing explanations, enhancing error messages, identifying bugs, and creating instructional materials and programming assignments.</li>
</ul>
</section>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<ul>
<li>The methodology section details the participant recruitment process, the survey study, and the interview study conducted to evaluate research questions.</li>
<li>It provides insights into the design of survey questions and interview questions, along with the analysis methods used for both studies.</li>
</ul>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<section id="frequency-of-usage" class="level3">
<h3 class="anchored" data-anchor-id="frequency-of-usage">Frequency of Usage</h3>
<ul>
<li>Students heavily rely on internet resources for help-seeking, but generative AI tools like ChatGPT are also used, particularly for tasks like debugging and writing code.</li>
<li>ChatGPT usage varies across tasks, with students utilizing it more for certain tasks.</li>
</ul>
</section>
<section id="context-of-usage" class="level3">
<h3 class="anchored" data-anchor-id="context-of-usage">Context of Usage</h3>
<ul>
<li>Students’ use of help-seeking resources varies across different tasks, with the internet being the most preferred method overall.</li>
<li>The experiences with using generative AI tools like ChatGPT for learning new concepts, writing code, debugging, and developing test cases are detailed with quotes from the interviews.</li>
</ul>
</section>
<section id="factors-influencing-usage" class="level3">
<h3 class="anchored" data-anchor-id="factors-influencing-usage">Factors Influencing Usage</h3>
<ul>
<li>Trust, trade-offs between convenience and quality, social aspects, and the ability for iteration are explored as factors influencing students’ usage of generative AI tools.</li>
<li>The perceived trade-off between efficient and accurate help, the social dynamics of help-seeking, and the potential for rapid iteration and follow-up questions with generative AI tools are highlighted.</li>
</ul>
</section>
</section>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<ul>
<li>The discussion provides insights into students’ early adoption of generative AI tools for help-seeking and the significant barriers that still exist.</li>
<li>It emphasizes the importance of students’ ability to use generative AI tools effectively and the need for instructors to create pedagogical materials that guide students in maximizing the utility of these tools.</li>
<li>The limitations of the study and the need for future research with larger and more diverse samples are acknowledged.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>The study provides critical insights into how students are incorporating generative AI tools into their help-seeking process and the diverse patterns in their utilization and preferences.</li>
<li>It highlights the potential benefits and challenges associated with using generative AI tools for help-seeking and the need for additional research to understand students’ abilities to use these tools effectively.</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02262v1">http://arxiv.org/abs/2401.02262v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02262v1">https://browse.arxiv.org/html/2401.02262v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>11637</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>education</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/The_Effects_of_Generative_AI_on_Computing_Students_Help_Seeking_Preferences/2024-01-04-The_Effects_of_Generative_AI_on_Computing_Students_Help_Seeking_Preferences.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02262v1/extracted/5330212/figs/rq-1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Text2MDT: Extracting Medical Decision Trees from Medical Texts</title>
  <dc:creator>Wei Zhu, Wenfeng Li, Xing Tian, Pengfei Wang, Xiaoling Wang, Jin Chen, Yuanbin Wu, Yuan Ni, Guotong Xie</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Text2MDT_Extracting_Medical_Decision_Trees_from_Medical_Texts/2024-01-04-Text2MDT_Extracting_Medical_Decision_Trees_from_Medical_Texts.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Text2MDT_Extracting_Medical_Decision_Trees_from_Medical_Texts/https:/browse.arxiv.org/html/2401.02034v1/x1.png" class="img-fluid"></p>
<section id="summary-of-text2mdt-extracting-medical-decision-trees-from-medical-texts" class="level1">
<h1>Summary of “Text2MDT: Extracting Medical Decision Trees from Medical Texts”</h1>
<section id="major-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h2>
<ol type="1">
<li><strong>Text2MDT</strong>: The paper proposes a novel task, <strong>Text2MDT</strong>, which aims to automatically extract <strong>medical decision trees (MDTs)</strong> from medical texts such as medical guidelines and textbooks. This is significant for the development of clinical decision support systems.</li>
<li><strong>End-to-end vs.&nbsp;Pipeline Framework</strong>: The paper investigates both an end-to-end framework and a pipeline framework for the Text2MDT task and demonstrates that large language models (LLMs) show promising results in automated MDT extraction.</li>
<li><strong>Open-Sourced Dataset and Source Code</strong>: The study contributes to the field by constructing the first Text2MDT benchmark dataset and making it openly available to facilitate further research.</li>
</ol>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>The development of clinical decision support systems, which rely on medical decision processes modeled as MDTs, has drawn significant attention in the medical field.</li>
<li>Current methods for constructing MDTs rely on manual tree construction, which is time-consuming and laborious, leading to a need for automated pipelines for precise MDT extraction. This motivates the proposal of the Text2MDT task.</li>
</ul>
</section>
<section id="text2mdt-task" class="level2">
<h2 class="anchored" data-anchor-id="text2mdt-task">Text2MDT Task</h2>
<ul>
<li><strong>Structure</strong>: The knowledge of a medical decision process embedded in the medical text is modeled as a binary decision tree consisting of condition nodes and decision nodes, linked by the logical relationships</li>
</ul>
</section>
<section id="data-collection-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="data-collection-and-evaluation">Data Collection and Evaluation</h2>
<ul>
<li><strong>Data Collection</strong>: A Text2MDT dataset was constructed using clinical practice guidelines and clinical medicine textbooks, and medical practitioners evaluated the ability of medical texts and decision trees to represent the medical decision process.</li>
<li><strong>Manual Evaluation</strong>: The quality of the annotated MDTs was evaluated by medical practitioners and individuals without a medical background.</li>
</ul>
</section>
<section id="methods-of-modeling-text2mdt" class="level2">
<h2 class="anchored" data-anchor-id="methods-of-modeling-text2mdt">Methods of modeling Text2MDT</h2>
<ul>
<li><strong>Pipelined Framework</strong>: The study investigates triplet extraction, node grouping, and tree assembling as subtasks for the pipeline framework. Both encoder-based and LLM-based methods are explored.</li>
<li><strong>End-to-end Framework</strong>: The paper proposes various COT-style generation methods for the end-to-end framework, considering the complexity of the Text2MDT task and the potential benefit of COT reasoning.</li>
</ul>
</section>
<section id="experiments-and-results" class="level2">
<h2 class="anchored" data-anchor-id="experiments-and-results">Experiments and Results</h2>
<ul>
<li><strong>Evaluation Metrics</strong>: The study uses metrics such as triplet precision, recall, and F1 scores for triplet extraction, edit distance-based metrics for node grouping, and additional metrics for tree assembling.</li>
<li><strong>Performance Findings</strong>: The study shows competitive results for MedBERT-based methods and demonstrates the potential of COT-style reasoning in improving the performance of generative LMs on the Text2MDT task.</li>
</ul>
</section>
<section id="limitations-and-critique" class="level2">
<h2 class="anchored" data-anchor-id="limitations-and-critique">Limitations and Critique</h2>
<ul>
<li>The study acknowledges limitations related to the expressiveness of the tree, limited logic expression of nodes, and text length constraints. Further improvements are identified as future work.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>The paper concludes with the significance of the proposed Text2MDT task for automated extraction of MDTs and highlights the contributions of the study, including the construction of the Text2MDT dataset and the exploration of novel method frameworks.</li>
<li>Additionally, the study identifies potential future work to address the limitations and challenges encountered in the investigation.</li>
</ul>
</section>
<section id="critique" class="level2">
<h2 class="anchored" data-anchor-id="critique">Critique</h2>
<p>The paper provides a comprehensive overview of the Text2MDT task and presents valuable contributions to the field of automated MDT extraction. However, a more detailed discussion of potential challenges and future directions for improving the proposed methods would enhance the paper’s completeness. Additionally, addressing the limitations of the proposed framework and its applicability in real-world clinical settings would provide a more comprehensive evaluation of the study’s contributions.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02034v1">http://arxiv.org/abs/2401.02034v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02034v1">https://browse.arxiv.org/html/2401.02034v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>True</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>13994</td>
</tr>
</tbody>
</table>


</section>
</section>

 ]]></description>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Text2MDT_Extracting_Medical_Decision_Trees_from_Medical_Texts/2024-01-04-Text2MDT_Extracting_Medical_Decision_Trees_from_Medical_Texts.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02034v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>LLaMA Pro: Progressive LLaMA with Block Expansion</title>
  <dc:creator>Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/LLaMA_Pro_Progressive_LLaMA_with_Block_Expansion/2024-01-04-LLaMA_Pro_Progressive_LLaMA_with_Block_Expansion.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/LLaMA_Pro_Progressive_LLaMA_with_Block_Expansion/https:/browse.arxiv.org/html/2401.02415v1/x2.png" class="img-fluid"></p>
<section id="main-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="main-takeaways">Main Takeaways</h3>
<ol type="1">
<li><p><strong>Large Language Models (LLMs) Post-Pretraining</strong>: The paper introduces a novel post-pretraining method for LLMs, termed “block expansion,” which aims to inject new domain-specific knowledge while preserving the model’s original general capabilities.</p></li>
<li><p><strong>LLaMA Pro Model</strong>: The study presents LLaMA Pro, an LLM with 8 added blocks, pre-trained on extensive code and math data, which excels in both general and domain-specific tasks.</p></li>
<li><p><strong>Superior Performance</strong>: LLaMA Pro’s instruction-following counterpart achieves state-of-the-art performance across a wide variety of tasks, demonstrating its superiority over existing open models in the LLaMA family and its potential as an intelligent agent.</p></li>
</ol>
</section>
<section id="related-work" class="level3">
<h3 class="anchored" data-anchor-id="related-work">Related Work</h3>
<ul>
<li><strong>Advancements in Large Language Models:</strong> The paper builds upon the developments in large language models and provides a methodology for specializing large language models in the domain of code.</li>
<li><strong>Post-Pretraining:</strong> The study discusses the two-step process of initial general-domain pretraining followed by domain-specific training observed in language model applications.</li>
<li><strong>Progressive Learning:</strong> The paper highlights progressive training techniques that have gained attention for accelerating the training of large-scale models in NLP research.</li>
</ul>
</section>
<section id="method" class="level3">
<h3 class="anchored" data-anchor-id="method">Method</h3>
<ul>
<li><strong>Block Expansion:</strong> The paper details the block expansion method for LLMs, incorporating an identity block after each block in the original model. This method aims to enhance the model’s domain-specific abilities while preserving its original general capabilities.</li>
<li><strong>SFT Results:</strong> LLaMA Pro - Instruct attains state-of-the-art performance compared to other fine-tuned models, showcasing its more comprehensive capabilities.</li>
</ul>
</section>
<section id="experiments" class="level3">
<h3 class="anchored" data-anchor-id="experiments">Experiments</h3>
<ul>
<li><strong>Pretrain Results:</strong> LLaMA Pro effectively balances natural language processing and coding capabilities, maintaining its general performance while excelling in code-related tasks. It outperforms both general-purpose and code-oriented pretrained models.</li>
<li><strong>SFT Results:</strong> LLaMA Pro - Instruct achieves superior performance in code and math tasks, as well as in multi-turn interactions and chatbot scenarios, compared to other models in the LLaMA family.</li>
<li><strong>Ablation Study:</strong> The study evaluates various training strategies, including LoRA, fine-tuning, and block expansion, and demonstrates the scalability and adaptive performance of the block expansion method with added blocks.</li>
</ul>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<p>The paper provides valuable insights into post-pretraining methods for LLMs and presents a promising approach for developing advanced language agents. However, some potential problems include the extensive computational resources and domain-specific datasets required for pretraining and the potential trade-offs between preserving general capabilities and enhancing domain-specific knowledge. Additionally, the scalability and effectiveness of the block expansion method need to be further validated across different domains and tasks.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02415v1">http://arxiv.org/abs/2401.02415v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02415v1">https://browse.arxiv.org/html/2401.02415v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>8377</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>prompt-engineering</category>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/LLaMA_Pro_Progressive_LLaMA_with_Block_Expansion/2024-01-04-LLaMA_Pro_Progressive_LLaMA_with_Block_Expansion.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02415v1/x2.png" medium="image" type="image/png"/>
</item>
<item>
  <title>LLM Augmented LLMs: Expanding Capabilities through Composition</title>
  <dc:creator>Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/LLM_Augmented_LLMs_Expanding_Capabilities_through_Composition/2024-01-04-LLM_Augmented_LLMs_Expanding_Capabilities_through_Composition.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/LLM_Augmented_LLMs_Expanding_Capabilities_through_Composition/https:/browse.arxiv.org/html/2401.02412v1/x1.png" class="img-fluid"></p>
<p><strong>Major Takeaways</strong> - The paper introduces the concept of Composition to Augment Language Models (CALM) which enables the composition of existing foundational language models with more specific models to enable newer capabilities. - The CALM framework introduces cross-attention between models to compose their representations and enable new capabilities, allowing for the reuse of existing models with established capabilities. - The paper demonstrates the practical applications of CALM in language inclusivity and code generation, showing significant improvements in translation, arithmetic reasoning, and code-related tasks.</p>
<p><strong>Introduction</strong> - Large Language Models (LLMs) have foundational capabilities and have been fine-tuned for domain-specific capabilities, resulting in the development of several specialized large models with domain-specific capabilities. - The paper aims to enable the composition of an anchor model with a domain-specific augmenting model to enable new capabilities, such as composing an augmenting model’s code understanding capability with an anchor LLM’s language generation capability to enable code-to-text generation capability.</p>
<p><strong>The CALM Framework</strong> - CALM aims to compose an anchor model and an augmenting model to enable new capabilities as a composition of capabilities of the two individual models. - It operates over a selected set of layers from the anchor and augmenting models and introduces a small number of trainable parameters over these layers. - The composition training data depicts a “combined skill” of the given models for the target composition domain and is used to learn the composition parameters.</p>
<p><strong>Experiments</strong> - The paper demonstrates the effectiveness of CALM in three domains: key-value arithmetic, low-resource language inclusivity, and code completion and explanation tasks. - The experiments show the significant improvements achieved by composing an augmenting model with an anchor LLM, surpassing the individual models and versions that have been fine-tuned for the specific tasks.</p>
<p><strong>Critique</strong> - The paper lacks a discussion on the potential limitations or challenges of the CALM framework, such as its scalability to larger models or its adaptability to diverse languages and domains. - The experimental results could benefit from a more extensive comparison with other relevant methods or frameworks to establish the unique advantages of CALM.</p>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02412v1">http://arxiv.org/abs/2401.02412v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02412v1">https://browse.arxiv.org/html/2401.02412v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>5397</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/LLM_Augmented_LLMs_Expanding_Capabilities_through_Composition/2024-01-04-LLM_Augmented_LLMs_Expanding_Capabilities_through_Composition.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02412v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Using LLM to select the right SQL Query from candidates</title>
  <dc:creator>Zhenwen Li, Tao Xie</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Using_LLM_to_select_the_right_SQL_Query_from_candidates/2024-01-04-Using_LLM_to_select_the_right_SQL_Query_from_candidates.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Using_LLM_to_select_the_right_SQL_Query_from_candidates/https:/browse.arxiv.org/html/2401.02115v1/x1.png" class="img-fluid"></p>
<section id="summary-of-using-llm-to-select-the-right-sql-query-from-candidates" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-using-llm-to-select-the-right-sql-query-from-candidates">Summary of “Using LLM to select the right SQL Query from candidates”</h3>
<section id="major-findings" class="level4">
<h4 class="anchored" data-anchor-id="major-findings">Major Findings</h4>
<ol type="1">
<li><strong>Automatic Test Case Generation</strong>: The paper proposes a method to automatically generate test cases for text-to-SQL, without ground truth SQL queries, and conducts experiments to explore how to generate easily predicted databases for large language models (LLMs) and design easy-to-understand prompts.</li>
<li><strong>Re-rank Method</strong>: The paper introduces a re-rank method to select the right SQL query from a candidate list and demonstrates its effectiveness on the validation dataset of Spider, showing a 3.6% improvement in the performance of state-of-the-art text-to-SQL models.</li>
<li><strong>Hyper-parameter Optimization</strong>: Through experiments, the study identifies optimal hyper-parameters for generating test cases, such as database size, naturalness of database contents, format of database contents, and number of examples. It also highlights the effectiveness of constraining the range of numbers in database columns participating in aggregation/sort operations.</li>
</ol>
</section>
<section id="introduction" class="level4">
<h4 class="anchored" data-anchor-id="introduction">Introduction</h4>
<ul>
<li>Text-to-SQL is the task of translating natural language into a SQL query, and the top-performing models often generate a list of candidate SQL queries, with the best query not always at the top of the list.</li>
<li>Previous studies have focused on re-ranking the candidate SQL queries, but automatic test case generation for text-to-SQL is an understudied field.</li>
</ul>
</section>
<section id="test-case-generation" class="level4">
<h4 class="anchored" data-anchor-id="test-case-generation">Test Case Generation</h4>
<ul>
<li>The method consists of database generation and using LLMs to predict the expected execution results.</li>
<li>Database generation involves fuzzing and random selection methods, exploring the impact of maximum table size and naturalness of database contents.</li>
<li>LLMs are guided by prompts containing the NL question, database representation, and examples to predict expected execution results.</li>
</ul>
</section>
<section id="candidate-selection" class="level4">
<h4 class="anchored" data-anchor-id="candidate-selection">Candidate Selection</h4>
<ul>
<li>The paper proposes a three-step method to select the right SQL query, involving candidate list classification, test suite generation, and re-ranking based on pass numbers on test cases and their generation probabilities.</li>
</ul>
</section>
<section id="experiment" class="level4">
<h4 class="anchored" data-anchor-id="experiment">Experiment</h4>
<ul>
<li>The study conducts experiments on the Spider dataset, using GPT-4-turbo and GPT-4 to generate test cases and state-of-the-art models like DAIL-SQL and RESDSQL to generate candidate lists.</li>
<li>Results indicate a 3.6% improvement for DAIL-SQL and a 2% improvement for RESDSQL after applying the proposed re-rank methods.</li>
</ul>
</section>
<section id="hyper-parameter-optimization" class="level4">
<h4 class="anchored" data-anchor-id="hyper-parameter-optimization">Hyper-parameter Optimization</h4>
<ul>
<li>The study explores hyper-parameters related to database generation and prompt design, identifying optimal values and showing the effectiveness of constraining number ranges in certain columns.</li>
</ul>
</section>
<section id="related-work" class="level4">
<h4 class="anchored" data-anchor-id="related-work">Related Work</h4>
<ul>
<li>The paper discusses the use of LLMs in text-to-SQL, the relationship to previous re-ranking studies, and the advantages of its database generation algorithm compared to previous work.</li>
</ul>
</section>
<section id="conclusion" class="level4">
<h4 class="anchored" data-anchor-id="conclusion">Conclusion</h4>
<ul>
<li>The study emphasizes the efficacy of using test cases to re-rank candidate lists for text-to-SQL, calling for further exploration in this research direction.</li>
</ul>
</section>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<p>The paper presents an innovative approach to test case generation and re-ranking of candidate SQL queries, demonstrating notable improvements in model performance. However, there are some potential limitations: 1. <strong>Prediction Accuracy of LLMs</strong>: The study acknowledges that only about 60% of the test cases generated are correct, raising questions about the overall reliability of using LLMs to predict expected execution results. 2. <strong>Complexity and Token Consumption</strong>: The re-rank method’s reliance on OpenAI’s API for generating test cases multiple times highlights potential challenges in scalability and token consumption for large-scale applications. 3. <strong>Database Generation Limitations</strong>: The limitations of the proposed database generation method, including its inability to distinguish some SQL queries, could impact the overall effectiveness of the test case generation process.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02115v1">http://arxiv.org/abs/2401.02115v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02115v1">https://browse.arxiv.org/html/2401.02115v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>7353</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>prompt-engineering</category>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Using_LLM_to_select_the_right_SQL_Query_from_candidates/2024-01-04-Using_LLM_to_select_the_right_SQL_Query_from_candidates.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02115v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Understanding LLMs: A Comprehensive Overview from Training to Inference</title>
  <dc:creator>Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Understanding_LLMs_A_Comprehensive_Overview_from_Training_to_Inference/2024-01-04-Understanding_LLMs_A_Comprehensive_Overview_from_Training_to_Inference.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Understanding_LLMs_A_Comprehensive_Overview_from_Training_to_Inference/https:/browse.arxiv.org/html/2401.02038v1/x1.png" class="img-fluid"></p>
<section id="major-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="major-takeaways">Major Takeaways:</h3>
<ol type="1">
<li><p><strong>Evolution of Large Language Models (LLMs)</strong>: The introduction of ChatGPT has led to the popular use of LLMs for addressing downstream tasks. The focus is now on cost-efficient training and deployment of LLMs, representing the future development trend.</p></li>
<li><p><strong>Training Techniques</strong>: LLMs training includes aspects such as data preprocessing, training architecture, pre-training tasks, parallel training, and model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization.</p></li>
<li><p><strong>Fine-Tuning</strong>: The paper categorizes fine-tuning techniques into supervised fine-tuning, alignment tuning, and parameter-efficient tuning. The supervision of fine-tuning involves adjusting the model based on large-scale pre-training.</p></li>
</ol>
</section>
<section id="background-knowledge" class="level3">
<h3 class="anchored" data-anchor-id="background-knowledge">Background Knowledge</h3>
<p>The section provides an overview of language modeling in the context of natural language processing (NLP) and the evolution of language models from statistical language models (SLM) to neural language models (NLM) and pre-trained language models (PLM). It also details the Transformer architecture, self-attention, encoder-decoder architecture, positional embedding, and prompt learning as widely adopted machine learning approach.</p>
</section>
<section id="training-of-large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="training-of-large-language-models">Training of Large Language Models</h3>
<ul>
<li><strong>Data Preparation and Preprocessing</strong>: Discusses data pre-training tasks such as language modeling, and model pre-training tasks, including data parallel, model parallel, mixed precision training, offloading, overlapping, and checkpoint mechanisms.</li>
<li><strong>Supervised Fine-Tuning</strong>: The paper categorizes fine-tuning techniques into supervised fine-tuning, alignment tuning, and parameter-efficient tuning. The supervision of fine-tuning involves adjusting the model based on large-scale pre-training.</li>
</ul>
</section>
<section id="model-training" class="level3">
<h3 class="anchored" data-anchor-id="model-training">Model Training</h3>
<ul>
<li><strong>Parallel Training</strong>: Discusses data parallel, distributed data parallel, model parallel and ZeRO framework.</li>
<li><strong>Mixed Precision Training</strong>: Details the use of 16-bit floating-point numbers to reduce memory usage and communication overhead.</li>
<li><strong>Offloading</strong>: Discusses the idea of moving the optimizer’s parameters from the GPU to the CPU.</li>
<li><strong>Overlapping</strong>: Describes asynchronous memory operations to optimize the training process.</li>
<li><strong>Checkpoint</strong>: Details the use of a checkpoint mechanism to optimize the backward propagation process.</li>
</ul>
</section>
<section id="fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning">Fine-Tuning</h3>
<ul>
<li><strong>Supervised Fine-Tuning</strong>: The core concept involves adjusting the model in a supervised manner on the basis of large-scale pre-training.</li>
<li><strong>Alignment Tuning</strong>: Aligns the model with specific task requirements, task prompt, or examples.</li>
<li><strong>Parameter-Efficient Tuning</strong>: Designed to fine-tune the model with minimal additional parameters.</li>
</ul>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<p>The article lacks a clear distinction between the literature review and original contributions, making it challenging to identify the author’s unique position or perspective on the subject matter. Additionally, some sections provide detailed technical descriptions that may be overwhelming for readers without a strong background in NLP and machine learning. Finally, the absence of empirical evidence or case studies limits the practical applicability of the paper’s findings.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02038v1">http://arxiv.org/abs/2401.02038v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02038v1">https://browse.arxiv.org/html/2401.02038v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>True</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>21883</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>hci</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Understanding_LLMs_A_Comprehensive_Overview_from_Training_to_Inference/2024-01-04-Understanding_LLMs_A_Comprehensive_Overview_from_Training_to_Inference.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02038v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models</title>
  <dc:creator>Songbo Hu, Xiaobin Wang, Zhangdie Yuan, Anna Korhonen, Ivan Vulić</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/DIALIGHT_Lightweight_Multilingual_Development_and_Evaluation_of_Task_Oriented_Dialogue_Systems_with_Large_Language_Models/2024-01-04-DIALIGHT_Lightweight_Multilingual_Development_and_Evaluation_of_Task_Oriented_Dialogue_Systems_with_Large_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/DIALIGHT_Lightweight_Multilingual_Development_and_Evaluation_of_Task_Oriented_Dialogue_Systems_with_Large_Language_Models/https:/browse.arxiv.org/html/2401.02208v1/x1.png" class="img-fluid"></p>
<section id="summary-of-lightweight-multilingual-development-and-evaluation-of-task-oriented-dialogue-systems-with-large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-lightweight-multilingual-development-and-evaluation-of-task-oriented-dialogue-systems-with-large-language-models">Summary of “: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models”</h3>
<section id="key-findings" class="level4">
<h4 class="anchored" data-anchor-id="key-findings">Key Findings</h4>
<ul>
<li><strong>Task-Oriented Dialogue (ToD) systems</strong> facilitate interactions between human users and system agents, focusing on specific tasks such as booking hotels or providing domain-specific information.</li>
<li>The prevailing approach for ToD system development has been fine-tuning <strong>Pretrained Language Models (PLMs)</strong>, but there’s a shift towards <strong>Large Language Models (LLMs)</strong> with in-context learning capabilities.</li>
<li>While PLM fine-tuning leads to higher accuracy and coherence, LLM-based systems excel in producing diverse and likeable responses but face challenges in adherence to task-specific instructions and generating outputs in multiple languages.</li>
</ul>
</section>
<section id="introduction" class="level4">
<h4 class="anchored" data-anchor-id="introduction">Introduction</h4>
<ul>
<li>ToD systems serve as access points to cutting-edge AI applications and drivers of technological expansion.</li>
<li>The shift in ToD system development from fine-tuning PLMs to relying on LLMs’ in-context learning and generalization capabilities is highlighted.</li>
</ul>
</section>
<section id="related-work" class="level4">
<h4 class="anchored" data-anchor-id="related-work">Related Work</h4>
<ul>
<li>*** is a novel addition to the landscape of ToD system toolkits, offering support for in-context learning (ICL) compared to existing frameworks.</li>
<li>It aims to lower entry barriers and facilitate comprehensive comparative analyses between PLM fine-tuning and ICL-based systems.</li>
</ul>
</section>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<ul>
<li>The paper acknowledges the limitations of the toolkit, such as focusing only on text input and its underperformance compared to more sophisticated systems. It also raises concerns about the dominance of English instructions biasing model outputs towards English, indicating potential biases in the toolkit’s approach.</li>
<li>The superficial tone and stylized infoboxes find little impact in interpreting the utility or contribution of the product. This makes the paper overlook potential areas.</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.02208v1">http://arxiv.org/abs/2401.02208v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.02208v1">https://browse.arxiv.org/html/2401.02208v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>9836</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/DIALIGHT_Lightweight_Multilingual_Development_and_Evaluation_of_Task_Oriented_Dialogue_Systems_with_Large_Language_Models/2024-01-04-DIALIGHT_Lightweight_Multilingual_Development_and_Evaluation_of_Task_Oriented_Dialogue_Systems_with_Large_Language_Models.html</guid>
  <pubDate>Thu, 04 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.02208v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Social Media Ready Caption Generation for Brands</title>
  <dc:creator>Himanshu Maheshwari, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivasan</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Social_Media_Ready_Caption_Generation_for_Brands/2024-01-03-Social_Media_Ready_Caption_Generation_for_Brands.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Social_Media_Ready_Caption_Generation_for_Brands/https:/browse.arxiv.org/html/2401.01637v1/x1.png" class="img-fluid"></p>
<section id="major-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h3>
<ol type="1">
<li><strong>Brand personality</strong> plays a crucial role in consumer perception and brand marketing, especially in the context of social media advertisements. Aligning brand personalities with social media captions is essential for successful digital marketing.</li>
<li>The proposed framework consists of two parts: automatic image captioning and large language model (LLM) based Instagram caption generation, allowing for both zero/few-shot and fine-tuning capabilities.</li>
<li>The framework demonstrates effectiveness in generating catchy social media captions aligned with the target brand personality and image, outperforming existing models in terms of caption quality and relevance.</li>
</ol>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>The increasing consumer engagement on social media platforms has led brands to focus on advertising through captivating captions, engaging images, and popular hashtags.</li>
<li>Brand personalities significantly influence consumer behavior, and aligning them with social media posts and captions has become essential for successful digital marketing.</li>
</ul>
</section>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">Methodology</h3>
<ul>
<li>The proposed framework comprises automatic image captioning using a vision-language model and a large language model (LLM) for Instagram caption generation aligned with brand personalities.</li>
<li>Two variants of the LLM framework are explored: fine-tuned LLM and zero/few-shot GPT, offering flexibility based on user needs and data privacy concerns.</li>
</ul>
</section>
<section id="dataset" class="level3">
<h3 class="anchored" data-anchor-id="dataset">Dataset</h3>
<ul>
<li>A new dataset for the task is created by scraping images and captions from public Instagram accounts, ensuring alignment with brand personalities.</li>
<li>The dataset’s quality and limitations are thoroughly examined, highlighting the need for a high-quality dataset for accurate evaluations.</li>
</ul>
</section>
<section id="evaluation-metric" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-metric">Evaluation Metric</h3>
<ul>
<li>CLIPScore and semantic similarity metrics are used to assess the relevance of generated captions to the original image and ground truth captions.</li>
<li>G-Eval is utilized to evaluate brand personality alignment, demonstrating high correlation with human judgment.</li>
</ul>
</section>
<section id="results-and-discussion" class="level3">
<h3 class="anchored" data-anchor-id="results-and-discussion">Results and Discussion</h3>
<ul>
<li>The proposed framework outperforms existing models, generating captions aligned with the target personality and additional user-provided attributes.</li>
<li>Qualitative and quantitative results showcase the effectiveness of the framework in generating catchy, personality-aligned social media captions.</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<ul>
<li>The paper introduces a novel task of generating brand-specific Instagram captions aligned with brand personalities, addressing limitations in existing literature, datasets, and evaluation metrics.</li>
<li>The framework provides insights and opportunities for future research in marketing and multimodal Instagram caption generation.</li>
</ul>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<p>The paper provides a comprehensive approach to brand-specific Instagram caption generation; however, potential limitations include the reliance on GPT, which may limit scalability due to cost. Additionally, the reliance on a scraped dataset from public Instagram accounts may introduce biases and limitations in the model’s generalizability to diverse brand personalities and marketing contexts. Further, the effectiveness of the framework in real-world marketing settings remains to be validated through practical applications.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01637v1">http://arxiv.org/abs/2401.01637v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01637v1">https://browse.arxiv.org/html/2401.01637v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>7430</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>hci</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Social_Media_Ready_Caption_Generation_for_Brands/2024-01-03-Social_Media_Ready_Caption_Generation_for_Brands.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01637v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Multilingual Instruction Tuning With Just a Pinch of Multilinguality</title>
  <dc:creator>Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Multilingual_Instruction_Tuning_With_Just_a_Pinch_of_Multilinguality/2024-01-03-Multilingual_Instruction_Tuning_With_Just_a_Pinch_of_Multilinguality.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Multilingual_Instruction_Tuning_With_Just_a_Pinch_of_Multilinguality/https:/browse.arxiv.org/html/2401.01854v1/x1.png" class="img-fluid"></p>
<section id="major-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h3>
<ul>
<li>Multilingual instruction tuning of a multilingual large language model (LLM) with a small set of multilingual examples can significantly improve multilingual instruction-following capabilities, even for languages unseen during tuning.</li>
<li>Training on a mixture of languages can lead to models with comparable or superior performance in several languages compared to models tuned on a single language, despite using fewer examples in those languages.</li>
<li>Adding just a few languages to the instruction tuning set can improve cross-lingual generalization for languages unseen during tuning.</li>
</ul>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>The paper investigates the impact of multilinguality during instruction tuning of a multilingual LLM on instruction-following across languages. The authors address the need for these models to operate on a wide range of languages to be globally applicable.</p>
</section>
<section id="experimental-setup" class="level3">
<h3 class="anchored" data-anchor-id="experimental-setup">Experimental Setup</h3>
<p>The study uses open-ended instructions and a modern multilingual pretrained LLM, and evaluates instruction-following abilities per language in a controlled setting using generations of monolingually tuned models as a baseline.</p>
<ul>
<li><strong>Data:</strong> Datasets of high-quality open-ended instructions are used, with translations created for 11 diverse languages using the Google Translate API.</li>
<li><strong>Evaluation Method:</strong> A side-by-side automatic evaluation protocol is used, where an LLM assesses two responses for a single prompt to identify the superior one.</li>
</ul>
</section>
<section id="how-much-multilinguality-is-needed-for-multilingual-instruction-tuning" class="level3">
<h3 class="anchored" data-anchor-id="how-much-multilinguality-is-needed-for-multilingual-instruction-tuning">How Much Multilinguality Is Needed For Multilingual Instruction Tuning?</h3>
<p>The paper examines the impact of multilingual data during instruction tuning and finds that monolingual instruction tuning yields multilingual abilities. A small number of multilingual examples and languages can improve instruction-following and cross-lingual generalization.</p>
</section>
<section id="potential-factors-of-cross-lingual-transfer" class="level3">
<h3 class="anchored" data-anchor-id="potential-factors-of-cross-lingual-transfer">Potential Factors of Cross-Lingual Transfer</h3>
<p>The authors explore the impact of language similarity and fraction of data in pretraining on cross-lingual transfer efficacy and find weak correlations.</p>
</section>
<section id="related-work" class="level3">
<h3 class="anchored" data-anchor-id="related-work">Related Work</h3>
<p>The study relates to previous work on cross-lingual transfer and multilingual instruction tuning, emphasizing its findings in the context of massively multilingual instruction-following LLMs.</p>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<p>The study’s reliance on translated data introduces potential noise, limiting the generalizability of its findings. Additionally, the study’s experiments encompass a limited number of languages and LLMs, opening up future research opportunities for scalability and generalization.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01854v1">http://arxiv.org/abs/2401.01854v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01854v1">https://browse.arxiv.org/html/2401.01854v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>8421</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Multilingual_Instruction_Tuning_With_Just_a_Pinch_of_Multilinguality/2024-01-03-Multilingual_Instruction_Tuning_With_Just_a_Pinch_of_Multilinguality.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01854v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries</title>
  <dc:creator>Akash Ghosh, Arkadeep Acharya, Prince Jha, Aniket Gaudgaul, Rajdeep Majumdar, Sriparna Saha, Aman Chadha, Raghav Jain, Setu Sinha, Shivani Agarwal</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/MedSumm_A_Multimodal_Approach_to_Summarizing_Code_Mixed_Hindi_English_Clinical_Queries/2024-01-03-MedSumm_A_Multimodal_Approach_to_Summarizing_Code_Mixed_Hindi_English_Clinical_Queries.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/MedSumm_A_Multimodal_Approach_to_Summarizing_Code_Mixed_Hindi_English_Clinical_Queries/https:/browse.arxiv.org/html/2401.01596v1/x1.png" class="img-fluid"></p>
<section id="major-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h3>
<ol type="1">
<li><p>The paper addresses the task of <strong>multimodal medical question summarization</strong> for codemixed input in a low-resource setting. It introduces the <strong>Multimodal Medical Codemixed Question Summarization (MMCQS) dataset</strong>, combining Hindi-English codemixed medical queries with visual aids to enrich the representation of a patient’s medical condition.</p></li>
<li><p>The proposed <strong>MedSumm framework</strong> leverages both Large Language Models (LLMs) and Vision Language Models (VLMs) to integrate visual information from images, demonstrating the value of integrating visual information to improve the creation of medical summaries, with the potential to increase access to quality healthcare and promote health equity.</p></li>
<li><p>The paper introduces a <strong>novel metric MMFCM</strong> to quantify how well the model captures the multimodal information in the generated summary.</p></li>
</ol>
</section>
<section id="qualitative-analysis" class="level3">
<h3 class="anchored" data-anchor-id="qualitative-analysis">Qualitative Analysis</h3>
<ul>
<li>The study suggests that all models perform better in a multimodal setting, capturing important visual information conveyed through the images and predicting the exact disorder phrase. However, some models demonstrate a tendency of hallucination and generation of facts out of context.</li>
</ul>
</section>
<section id="critique-and-potential-problems" class="level3">
<h3 class="anchored" data-anchor-id="critique-and-potential-problems">Critique and Potential Problems</h3>
<ul>
<li><p>The paper acknowledges limitations such as confining the task to a limited set of symptoms conducive to image sharing, which may lead to potentially erroneous information in the summary when introducing an image outside this scope. It is prudent to engage a medical expert for ultimate verification, particularly in high-stakes scenarios.</p></li>
<li><p>While the multimodal model shows promise, it is necessary to consider its role as a tool, not a substitute for medical professionals, particularly in scenarios involving high-stakes medical decisions.</p></li>
<li><p>The paper’s reliance on automatic evaluation metrics such as ROUGE, BLEU, and BERT score may not fully capture the nuanced quality of summaries in the medical domain, suggesting the need for human evaluation and verification by medical professionals to ensure accuracy and relevance.</p></li>
</ul>
<p>Overall, the paper’s focus on multimodal medical question summarization and the introduction of the MMCQS dataset and MedSumm framework offer valuable contributions to the field, but it is important to consider potential limitations and the need for further validation and ethical considerations in real-world medical applications.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01596v1">http://arxiv.org/abs/2401.01596v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01596v1">https://browse.arxiv.org/html/2401.01596v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>6480</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>social-sciences</category>
  <category>hci</category>
  <guid>https://bayesian-beagle.netlify.app/posts/MedSumm_A_Multimodal_Approach_to_Summarizing_Code_Mixed_Hindi_English_Clinical_Queries/2024-01-03-MedSumm_A_Multimodal_Approach_to_Summarizing_Code_Mixed_Hindi_English_Clinical_Queries.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01596v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>PLLaMa: An Open-source Large Language Model for Plant Science</title>
  <dc:creator>Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/PLLaMa_An_Open_source_Large_Language_Model_for_Plant_Science/2024-01-03-PLLaMa_An_Open_source_Large_Language_Model_for_Plant_Science.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/PLLaMa_An_Open_source_Large_Language_Model_for_Plant_Science/https:/browse.arxiv.org/html/2401.01600v1/x1.png" class="img-fluid"></p>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings</h3>
<ol type="1">
<li><p><strong>PLLaMa</strong> is an open-source language model developed with a specific focus on plant science, integrating a comprehensive database of over 1.5 million scholarly articles in the field. The initial tests indicate significant improvement in understanding plant science-related topics.</p></li>
<li><p>The paper highlights the importance of domain-specific knowledge for enhancing the proficiency of large language models in specialized fields, such as plant science, and introduces an international panel of professionals to verify the accuracy of the model’s responses.</p></li>
<li><p>The development process and model’s checkpoints and source codes are made accessible to the scientific community, thereby facilitating further research and development.</p></li>
</ol>
</section>
<section id="introduction-to-large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="introduction-to-large-language-models">Introduction to Large Language Models</h3>
<ul>
<li>Large Language Models (LLMs) like <strong>OpenAI’s ChatGPT</strong>, while remarkable in natural language understanding, face limitations in specialized domains such as plant science due to their generic training and high API costs.</li>
<li>Publicly accessible models like <strong>LLaMa-2</strong> sometimes underperform in specialized tasks due to the absence of domain-specific data in their initial training.</li>
</ul>
</section>
<section id="development-process" class="level3">
<h3 class="anchored" data-anchor-id="development-process">Development Process</h3>
<ul>
<li>PLLaMa’s development involved extended pretraining with a comprehensive corpus of academic articles in plant science, followed by instruction-based fine-tuning.</li>
<li>The model’s proficiency was evaluated through an initial plant science quiz, demonstrating its utility in the field.</li>
</ul>
</section>
<section id="benchmark-and-zero-shot-case-study" class="level3">
<h3 class="anchored" data-anchor-id="benchmark-and-zero-shot-case-study">Benchmark and Zero-shot Case Study</h3>
<ul>
<li>The initial evaluation of PLLaMa on a plant science quiz yielded an accuracy of around 80% on multi-choice questions.</li>
<li>The model’s responses to zero-shot questions were confirmed to be accurate and useful by a team of global experts.</li>
</ul>
</section>
<section id="conclusion-and-future-work" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-and-future-work">Conclusion and Future Work</h3>
<ul>
<li>The paper emphasizes the importance of strengthening fundamental language models with domain-specific knowledge and outlines plans for future releases, including a more comprehensive instruction-tuning dataset and a more thorough model evaluation.</li>
</ul>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<p>The paper effectively presents the development of PLLaMa as a specialized language model for plant science, but it could benefit from more specific details on the model’s performance metrics and potential limitations in real-world plant science applications. Additionally, while the open-source nature of PLLaMa is highlighted, further information on model accessibility and potential usage scenarios in plant science research would enhance the paper’s practical relevance.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01600v1">http://arxiv.org/abs/2401.01600v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01600v1">https://browse.arxiv.org/html/2401.01600v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>8053</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/PLLaMa_An_Open_source_Large_Language_Model_for_Plant_Science/2024-01-03-PLLaMa_An_Open_source_Large_Language_Model_for_Plant_Science.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01600v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>De-Hallucinator: Iterative Grounding for LLM-Based Code Completion</title>
  <dc:creator>Aryaz Eghbali, Michael Pradel</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/De_Hallucinator_Iterative_Grounding_for_LLM_Based_Code_Completion/2024-01-03-De_Hallucinator_Iterative_Grounding_for_LLM_Based_Code_Completion.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/De_Hallucinator_Iterative_Grounding_for_LLM_Based_Code_Completion/https:/browse.arxiv.org/html/2401.01701v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level1">
<h1>Summary</h1>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways</h2>
<ul>
<li>Large language models (LLMs) have been successful in code completion, but they lack knowledge of project-specific APIs, resulting in inaccurate completions and “hallucinated” code.</li>
<li>De-Hallucinator addresses this challenge by iteratively querying the LLM with increasingly suitable context information, thus improving the predicted code and recall of correctly predicted API usages.</li>
<li>The approach is language-agnostic and designed to work with any off-the-shelf LLM trained on code, making it a versatile solution for improving code completion accuracy.</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<ul>
<li>Large language models (LLMs) have shown promise in code completion tasks, but they lack project-specific API knowledge, leading to incomplete and inaccurate code predictions.</li>
</ul>
</section>
<section id="approach" class="level2">
<h2 class="anchored" data-anchor-id="approach">Approach</h2>
<section id="static-pre-analysis" class="level3">
<h3 class="anchored" data-anchor-id="static-pre-analysis">Static Pre-Analysis</h3>
<ul>
<li>The approach utilizes CodeQL to statically analyze code and extract API references for fast retrieval during the code completion process. The extracted API references are then indexed for efficient querying.</li>
</ul>
</section>
<section id="retrieval-of-related-apis" class="level3">
<h3 class="anchored" data-anchor-id="retrieval-of-related-apis">Retrieval of Related APIs</h3>
<ul>
<li>De-Hallucinator retrieves relevant API references based on similarity to the input code, providing a ranked list of project-specific API references to be added to the prompt.</li>
</ul>
</section>
<section id="prompt-construction" class="level3">
<h3 class="anchored" data-anchor-id="prompt-construction">Prompt Construction</h3>
<ul>
<li>The augmented prompt is designed to resemble “normal” code and consists of a commented block of relevant API references followed by the original prompt.</li>
</ul>
</section>
<section id="integration-with-the-llm" class="level3">
<h3 class="anchored" data-anchor-id="integration-with-the-llm">Integration with the LLM</h3>
<ul>
<li>De-Hallucinator queries the LLM as a black box and post-processes the completion to make it syntactically correct and remove extraneous completions.</li>
</ul>
</section>
</section>
<section id="evaluation" class="level2">
<h2 class="anchored" data-anchor-id="evaluation">Evaluation</h2>
<ul>
<li>The approach is evaluated on four state-of-the-art LLMs for code completion, demonstrating consistent improvements in predicted code, edit distance, and recall of correctly predicted API usages compared to querying the model with a fixed prompt.</li>
</ul>
</section>
</section>
<section id="critique" class="level1">
<h1>Critique</h1>
<ul>
<li>The paper does not address potential trade-offs or limitations of the De-Hallucinator approach.</li>
<li>There is no discussion regarding the scalability of the approach to larger codebases or its real-world applicability.</li>
<li>The evaluation could benefit from a broader set of programming languages and a comparison with other state-of-the-art code completion techniques.</li>
</ul>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01701v1">http://arxiv.org/abs/2401.01701v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01701v1">https://browse.arxiv.org/html/2401.01701v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>True</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>14084</td>
</tr>
</tbody>
</table>


</section>
</section>

 ]]></description>
  <category>robustness</category>
  <category>programming</category>
  <guid>https://bayesian-beagle.netlify.app/posts/De_Hallucinator_Iterative_Grounding_for_LLM_Based_Code_Completion/2024-01-03-De_Hallucinator_Iterative_Grounding_for_LLM_Based_Code_Completion.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01701v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers</title>
  <dc:creator>Aleksandar Stanić, Sergi Caelles, Michael Tschannen</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Towards_Truly_Zero_shot_Compositional_Visual_Reasoning_with_LLMs_as_Programmers/2024-01-03-Towards_Truly_Zero_shot_Compositional_Visual_Reasoning_with_LLMs_as_Programmers.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Towards_Truly_Zero_shot_Compositional_Visual_Reasoning_with_LLMs_as_Programmers/https:/browse.arxiv.org/html/2401.01974v1/extracted/5328832/figures/refcoco_skiers_blur.png" class="img-fluid"></p>
<section id="towards-truly-zero-shot-compositional-visual-reasoning-with-llms-as-programmers" class="level1">
<h1>Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers</h1>
<section id="major-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h2>
<ul>
<li>Large language models (LLMs) demonstrate strong performance on compositional visual question answering, visual grounding, and video temporal reasoning tasks. However, their performance heavily relies on human-engineered in-context examples (ICEs) in the prompt.</li>
<li>The presented framework introduces spatially and temporally abstract routines and leverages a small number of labeled examples to automatically generate in-context examples, thus avoiding the need for human-created ICEs.</li>
<li>The framework leads to consistent gains in performance, makes LLMs as controllers setup more robust, and removes the need for human engineering of ICEs across various visual reasoning tasks.</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Compositional visual question answering necessitates understanding visual information in images and the structure of the question, posing a challenge for end-to-end neural networks, especially in tasks requiring compositional reasoning, spatial reasoning, and counting. A promising alternative involves LLMs as controllers, orchestrating a set of visual tools to decompose tasks into subtasks and solve them by utilizing abstract routines.</p>
</section>
<section id="llms-as-programmers-for-visual-reasoning-framework" class="level2">
<h2 class="anchored" data-anchor-id="llms-as-programmers-for-visual-reasoning-framework">LLMs as programmers for visual reasoning framework</h2>
<ul>
<li><strong>Background:</strong> The ViperGPT approach uses an LLM (Codex) and a tools API to generate scripts to solve visual queries, with the prompt consisting of API functions, docstrings, and query-code examples of their use.</li>
<li><strong>Abstract API through visual routines:</strong> The framework introduces spatially and temporally abstract routines to reduce the LLM’s burden of strong spatial and temporal reasoning.</li>
<li><strong>Automatic generation of in-context examples:</strong> Using a few labeled examples, the framework generates query-code examples in a zero-shot manner, thereby eliminating human engineering of ICEs.</li>
<li><strong>Self-correction:</strong> The framework enables LLMs to perform self-debugging and self-tuning to correct generated code when execution fails without any ground truth labels.</li>
</ul>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<ul>
<li><strong>Tasks:</strong> Evaluation is conducted on datasets such as RefCOCO, RefCOCO+, GQA, and NExT-QA, assessing a diverse set of capabilities including visual grounding, compositional image question answering, and video temporal reasoning.</li>
<li><strong>Vision and Language Models:</strong> The framework uses a code instruction-tuned version of PaLM2 for code generation and various vision models for object detection, depth estimation, and image captioning.</li>
<li><strong>Self-Correction:</strong> Results show that self-tuning, dynamic object detector thresholds, and generated in-context examples lead to improved performance across tasks.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The framework showcased consistent performance gains while removing the need for human engineering of ICEs and demonstrating the potential of LLMs as controllers for visual reasoning.</p>
</section>
<section id="critique-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="critique-and-future-work">Critique and Future Work</h2>
<p>While the framework shows promise, further research is needed to optimize the Abstract API routines and automate prompt engineering with natural language dataset specifications. Additionally, the creation of better benchmarks for evaluating compositional visual reasoning is needed to maximize the framework’s potential. There should also be continued exploration of LLMs’ self-correction capabilities.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01974v1">http://arxiv.org/abs/2401.01974v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01974v1">https://browse.arxiv.org/html/2401.01974v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>13256</td>
</tr>
</tbody>
</table>


</section>
</section>

 ]]></description>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Towards_Truly_Zero_shot_Compositional_Visual_Reasoning_with_LLMs_as_Programmers/2024-01-03-Towards_Truly_Zero_shot_Compositional_Visual_Reasoning_with_LLMs_as_Programmers.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01974v1/extracted/5328832/figures/refcoco_skiers_blur.png" medium="image" type="image/png"/>
</item>
<item>
  <title>GPT-4V(ision) is a Generalist Web Agent, if Grounded</title>
  <dc:creator>Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/GPT_4V(ision)_is_a_Generalist_Web_Agent_if_Grounded/2024-01-03-GPT_4V(ision)_is_a_Generalist_Web_Agent_if_Grounded.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/GPT_4V(ision)_is_a_Generalist_Web_Agent_if_Grounded/https:/browse.arxiv.org/html/2401.01614v1/x1.png" class="img-fluid"></p>
<section id="summary-of-gpt-4vision-as-a-generalist-web-agent-if-grounded" class="level1">
<h1>Summary of “GPT-4V(ision) as a Generalist Web Agent, if Grounded”</h1>
<section id="major-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="major-takeaways">Major Takeaways</h2>
<ul>
<li><strong>LMMs</strong> like GPT-4V have great potential as <strong>generalist web agents</strong>, outperforming text-only LLMs like GPT-4 and smaller models specifically fine-tuned for web agents in completing tasks on live websites.</li>
<li>Grounding, especially <strong>element grounding</strong>, remains a substantial challenge, with the best strategies still exhibiting a performance gap with oracle grounding. <strong>Grounding via textual choices</strong> was the most effective approach, outperforming image annotation strategies, but still faced challenges with identical elements on webpages.</li>
<li><strong>In-context learning (ICL)</strong> with large models showed better generalization to unseen websites compared to supervised fine-tuning (SFT) methods, making it a more compelling solution for generalist web agents, especially in scenarios lacking annotations or requiring strong generalization capabilities.</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The paper explores the potential of LMMs as generalist web agents, defining generalist web agents as those that can follow natural language instructions and complete tasks on any real-world website.</p>
</section>
<section id="seeact" class="level2">
<h2 class="anchored" data-anchor-id="seeact">SeeAct</h2>
<ul>
<li>Aims to investigate the capabilities of <strong>GPT-4V</strong> as a generalist web agent by generating action descriptions and identifying webpage elements for completing tasks on websites.</li>
<li>Formulation includes two essential capabilities: <strong>Action Generation</strong> and <strong>Element Grounding</strong> for identifying HTML elements at each step.</li>
</ul>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<ul>
<li><strong>Dataset</strong>: Evaluated on the <strong>Mind2Web</strong> benchmark, encompassing over 2,000 tasks on real-world websites.</li>
<li><strong>Methods</strong>: SeeAct, baselines such as FLAN-T5 and BLIP2-T5, and in-context learning methods using GPT-3.5 and GPT-4 are compared.</li>
<li><strong>Offline Evaluation</strong>: Shows potential of GPT-4V as a web agent with <strong>oracle grounding</strong> method achieving notable success rates, but still exhibiting a substantial gap with proposed strategies. In-context learning methods demonstrate better generalization to unseen websites compared to supervised fine-tuning methods.</li>
<li><strong>Online Evaluation</strong>: Demonstrates a substantial discrepancy with offline evaluations, indicating that multiple viable plans for the same task impact model performance.</li>
</ul>
</section>
<section id="results-and-analysis" class="level2">
<h2 class="anchored" data-anchor-id="results-and-analysis">Results and Analysis</h2>
<ul>
<li><strong>Whole Task Success Rate</strong>: SeeActChoice outperforms existing methods on live websites, showcasing its potential as a generalist web agent. Surpassed fine-tuned models like FLAN-T5-XL in online evaluation, despite showing lower step success rates in offline evaluation.</li>
<li><strong>Error Analysis</strong>: Showed challenges in grounding via textual choices and image annotation, with challenges of identical elements and hallucination errors.</li>
<li><strong>Knowledge and Reasoning</strong>: Tasks requiring knowledge and reasoning displayed GPT-4V’s capabilities in identifying specific details like IATA codes and geographic locations.</li>
<li><strong>Path Variation and Error Correction</strong>: Demonstrates the model’s flexibility in finding alternative paths to task completion and awareness of error correction during the task.</li>
</ul>
</section>
<section id="critique" class="level2">
<h2 class="anchored" data-anchor-id="critique">Critique</h2>
<ul>
<li>The major findings are promising, but the discrepancy between offline and online evaluations raises questions about the robustness of the evaluation protocols and the need for better alignment between the two.</li>
<li>The focus on the specific dataset Mind2Web and the limited subset used for experiments may limit the generalizability of the findings.</li>
</ul>
<p>Overall, the paper provides valuable insights into the potential of large multimodal models as generalist web agents and highlights the challenges and future research directions in this domain. It opens up discussions on the practical implications and ethical considerations of deploying such models in real-world web environments.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01614v1">http://arxiv.org/abs/2401.01614v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01614v1">https://browse.arxiv.org/html/2401.01614v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>12123</td>
</tr>
</tbody>
</table>


</section>
</section>

 ]]></description>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/GPT_4V(ision)_is_a_Generalist_Web_Agent_if_Grounded/2024-01-03-GPT_4V(ision)_is_a_Generalist_Web_Agent_if_Grounded.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01614v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering</title>
  <dc:creator>Pierre Erbacher, Louis Falissar, Vincent Guigue, Laure Soulier</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Navigating_Uncertainty_Optimizing_API_Dependency_for_Hallucination_Reduction_in_Closed_Book_Question_Answering/2024-01-03-Navigating_Uncertainty_Optimizing_API_Dependency_for_Hallucination_Reduction_in_Closed_Book_Question_Answering.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Navigating_Uncertainty_Optimizing_API_Dependency_for_Hallucination_Reduction_in_Closed_Book_Question_Answering/https:/browse.arxiv.org/html/2401.01780v1/extracted/5328477/images/cute3.png" class="img-fluid"></p>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings</h3>
<ol type="1">
<li><strong>Large Language Models (LLM) are prone to producing inaccurate or false responses</strong>, commonly known as hallucinations, when faced with factual questions.</li>
<li><strong>Searching in a large collection of documents introduces additional computational and time costs</strong> in augmenting LLMs with the ability to search on external information sources.</li>
<li>The proposed model self-estimates its ability to answer directly or request an external tool resulting in the API being utilized only 62% of the time.</li>
</ol>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<ul>
<li>Language models have demonstrated remarkable performances in natural language processing tasks.</li>
<li>Large language models are prone to hallucinations, and the existing approaches to tackle this issue involve using external techniques to detect and mitigate hallucinations.</li>
</ul>
</section>
<section id="learning-when-to-search-with-llms" class="level3">
<h3 class="anchored" data-anchor-id="learning-when-to-search-with-llms">Learning when to search with LLMs</h3>
<ul>
<li>Problem formalization involves training an LLM to query external resources instead of generating hallucinations or to generate answers directly.</li>
<li>The paper proposes a Hallucination Masking Mechanism (HalM) allowing to mask wrong answers with an API call token instead of hallucinating an answer.</li>
</ul>
</section>
<section id="evaluation-protocol" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-protocol">Evaluation protocol</h3>
<ul>
<li><strong>Datasets</strong>: Natural Question Open (NQ) and TriviaQA (TQA) datasets are considered for the experiments.</li>
<li><strong>Metrics</strong>: F1-scores are used to evaluate model performances.</li>
</ul>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<ul>
<li>The proposed Hallucination Masking Mechanism (HalM) reduces hallucinations and enables LLMs to internally assess their ability to answer queries.</li>
<li>The LoRA strategy consistently outperforms the PPL-T strategy for most metrics.</li>
</ul>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<ul>
<li>The proposed approach enables LLMs to endogenously identify their potential for hallucination better than perplexity-based methods.</li>
<li>The approach also enables large language models to condition their generation on their ability to answer appropriately, a crucially important feature in reducing hallucinations.</li>
</ul>
</section>
<section id="critique" class="level3">
<h3 class="anchored" data-anchor-id="critique">Critique</h3>
<ul>
<li>The experiments are limited to in-domain hallucination detection, potentially reducing the generalizability of the findings.</li>
<li>The paper should provide a more comprehensive comparison with existing state-of-the-art approaches to reducing hallucinations in language models.</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-06</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.01780v1">http://arxiv.org/abs/2401.01780v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.01780v1">https://browse.arxiv.org/html/2401.01780v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>6011</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>robustness</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Navigating_Uncertainty_Optimizing_API_Dependency_for_Hallucination_Reduction_in_Closed_Book_Question_Answering/2024-01-03-Navigating_Uncertainty_Optimizing_API_Dependency_for_Hallucination_Reduction_in_Closed_Book_Question_Answering.html</guid>
  <pubDate>Wed, 03 Jan 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2401.01780v1/extracted/5328477/images/cute3.png" medium="image" type="image/png"/>
</item>
</channel>
</rss>
