
---
title: "Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM"
id: "2402.00097v1"
description: "TL;DR: SymPrompt improves large language model test generation for complex software units."
author: Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma, Murali Krishna Ramanathan, Baishakhi Ray
date: "2024-01-31"
image: "https://browse.arxiv.org/html/2402.00097v1/extracted/5380982/figures/working_example_fm.png"
categories: ['robustness', 'prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.00097v1/extracted/5380982/figures/working_example_fm.png)

### **Summary:**
The article introduces SymPrompt, a code-aware prompting strategy for large language models (LLMs) in test generation. It aims to improve test generation quality by optimizing the test generation context and correcting errors in model outputs. SymPrompt breaks down the test generation process into a multi-stage sequence, each driven by a specific prompt aligned with the execution paths of the method under test. The approach significantly enhances the generation of comprehensive test suites with a recent open source LLM, CodeGen2, and achieves substantial improvements in the ratio of correct test generations and coverage.

### **Major Findings:**
1. **Code-Aware Prompting Strategy:** SymPrompt is a code-aware prompting strategy for LLMs in test generation, breaking down the process into a multi-stage sequence aligned with the execution paths of the method under test.
2. **Improved Test Generation Quality:** SymPrompt significantly enhances the generation of comprehensive test suites with a recent open source LLM, CodeGen2, and achieves substantial improvements in the ratio of correct test generations and coverage.
3. **Impact on Large Models:** When given a specific instruction prompt to analyze execution path constraints, GPT-4 is capable of generating its own path constraint prompts, which improves the coverage of its generating test suites by a factor of 2Ã— over prompting strategies from recent prior work.

### **Analysis and Critique:**
- The article provides a comprehensive and innovative approach to test generation with LLMs, addressing the limitations of existing methods and significantly improving the quality of test suites.
- The study demonstrates the effectiveness of SymPrompt in improving test generation quality and coverage, especially when applied to large language models like GPT-4.
- The article's findings have significant implications for software testing and development, offering a promising solution to the challenges of automated test generation.

Overall, the article presents a well-structured and coherent approach to improving test generation with LLMs, providing valuable insights and contributions to the field of software testing. The critical analysis highlights the strengths and potential impact of the proposed approach. However, further research and real-world application of SymPrompt are necessary to validate its effectiveness in practical software development scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2402.00097v1](https://arxiv.org/abs/2402.00097v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00097v1](https://browse.arxiv.org/html/2402.00097v1)       |
| Truncated       | False       |
| Word Count       | 11229       |