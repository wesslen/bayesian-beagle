
---
title: "$\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for In-Context Learning"
id: "2402.13874v1"
description: "Large language models need sequential examples for in-context learning, ùöÇùöé2superscriptùöÇùöé2^{2}Se start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT method improves selection."
author: Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Furu Wei, Qi Zhang
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13874v1/x1.png"
categories: ['prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13874v1/x1.png)

### **Summary:**
- The article introduces a method called Sequential Example Selection for In-Context Learning (SESEL) to address the problem of selecting examples for in-context learning (ICL) in large language models (LLMs).
- SESEL leverages the LLM‚Äôs feedback on varying context to capture inter-relationships and sequential information among examples, enhancing the contextuality and relevance of ICL prompts.
- The method utilizes beam search to seek and construct example sequences, resulting in significant improvements in performance across 23 natural language processing (NLP) tasks.

### **Major Findings:**
1. SESEL leverages the LLM‚Äôs feedback on varying context to capture inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts.
2. The method utilizes beam search to seek and construct example sequences, enhancing both quality and diversity.
3. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that SESEL markedly surpasses competitive baselines and achieves 42% relative improvement over random selection.

### **Analysis and Critique:**
- The article provides a comprehensive and effective method for example selection in the context of in-context learning. However, the limitations and potential biases within LLMs could influence the results of the method.
- The study primarily utilized GPT-Neo-2.7B for experiments, which may have computational limitations and could benefit from exploring more capable models.
- The method's effectiveness and robustness in example selection are highlighted, but further exploration of strategies to achieve fair and explainable outcomes from complex models is essential.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13874v1](https://arxiv.org/abs/2402.13874v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13874v1](https://browse.arxiv.org/html/2402.13874v1)       |
| Truncated       | False       |
| Word Count       | 9847       |