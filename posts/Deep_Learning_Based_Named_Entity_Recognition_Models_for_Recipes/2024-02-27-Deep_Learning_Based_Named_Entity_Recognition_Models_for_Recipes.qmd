
---
title: "Deep Learning Based Named Entity Recognition Models for Recipes"
id: "2402.17447v1"
description: "Automated protocols for recognizing recipe text entities are valuable for various applications. Fine-tuned spaCy-transformer is best."
author: Mansi Goel, Ayush Agarwal, Shubham Agrawal, Janak Kapuriya, Akhil Vamshi Konam, Rishabh Gupta, Shrey Rastogi, Niharika, Ganesh Bagler
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17447v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17447v1/x1.png)

### **Summary:**
- Recipes are unstructured text, and named entities are their building blocks.
- Named entity recognition (NER) is a technique for extracting information from unstructured or semi-structured data with known labels.
- Deep learning-based models, such as BERT, DistilBERT, RoBERTa, and DistilRoBERTa, and NLP frameworks, such as spaCy and flair, have been implemented to find the named entities in the ingredients section of recipes.

### Major Findings:
1. **Dataset Creation**
   - Manually annotated data consisting of 6,611 ingredient phrases were sourced from RecipeDB.
   - An augmented dataset comprising 26,445 ingredient phrases was created by label-wise token replacement, synonym replacement, and shuffling with segments.
   - A machine-annotated dataset of 349,762 unique ingredient phrases from RecipeDB was created involving semi-automated processing protocol and human curation.

2. **Model Evaluation**
   - The spaCy-transformer emerged as the best model with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated, augmented, and machine-annotated datasets, respectively.
   - Distil-variants frequently outperformed the base BERT models, indicating their effectiveness in capturing the data's inherent nature.

3. **Analysis of Few-Shot Prompting on LLMs**
   - Pre-trained LLMs have limited exposure to food and culinary datasets during their initial pretraining, affecting their performance in in-context learning, especially in food-related named entity recognition.

### Analysis and Critique:
- The study is limited in certain aspects of culinary context, nuances of data, and modeling paradigm.
- The study focuses only on ingredient phrases and does not account for the recipe instructions, which often carry semantic information about cooking that encodes cultural nuances.
- The static pre-trained models come with inherent biases and might not be fine-tuned to capture the nuances of the food lexicon.
- The study may be extended to include LLM fine-tuning, implementing NERs on cooking instruction, prompt engineering for LLMs for NER on recipes, soft prompt tuning, chain of thought, and implementation of multilingual NER.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.17447v1](https://arxiv.org/abs/2402.17447v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17447v1](https://browse.arxiv.org/html/2402.17447v1)       |
| Truncated       | False       |
| Word Count       | 6373       |