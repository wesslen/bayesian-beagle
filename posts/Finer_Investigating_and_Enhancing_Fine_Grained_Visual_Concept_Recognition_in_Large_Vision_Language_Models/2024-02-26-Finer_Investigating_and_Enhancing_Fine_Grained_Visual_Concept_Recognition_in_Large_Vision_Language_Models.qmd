
---
title: "Finer: Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models"
id: "2402.16315v1"
description: "Recent LVLMs struggle with fine-grained visual categorization, proposing a new evaluation benchmark. Code and dataset available."
author: Jeonghwan Kim, Heng Ji
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16315v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16315v1/x1.png)

### Summary:
- Recent advances in Large Vision-Language Models (LVLMs) have led to the ability to generate high-level, image-grounded explanations with ease.
- However, the study reveals that LVLMs have shortcomings in fine-grained visual categorization (FGVC) across six different benchmark settings.
- The study proposes a multiple granularity attribute-centric evaluation benchmark, Finer, to evaluate LVLMs' fine-grained visual comprehension ability and provide significantly improved explainability.

### Major Findings:
1. LVLMs exhibit significant deterioration in terms of classification performance across all the five baselines, with some even reaching near 0% in EM score.
2. The text-only input, which contains the detailed physical attributes of a concept, outperforms the image-only input, which is the model's zero-shot performance on fine-grained classification.
3. The model-generated attributes from image-only input are more generic compared to those from text-only input, suggesting that the models fail to properly observe the fine-grained details of a concept.

### Analysis and Critique:
- The study highlights the lack of fine-grained image comprehension ability of instruction-tuned LVLMs across various real-life objects.
- The persistence of modality gap within the LVLMs is revealed, showing a discrepancy in how the two modalities are processed by these models.
- The study constructs a novel attribute-centric, multi-granularity benchmark on FGVC datasets to open up a new direction for future works to measure LVLMs' modality gap and their fine-grained image understanding capability. 
- The study acknowledges limitations such as intra-concept variance in images and the selection of baseline models, suggesting areas for future research and improvement.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.16315v1](https://arxiv.org/abs/2402.16315v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16315v1](https://browse.arxiv.org/html/2402.16315v1)       |
| Truncated       | False       |
| Word Count       | 8207       |