
---
title: "Long Input Benchmark for Russian Analysis"
id: "2408.02439v1"
description: "LIBRA: A Russian benchmark for evaluating LLMs' long-text understanding, with 21 datasets and varying complexity levels."
author: Igor Churin, Murat Apishev, Maria Tikhonova, Denis Shevelev, Aydar Bulatov, Yuri Kuratov, Sergej Averkiev, Alena Fenogenova
date: "2024-08-05"
image: "../../../bayesian-beagle.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

# Summary:

**Summary:**

- The paper introduces LIBRA, a new benchmark for long context understanding in Russian, which includes 21 tasks for LLM evaluation.
- LIBRA aims to evaluate a large scope of LLMs, including pretrain models and models with supervised finetuning (SFT) with any system prompt.
- The tasks in LIBRA are divided into 4 complexity groups, and the datasets have several subsets of various context lengths ranging from 4k up to 128k tokens.
- The main purpose of the benchmark is to create a reliable instrument for the long context understanding evaluation, enabling the study of the model’s ability to solve various tasks of different complexity with respect to the input context length.
- The paper also presents a methodology for the evaluation of long-context abilities of LLMs for the Russian language and publicly releases a set of 21 datasets of various skills and complexities in Russian which form the LIBRA benchmark.
- The paper also provides a codebase and public leaderboard for LIBRA to guide forthcoming research.

**Major Findings:**

1. LIBRA is a new benchmark for long context understanding in Russian, which includes 21 tasks for LLM evaluation.
2. The tasks in LIBRA are divided into 4 complexity groups, and the datasets have several subsets of various context lengths ranging from 4k up to 128k tokens.
3. The main purpose of the benchmark is to create a reliable instrument for the long context understanding evaluation, enabling the study of the model’s ability to solve various tasks of different complexity with respect to the input context length.

**Analysis and Critique:**

- The paper presents a new benchmark for long context understanding in Russian, which is a significant contribution to the field.
- The division of tasks into 4 complexity groups and the use of datasets with various context lengths ranging from 4k up to 128k tokens is a well-thought-out approach to evaluate the model’s ability to solve various tasks of different complexity with respect to the input context length.
- The paper also provides a codebase and public leaderboard for LIBRA, which is a valuable resource for researchers and practitioners in the field.
- However, the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2408.02439v1](https://arxiv.org/abs/2408.02439v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.02439v1](https://browse.arxiv.org/html/2408.02439v1)       |
| Truncated       | False       |
| Word Count       | 8590       |