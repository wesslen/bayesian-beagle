
---
title: "LLMs achieve adult human performance on higher-order theory of mind tasks"
id: "2405.18870v1"
description: "GPT-4 and Flan-PaLM demonstrate near-human performance in understanding multiple mental states, surpassing humans in 6th order inferences, with implications for user-facing LLM applications."
author: Winnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael McKibben, Tatenda Kanyere, Alison Lentz, Blaise Aguera y Arcas, Robin I. M. Dunbar
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.18870v1/extracted/5596702/images/LLM_ment_without_labels.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.18870v1/extracted/5596702/images/LLM_ment_without_labels.png)

### Summary:

This paper examines the extent to which large language models (LLMs) have developed higher-order theory of mind (ToM), the human ability to reason about multiple mental and emotional states in a recursive manner. The authors introduce a handwritten test suite, Multi-Order Theory of Mind Q&A, and use it to compare the performance of five LLMs to a newly gathered adult human benchmark. The results show that GPT-4 and Flan-PaLM reach adult-level and near adult-level performance on ToM tasks overall, and that GPT-4 exceeds adult performance on 6th order inferences. The findings suggest that there is an interplay between model size and finetuning for the realization of ToM abilities, and that the best-performing LLMs have developed a generalized capacity for ToM. Given the role that higher-order ToM plays in a wide range of cooperative and competitive human behaviors, these findings have significant implications for user-facing LLM applications.

### Major Findings:

1. GPT-4 and Flan-PaLM reach at-human or near-human performance on ToM tasks, respectively.
2. GPT-4 exceeds adult performance on 6th order inferences.
3. There is an interplay between model size and finetuning for the realization of ToM abilities in LLMs.

### Analysis and Critique:

The paper presents a novel and important contribution to the field of LLM research by examining the extent to which LLMs have developed higher-order ToM. The use of a handwritten test suite and comparison to a newly gathered adult human benchmark provides a robust and reliable measure of LLM performance on ToM tasks. However, the paper does not address the potential limitations of using a handwritten test suite, such as the possibility of overfitting to the specific test suite or the lack of generalizability to other tasks. Additionally, the paper does not discuss the potential implications of LLMs with higher-order ToM for user-facing applications, such as the potential for advanced persuasion, manipulation, and exploitation behaviors. Further research is needed to understand the potential risks and benefits of LLMs with higher-order ToM and to devise technical guardrails and design principles that mitigate these risks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.18870v1](https://arxiv.org/abs/2405.18870v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.18870v1](https://browse.arxiv.org/html/2405.18870v1)       |
| Truncated       | False       |
| Word Count       | 11809       |