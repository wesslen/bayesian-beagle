
---
title: "Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning"
id: "2402.04833v1"
description: "Selecting longest responses consistently outperforms sophisticated methods for LLM fine-tuning."
author: Hao Zhao, Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion
date: "2024-02-07"
image: "../../img/2402.04833v1/image_1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.04833v1/image_1.png)

### Summary:

- The article explores the importance of instruction fine-tuning (IFT) for large language models (LLMs) and the effectiveness of using long instructions as a baseline for IFT. It compares the performance of different methods for selecting high-quality examples for IFT and demonstrates that selecting the 1,000 longest instructions from standard datasets consistently outperforms more sophisticated methods. The article also discusses the role of response length in instruction-following tasks using LLMs and evaluates the performance of the Llama-2-7B model fine-tuned on the Alpaca-1k-longest dataset. Additionally, it provides details about the IFT datasets used in the experiments and presents preference evaluations and comparisons of various LLM fine-tuning methods on different datasets. The article also includes examples of responses to specific prompts and discusses the potential consequences of humanity never discovering electricity.

### Major Findings:

1. Selecting the 1,000 longest instructions from standard datasets consistently outperforms more sophisticated methods for IFT.
2. Longer and more detailed instructions positively influence the performance of LLMs on quantitative tasks.
3. Fine-tuning on datasets with longer responses consistently leads to higher preferences on average than existing methods.

### Analysis and Critique:

- The findings suggest that selecting long instructions as a baseline for IFT can lead to improved performance of fine-tuned LLMs, challenging the current understanding of high-quality IFT datasets and their impact on fine-tuned model performance in standard NLP benchmarks. However, the article could benefit from further exploration of the potential biases and limitations in the experimental methodology. Additionally, the qualitative analysis of responses to specific prompts provides valuable insights into the quality of responses generated by LLMs fine-tuned on different datasets. However, further research is needed to fully understand the implications of these findings for instruction-following performance and factuality in LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.04833v1](https://arxiv.org/abs/2402.04833v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04833v1](https://browse.arxiv.org/html/2402.04833v1)       |
| Truncated       | True       |
| Word Count       | 29117       |