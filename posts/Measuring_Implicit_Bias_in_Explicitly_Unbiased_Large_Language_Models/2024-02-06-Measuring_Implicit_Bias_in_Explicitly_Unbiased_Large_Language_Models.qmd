
---
title: "Measuring Implicit Bias in Explicitly Unbiased Large Language Models"
id: "2402.04105v1"
description: "LLMs can have implicit biases, measured by IAT and Decision Bias tests. Bias found in 6 LLMs."
author: Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, Thomas L. Griffiths
date: "2024-02-06"
image: "../../img/2402.04105v1/image_1.png"
categories: ['hci', 'social-sciences', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.04105v1/image_1.png)

### Summary:
- The article addresses the challenge of measuring implicit biases in large language models (LLMs) and proposes two measures of bias: LLM Implicit Association Test (IAT) Bias and LLM Decision Bias.
- Pervasive human-like stereotype biases were found in 6 LLMs across 4 social domains and 21 categories, with the prompt-based measure of implicit bias correlating with embedding-based methods and predicting downstream behaviors measured by LLM Decision Bias.
- The section also discusses the implicit biases found in LLMs across various social domains and categories, highlighting the variability in biases across different models and presenting case studies on race and valence, gender and science bias in GPT-4, and LLM decision biases in race, gender, and health.
- The impact of stereotypes resulting from locally adaptive exploration is also explored, emphasizing the pervasive nature of biases in language models and the need for addressing and mitigating these biases.
- Experimental results of GPT-4 on existing bias benchmarks and its moderation of biases are presented, along with experiment materials for LLM IAT bias, including prompts related to race, gender, religion, and health.
- Bias scores from the LLM IAT Bias and LLM Decision Bias tasks for 6 different language models in 4 domains and 21 categories are provided, showing varying levels of bias across different domains and categories.
- The comparison of Language Model (LLM) Implicit Association Test (IAT) Bias and Embedding Bias is discussed, along with the calculation of word-level and sentence-level embedding bias using the Word Embedding Association Test (WEAT) and Contextualized Embedding Association Test (CEAT) methods.

### Major Findings:
1. Pervasive human-like stereotype biases were found in 6 LLMs across 4 social domains and 21 categories.
2. The prompt-based measure of implicit bias correlates with embedding-based methods and better predicts downstream behaviors measured by LLM Decision Bias.
3. Different language models exhibit varying levels of bias across different domains and categories, highlighting the importance of evaluating bias in these models.

### Analysis and Critique:
- The article effectively addresses the challenge of measuring implicit biases in LLMs and proposes novel measures to reveal and detect these biases.
- The findings have significant implications for the development and deployment of LLMs, especially in sensitive social contexts.
- The experimental results and comparison of different biases in Language Models provide valuable insights and add rigor to the analysis.
- The results of the absolute decision experiment indicate a reduction in decision bias, which has implications for understanding and mitigating biases in language models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.04105v1](https://arxiv.org/abs/2402.04105v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04105v1](https://browse.arxiv.org/html/2402.04105v1)       |
| Truncated       | True       |
| Word Count       | 27284       |