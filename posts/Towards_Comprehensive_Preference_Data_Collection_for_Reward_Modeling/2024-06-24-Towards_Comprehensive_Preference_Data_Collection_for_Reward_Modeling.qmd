
---
title: "Towards Comprehensive Preference Data Collection for Reward Modeling"
id: "2406.16486v1"
description: "New framework for RLHF preference data collection improves quality, diversity, and reduces human labor."
author: Yulan Hu, Qingyang Li, Sheng Ouyang, Ge Chen, Kaihui Chen, Lijun Mei, Xucheng Ye, Fuzheng Zhang, Yong Liu
date: "2024-06-24"
image: "https://browse.arxiv.org/html/2406.16486v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.16486v1/x1.png)

### Summary:

The paper presents a comprehensive study on collecting preference data for training reward models (RMs) in the context of Reinforcement Learning from Human Feedback (RLHF). The proposed framework aims to gather high-quality preference data by decomposing the process into four sub-steps: Prompt Generation, Response Generation, Response Filtering, and Human Labeling. The framework combines AI filtering with human intervention to effectively reflect human preferences while significantly reducing the amount of human labor required. The experiments conducted on preference data collected at different stages demonstrate that performance enhancement is achieved as the quality of the preference data improves.

### Major Findings:

1. The proposed framework decomposes the preference data collection process into four sub-steps: Prompt Generation, Response Generation, Response Filtering, and Human Labeling, ensuring the collection of high-quality preferences while reducing reliance on human labor.
2. The framework combines AI filtering with human intervention, effectively reflecting human preferences while significantly reducing the amount of human labor required.
3. The experiments conducted on preference data collected at different stages demonstrate that performance enhancement is achieved as the quality of the preference data improves.

### Analysis and Critique:

1. The paper provides a detailed and structured approach to collecting high-quality preference data for RM training, addressing the lack of thorough investigation in this area.
2. The framework's reliance on AI filtering and human intervention could potentially introduce biases or limitations, as the AI models used for filtering may not perfectly align with human preferences, and human annotators may introduce subjectivity.
3. The long-term data production pipeline in the proposed framework may not facilitate the collection of enough training data in a short period of time, making it more suitable for the later stages of RM optimization and for optimizing certain specific verticals.
4. The paper does not discuss the scalability of the proposed framework, which could be a potential limitation when dealing with large-scale preference data collection.
5. The paper does not provide a comparison with other existing methods for preference data collection, making it difficult to evaluate the proposed framework's performance against alternative approaches.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.16486v1](https://arxiv.org/abs/2406.16486v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.16486v1](https://browse.arxiv.org/html/2406.16486v1)       |
| Truncated       | False       |
| Word Count       | 3102       |