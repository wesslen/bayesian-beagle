
---
title: "A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution"
id: "2408.06272v1"
description: "Our QA model, based on RAG and LLM, outperforms GPT-3.5 and GPT-4o in cyber-attack investigation and attribution, providing answer sources and reducing hallucination."
author: Sampath Rajapaksha, Ruby Rani, Erisa Karafili
date: "2024-08-12"
image: "https://browse.arxiv.org/html/2408.06272v1/extracted/5786891/images/CyberThreaDQA.png"
categories: ['production', 'architectures', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.06272v1/extracted/5786891/images/CyberThreaDQA.png)

### Summary:

The paper introduces a Retrieval Augmented Generation (RAG)-based question-answering (QA) model for cyber-attack investigation and attribution. The model uses a knowledge base (KB) containing curated information about cyber-attacks and a Large Language Model (LLM) to provide answers to user queries. The model was tested with various types of questions and compared to OpenAI's GPT-3.5 and GPT-4o LLMs. The RAG-based QA model outperformed the GPT models by providing the source of the answers and overcoming the hallucination limitations of the GPT models. The model also performed better when given few-shot examples rather than zero-shot instructions.

### Major Findings:

1. The RAG-based QA model outperforms OpenAI's GPT models by providing the source of the answers and overcoming the hallucination limitations of the GPT models.
2. The RAG-based QA model generates better answers when given few-shot examples rather than zero-shot instructions.
3. The RAG-based QA model uses a KB containing curated information about cyber-attacks and a LLM to provide answers to user queries.

### Analysis and Critique:

The paper presents a novel approach to cyber-attack investigation and attribution using a RAG-based QA model. The model's ability to provide the source of the answers and overcome the hallucination limitations of the GPT models is a significant advantage. However, the paper does not provide a detailed comparison of the RAG-based QA model to other existing models for cyber-attack investigation and attribution. Additionally, the paper does not discuss the limitations of the RAG-based QA model, such as the potential for the model to generate incorrect answers if the KB contains incorrect information. Further research is needed to evaluate the effectiveness of the RAG-based QA model in real-world scenarios and to compare it to other existing models.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.06272v1](https://arxiv.org/abs/2408.06272v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.06272v1](https://browse.arxiv.org/html/2408.06272v1)       |
| Truncated       | False       |
| Word Count       | 7723       |