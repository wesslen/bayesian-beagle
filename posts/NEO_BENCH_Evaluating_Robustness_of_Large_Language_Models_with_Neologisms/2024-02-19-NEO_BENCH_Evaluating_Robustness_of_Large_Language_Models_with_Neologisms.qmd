
---
title: "NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms"
id: "2402.12261v1"
description: "Neologisms impact LLM performance, benchmark shows lower perplexities with later knowledge cutoff dates."
author: Jonathan Zheng, Alan Ritter, Wei Xu
date: "2024-02-19"
image: "../../img/2402.12261v1/image_1.png"
categories: ['production', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.12261v1/image_1.png)

### Summary:
- The emergence of neologisms and their impact on large language models (LLMs) is discussed, with the introduction of NEO-BENCH to evaluate LLMs' ability to understand and process neologisms.
- Neologisms significantly impact LLMs, with newer and larger LLMs performing better on tasks involving neologisms.
- The challenges and methods for detecting neologisms are highlighted, with the introduction of NEO-BENCH to test the ability of LLMs to generalize on neologisms.
- The creation of the NEO-BENCH dataset for neologism detection is detailed, including data collection, analysis, and tasks for machine translation and rank classification with perplexity.
- The evaluation of language models for perplexity ranking, cloze question answering, and definition generation tasks is presented, comparing the performance of different models.

### Major Findings:
1. Neologisms significantly impact LLMs, with newer and larger LLMs performing better on tasks involving neologisms.
2. The introduction of NEO-BENCH provides a valuable resource for evaluating LLMs' robustness in handling language change.
3. The performance of various language models in generating coherent sequences, answering cloze questions, and defining neologisms has implications for improving language model capabilities.

### Analysis and Critique:
- The findings provide valuable insights into the impact of neologisms on LLMs and the challenges they pose for natural language processing tasks.
- The introduction of NEO-BENCH and the collection of neologisms provide a comprehensive resource for evaluating LLMs' robustness in handling language change.
- The limitations of existing methods for detecting neologisms and the need for more comprehensive and dynamic approaches to collect and evaluate neologisms are highlighted.
- The performance of different language models in generating definitions for specific terms sheds light on the potential for automated definition generation and the need for further improvement in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-20       |
| Abstract | [https://arxiv.org/abs/2402.12261v1](https://arxiv.org/abs/2402.12261v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12261v1](https://browse.arxiv.org/html/2402.12261v1)       |
| Truncated       | True       |
| Word Count       | 24207       |