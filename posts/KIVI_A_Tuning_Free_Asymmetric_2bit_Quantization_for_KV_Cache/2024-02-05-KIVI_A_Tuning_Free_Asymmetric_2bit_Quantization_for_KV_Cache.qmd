
---
title: "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
id: "2402.02750v1"
description: "KV cache size limits LLM efficiency, but KIVI algorithm reduces memory usage and increases throughput."
author: Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu
date: "2024-02-05"
image: "../../../bayesian-beagle.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:
- Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request. Yet, the key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage. This memory demand increases with larger batch sizes and longer context lengths. Additionally, the inference speed is limited by the size of KV cache, as the GPUâ€™s SRAM must load the entire KV cache from the main GPU memory for each token generated, causing the computational core to be idle during this process.
- A straightforward and effective solution to reduce KV cache size is quantization, which decreases the total bytes taken by KV cache. However, there is a lack of in-depth studies that explore the element distribution of KV cache to understand the hardness and limitation of KV cache quantization. To fill the gap, the authors conducted a comprehensive study on the element distribution in KV cache of popular LLMs. Their findings indicate that the key cache should be quantized per-channel, i.e., group elements along the channel dimension and quantize them together. In contrast, the value cache should be quantized per-token.
- Based on the above insights, the authors proposed KIVI, a plug-and-play extreme low-bit KV cache quantization method. KIVI quantizes key cache per-channel and quantizes value cache per-token. The per-token value cache quantization aligns well with the streaming nature of auto-regressive inference, allowing newly quantized tensors to be directly appended to the existing quantized value cache by token dimension.

### Major Findings:
1. Efficiently serving large language models (LLMs) requires batching many requests together to reduce the cost per request.
2. The key-value (KV) cache, which stores attention keys and values to avoid re-computations, significantly increases memory demands and becomes the new bottleneck in speed and memory usage.
3. The proposed KIVI algorithm quantizes key cache per-channel and quantizes value cache per-token, enabling extreme low-bit KV cache quantization.

### Analysis and Critique:
- The proposed KIVI algorithm provides a promising solution to the challenges associated with large language models. However, the study could benefit from further validation on a wider range of LLMs and real-world applications to assess its generalizability and practical utility. Additionally, the authors should consider addressing potential trade-offs and limitations associated with the proposed quantization approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-06       |
| Abstract | [https://arxiv.org/abs/2402.02750v1](https://arxiv.org/abs/2402.02750v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.02750v1](https://browse.arxiv.org/html/2402.02750v1)       |
| Truncated       | False       |
| Word Count       | 13762       |