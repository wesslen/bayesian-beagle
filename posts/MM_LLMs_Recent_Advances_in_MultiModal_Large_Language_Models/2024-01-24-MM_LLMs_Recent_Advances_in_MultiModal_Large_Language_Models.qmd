
---
title: "MM-LLMs: Recent Advances in MultiModal Large Language Models"
id: "2401.13601v2"
description: "MM-LLMs have evolved and can support MM inputs and outputs. This survey provides design, models, performance, and future directions."
author: ['Duzhen Zhang', 'Yahan Yu', 'Chenxing Li', 'Jiahua Dong', 'Dan Su', 'Chenhui Chu', 'Dong Yu']
date: "2024-01-24"
image: "https://browse.arxiv.org/html/2401.13601v2/x1.png"
categories: ['education', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.13601v2/x1.png)

**Summary of the Article:**
The article provides a comprehensive survey of MultiModal Large Language Models (MM-LLMs), focusing on recent advancements in this field, highlighting the model architecture, training pipeline, state-of-the-art (SOTA) models, benchmarks and performance, training recipes, future directions, and related surveys. It outlines the progression of MM-LLMs from MM understanding to generation, introduces several impactful MM-LLMs, and emphasizes the need for more challenging benchmarks and continual improvement in areas such as mobile deployment and embodied intelligence. The article also contributes by establishing a real-time tracking website for ongoing MM-LLMs developments and envisions avenues for future MM-LLMs research.

### Major Findings:
1. MM-LLMs leverage Large Language Models (LLMs) to handle MultiModal (MM) inputs, showcasing impressive capabilities in MM content comprehension and generation.
2. The article introduces several SOTA MM-LLMs, each distinguished by specific formulations and developmental trends, emphasizing continual refinement of the training pipeline through Supervised Fine-Tuning and Reinforcement Learning from Human Feedback.
3. Training recipes for MM-LLMs suggest the incorporation of higher image resolution, high-quality Supervised Fine-Tuning data, and re-blending of text-only instruction data during fine-tuning.

### Analysis and Critique:
The article provides a comprehensive overview of MM-LLMs, addressing various aspects from model architecture to future directions. However, it lacks a critical analysis of potential limitations, biases, or conflicting evidence in the field. It would have been beneficial to include a discussion of potential challenges or areas requiring further research, especially in continually fine-tuning MM-LLMs, establishing more challenging benchmarks, and addressing the limitations of the surveyed MM-LLMs. Additionally, the article would benefit from clearer organization and section headings to aid in navigating the extensive content.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-29       |
| Abstract | [http://arxiv.org/abs/2401.13601v2](http://arxiv.org/abs/2401.13601v2)        |
| HTML     | [https://browse.arxiv.org/html/2401.13601v2](https://browse.arxiv.org/html/2401.13601v2)       |
| Truncated       | False       |
| Word Count       | 10356       |