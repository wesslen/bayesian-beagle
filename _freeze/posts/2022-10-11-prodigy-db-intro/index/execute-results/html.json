{
  "hash": "d5d4ca8f39ef4a7df3db3c0e895d6fc1",
  "result": {
    "markdown": "---\ntitle: \"Retrieving annotations in Prodigy\"\nauthor: \"Ryan Wesslen\"\ndate: \"2022-10-11\"\ndescription: \"How to use Prodigy database recipes for handling annotated data.\"\ncategories: [prodigy, database, sqlite]\nexecute: \n  enabled: true\n---\n\nBy default, [Prodigy](https://prodi.gy/) includes a [SQLite](https://www.sqlite.org/index.html) database to save annotations. This enables using SQLite to be used out of the box without additional configuration. While there are many ways to customize the database, many users may be able to interact with their annotations only requiring three helpful Prodigy recipes: [`db-in`](#db-in), [`db-out`](#db-out), and [`db-merge`](#db-merge).\n\nIn this post, I want to briefly review these recipes. Near the end, I'll also show how you can interact with [Prodigy's default database too from Python](#accessing-the-database-programmatically) to enable a more programmatic approach. We won't cover all aspects of Prodigy's database but I encourage the interested reader to see the [Prodigy database documentation](https://prodi.gy/docs/api-database) or [Prodigy Support issues tagged as database](https://support.prodi.gy/tag/database).\n\n\n## `db-out`\n\nLet's start first with the problem of exporting out annotations to files. For this, we'll use the `db-out` recipe that includes two arguments:\n1. `my_dataset` which is the Prodigy dataset we want to export\n2. `> /path/to/data.jsonl` which is the file location of the exported dataset. \n\nTo see this, let's first say we have a dataset named `ceo-letters`. It is 10 sentences from a CEO's Letter to Shareholders.\n\n\n```bash\npython -m prodigy db-out my_dataset > /path/to/data.jsonl\n```\n\n```bash\npython -m prodigy db-out  | jq \n```\n\nTODO: show example\n\n## `db-in`\n\nThe [`db-in`](https://prodi.gy/docs/recipes#db-in) is a recipe to load in new examples into your Prodigy database.\n\n:::{.callout-note}\nThe `db-in` command is typically used to import existing annotations into your Prodigy datasets – for example, if you've already labelled data with some other process and want to combine it with new annotations or if you want to re-import annotations to a new dataset.\n\nIf you just want to annotate data, you do not have to import anything upfront – you can just start the server with your input data and Prodigy will stream it in, let you annotate and save the collected annotations to the database.\n:::\n\nProdigy prefers [newline-delimited JSON](https://jsonlines.org/) (or JSONL), as it can contain detailed information and meta data and allows being read in line by line. \n\nUnlike regular JSON, JSONL doesn’t require parsing the entire file, which results in overall better performance when working with large volumes of text.\n\nTODO: change to [NER example]()\n\nAs an example, we'll use an annotation that is in the format of the [`ner_manual` user interface](https://prodi.gy/docs/api-interfaces#ner_manual) with one annotation:\n\n```json\n{\n  \"text\": \"First look at the new MacBook Pro\",\n  \"spans\": [\n    {\"start\": 22, \"end\": 33, \"label\": \"PRODUCT\", \"token_start\": 5, \"token_end\": 6}\n  ],\n  \"tokens\": [\n    {\"text\": \"First\", \"start\": 0, \"end\": 5, \"id\": 0},\n    {\"text\": \"look\", \"start\": 6, \"end\": 10, \"id\": 1},\n    {\"text\": \"at\", \"start\": 11, \"end\": 13, \"id\": 2},\n    {\"text\": \"the\", \"start\": 14, \"end\": 17, \"id\": 3},\n    {\"text\": \"new\", \"start\": 18, \"end\": 21, \"id\": 4},\n    {\"text\": \"MacBook\", \"start\": 22, \"end\": 29, \"id\": 5},\n    {\"text\": \"Pro\", \"start\": 30, \"end\": 33, \"id\": 6}\n  ]\n}\n```\n\nFor input `.jsonl` files, the key `\"text\"` required and contains the text contents of the document typically a sentence. \n\nThe `db-in` recipe requires two arguments:\n\n1. the name of the new dataset (`new_dataset`)\n2. a path to the input file (`/path/to/data.jsonl`)\n\n```bash\npython -m prodigy db-in new_dataset /path/to/data.jsonl\n```\n\nFor example, let's say we have in our current working folder a file named `ner-sample.jsonl` that includes 1 documents, each with a sentence from a CEO's Letter to Shareholder. We can then read that file into the database as:\n\n```bash\npython -m prodigy db-in ner-sample ner-sample.jsonl\n✔ Created dataset 'ner_sample' in database SQLite\n✔ Imported 1 annotations to 'ner_sample' (session\n2022-10-11_13-22-26) in database SQLite\nFound and keeping existing \"answer\" in 0 examples\n```\n\nA few points to note. First, by running this command, Prodigy will create a new dataset named `ner_sample` into our default SQLite database. \n\nTODO: explain other aspects. [Example of db-in options](https://support.prodi.gy/t/split-a-ner-manual-dataset-into-smaller-texts/5713/4)\n\n:::{.callout-note}\nWhat if your documents are not on a sentence level to start with? That's okay, you can use spaCy and its sentence segmenter in the `en_core_web_sm` model.\n\n```json\n{\"text\": \"This is a sentence. This is another sentence. And then this is a sentence.\"}\n```\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport spacy\nimport srsly\n\nnlp = spacy.load(\"en_core_web_sm\")\nexamples = srsly.read_jsonl(\"sentences.jsonl\")\ntexts = (eg[\"text\"] for eg in examples)\n\nsentences = []\nfor doc in nlp.pipe(texts):\n  for sent in doc.sents:\n    sentences.append({\"text\": sent.text}) \n\nprint(sentences)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[{'text': 'This is a sentence.'}, {'text': 'This is another sentence.'}, {'text': 'And then this is a sentence.'}]\n```\n:::\n:::\n\n\nYou can use `srsly.write_jsonl` to export the data to a `.jsonl`:\n\n```python\nsrsly.write_jsonl(output_loc, new_examples)\n```\n\nAlternatively, with Prodigy you can train your own sentence segmenter or use one of spaCy universe's projects like [`pySBD`](https://spacy.io/universe/project/python-sentence-boundary-disambiguation).\n\nLast, several off-the-shelf Prodigy recipes like [`ner.correct`](https://prodi.gy/docs/recipes#ner-correct) or [`ner.teach`](https://prodi.gy/docs/recipes#ner-teach) use a sentence segmenter as it's default behavior. You may turn off this behavior to not segment sentences by adding `--unsegmented` to the recipe.\n:::\n\n\n## `db-merge`\n\nTODO: explain db-merge.\n\n```\npython -m prodigy db-merge new_dataset  \n```\n\nTODO: Give an illustrative example\n\n\nTODO: [Problem with `db-merge` memory](https://support.prodi.gy/t/prodigy-ner-train-recipe-getting-killed-by-oom/3453/2).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}