
---
title: "Ask LLMs Directly, What shapes your bias?: Measuring Social Bias in Large Language Models"
id: "2406.04064v1"
description: "This paper proposes a method to quantify social biases in LLMs by considering diverse social perceptions, offering a more nuanced understanding of bias."
author: Jisu Shin, Hoyun Song, Huije Lee, Soyeong Jeong, Jong C. Park
date: "2024-06-06"
image: "https://browse.arxiv.org/html/2406.04064v1/extracted/5648652/Images/1Introduction/concept_fig_3.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.04064v1/extracted/5648652/Images/1Introduction/concept_fig_3.png)

### Summary:
- The paper proposes a novel strategy to intuitively quantify social perceptions and suggests metrics to evaluate social biases within large language models (LLMs) by aggregating diverse social perceptions.
- The study aims to investigate how social perceptions from various viewpoints contribute to the development of social bias in LLMs.
- The experimental results show the quantitative demonstration of social attitude in LLMs by examining social perception.
- The analysis conducted shows that the proposed metrics capture the multi-dimensional aspects of social bias, enabling a fine-grained and comprehensive investigation of bias in LLMs.

### Major Findings:
1. The paper introduces a methodology to directly measure social perceptions in a QA format and aggregate the social perceptions to quantify bias.
2. The study proposes three novel metrics for measuring social biases: Target Bias (TB), Bias Amount (BAmt), and Persona Bias (PB).
3. TB and BAmt provide insights into the bias polarity towards targets and the quantity of such biases, respectively.
4. PB uniquely assesses the variance in social perception based on a demographic identity perceived by LLMs.

### Analysis and Critique:
- The paper's approach to quantifying social perceptions and measuring biases in LLMs is a significant contribution to the field.
- The proposed metrics allow for a more comprehensive and fine-grained analysis of bias in LLMs, which is a significant improvement over previous methods.
- However, the paper does not discuss the potential limitations or biases that may be introduced by the persona-assigning approach.
- The study also does not address the potential impact of the context or the performance of toxicity and sentiment classifiers on the results.
- Further research is needed to validate the proposed metrics and evaluate their effectiveness in different contexts and with different LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.04064v1](https://arxiv.org/abs/2406.04064v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.04064v1](https://browse.arxiv.org/html/2406.04064v1)       |
| Truncated       | False       |
| Word Count       | 5188       |