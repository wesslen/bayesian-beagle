
---
title: "Evaluating Large Language Models with Grid-Based Game Competitions: An Extensible LLM Benchmark and Leaderboard"
id: "2407.07796v2"
description: "This study introduces a benchmark for LLMs using grid-based games, revealing variations in performance across different games and prompt types."
author: Oguzhan Topsakal, Colby Jacob Edell, Jackson Bailey Harper
date: "2024-07-11"
image: "https://browse.arxiv.org/html/2407.07796v2/extracted/5724064/images/TicTacToe-list.png"
categories: ['hci', 'education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.07796v2/extracted/5724064/images/TicTacToe-list.png)

# Summary

This study introduces a novel and extensible benchmark for large language models (LLMs) using grid-based games such as Tic-Tac-Toe, Connect Four, and Gomoku. The open-source game simulation code, available on GitHub, allows LLMs to compete and generates detailed data files in JSON, CSV, TXT, and PNG formats for leaderboard rankings and further analysis. The study presents the results of games among leading LLMs, including Claude 3.5 Sonnet and Claude 3 Sonnet by Anthropic, Gemini 1.5 Pro and Gemini 1.5 Flash by Google, GPT-4 Turbo and GPT-4o by OpenAI, and Llama3-70B by Meta. The results revealed significant variations in LLM performance across different games and prompt types. The study enhances our understanding of LLMs’ capabilities in playing games they were not specifically trained for, helping to assess their rule comprehension and strategic thinking.

## Major Findings:

1. LLMs perform relatively well in simpler formats, such as list prompts for Tic-Tac-Toe and Connect Four, but their performance declines with more complex prompts, especially those involving illustrations and images.
2. LLMs show a tendency to make invalid moves when faced with more complex prompts, underscoring the need for improved strategic decision-making processes.
3. The study reveals both the strengths and limitations of LLMs, pointing to the need for ongoing research to enhance their ability to process complex and visual data, improve decision-making processes, and develop more sophisticated benchmarking tools.

## Analysis and Critique:

This study provides a valuable contribution to the field by introducing a novel and extensible benchmark for LLMs using grid-based games. The use of open-source game simulation code and the generation of detailed data files in various formats facilitate further analysis and comparison of LLM performance. However, the study has some limitations. The focus on a select group of LLMs might not capture the full diversity of strategic approaches across available models. Additionally, the simplicity of the games used in this benchmark may not challenge LLMs’ strategic capabilities as much as more complex games like chess or Go might.

Future work could explore several promising directions to extend research and deepen our understanding

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.07796v2](https://arxiv.org/abs/2407.07796v2)        |
| HTML     | [https://browse.arxiv.org/html/2407.07796v2](https://browse.arxiv.org/html/2407.07796v2)       |
| Truncated       | False       |
| Word Count       | 13403       |