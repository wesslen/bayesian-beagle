
---
title: "Multilingual Trolley Problems for Language Models"
id: "2407.02273v1"
description: "LLMs' moral decisions vary by language; more aligned with English, Korean, Hungarian, and Chinese, less with Hindi and Somali. Fairness dominates GPT-4's choices, utilitarianism for GPT-3. Language inequality exists in moral decision-making."
author: Zhijing Jin, Sydney Levine, Max Kleiman-Weiner, Giorgio Piatti, Jiarui Liu, Fernando Gonzalez Adauto, Francesco Ortu, András Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Schölkopf
date: "2024-07-02"
image: "https://browse.arxiv.org/html/2407.02273v1/extracted/5705672/fig/fig_example.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.02273v1/extracted/5705672/fig/fig_example.png)

### Summary:

The paper introduces Multilingual Trolley Problems (MultiTP), a dataset to evaluate the morality of large language models (LLMs) in 100+ languages. The dataset is grounded in moral philosophy and psychology, using the "trolley problem" task. MultiTP allows for controlled variations across specified parameters and is multilingual, covering 100+ languages. The study reveals that LLMs tend to align more closely with human moral preferences in some languages, such as English, Korean, Hungarian, and Chinese, while showing less alignment in languages such as Hindi and Somali (in Africa). This variance highlights the presence of "language inequality," manifesting itself as different levels of model performance and moral reasoning between languages.

### Major Findings:

1. LLMs are more aligned with human preferences in languages such as English, Korean, Hungarian, and Chinese, but less aligned in languages such as Hindi and Somali (in Africa).
2. Fairness is the most dominant supporting reason behind GPT-4’s decisions and utilitarianism by GPT-3.
3. The study reveals "language inequality" in a series of meta-properties of moral decision making.

### Analysis and Critique:

The paper provides a valuable contribution to the understanding of LLMs' moral decision-making in different languages and cultures. However, there are some limitations and potential biases that should be considered:

1. The study focuses on a single task, the trolley problem, which may not fully capture the complexity of moral decision-making in real-world situations.
2. The study uses a limited set of languages, which may not fully represent the diversity of human languages and cultures.
3. The study does not address the potential biases and limitations of the LLMs themselves, which may affect their moral decision-making.
4. The study does not consider the potential impact of the LLMs' moral decision-making on real-world applications, such as autonomous vehicles or other AI systems.

Overall, the paper provides a valuable contribution to the understanding of LLMs' moral decision-making in different languages and cultures. However, further research is needed to address the limitations and potential biases of the study.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02273v1](https://arxiv.org/abs/2407.02273v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02273v1](https://browse.arxiv.org/html/2407.02273v1)       |
| Truncated       | False       |
| Word Count       | 12265       |