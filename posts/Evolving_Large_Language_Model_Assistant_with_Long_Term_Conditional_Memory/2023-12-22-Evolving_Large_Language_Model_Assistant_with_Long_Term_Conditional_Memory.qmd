
---
title: "Evolving Large Language Model Assistant with Long-Term Conditional Memory"
id: "2312.17257v1"
description: "AI assistants like ChatGPT with long-term memory improve responses using past dialogue, tested on different datasets."
author: ['Ruifeng Yuan', 'Shichao Sun', 'Zili Wang', 'Ziqiang Cao', 'Wenjie Li']
date: "2023-12-22"
image: "https://browse.arxiv.org/html/2312.17257v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.17257v1/x1.png)

# Evolving Large Language Model Assistant with Long-Term Conditional Memory

## Key Findings

- The paper presents an evolving large language model assistant that utilizes **verbal long-term memory** to preserve knowledge and experience from previous dialogues to improve future responses.
- **Conditional memory** is proposed as a new memorizing mechanism to address the shortcomings of existing methods in preserving and utilizing critical information from dialogues.
- The study evaluates the model on three constructed test datasets focusing on different abilities required by an AI assistant with long-term memory and finds that conditional memory achieves relatively better results.

## Introduction
- The rapid development of large language models has led to the widespread use of AI assistants such as ChatGPT, which provide assistance through dialogue interactions.
- However, current AI assistants lack the ability to preserve information from previous dialogue sessions, hindering their capacity to learn and improve responses over time.

## Proposed Framework
- The evolving large language model assistant is made up of an existing LLM assistant, a **memory**, and a prompt-based wrapper responsible for interactions between the assistant and the memory.
- The wrapper constructs memory records from ongoing dialogues and stores them in the memory, which is later retrieved to enhance the quality of responses.

## Memory Construction
- The study explores three types of memory construction mechanisms: History-Based Memory, Summary-Based Memory, and **Conditional Memory**.
- Conditional Memory is proposed to selectively memorize crucial information based on the importance of each utterance.

## Memory Retrieval and Application
- The retrieval of memory records is conducted using dense retrieval, and a **self-reflection mechanism** is employed to determine the usefulness of retrieved information in response generation.

## Evaluation
- The model is evaluated on three test datasets focusing on different aspects: continuing previous dialogues, learning new knowledge, and learning from human feedback.
- The results show that conditional memory outperforms other forms of memory in learning new knowledge and learning from human feedback.

## Critique
- The study relies on small-scale test datasets, limiting the generalizability of the findings to real-world scenarios with larger and more diverse data.
- The paper mainly investigates the foundational aspects of the proposed idea, leaving other key aspects such as the time stamp or forgetting mechanism unexplored.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [http://arxiv.org/abs/2312.17257v1](http://arxiv.org/abs/2312.17257v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.17257v1](https://browse.arxiv.org/html/2312.17257v1)       |
| Truncated       | False       |
| Word Count       | 8298       |