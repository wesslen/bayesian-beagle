
---
title: "TAMIGO: Empowering Teaching Assistants using LLM-assisted viva and code assessment in an Advanced Computing Class"
id: "2407.16805v1"
description: "LLMs aid TAs in assessing viva and code, offering constructive feedback, but may hallucinate and require alignment with rubrics."
author: Anishka IIITD, Diksha Sethi, Nipun Gupta, Shikhar Sharma, Srishti Jain, Ujjwal Singhal, Dhruv Kumar
date: "2024-07-23"
image: "https://browse.arxiv.org/html/2407.16805v1/extracted/5750856/files/images/phases.png"
categories: ['programming', 'education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.16805v1/extracted/5750856/files/images/phases.png)

### Summary:

- The paper explores the application of Large Language Models (LLMs) in assisting teaching assistants (TAs) with viva and code assessments in an advanced computing class on distributed systems in an Indian University.
- The authors developed TAMIGO, an LLM-based system for TAs to evaluate programming assignments, using OpenAI's GPT-3.5-Turbo model.
- TAMIGO was used to generate questions for viva assessments and provide feedback on student answers, as well as evaluate student code submissions.
- The study evaluates the quality of LLM-generated viva questions, model answers, feedback on viva answers, and feedback on student code submissions.
- The results indicate that LLMs are highly effective at generating viva questions with sufficient context, but the feedback on viva answers exhibited occasional hallucinations, impacting accuracy.
- The feedback on code submissions was comprehensive, though improvements are needed to better match the course rubric.

### Major Findings:

1. LLMs are highly effective at generating questions with sufficient context for viva assessments.
2. The feedback on viva answers exhibited occasional hallucinations, impacting accuracy.
3. Despite occasional hallucinations, the feedback was generally constructive and balanced, aiding TAs without overwhelming them.
4. The feedback on code submissions was comprehensive, though improvements are needed to better match the course rubric.

### Analysis and Critique:

- The study provides valuable insights into the potential benefits and challenges of integrating LLMs into educational settings.
- The use of LLMs for generating viva questions and providing feedback on student answers can save time and effort for TAs, while also ensuring a thorough understanding of student submissions.
- However, the occasional hallucinations in the feedback on viva answers highlight the need for further refinement and improvement in LLM-based systems.
- The study also highlights the need for better alignment of LLM-generated feedback with the course rubric for code evaluations.
- Future research should focus on refining these systems to mitigate inaccuracies and further align their output with instructor-provided rubrics.
- The study does not evaluate student perception using AI-based evaluators, which could provide insights into how students perceive and

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.16805v1](https://arxiv.org/abs/2407.16805v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.16805v1](https://browse.arxiv.org/html/2407.16805v1)       |
| Truncated       | False       |
| Word Count       | 6849       |