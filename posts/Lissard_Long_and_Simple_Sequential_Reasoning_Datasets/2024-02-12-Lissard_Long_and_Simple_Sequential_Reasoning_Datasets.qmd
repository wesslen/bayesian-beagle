
---
title: "Lissard: Long and Simple Sequential Reasoning Datasets"
id: "2402.07859v1"
description: "Language models struggle with repetitive tasks on long sequences, as shown in Lissard benchmark."
author: Mirelle Bueno, Roberto Lotufo, Rodrigo Nogueira
date: "2024-02-12"
image: "../../img/2402.07859v1/image_1.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.07859v1/image_1.png)

### **Summary:**
- The article introduces the Lissard benchmark, which evaluates the ability of language models to process and generate wide-range sequence lengths requiring repetitive procedural execution.
- The benchmark comprises seven tasks and evaluates open-source and proprietary models, showing a consistent decline in performance across all models as the complexity of the sequence increases.

### Major Findings:
1. The efficacy of language models, particularly in reasoning tasks, is significantly impacted by longer text lengths than those seen in training.
2. Recent research has tried to address this challenge by modifications to the positional embeddings or by using prompting strategies such as scratchpad and chain-of-thought reasoning.
3. The Lissard benchmark is designed to evaluate the ability of models on tasks that require the use of repetitive simple rules, whose difficulty increases with respect to the sequence length.

### Analysis and Critique:
- The article provides a comprehensive evaluation of the limitations of state-of-the-art language models in processing and generating text as lengths increase.
- The benchmark introduces a control mechanism called "key entities" to systematically increase task complexity in tandem with sequence length, providing more control and enabling a detailed analysis of model performance.
- The study highlights the need for benchmarks that can explicitly manipulate and test the impact of sequence length on model performance, addressing the limitations of existing benchmarks in evaluating model performance degradation within the context of length generalization.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.07859v1](https://arxiv.org/abs/2402.07859v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07859v1](https://browse.arxiv.org/html/2402.07859v1)       |
| Truncated       | False       |
| Word Count       | 7052       |