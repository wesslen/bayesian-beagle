
---
title: "Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents"
id: "2402.11651v1"
description: "LLMs need better tool use; using negative examples improves model performance. Code and data available."
author: Renxi Wang, Haonan Li, Xudong Han, Yixuan Zhang, Timothy Baldwin
date: "2024-02-18"
image: "https://browse.arxiv.org/html/2402.11651v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.11651v1/x1.png)

### Summary:
- Large language models (LLMs) have been successful as agents, but they are not optimized for tool use during training or alignment.
- Previous work has collected interaction trajectories between GPT-4 and environments and fine-tuned smaller models with them.
- This paper contends that LLMs can learn from failures through appropriate data cleaning and fine-tuning strategies.
- Experimental results demonstrate that incorporating negative examples enhances model performance by a large margin.

### Major Findings:
1. LLMs can learn from negative examples through fine-tuning.
2. Incorporating negative examples enhances model performance by a large margin.
3. Negative-aware training (NAT) outperforms traditional methods that solely use positive examples or naively combine positive and negative examples.

### Analysis and Critique:
- The paper demonstrates the value of negative trajectories and introduces a negative-aware training approach, allowing LLM-based trained agents to effectively learn from both positive and negative examples.
- The study validates the broad applicability and effectiveness of learning from negative examples, enabling models to acquire information akin to positive examples across various tasks and prompting strategies.
- The paper acknowledges limitations, such as the requirement for ground truth labels and the lack of experimentation with more diverse and powerful models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-20       |
| Abstract | [https://arxiv.org/abs/2402.11651v1](https://arxiv.org/abs/2402.11651v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11651v1](https://browse.arxiv.org/html/2402.11651v1)       |
| Truncated       | False       |
| Word Count       | 5764       |