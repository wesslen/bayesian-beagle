
---
title: "Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation"
id: "2401.08417v1"
description: "13B LLM-based translation models have shortcomings, but new approach improves performance to match or exceed competition winners."
author: Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, Young Jin Kim
date: "2024-01-16"
image: "../../../bayesian-beagle.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:

The article scrutinizes the quality of reference translations in machine translation tasks and introduces the Contrastive Preference Optimization (CPO) method for preference learning. It investigates whether improved translation scores reflect genuinely better translations or if they simply align more closely with the evaluation model’s preferences. The study also presents comprehensive results for en→xx and xx→en translations, compares the performance of recently released large language model (LLM)-based translators, and discusses the impact of human-labeled preference data on translation performance.

### Major Findings:
1. The quality of human-written parallel data may not consistently represent the highest quality, and the proposed CPO method aims to guide the model in developing a propensity for generating 'better' translations while simultaneously learning to reject 'worse' ones.
2. The CPO method demonstrates effectiveness in enhancing translation performance, indicating its potential for improving machine translation systems.
3. The relationship between preferred data and translation performance is complex and not straightforward, highlighting the need for a more nuanced understanding of the factors influencing translation quality.

### Analysis and Critique:
The article's findings have significant implications for the development and evaluation of machine translation models. The introduction of the CPO method addresses limitations in training models exclusively towards replicating reference translations, potentially leading to improved translation quality. However, the study also highlights the complex relationship between preferred data and translation performance, emphasizing the need for a more nuanced understanding of the factors influencing translation quality. The limitations of traditional metrics like BLEU and the importance of using reference-free metrics for evaluating advanced translation models are also underscored. Further research is needed to investigate the impact of human-labeled data on translation quality and to address potential biases in the evaluation process.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2401.08417v1](https://arxiv.org/abs/2401.08417v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.08417v1](https://browse.arxiv.org/html/2401.08417v1)       |
| Truncated       | True       |
| Word Count       | 24925       |