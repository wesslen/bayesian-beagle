
---
title: "Measuring and Benchmarking Large Language Models' Capabilities to Generate Persuasive Language"
id: "2406.17753v1"
description: "LLMs can produce persuasive text; new dataset measures this ability, enabling comparison of different LLMs and highlighting the impact of system prompts."
author: Amalie Brogaard Pauli, Isabelle Augenstein, Ira Assent
date: "2024-06-25"
image: "https://browse.arxiv.org/html/2406.17753v1/x2.png"
categories: ['hci', 'prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17753v1/x2.png)

### Summary:

The study focuses on measuring and benchmarking the ability of Large Language Models (LLMs) to produce persuasive text. Unlike previous work, which focuses on specific domains or types of persuasion, this research conducts a general study across various domains. The authors construct a new dataset, Persuasive-Pairs, consisting of short texts and their rewritten versions with amplified or diminished persuasive language. The dataset is multi-annotated on a relative scale for persuasive language. The authors also train a regression model to predict a score of persuasive language between text pairs, which can be used to benchmark and compare different LLMs. The study finds that different 'personas' in the system prompt of LLaMA3 change the persuasive language in the text substantially, even when only instructed to paraphrase.

### Major Findings:

1. The study constructs a new dataset, Persuasive-Pairs, consisting of 2697 short-text pairs annotated for relative persuasive language on a scale.
2. A regression model is trained to score relatively persuasive language of text pairs, which generalizes well across domains.
3. The study shows an example of benchmarking different LLMs' capabilities to generate persuasive language and finds that different personas in system prompts affect the degree of persuasiveness when prompted to paraphrase with no instructions regarding persuasiveness.

### Analysis and Critique:

The study provides a valuable contribution to the field by measuring and benchmarking the ability of LLMs to produce persuasive text across various domains. The construction of the Persuasive-Pairs dataset and the training of a regression model to score persuasive language are significant achievements. However, the study's scope is limited to English language texts, and the annotators are recruited from specific demographics, which may limit the dataset's cultural diversity. Additionally, the study does not examine other shallow features that may impact the measure of persuasiveness or explain what makes the text more persuasive. Further research is needed to address these limitations and expand the study's scope.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17753v1](https://arxiv.org/abs/2406.17753v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17753v1](https://browse.arxiv.org/html/2406.17753v1)       |
| Truncated       | False       |
| Word Count       | 10078       |