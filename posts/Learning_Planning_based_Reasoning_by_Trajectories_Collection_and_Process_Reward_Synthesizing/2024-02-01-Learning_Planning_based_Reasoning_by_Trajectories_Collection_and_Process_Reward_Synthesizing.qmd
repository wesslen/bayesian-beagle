
---
title: "Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing"
id: "2402.00658v1"
description: "LLMs have reasoning flaws, but a new framework improves planning-based reasoning. Outperforms GPT-3.5-Turbo."
author: Fangkai Jiao, Chengwei Qin, Zhengyuan Liu, Nancy F. Chen, Shafiq Joty
date: "2024-02-01"
image: "../../img/2402.00658v1/image_1.png"
categories: ['robustness', 'production', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.00658v1/image_1.png)

### Summary:
The article discusses the challenges and limitations of Large Language Models (LLMs) in generating reliable and faithful rationales for complex reasoning tasks. It introduces planning-based reasoning and process supervision as approaches to improve the reliability of LLM-generated rationales. The authors propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. The section also introduces a method for estimating expected values in reasoning processes and training a process reward model to assign rewards to intermediate states/actions. The evaluation of the quality of the reasoning process using GPT-4 is discussed, and a logical reasoning problem with different reasoning processes is presented and evaluated based on specific criteria.

**Key Terms:**
- Large Language Models (LLMs)
- Planning-based reasoning
- Process supervision
- Direct preference optimization (DPO)
- Expected value
- Process reward model
- Trajectory reward
- GPT-4
- Rationale quality
- Logical reasoning

### Major Findings:
1. The proposed framework for planning-based reasoning through DPO shows effectiveness in surpassing strong counterparts like GPT-3.5-Turbo in logical reasoning benchmarks.
2. The method for estimating expected values and training a process reward model provides valuable insights into assigning rewards to intermediate states/actions in reasoning processes.
3. The evaluation using GPT-4 demonstrates the effectiveness of process-supervised DPO in improving the reasonableness and simplicity of the intermediate reasoning process.

### Analysis and Critique:
- The proposed framework offers a potential solution to improve the reliability and faithfulness of LLM-generated rationales, addressing a crucial challenge in artificial intelligence and natural language processing.
- The method for estimating expected values and training a process reward model is crucial for optimizing policy models for reasoning tasks.
- The evaluation using GPT-4 provides valuable insights into the quality of reasoning processes and the impact of process supervision in enhancing reasoning quality.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.00658v1](https://arxiv.org/abs/2402.00658v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00658v1](https://browse.arxiv.org/html/2402.00658v1)       |
| Truncated       | True       |
| Word Count       | 16125       |