
---
title: "HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model"
id: "2408.09559v1"
description: "HiAgent improves LLM-based agents' performance in long-horizon tasks by managing working memory with subgoals, achieving a twofold success rate increase."
author: Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, Ping Luo
date: "2024-08-18"
image: "../../img/2408.09559v1/image_1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](../../img/2408.09559v1/image_1.png)

# Summary:

The paper "HIAGENT: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model" introduces a novel framework called HIAGENT, which leverages subgoals as memory chunks to manage the working memory of LLM-based agents hierarchically. The framework is inspired by human problem-solving strategies and aims to address the poor performance of LLM-based agents when handling long-horizon tasks.

## Major Findings:

1. HIAGENT outperforms the baseline model across all tasks, with an overall success rate more than double that of the baseline model.
2. HIAGENT is more efficient than the baseline model, accomplishing tasks with fewer steps, in less runtime, and using shorter context.
3. The ablation study confirms the effectiveness of the individual modules of HIAGENT.
4. HIAGENT is more likely to generate executable actions than the baseline model, even with longer steps.
5. The observed performance improvements in HIAGENT are statistically significant compared to the baseline model.

## Analysis and Critique:

The paper presents a well-structured and coherent summary of the proposed HIAGENT framework. The authors provide a clear motivation for the research and a detailed explanation of the framework's components. The experimental results demonstrate the effectiveness and efficiency of HIAGENT in handling long-horizon tasks.

However, the paper could benefit from a more in-depth discussion of the limitations and potential biases of the proposed framework. For instance, the authors could discuss the potential challenges of applying HIAGENT to other domains or tasks, as well as the computational resources required to implement the framework. Additionally, the paper could provide more details on the evaluation metrics used in the experiments and the criteria for selecting the final parameter settings.

Overall, the paper presents a promising approach to improving the performance of LLM-based agents in handling long-horizon tasks. The proposed HIAGENT framework offers a flexible and effective solution to managing working memory, which could inspire further research in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.09559v1](https://arxiv.org/abs/2408.09559v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.09559v1](https://browse.arxiv.org/html/2408.09559v1)       |
| Truncated       | False       |
| Word Count       | 21678       |