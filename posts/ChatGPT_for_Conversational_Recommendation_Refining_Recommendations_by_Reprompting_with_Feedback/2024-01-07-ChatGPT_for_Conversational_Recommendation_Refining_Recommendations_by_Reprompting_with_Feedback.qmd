
---
title: "ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback"
id: "2401.03605v1"
description: "ChatGPT is investigated as a conversational recommendation system, and reprompting with feedback improves relevancy while mitigating popularity bias."
author: Kyle Dylan Spurlock, Cagla Acun, Esin Saka, Olfa Nasraoui
date: "2024-01-07"
image: "https://browse.arxiv.org/html/2401.03605v1/extracted/5334641/figures/methodology.png"
categories: ['programming', 'hci', 'recommender', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.03605v1/extracted/5334641/figures/methodology.png)

### Summary

In the paper "ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback," the authors investigate the effectiveness of **ChatGPT** as a conversational recommendation system. They develop a pipeline to rigorously evaluate ChatGPT's recommendation ability using conversation and explore the impact of popularity bias in its recommendations.

#### Major Takeaways

1. **Conversation Improves Recommendation Relevancy:** Reprompting ChatGPT with feedback is found to be an effective strategy to improve recommendation relevancy.

2. **ChatGPT Outperforms Baseline Models:** ChatGPT is significantly better than both a random and traditional recommender systems, showcasing its utility in zero-shot recommendation tasks.

3. **Popularity Bias Mitigation:** The study examines strategies to counteract popularity bias in ChatGPT's recommendations, highlighting the potential for more novel recommendations.

### Methodology

- **Prompt Engineering**: The paper discusses the three predominant means of communication with LLMs through prompting: zero-shot, few-shot, and chain-of-thought prompting.

- **Language Models as Recommenders**: The extensive domain knowledge encapsulated in Large Language Models (LLMs) has recently captured interest for their use in recommendation tasks.

- **Algorithmic Recourse**: The paper draws parallels with the concept of algorithmic recourse and highlights the differences in the context of a recommendation system.

### Experiments

- **Effect of Embedding Content**: The study validates the impact of embedding content on recommendation results and identifies content level 4 embeddings as the most suitable for experimentation.

- **Iterative Feedback Analysis**: The authors explore the impact of reprompting with feedback during a conversation and highlight its significant impact on recommendation performance.

- **ChatGPT as a Top-n Recommender**: The study compares ChatGPT with NMF as a baseline model and showcases ChatGPT's significant improvement over a random baseline.

- **Popularity Bias Analysis**: The authors explore strategies to counteract popularity bias in ChatGPT's recommendations and highlight the trade-off between recommendation variety and performance.

### Conclusion

The paper concludes that reprompting ChatGPT with feedback significantly improves recommendation performance. Additionally, ChatGPT outperforms baseline models and strategies to counteract popularity bias in recommendations are proposed. The authors highlight the limitations of the study and suggest future work in comparing ChatGPT's performance against other recommendation algorithms.

### Critique

The paper provides valuable insights into the use of ChatGPT for conversational recommendation. However, the study's limitations include dependency on a relatively large amount of text data and the use of an older movie dataset. Additionally, the study lacks comparable models to compare with ChatGPT in the pipeline, suggesting a need for further investigation.

Overall, the study presents valuable findings but should address the limitations and potential biases in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-12       |
| Abstract | [http://arxiv.org/abs/2401.03605v1](http://arxiv.org/abs/2401.03605v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.03605v1](https://browse.arxiv.org/html/2401.03605v1)       |
| Truncated       | False       |
| Word Count       | 10455       |