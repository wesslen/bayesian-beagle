
---
title: "RedWhale: An Adapted Korean LLM Through Efficient Continual Pretraining"
id: "2408.11294v1"
description: "RedWhale, a Korean-focused NLP model, outperforms others, reducing training time and costs while maintaining accuracy."
author: Anh-Dung Vo, Minseong Jung, Wonbeen Lee, Daewoo Choi
date: "2024-08-21"
image: "https://browse.arxiv.org/html/2408.11294v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.11294v1/x1.png)

# Summary

The research paper introduces RedWhale, a large language model (LLM) specifically designed for Korean language processing. The model addresses the challenges of Korean's non-alphabetic token structure and the computational demands of LLM training. The paper presents a four-step process for effectively adapting an English LLM to Korean, which includes enhancing Korean corpus quality, adapting an efficient Korean tokenizer, initializing model weights effectively, and implementing comprehensive multistage training.

RedWhale is developed using an efficient continual pretraining approach, which includes a comprehensive Korean corpus preprocessing pipeline, a specialized tokenizer, an optimized model initialization technique, and a multistage pretraining strategy. These innovations collectively reduce training time and computational costs while maintaining high levels of accuracy and comprehension.

Experimental results demonstrate that RedWhale outperforms other leading models on Korean NLP benchmarks, including the Korean Balanced Evaluation of Significant Tasks (KoBEST). The model showed no signs of convergence even after pretraining on 9.7 billion tokens, indicating the potential for further improvements with additional training.

# Major Findings

1. RedWhale, a model specifically tailored for Korean language processing, outperforms other leading models on Korean NLP benchmarks, including KoBEST.
2. The efficient continual pretraining approach used in RedWhale reduces training time and computational costs while maintaining high levels of accuracy and comprehension.
3. RedWhale showed no signs of convergence even after pretraining on 9.7 billion tokens, indicating the potential for further improvements with additional training.

# Analysis and Critique

The paper presents a well-structured and coherent summary of the research, highlighting the major findings and contributions of the study. The proposed method for adapting an English LLM to Korean is comprehensive and addresses the specific challenges of Korean language processing. The use of an efficient continual pretraining approach, along with a specialized tokenizer and optimized model initialization, is a significant contribution to the field.

However, the paper does not provide a detailed comparison of RedWhale with other existing models for Korean language processing. While the experimental results demonstrate the superior performance of RedWhale on KoBEST, a more comprehensive comparison with other models would provide a better understanding

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.11294v1](https://arxiv.org/abs/2408.11294v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.11294v1](https://browse.arxiv.org/html/2408.11294v1)       |
| Truncated       | False       |
| Word Count       | 13631       |