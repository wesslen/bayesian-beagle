
---
title: "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning"
id: "2402.11411v1"
description: "VLLMs merge vision and language models, but can hallucinate. POVID reduces hallucinations and improves performance."
author: Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao
date: "2024-02-18"
image: "../../img/2402.11411v1/image_1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11411v1/image_1.png)

### Summary:
- Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks.
- These models suffer from "hallucinations," where the language model generates content that is not grounded in the image.
- The proposed approach, POVID, addresses the hallucination problem as an alignment issue and tackles it with preference tuning.
- POVID generates feedback data with AI models, using ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data.
- The approach is automated, scalable, and effectively reduces hallucinations while improving model performance across standard benchmarks.

### Major Findings:
1. POVID effectively reduces hallucinations in VLLMs compared to other preference fine-tuning strategies.
2. POVID improves performance compared to other benchmarks and tasks like VQA.
3. Hallucinating textual responses and image distortion benefit performance.

### Analysis and Critique:
- The proposed approach, POVID, effectively addresses the issue of hallucinations in VLLMs and improves model performance.
- The use of AI-generated dispreferred responses and image distortion contributes to the success of POVID in reducing hallucinations and improving modality alignment.
- The results of the study demonstrate the promise of POVID in enhancing VLLM-related tasks and outperforming prior approaches.
- The approach is automated and scalable, making it a valuable contribution to the field of multimodal learning and language models.

Overall, the article provides a comprehensive and effective solution to the issue of hallucinations in VLLMs, with promising results and potential for further advancements in the field.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.11411v1](https://arxiv.org/abs/2402.11411v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11411v1](https://browse.arxiv.org/html/2402.11411v1)       |
| Truncated       | False       |
| Word Count       | 14186       |