
---
title: "Small But Funny: A Feedback-Driven Approach to Humor Distillation"
id: "2402.18113v1"
description: "LLMs help SLMs with complex tasks, but feedback improves performance more than imitation alone."
author: Sahithya Ravi, Patrick Huber, Akshat Shrivastava, Aditya Sagar, Ahmed Aly, Vered Shwartz, Arash Einolghozati
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18113v1/x1.png"
categories: ['education', 'hci', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18113v1/x1.png)

### Summary:
- The article explores the use of feedback-driven knowledge distillation to transfer complex language abilities, specifically humor generation, from Large Language Models (LLMs) to Small Language Models (SLMs).
- The study hypothesizes that creative tasks like humor generation may be hard to learn by imitation alone and investigates the use of feedback from the LLM as a "critic" to evaluate the student's performance.
- Experiments reveal that incorporating feedback significantly narrows the performance gap between SLMs and LLMs in humor generation, highlighting the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation.

### Major Findings:
1. The study explores the use of feedback-driven knowledge distillation to transfer complex language abilities, specifically humor generation, from LLMs to SLMs.
2. Incorporating feedback from the LLM as a "critic" significantly narrows the performance gap between SLMs and LLMs in humor generation.
3. The study highlights the potential of using feedback as an additional dimension to data when transferring complex language abilities via distillation.

### Analysis and Critique:
- The article provides valuable insights into the potential of feedback-driven knowledge distillation for transferring complex language abilities, such as humor generation, from LLMs to SLMs.
- The study addresses limitations of relying solely on imitation for creative tasks and proposes a novel approach involving feedback from the LLM as a "critic" to guide the student model.
- The article acknowledges potential biases and limitations in LLM-based evaluation and feedback, emphasizing the need for further research in mitigating biases and ensuring cultural sensitivity in humor generation.
- The study raises ethical considerations related to data sources, offensive content, and cultural references, highlighting the importance of responsible and inclusive language model training and evaluation.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.18113v1](https://arxiv.org/abs/2402.18113v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18113v1](https://browse.arxiv.org/html/2402.18113v1)       |
| Truncated       | False       |
| Word Count       | 7067       |