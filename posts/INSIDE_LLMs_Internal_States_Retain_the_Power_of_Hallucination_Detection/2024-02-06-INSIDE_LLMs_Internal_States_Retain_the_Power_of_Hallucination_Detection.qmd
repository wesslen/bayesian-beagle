
---
title: "INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection"
id: "2402.03744v1"
description: "LLMs' internal states used for hallucination detection with EigenScore metric. Test time feature clipping explored."
author: Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, Jieping Ye
date: "2024-02-06"
image: "../../../bayesian-beagle.png"
categories: ['architectures', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The article addresses the issue of knowledge hallucination in Large Language Models (LLMs) and proposes a method called INternal States for hallucInation DEtection (INSIDE) to detect hallucinations.
- The proposed method leverages the internal states of LLMs to measure semantic consistency/diversity in the dense embedding space and introduces an EigenScore metric to evaluate responses' self-consistency.
- The evaluation of two representative open source LLMs, LLaMA and OPT, in experiments is discussed, including the evaluation metrics used, comparison with baselines, correctness measures, implementation details, main results, ablation studies, sensitivity to correctness measures, and sensitivity to hyperparameters.
- The performance of the proposed EigenScore method for detecting hallucinations in LLMs on the TruthfulQA dataset is evaluated, demonstrating its superiority over baseline methods and competitive methods on the CoQA dataset.
- The computational cost comparison in LLaMA-7B and LLaMA-13B is presented, along with a comparison of different methods in terms of inference cost, performance evaluation with exact match as the correctness measure, and an ablation study of determining the clipping threshold with different techniques.
- The impact of feature clipping on the performance of a language generation model is evaluated, showing that after applying feature clipping, overconfident generations can be appropriately suppressed, and some self-consistent hallucinations are finally identified.

### Major Findings:
1. The proposed INSIDE method effectively detects and prevents knowledge hallucination in LLMs by leveraging internal states and introducing the EigenScore metric.
2. The EigenScore method demonstrates superior performance in detecting hallucinations across various datasets and models, with computational efficiency compared to other approaches.
3. Feature clipping shows potential in improving the accuracy and reliability of language generation models by suppressing overconfident generations and identifying self-consistent hallucinations.

### Analysis and Critique:
- The article provides valuable insights into the detection and prevention of knowledge hallucination in LLMs, offering a novel method and demonstrating its effectiveness through extensive experiments.
- However, the limitations and future work of the proposed method are acknowledged, indicating the need for further research to address these limitations and enhance the practical applicability of the method.
- The findings and methodologies presented in the article contribute to the development of more accurate and trustworthy natural language processing systems, but potential biases and methodological issues should be carefully considered in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.03744v1](https://arxiv.org/abs/2402.03744v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.03744v1](https://browse.arxiv.org/html/2402.03744v1)       |
| Truncated       | True       |
| Word Count       | 20552       |