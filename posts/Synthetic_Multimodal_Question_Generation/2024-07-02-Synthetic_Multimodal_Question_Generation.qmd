
---
title: "Synthetic Multimodal Question Generation"
id: "2407.02233v1"
description: "SMMQG generates style-specific MMRAG questions from multimodal documents, rivaling human-generated data quality."
author: Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Pakazad, Tongshuang Wu, Graham Neubig
date: "2024-07-02"
image: "https://browse.arxiv.org/html/2407.02233v1/extracted/5705436/images/intro_diagram.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.02233v1/extracted/5705436/images/intro_diagram.png)

# Synthetic Multimodal Question Generation

## Summary:

The paper introduces a method for Synthetic Multimodal Question Generation (SMMQG), a framework that leverages the interplay between a retriever, a large language model (LLM), and a large multimodal model (LMM) to generate question-answer pairs directly from multimodal documents. SMMQG enables fine-grained control over the styles and modalities of questions, and is capable of producing both unimodal and cross-modal questions. The authors use SMMQG to generate an MMRAG dataset of 1024 questions over Wikipedia documents and evaluate state-of-the-art models using it, revealing insights into model performance that are attainable only through style- and modality-specific evaluation data. A human study is conducted to measure the quality of the synthetic data, which is found to be on par with the quality of the crowdsourced benchmark MMQA.

## Major Findings:

1. SMMQG is a powerful approach to question-answering over multimodal documents, enabling fine-grained control over the styles and modalities of questions.
2. The quality of the synthetic data generated by SMMQG is on par with the quality of the crowdsourced benchmark MMQA, as demonstrated by a human study.
3. Evaluation results using the SMMQG dataset strongly concur with those obtained using MMQA, demonstrating that the synthetic dataset can be used in place of MMQA for model selection.

## Analysis and Critique:

The paper presents a novel and promising approach to generating synthetic multimodal question-answer pairs, addressing a key challenge in evaluating MMRAG systems. The use of a large language model and a large multimodal model in conjunction with a retriever allows for the generation of diverse and high-quality questions and answers. The evaluation of state-of-the-art models using the SMMQG dataset provides valuable insights into model performance, and the human study confirms the quality of the synthetic data.

However, the paper does not discuss potential limitations or biases in the SMMQG framework, nor does it address the issue of generalizability to

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02233v1](https://arxiv.org/abs/2407.02233v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02233v1](https://browse.arxiv.org/html/2407.02233v1)       |
| Truncated       | False       |
| Word Count       | 13736       |