
---
title: "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks"
id: "2401.13649v1"
description: "VisualWebArena benchmarks multimodal web agents for visually grounded tasks, addressing limitations in existing benchmarks."
author: Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried
date: "2024-01-24"
image: "../../../bayesian-beagle.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:

The article introduces VisualWebArena, a benchmark suite designed to evaluate the performance of multimodal web agents on visually grounded tasks. It comprises 910 diverse web-based tasks that require agents to process image-text inputs, interpret natural language instructions, and execute actions on websites. The tasks are split across three web environments and classified into difficulty levels. The authors conduct an extensive evaluation of state-of-the-art large language model (LLM) and vision-language model (VLM) agents, highlighting the limitations of text-only LLM agents and the potential of multimodal models. The performance of GPT-4 models is analyzed across different action and visual difficulty levels, with the GPT-4V + SoM agent showing promising results. The article also provides insights into the performance and failure modes of VLM agents in various scenarios.

### Major Findings:
1. VisualWebArena provides a comprehensive benchmark for assessing the visual and reasoning skills of autonomous agents in web-based environments.
2. The GPT-4V + SoM agent shows improved performance on hard visual tasks, indicating the potential of multimodal models in simplifying action spaces and improving performance on visually complex websites.
3. The performance of VLM agents tends to improve with a greater number of in-context examples, suggesting the potential for enhancing the capabilities of these agents through additional training data.

### Analysis and Critique:
The article presents a valuable benchmark suite and provides insights into the performance of state-of-the-art language models on visually grounded tasks. However, the study could benefit from further exploration of the limitations and potential biases in the evaluation process. Additionally, the article could address the need for further research on enhancing the reasoning, visual understanding, and planning abilities of autonomous agents in web environments. Overall, the article offers significant contributions to the field of autonomous agents and sets the stage for future advancements in multimodal language models and their applications in web navigation tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-31       |
| Abstract | [https://arxiv.org/abs/2401.13649v1](https://arxiv.org/abs/2401.13649v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.13649v1](https://browse.arxiv.org/html/2401.13649v1)       |
| Truncated       | True       |
| Word Count       | 20061       |