
---
title: "A Transcription Prompt-based Efficient Audio Large Language Model for Robust Speech Recognition"
id: "2408.09491v1"
description: "New method reduces errors, eliminates repetition in audio-LLM speech recognition, improving performance in noisy environments."
author: Yangze Li, Xiong Wang, Songjun Cao, Yike Zhang, Long Ma, Lei Xie
date: "2024-08-18"
image: "https://browse.arxiv.org/html/2408.09491v1/x1.png"
categories: ['social-sciences', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.09491v1/x1.png)

### Summary:

The paper proposes a transcription prompt-based audio-LLM to address the issues of substitution errors and decoding repetition in speech recognition tasks. The approach introduces an ASR expert as a transcription tokenizer and a hybrid AR NAR decoding method. Experiments on the 10k-hour WenetSpeech Mandarin corpus show that the proposed method decreases CER by 12.2% and 9.6% on Test_Net and Test_Meeting evaluation sets, respectively, compared to the baseline. Notably, the decoding repetition rate is reduced to zero, indicating that the repetition problem has been fundamentally solved.

### Major Findings:

1. The proposed transcription prompt-based audio-LLM effectively addresses substitution errors and decoding repetition in speech recognition tasks.
2. The hybrid AR NAR decoding approach fundamentally solves the decoding repetition problem and achieves a lower ASR decoding RTF.
3. The proposed method significantly improves speech recognition performance, with a 12.2% and 9.6% relative decrease in CER on Test_Net and Test_Meeting evaluation sets, respectively.

### Analysis and Critique:

The paper presents a promising approach to improving speech recognition performance in noisy environments. The use of an ASR expert as a transcription tokenizer and a hybrid AR NAR decoding method effectively addresses the issues of substitution errors and decoding repetition. However, the paper does not discuss the potential limitations or biases of the proposed method. Additionally, the method's performance on other languages or datasets is not evaluated, which could limit its generalizability. Further research is needed to evaluate the proposed method's performance on other languages and datasets and to address any potential limitations or biases.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.09491v1](https://arxiv.org/abs/2408.09491v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.09491v1](https://browse.arxiv.org/html/2408.09491v1)       |
| Truncated       | False       |
| Word Count       | 3902       |