{"id": "2312.12321v1", "text": "# Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n## Abstract\n- Investigates the fragility of state-of-the-art (SOTA) open-source LLMs under simple optimization-free attacks called **priming attacks**. \n- These attacks bypass alignment from safety training and can improve the attack success rate on harmful behaviors by up to 3.3\u00d7.\n\n## 1 Introduction\n- Discusses the need for safety training for LLMs and previous research on circumventing safety alignment in open-source LLMs.\n- Introduces the concept of allowing unrestricted inputs for open-source LLMs. \n\n## 2 Methodology & Results\n- Describes the methodology for few-shot priming attacks involving prompting a non-safety-trained helper LLM and evaluates the attack success rate (ASR) on harmful behaviors.\n- Presents results showing that priming attacks outperform baselines for all models.\n\n## 3 Conclusion\n- Summarizes the findings and highlights the fragility of current LLM safety measures under practical assumptions, raising concerns for the future of open-sourcing LLMs.\n\n## Appendices\n- Details on the experimental setup, few-shot prompt format, Llama Guard task instructions, manual evaluation benchmark, manual evaluation vs. Llama Guard, and runtime comparison.", "meta": {"url": "https://browse.arxiv.org/html/2312.12321v1", "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks", "subtitle": "LLMs are vulnerable to priming attacks that bypass safety training, increasing attack success rates on harmful behaviors.", "categories": ["security", "open-source"], "publish_date": "2023-12-19", "model": "gpt-3.5-turbo-1106", "image": null, "word_count": 4388, "is_truncated": false}}
