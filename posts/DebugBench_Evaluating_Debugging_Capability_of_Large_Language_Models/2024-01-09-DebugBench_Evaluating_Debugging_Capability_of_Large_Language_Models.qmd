
---
title: "DebugBench: Evaluating Debugging Capability of Large Language Models"
id: "2401.04621v1"
description: "LLMs' debugging capability evaluated with 'DebugBench' benchmark, showing mixed performance and bug category complexity."
author: Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, Maosong Sun
date: "2024-01-09"
image: "../../../bayesian-beagle.png"
categories: ['architectures', 'robustness', 'programming', 'production']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:

The article introduces DebugBench, a new benchmark for evaluating the debugging capability of Large Language Models (LLMs). It evaluates closed-source and open-source models across 18 types of programming errors in three different scenarios, highlighting the challenges and differences in performance. The section also provides prompts used in bug implantation and model evaluation, as well as examples of buggy code and their respective bug explanations, along with the debugging process and test results.

### Major Findings:
1. The closed-source models exhibit lower debugging performance compared to humans, but demonstrate significant time efficiency.
2. Open-source models struggle to produce effective debugging responses, particularly for logic bugs.
3. The challenge of debugging varies with the type of bug, with syntax and reference errors being comparatively simpler to spot and rectify.

### Analysis and Critique:
The introduction of DebugBench addresses the limitations of previous evaluations of LLMs' debugging ability and provides a comprehensive benchmark for assessing LLMs' performance in debugging various types of bugs. The findings have implications for the development and improvement of LLMs for debugging purposes, as well as for understanding the interplay between debugging and coding tasks. The prompts used in bug implantation and model evaluation play a crucial role in directing the models to perform specific tasks related to bug identification and code correction. The examples of buggy code and the subsequent debugging process emphasize the significance of attention to detail in coding and the potential impact of even small errors on the functionality of the program. However, the article could benefit from further discussion on the limitations and potential biases in the evaluation process, as well as areas that require further research, such as the correlation between debugging and coding tasks in closed-source models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2401.04621v1](https://arxiv.org/abs/2401.04621v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.04621v1](https://browse.arxiv.org/html/2401.04621v1)       |
| Truncated       | True       |
| Word Count       | 20557       |