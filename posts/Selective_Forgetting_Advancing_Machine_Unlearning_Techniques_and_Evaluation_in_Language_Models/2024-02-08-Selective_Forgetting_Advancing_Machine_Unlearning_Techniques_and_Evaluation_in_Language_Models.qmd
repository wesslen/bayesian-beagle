
---
title: "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models"
id: "2402.05813v1"
description: "Study investigates Machine Unlearning (MU) for selective forgetting in language models, proposes evaluation metrics and annotation method."
author: Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, Georg Gottlob
date: "2024-02-08"
image: "../../img/2402.05813v1/image_1.png"
categories: ['social-sciences', 'architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.05813v1/image_1.png)

The paper presents a novel selective unlearning method for language models, called SeUL, which minimizes negative impacts on model capabilities by focusing on specific sequence spans. It introduces specialized evaluation metrics, S-EL and S-MA, designed to assess the forgetting of sensitive information. The paper also proposes efficient automatic online and offline sensitive span annotation methods to support the overall unlearning framework.

The main results reported in the paper are average scores across multiple classification and dialogue systems. The results demonstrate that SeUL generally exhibits superior effectiveness in unlearning sensitive information compared to the baseline method, Kul. SeUL demonstrates comparable results on classification datasets but significantly better performance on dialogue datasets. Additionally, SeUL's training and evaluation rely on automatic online or offline annotated sensitive spans, and the paper provides a detailed analysis of the reliability of the annotation methods.

The paper also discusses the limitations of the study, including the sparse sensitive information in the datasets and the potential for inaccuracies in the automatic annotation methods.

In addition, the paper provides further experimental results, annotation details, and additional analysis to support the main findings.

Overall, the paper presents a comprehensive and well-structured approach to selective unlearning in language models, with a critical analysis of the methods and their limitations. The proposed evaluation metrics and annotation methods provide a strong foundation for future research in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.05813v1](https://arxiv.org/abs/2402.05813v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.05813v1](https://browse.arxiv.org/html/2402.05813v1)       |
| Truncated       | False       |
| Word Count       | 15052       |