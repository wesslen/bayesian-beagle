
---
title: "GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models"
id: "2407.02936v1"
description: "GraCoRe benchmark evaluates LLMs' graph comprehension and reasoning, revealing insights on semantic enrichment, node ordering, and text length impact."
author: Zike Yuan, Ming Liu, Hui Wang, Bing Qin
date: "2024-07-03"
image: "https://browse.arxiv.org/html/2407.02936v1/x1.png"
categories: ['hci', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.02936v1/x1.png)

### Summary:

The paper presents GraCoRe, a benchmark for systematically assessing the graph comprehension and reasoning abilities of Large Language Models (LLMs). The benchmark uses a three-tier hierarchical taxonomy to categorize and test models on pure graph and heterogeneous graphs, subdividing capabilities into 10 distinct areas tested through 19 tasks. The benchmark includes 11 datasets with 5,140 graphs of varying complexity. The authors evaluated three closed-source and seven open-source LLMs, conducting thorough analyses from both ability and task perspectives. Key findings reveal that semantic enrichment enhances reasoning performance, node ordering impacts task success, and the ability to process longer texts does not necessarily improve graph comprehension or reasoning.

### Major Findings:

1. Semantic enrichment enhances reasoning performance: LLMs perform better on graph reasoning tasks enriched with semantic information compared to tasks involving purely structural graph reasoning, indicating that textual information can enhance the graph reasoning abilities of LLMs.
2. Node ordering impacts task success: Models exhibit high sensitivity to the ordering of nodes in textual graph data. An ordered naming of nodes can significantly improve model performance on graph tasks.
3. Ability to process longer texts does not improve graph comprehension or reasoning: The capability of models to handle longer text inputs does not affect their performance in graph understanding and reasoning tasks, regardless of whether the graphs are complex with long textual descriptions or simple with short descriptions.

### Analysis and Critique:

While the paper provides a comprehensive benchmark for evaluating LLMs on graph comprehension and reasoning tasks, there are some potential limitations and areas for improvement.

1. Limited generalization: The benchmark predominantly tests either pure graphs or heterogeneous graphs in isolation, failing to provide a unified and systematic evaluation across both graph structures.
2. Lack of clear definition regarding model capabilities: Traditional benchmarks for LLMs on graphs are primarily task-driven, inadequately assessing the specific abilities of LLMs on graph data.
3. Insufficient variety in model types and task categories: Current benchmarks neither offer a clear classification of task types nor test a wide range of models.

To address these challenges, future work should focus on developing better benchmarks that evaluate LLMs' capabilities more comprehens

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02936v1](https://arxiv.org/abs/2407.02936v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02936v1](https://browse.arxiv.org/html/2407.02936v1)       |
| Truncated       | False       |
| Word Count       | 5687       |