
---
title: "Eliciting the Priors of Large Language Models using Iterated In-Context Learning"
id: "2406.01860v1"
description: "LLMs' knowledge can be captured as Bayesian prior distributions, which align with human priors. This method was used to predict speculative events like superhuman AI development."
author: Jian-Qiao Zhu, Thomas L. Griffiths
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.01860v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.01860v1/x1.png)

### Summary:

The paper presents a prompt-based workflow for eliciting prior distributions from Large Language Models (LLMs) using iterated learning, a Markov chain Monte Carlo method. The authors validate their method in settings where iterated learning has previously been used to estimate the priors of human participants, such as causal learning, proportion estimation, and predicting everyday quantities. The results show that priors elicited from GPT-4 qualitatively align with human priors in these settings. The same method is then used to elicit priors from GPT-4 for speculative events, such as the timing of the development of superhuman AI.

### Major Findings:

1. The paper introduces a novel method for eliciting prior distributions from LLMs using iterated learning, a Markov chain Monte Carlo method.
2. The method is validated in settings where iterated learning has previously been used to estimate the priors of human participants, such as causal learning, proportion estimation, and predicting everyday quantities.
3. The results show that priors elicited from GPT-4 qualitatively align with human priors in these settings.
4. The same method is used to elicit priors from GPT-4 for speculative events, such as the timing of the development of superhuman AI.

### Analysis and Critique:

1. The paper provides a unique approach to understanding the decision-making processes of LLMs by eliciting their prior distributions.
2. The validation of the method in settings where iterated learning has been used to estimate human priors is a significant contribution, as it demonstrates the potential of LLMs to capture human-like decision-making processes.
3. The application of the method to elicit priors for speculative events is an interesting extension, as it allows for the exploration of LLMs' capabilities in predicting future events.
4. However, the paper does not discuss the limitations of the method or potential sources of error in the elicitation process.
5. Additionally, the paper does not provide a comparison of the results obtained using the proposed method with those obtained using other methods for eliciting priors from LLMs.
6. The paper also does not discuss the potential implications of the findings for the development and deployment of LLMs in real-world applications.


## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.01860v1](https://arxiv.org/abs/2406.01860v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.01860v1](https://browse.arxiv.org/html/2406.01860v1)       |
| Truncated       | False       |
| Word Count       | 9222       |