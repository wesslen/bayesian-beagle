
---
title: "LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages"
id: "2407.05975v1"
description: "LLMs struggle with low-resource languages. LLaMAX, a multilingual LLM, outperforms existing models in translation tasks across 100+ languages."
author: Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, Fei Yuan
date: "2024-07-08"
image: "https://browse.arxiv.org/html/2407.05975v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.05975v1/x1.png)

### Summary:

The paper presents LLaMAX, a series of open-sourced models that enhance the translation performance of the LLaMA series models across more than 100 languages. The authors conduct a comprehensive analysis of key techniques in multilingual continual pre-training, including vocabulary extension and data augmentation. The LLaMAX2 model, trained over 60 days using 24 A100 GPUs, significantly enhances translation capabilities and achieves comparable performance to the specialized translation model M2M-100-12B on the Flores-101 benchmark. The paper also provides extensive experiments on key technique design, comprehensive translation benchmark evaluation across various models, general task testing, and supervised fine-tuning on task-specific data, demonstrating the superiority of LLaMAX.

### Major Findings:

1. The LLaMAX series models enhance the translation performance of the LLaMA series models across more than 100 languages.
2. The LLaMAX2 model, trained over 60 days using 24 A100 GPUs, significantly enhances translation capabilities and achieves comparable performance to the specialized translation model M2M-100-12B on the Flores-101 benchmark.
3. The LLaMAX2 model demonstrates an average improvement of more than 10 spBLEU compared to baseline models in low-resource-centric translation.
4. The LLaMAX2 model shows significant performance enhancements even for languages not included in the training set when evaluated on Flores-200.
5. Enhancing translation capabilities also establishes a robust multilingual base model foundation, with an average improvement of 5 points over LLaMA2 on X-CSQA, XNLI, and MGSM tasks.

### Analysis and Critique:

The paper presents a significant contribution to the field of multilingual translation by introducing the LLaMAX series models, which enhance the translation performance of the LLaMA series models across more than 100 languages. The authors provide a comprehensive analysis of key techniques in multilingual continual pre-training, including vocabulary extension and data augmentation. The LLaMAX2 model, trained over 60 days using 24 A

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.05975v1](https://arxiv.org/abs/2407.05975v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.05975v1](https://browse.arxiv.org/html/2407.05975v1)       |
| Truncated       | False       |
| Word Count       | 10244       |