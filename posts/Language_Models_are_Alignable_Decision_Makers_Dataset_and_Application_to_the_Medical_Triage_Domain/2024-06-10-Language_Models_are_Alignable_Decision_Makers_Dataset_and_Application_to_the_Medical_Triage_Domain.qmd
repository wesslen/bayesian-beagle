
---
title: "Language Models are Alignable Decision-Makers: Dataset and Application to the Medical Triage Domain"
id: "2406.06435v1"
description: "New dataset for medical triage decision-making; LLMs used as ethical decision-makers, alignable to different attributes."
author: Brian Hu, Bill Ray, Alice Leung, Amy Summerville, David Joy, Christopher Funk, Arslan Basharat
date: "2024-06-10"
image: "https://browse.arxiv.org/html/2406.06435v1/x1.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.06435v1/x1.png)

### Summary:

The paper introduces a novel medical triage decision-making dataset, labeled with a set of decision-maker attributes (DMAs), to quantify model alignment using a new attribute-dependent accuracy metric. The authors propose a zero-shot prompting approach to align large language models (LLMs) to a set of DMAs, demonstrated through detailed analysis across different attributes and model types, sizes, and training techniques. They also extend a self-consistency module using weighted positive and negative samples, which improves model alignment. The paper presents a new, extensible, and versatile open-source software framework to enable research on human-aligned decision-making with LLMs.

### Major Findings:

1. The paper introduces a novel medical triage decision-making dataset, containing different scenarios labeled with DMAs, which allows for the quantification of model alignment using a new attribute-dependent accuracy metric.
2. The authors propose a new zero-shot prompting approach to align LLM decisions to a set of DMAs, demonstrated through detailed analysis across different attributes and model types, sizes, and training techniques.
3. The paper extends a self-consistency module using weighted positive and negative samples, which improves model alignment.
4. The authors present a new, extensible, and versatile open-source software framework to enable research on human-aligned decision-making with LLMs.

### Analysis and Critique:

1. The paper provides a valuable contribution to the field of human-aligned decision-making with LLMs, offering a novel dataset and a new approach to align LLMs to DMAs.
2. The proposed zero-shot prompting approach and the extension of the self-consistency module are promising techniques to improve model alignment.
3. The open-source software framework is a valuable resource for researchers in the field, enabling further exploration and development of human-aligned decision-making with LLMs.
4. However, the paper does not discuss potential limitations, unanswered questions, or conflicting evidence, which could be addressed in future work.
5. The paper also does not provide a comprehensive comparison with existing methods, which could help to better understand the advantages and disadvantages of the proposed approach.
6. The paper could benefit from a more detailed discussion of the potential applications and implications of the proposed methods,

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.06435v1](https://arxiv.org/abs/2406.06435v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06435v1](https://browse.arxiv.org/html/2406.06435v1)       |
| Truncated       | False       |
| Word Count       | 9086       |