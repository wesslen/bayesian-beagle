
---
title: "Beyond Generative Artificial Intelligence: Roadmap for Natural Language Generation"
id: "2407.10554v1"
description: "TL;DR: This paper reviews recent NLG surveys to identify gaps in LLMs and suggest future research directions."
author: María Miró Maestre, Iván Martínez-Murillo, Tania J. Martin, Borja Navarro-Colorado, Antonio Ferrández, Armando Suárez Cueto, Elena Lloret
date: "2024-07-15"
image: "https://browse.arxiv.org/html/2407.10554v1/extracted/5731695/denada.png"
categories: ['social-sciences', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.10554v1/extracted/5731695/denada.png)

**Summary:**

This paper conducts a review of a representative sample of surveys recently published in Natural Language Generation (NLG) to provide a research roadmap for the scientific community. The goal is to identify which NLG aspects are not suitably addressed by Large Language Models (LLMs) and suggest future lines of research. The paper discusses the evolution of NLG, from modular architectures to global approaches, and the current focus on developing larger LLMs. However, these models still lack precision and have problems generating texts faithfully like humans. The paper also highlights the need for further contextual knowledge and information modalities to improve LLM performance in more demanding tasks.

**Major Findings:**

1. Multimodality: LLMs need to improve their performance in handling different input formats, such as text, data, images, audio, and video. Current models tend to prioritize information from one modality over another, leading to an imbalance in knowledge acquisition.
2. Multilinguality: The predominance of English in NLG tasks poses a risk of missing semantic properties inherent to other languages. There is a need for extended approaches with each language as the central element of the architecture and for more original datasets in high and low-resourced languages.
3. Knowledge Integration and Controllable NLG: Including additional knowledge in neural models can enhance their performance. However, there are still gaps in effectively integrating knowledge and controlling the final attributes of a text.
4. Hallucination: State-of-the-art NLG tools suffer from hallucination, where generated text seems fluent and natural but is untrustworthy or illogical. This issue needs to be addressed to ensure the reliability of LLMs.

**Analysis and Critique:**

The paper provides a comprehensive review of recent surveys in NLG and identifies key research gaps that need to be addressed. However, the paper does not discuss the methodology used to select the surveys or the criteria for inclusion. Additionally, the paper does not provide a detailed analysis of the limitations of the reviewed surveys or the potential biases in their findings. The paper also does not discuss the potential impact of the identified research gaps on the development of NLG systems or the implications for the broader field of AI. Overall, the paper provides a valuable contribution to the field of NLG by highlight

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.10554v1](https://arxiv.org/abs/2407.10554v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.10554v1](https://browse.arxiv.org/html/2407.10554v1)       |
| Truncated       | False       |
| Word Count       | 8979       |