
---
title: "Chain-of-Thought Prompting of Large Language Models for Discovering and Fixing Software Vulnerabilities"
id: "2402.17230v1"
description: "Large language models significantly improve software vulnerability analysis tasks using chain-of-thought prompting."
author: Yu Nong, Mohammed Aldeen, Long Cheng, Hongxin Hu, Feng Chen, Haipeng Cai
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17230v1/x1.png"
categories: ['robustness', 'education', 'security', 'hci', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17230v1/x1.png)

### Summary

- Software vulnerabilities are a significant issue, leading to financial losses, service disruptions, and data breaches.
- Various approaches to defensive software vulnerability analysis have been proposed, including static/dynamic program analysis, hybrid techniques, and data-driven methods like machine/deep learning.
- Deep learning-based techniques face challenges such as the lack of sizable, quality-labeled task-specific datasets and poor generalization to real-world scenarios.
- Large language models (LLMs) have shown potential in various domains, especially through chain-of-thought (CoT) prompting.
- This paper explores using LLMs and CoT for three key software vulnerability analysis tasks: identifying specific vulnerabilities, discovering vulnerabilities of any type, and patching detected vulnerabilities.
- The authors introduce VSP, a unified, vulnerability-semantics-guided prompting approach, and compare it to five baselines for the three tasks across three LLMs and two datasets.
- Results show significant superiority of the CoT-inspired prompting (553.3%, 36.5%, and 30.8% higher F1 accuracy for vulnerability identification, discovery, and patching, respectively, on CVE datasets).
- Through in-depth case studies, the authors reveal current gaps in LLM/CoT for challenging vulnerability cases and propose improvements.

### Major Findings

1. Large language models (LLMs) with chain-of-thought (CoT) prompting can significantly improve software vulnerability analysis tasks, including vulnerability identification, discovery, and patching.
2. The authors' proposed VSP approach, a unified, vulnerability-semantics-guided prompting method, outperforms five baselines in these tasks across three LLMs and two datasets.
3. In-depth case studies reveal current gaps in LLM/CoT for challenging vulnerability cases and propose improvements for future research.

### Analysis and Critique

- The paper focuses on using LLMs and CoT for software vulnerability analysis tasks, which is a promising direction. However, the authors could further discuss potential limitations and challenges, such as the need for high-quality labeled datasets and the potential for overfitting or biases in LLM-generated outputs.


## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.17230v1](https://arxiv.org/abs/2402.17230v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17230v1](https://browse.arxiv.org/html/2402.17230v1)       |
| Truncated       | False       |
| Word Count       | 14354       |