
---
title: "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions"
id: "2402.18060v2"
description: "New medical datasets for evaluating LLM performance on complex, explainable clinical QA. Experiments show tougher than previous benchmarks."
author: Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.18060v2/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18060v2/x1.png)

### Summary:

- Large language models (LLMs) have shown impressive performance in answering medical questions, but they struggle with complex, real-world clinical cases.
- To address this issue, two new datasets, JAMA Clinical Challenge and Medbullets, are constructed for multiple-choice question-answering tasks with expert-written explanations.
- Four LLMs are evaluated on these datasets, and the results demonstrate that the new tasks are more challenging, with inconsistencies between automatic and human evaluations of model-generated explanations.

### Major Findings:

1. The two new datasets, JAMA Clinical Challenge and Medbullets, are designed to evaluate LLMs on more realistic and challenging clinical cases, with high-quality explanations provided by human experts.
2. Four LLMs are evaluated on these datasets, and the results show that the new tasks are more difficult, with lower scores compared to previous benchmarks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.18060v2](https://arxiv.org/abs/2402.18060v2)        |
| HTML     | [https://browse.arxiv.org/html/2402.18060v2](https://browse.arxiv.org/html/2402.18060v2)       |
| Truncated       | False       |
| Word Count       | 7933       |