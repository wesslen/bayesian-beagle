
---
title: "On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments"
id: "2407.08067v1"
description: "TL;DR: This study explores using large language models as Wizards in WoZ experiments, providing methodology and evaluation for their role-playing ability."
author: Jingchao Fang, Nikos Arechiga, Keiichi Namaoshi, Nayeli Bravo, Candice Hogan, David A. Shamma
date: "2024-07-10"
image: "https://browse.arxiv.org/html/2407.08067v1/extracted/5723601/Figures/Overview.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.08067v1/extracted/5723601/Figures/Overview.png)

### Summary:

The paper "On LLM Wizards: Identifying Large Language Modelsâ€™ Behaviors for Wizard of Oz Experiments" (2024) explores the potential of using large language models (LLMs) as Wizards in Wizard of Oz (WoZ) experiments. The authors propose an experiment lifecycle that allows researchers to safely integrate LLMs into WoZ experiments and interpret data generated from settings involving Wizards role-played by LLMs. The paper also introduces a heuristic-based evaluation framework to estimate LLMs' role-playing ability in WoZ experiments and reveal their behavior patterns at scale.

### Major Findings:

1. The proposed experiment lifecycle involves two stages: a coarse, cheap, and large-scale WoLs-to-Simulacrums setting (Stage 1) and a smaller-scale, human-facing experiment (Stage 2) conducted after experimenter intervention guided by the outcome of Stage 1.
2. The experiment lifecycle starts with a fully automated Stage 1, which allows the fast generation of synthetic, scenario-specific conversational data and observation of LLMs' behaviors in WoZ studies without risking human participants.
3. The heuristic evaluation framework comprises automatic metrics that can detect and quantify pitfalls in LLM-generated synthetic conversational data, complemented by human evaluation to further reveal LLMs' behavioral patterns in WoZ experiments.
4. The paper contributes a list of identified failure modes of LLMs in WoZ experiments with evidence from formal quantitative and qualitative evaluations.

### Analysis and Critique:

The paper presents a promising approach to integrating LLMs into WoZ experiments, offering a systematic evaluation of LLMs' role-playing ability and revealing their behavior patterns at scale. However, several potential limitations and areas for improvement should be considered:

1. The proposed experiment lifecycle and evaluation framework may not be applicable to all types of LLMs or conversation topics, as the paper only includes three conversation topics and uses GPT-4 as the LLM.
2. The heuristic evaluation framework may not capture all potential failure modes of LLMs, as it is based on a set of predefined metrics and may not account for unforeseen issues.
3. The paper does not discuss

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.08067v1](https://arxiv.org/abs/2407.08067v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.08067v1](https://browse.arxiv.org/html/2407.08067v1)       |
| Truncated       | False       |
| Word Count       | 13493       |