
---
title: "ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios"
id: "2401.00741v1"
description: "ToolEyes assesses large language model tool learning in authentic scenarios, uncovering limitations and guiding future research.

"
author: Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang
date: "2024-01-01"
image: "https://browse.arxiv.org/html/2401.00741v1/x2.png"
categories: ['robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.00741v1/x2.png)

# ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios

## Key Findings

- **ToolEyes** offers a fine-grained evaluation system for Large Language Models' (LLMs) tool learning capabilities, examining seven real-world scenarios and approximately 600 tools.
- The evaluation reveals that LLMs exhibit preference for specific scenarios and restricted cognitive abilities in tool learning, with larger model size exacerbating the hindrance to tool learning.
- The findings suggest the need for improvement in tool learning capabilities across all categories of LLMs.

## Evaluation System

### Scenario Construction
- ToolEyes formulates seven real-world scenarios, including **Text Generation, Data Understanding, Real-Time Search, Application Manipulation, Personal Life, Information Retrieval**, and **Financial Transactions**.
- Each scenario is equipped with a related set of tools, totaling 41 categories, 95 subcategories, and 568 tools.

### Tool Library Building
- The system establishes a tool library, serving as an interface for LLMs to interact with the environment.

### Human-Driven Data Generation
- Professionals were engaged to identify actual requirements by reviewing the tool documentation to ensure comprehensive coverage of different scenarios.

### LLMs Capability Evaluation
- ToolEyes evaluates LLMs across five essential capabilities: **format alignment, intent comprehension, behavior planning, tool selection**, and **answer organization**.

## Experiments

### Model Selection
- Experiments were conducted on ten LLMs from three sources: open-source, tool-oriented, and closed-source categories, including **LLaMA-2-chat, Vicuna-1.5, Text-davinci-003, GPT-3.5-turbo**, and **GPT-4.**

### Experimental Setup
- LLMs were assessed using a five-shot format for open-source models and zero-shot format for others, with specific prompt templates used during inference.

### Results in Different Scenarios
- LLMs exhibit scenario-specific preferences in tool learning, influenced by their optimization goals and training data.

### Results of Different LLMs Capabilities
- The present constraints in LLMs thinking skills present a substantial obstacle to tool learning, and LLMs with superior performance exhibit more effective problem-solving abilities.

### Why does NOT LLMs Capabilities Increase with Size?
- The study found that as the model size increases, there is a potential weakening of the instrumental learning capabilities within specific LLM families.

## Insights for Advancing Tool Learning
- Ideas for advancing tool learning include task construction considering model behavior, scenario generalization using diverse data, and capability enhancement addressing the "barrel effect."

## Related Works
- The paper discusses tool learning and evaluations for tool learning, highlighting the challenges in current tool learning research.

## Conclusion
- ToolEyes offers instructive insights to inform the development of tool learning and presents avenues for future research.

## Limitations
- The paper acknowledges limitations, including the absence of a novel LLM dedicated to tool learning and the associated costs of scoring using specific LLMs.

# References
- Key references include Tang et al. (2023), Wei et al. (2022b), Chen et al. (2023b), and Schick et al. (2023).

---

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-27       |
| Abstract | [http://arxiv.org/abs/2401.00741v1](http://arxiv.org/abs/2401.00741v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.00741v1](https://browse.arxiv.org/html/2401.00741v1)       |
| Truncated       | False       |
| Word Count       | 11381       |