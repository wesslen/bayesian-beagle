
---
title: "HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation"
id: "2402.09390v1"
description: "HGOT improves retrieval in LLMs, enhancing factuality by 7%."
author: Yihao Fang, Stephen W. Thomas, Xiaodan Zhu
date: "2024-02-14"
image: "../../img/2402.09390v1/image_1.png"
categories: ['robustness', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.09390v1/image_1.png)

### Summary:
- The Hierarchical Graph of Thoughts (HGOT) framework addresses factuality and hallucinations in large language models (LLMs) by utilizing a multi-layered graph approach for retrieval-augmented in-context learning. It refines self-consistency majority voting for answer selection and proposes a scoring mechanism for evaluating retrieved passages. Experimental results demonstrate the superior performance of HGOT compared to other methods.

### Major Findings:
1. The HGOT framework significantly enhances fact retrieval and reasoning in LLMs.
2. The HGOT algorithm effectively predicts answers and assesses thought and retrieval quality.
3. The Retrieve-then-Read model demonstrates competitive performance in factual information extraction and multi-hop reasoning tasks.

### Analysis and Critique:
- The HGOT framework shows promise in improving the reliability and trustworthiness of LLMs in high-stakes applications.
- The detailed procedures and metrics used in the HGOT algorithm highlight its effectiveness and versatility, with potential for optimization through hyperparameter searches.
- The competitive performance of the Retrieve-then-Read model in different datasets underscores its strengths and potential for question-answering tasks. The ablation study provides valuable insights for further optimization.
- The examples and examination of the FEVER, Open-SQuAD, and HotPotQA datasets illustrate the varying challenges and requirements of question-answering systems across different contexts.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.09390v1](https://arxiv.org/abs/2402.09390v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09390v1](https://browse.arxiv.org/html/2402.09390v1)       |
| Truncated       | True       |
| Word Count       | 22156       |