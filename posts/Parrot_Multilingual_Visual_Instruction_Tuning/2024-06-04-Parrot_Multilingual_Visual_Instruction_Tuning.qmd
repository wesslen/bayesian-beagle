
---
title: "Parrot: Multilingual Visual Instruction Tuning"
id: "2406.02539v1"
description: "Parrot method improves multilingual abilities in MLLMs like GPT-4V, outperforming on multimodal tasks with a new benchmark, MMMB."
author: Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02539v1/x2.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02539v1/x2.png)

### Summary:

The paper introduces Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level. Parrot aims to enhance the multilingual capabilities of Multimodal Large Language Models (MLLMs) by converting visual tokens into language-specific embeddings using a Mixture-of-Experts (MoE) module. The authors also present a new benchmark, the Massive Multilingual Multimodal Benchmark (MMMB), which includes 6 languages, 15 categories, and 12,000 questions. Parrot demonstrates state-of-the-art performance on multilingual MMBench and MMMB, as well as across a broad range of multimodal tasks.

### Major Findings:

1. The imbalanced Supervised Fine-Tuning (SFT) datasets, primarily composed of English-centric image-text pairs, lead to significantly reduced performance in non-English languages.
2. Parrot, a novel method that utilizes textual guidance to drive visual token alignment at the language level, enhances the multilingual capabilities of MLLMs.
3. The Massive Multilingual Multimodal Benchmark (MMMB) is introduced to address the lack of benchmarks for evaluating multilingual capabilities within the field.

### Analysis and Critique:

1. The paper effectively addresses the issue of multilingual erosion in MLLMs, which is a significant problem in the field.
2. The introduction of Parrot and the MMMB benchmark provides a valuable contribution to the research community, as it enables more accurate evaluation and comparison of multilingual MLLMs.
3. The paper could benefit from a more detailed analysis of the limitations and potential biases of the proposed method, as well as a discussion of the ethical implications of using MLLMs for multilingual tasks.
4. The paper could also provide more information on the potential applications and use cases of Parrot and the MMMB benchmark in real-world scenarios.
5. The paper could benefit from a more comprehensive evaluation of Parrot's performance across a wider range of languages and tasks, as well as a comparison with other state-of-the-art methods in the field.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02539v1](https://arxiv.org/abs/2406.02539v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02539v1](https://browse.arxiv.org/html/2406.02539v1)       |
| Truncated       | False       |
| Word Count       | 8233       |