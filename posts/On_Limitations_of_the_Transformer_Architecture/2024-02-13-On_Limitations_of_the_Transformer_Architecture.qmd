
---
title: "On Limitations of the Transformer Architecture"
id: "2402.08164v1"
description: "LLMs struggle with composing functions due to domain size, impacting mathematical tasks."
author: Binghui Peng, Srini Narayanan, Christos Papadimitriou
date: "2024-02-13"
image: "../../../bayesian-beagle.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
- The article discusses the limitations of the Transformer architecture in large language models (LLMs) and its inability to perform function composition tasks.
- It uses Communication Complexity to prove that Transformers are incapable of composing functions, even for small domains, and points out that mathematical tasks at the core of compositional tasks are unlikely to be solvable by Transformers.
- The article also discusses the shortcomings of Transformers in logical reasoning and compositional tasks, providing evidence from Computational Complexity.

### Major Findings:
1. The Transformer architecture is incapable of reliably computing function composition, even for small domains.
2. The article provides evidence from Computational Complexity to support the limitations of Transformers in logical reasoning and compositional tasks.
3. It highlights the challenges faced by Transformers in solving compositional tasks, such as multiplication of multi-digit integers and solving logical puzzles.

### Analysis and Critique:
- The article provides valuable insights into the limitations of the Transformer architecture in performing compositional tasks.
- However, the complexity arguments come with certain caveats, such as being asymptotic and relying on unproven conjectures in Computational Complexity.
- The article also raises the challenge of designing an attention layer that is immune to lower bound techniques while maintaining efficiency and effectiveness in practice.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-14       |
| Abstract | [https://arxiv.org/abs/2402.08164v1](https://arxiv.org/abs/2402.08164v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08164v1](https://browse.arxiv.org/html/2402.08164v1)       |
| Truncated       | False       |
| Word Count       | 7408       |