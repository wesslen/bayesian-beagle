
---
title: "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"
id: "2401.16788v1"
description: "Developing reliable evaluation methods for Large Language Models (LLMs) is challenging. ScaleEval framework assists in meta-evaluation."
author: Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu
date: "2024-01-30"
image: "https://browse.arxiv.org/html/2401.16788v1/x1.png"
categories: ['hci', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.16788v1/x1.png)

### **Summary:**
The article introduces ScaleEval, a scalable meta-evaluation framework for assessing the reliability and robustness of Large Language Models (LLMs) as evaluators. It addresses the challenges of evaluating LLMs across diverse tasks and scenarios, particularly in new, user-defined scenarios. The framework leverages multiple communicative LLM agents to assist human annotators in discerning the most capable LLMs as evaluators, significantly easing their workload. The article also discusses related work, the methodology of the framework, examined scenarios, and the results of three experiments.

### **Major Findings:**
1. **Challenges in LLM Evaluation:** Evaluating LLMs as evaluators across varied contexts continues to be challenging due to the lack of comprehensive benchmarks and the high cost of human annotation.
2. **ScaleEval Framework:** ScaleEval is a meta-evaluation framework that uses multi-agent debate to assess the performance of LLMs as evaluators. It supports multi-round discussions and minimizes human oversight, making it scalable and efficient.
3. **Performance of LLM Evaluators:** The article compares the performance of LLM evaluators, such as gpt-4-turbo, claude-2, and gpt-3.5-turbo, in various scenarios and criteria prompts, highlighting their capabilities and limitations.

### **Analysis and Critique:**
The article effectively addresses the challenges of evaluating LLMs as evaluators and proposes a novel framework, ScaleEval, to mitigate these challenges. However, the study primarily focuses on the performance of LLMs as evaluators and does not extensively discuss potential biases or limitations of the proposed framework. Additionally, while the experiments demonstrate the reliability of the meta-evaluation framework, further research is needed to explore the generalizability of the findings across different LLM models and evaluation scenarios. Overall, the article provides valuable insights into the meta-evaluation of LLMs as evaluators but could benefit from a more comprehensive discussion of potential limitations and future research directions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.16788v1](https://arxiv.org/abs/2401.16788v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.16788v1](https://browse.arxiv.org/html/2401.16788v1)       |
| Truncated       | False       |
| Word Count       | 6273       |