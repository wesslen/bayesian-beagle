
---
title: "MidiCaps -- A large-scale MIDI dataset with text captions"
id: "2406.02255v1"
description: "Introducing MidiCaps: A large-scale MIDI dataset with text captions for music and language processing research."
author: Jan Melechovsky, Abhinaba Roy, Dorien Herremans
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02255v1/extracted/5642374/intro_pic.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02255v1/extracted/5642374/intro_pic.png)

### Summary:

The authors introduce MidiCaps, a large-scale MIDI dataset with text captions, to enable research combining large language models (LLMs) with symbolic music. The dataset contains over 168k MIDI files with textual descriptions, including tempo, chord progression, time signature, instruments present, genre, and mood. The dataset is diverse, offering a rich source for training and evaluating models for tasks such as music information retrieval, music understanding, and cross-modal translation. The authors provide detailed statistics about the dataset and have assessed the quality of the captions in an extensive listening study.

### Major Findings:

1. MidiCaps is the first curated large-scale open dataset of MIDI-caption pairs, providing a comprehensive set of music-specific features extracted from MIDI files.
2. The dataset includes a text caption annotation framework tailored specifically for MIDI data, leveraging the in-context learning capability of large language models (LLMs) to generate captions using only a small number of feature-caption training pairs.
3. The authors have conducted a listening study to evaluate the quality of the generated captions, with participants rating the captions on various aspects such as overall matching, genre matching, mood matching, and tempo matching.

### Analysis and Critique:

1. The authors acknowledge that there are no publicly available MIDI caption datasets, making it difficult to compare their work to existing benchmarks.
2. The listening study relies on subjective ratings from participants, which may introduce bias and variability in the results.
3. The authors do not discuss any potential limitations or shortcomings of their approach, such as the quality of the extracted features or the performance of the LLM in generating accurate and diverse captions.
4. The authors do not provide a detailed analysis of the listening study results, making it difficult to assess the strengths and weaknesses of their approach.
5. The authors do not discuss any potential applications or use cases for the MidiCaps dataset, which could help to demonstrate the practical value of their work.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02255v1](https://arxiv.org/abs/2406.02255v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02255v1](https://browse.arxiv.org/html/2406.02255v1)       |
| Truncated       | False       |
| Word Count       | 5098       |