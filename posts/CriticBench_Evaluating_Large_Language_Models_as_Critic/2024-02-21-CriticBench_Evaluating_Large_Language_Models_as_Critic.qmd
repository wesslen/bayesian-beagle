
---
title: "CriticBench: Evaluating Large Language Models as Critic"
id: "2402.13764v1"
description: "CriticBench evaluates critique ability of Large Language Models across diverse tasks and response qualities."
author: Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-ling Mao
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13764v1/x1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13764v1/x1.png)

### Summary:
The paper introduces CriticBench, a benchmark designed to evaluate the critique abilities of Large Language Models (LLMs). It encompasses nine diverse tasks and evaluates four key critique ability dimensions: feedback, comparison, refinement, and meta-feedback. The paper conducts extensive evaluations of open-source and closed-source LLMs, revealing intriguing relationships between critique ability and tasks, response qualities, and model scales.

### Major Findings:
1. CriticBench exhibits significant advantages over previous benchmarks on critique evaluation, showing great diversity in response quality granularity, critique formats, critique dimensions, and data size.
2. The critique difficulty varies with tasks, where responses of mathematics and coding tasks are challenging for feedback and comparison.
3. The critique ability obeys the scaling law, i.e., LLMs with grander scale and general capabilities demonstrate better critique abilities.

### Analysis and Critique:
- The paper relies heavily on GPT-4 for subjective evaluation, which may not always align perfectly with human judgment.
- The evaluation is limited in the task scenarios, critique dimensions, critique formats, and test data size.
- The paper acknowledges the risks involved in using the Anthropic-HHH dataset, which contains harmful materials and hate speech.

The paper provides a comprehensive benchmark for evaluating LLMs' critique abilities, but it has limitations in terms of evaluation methods and potential risks associated with certain datasets. Further research is needed to address these limitations and enhance the benchmark's evaluation protocol.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13764v1](https://arxiv.org/abs/2402.13764v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13764v1](https://browse.arxiv.org/html/2402.13764v1)       |
| Truncated       | False       |
| Word Count       | 8453       |