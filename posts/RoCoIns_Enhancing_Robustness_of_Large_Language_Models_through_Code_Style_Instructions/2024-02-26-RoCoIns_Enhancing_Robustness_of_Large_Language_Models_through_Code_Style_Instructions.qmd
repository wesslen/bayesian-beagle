
---
title: "RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions"
id: "2402.16431v1"
description: "TL;DR: Code-style instructions improve robustness of Large Language Models, outperforming natural language instructions."
author: Yuansen Zhang, Xiao Wang, Zhiheng Xi, Han Xia, Tao Gui, Qi Zhang, Xuanjing Huang
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16431v1/x3.png"
categories: ['architectures', 'prompt-engineering', 'security', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16431v1/x3.png)

### **Summary:**
- Large Language Models (LLMs) have shown remarkable capabilities in following human instructions, but recent studies have raised concerns about their robustness when prompted with instructions containing textual adversarial samples.
- In this paper, the authors propose RoCoIns, a novel approach to enhance the robustness of LLMs against textual adversarial attacks by utilizing code-style instructions.
- Experiments on eight robustness datasets show that their method consistently outperforms prompting LLMs with natural language instructions.

### **Major Findings:**
1. Prompting LLMs with code-style instructions consistently outperforms prompting with natural language instructions, resulting in a 2.44 and 3.64 point reduction in Attack Success Rate (ASR) on text-davinci-003 and gpt-3.5-turbo, respectively.
2. The proposed adversarial context method further enhances the robustness of LLMs, leading to a decrease of 3.55 points in ASR for text-davinci-003 and 5.66 points for gpt-3.5-turbo.
3. Few-shot prompting significantly improves the robustness of LLMs, with an average ASR decrease of 16.82 and 13.75 points for text-davinci-003 and gpt-3.5-turbo, respectively.

### **Analysis and Critique:**
- The authors provide a comprehensive analysis of the advantages of using code-style instructions, including lower perplexity, improved robustness, and better focus on important parts of the input.
- The study also explores different code-style instructions, the influence of the number of in-context demonstrations, and the impact of different parts of the code-style prompts.
- The authors acknowledge limitations related to the closed-source nature of black-box models and the economic and environmental costs of querying advanced models.
- The paper provides a detailed discussion of related work, datasets, prompt designs, and detailed experimental results.

Overall, the study presents a compelling case for the effectiveness of code-style instructions in enhancing the robustness of LLMs, supported by thorough experimental analysis and critical evaluation. However, the limitations and potential biases of the study should be considered in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.16431v1](https://arxiv.org/abs/2402.16431v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16431v1](https://browse.arxiv.org/html/2402.16431v1)       |
| Truncated       | False       |
| Word Count       | 7657       |