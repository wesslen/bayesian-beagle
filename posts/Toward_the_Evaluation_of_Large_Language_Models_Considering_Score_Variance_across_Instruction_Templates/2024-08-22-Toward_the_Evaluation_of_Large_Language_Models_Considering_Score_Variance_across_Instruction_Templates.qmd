
---
title: "Toward the Evaluation of Large Language Models Considering Score Variance across Instruction Templates"
id: "2408.12263v1"
description: "Fair NLU evaluation of LLMs requires considering score variance between different instruction templates, using Sharpe score as a metric."
author: Yusuke Sakai, Adam Nohejl, Jiangnan Hang, Hidetaka Kamigaito, Taro Watanabe
date: "2024-08-22"
image: "https://browse.arxiv.org/html/2408.12263v1/x1.png"
categories: ['prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.12263v1/x1.png)

### Summary:

- The study focuses on the evaluation of large language models (LLMs) considering score variance across instruction templates.
- The existing evaluation methods do not account for the variance in scores due to differences in prompts, leading to unfair evaluation and comparison of natural language understanding (NLU) performance.
- The study provides English and Japanese cross-lingual datasets for evaluating the NLU performance of LLMs, including multiple instruction templates for fair evaluation and regular expressions to constrain the output format.
- The Sharpe score is proposed as an evaluation metric that takes into account the variance in scores between templates.
- Comprehensive analysis of English and Japanese LLMs reveals that the high variance among templates has a significant impact on the fair evaluation of LLMs.

### Major Findings:

1. The study highlights the importance of considering score variance across instruction templates when evaluating LLMs.
2. The proposed Sharpe score is an effective evaluation metric for measuring NLU performance in a fair manner, considering score variance between different instruction templates.
3. The study provides English and Japanese cross-lingual datasets for evaluating the NLU performance of LLMs, which include multiple instruction templates for fair evaluation and regular expressions to constrain the output format.

### Analysis and Critique:

- The study does not provide a detailed methodology for the creation of the cross-lingual datasets, which could impact the reproducibility of the results.
- The study does not provide a comparison of the proposed evaluation method with existing evaluation methods, which could help to establish the effectiveness of the proposed method.
- The study does not discuss the potential limitations of the proposed evaluation method, such as the impact of the choice of instruction templates on the evaluation results.
- The study does not provide a detailed analysis of the impact of the proposed evaluation method on the development of LLMs, which could help to establish its practical significance.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.12263v1](https://arxiv.org/abs/2408.12263v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.12263v1](https://browse.arxiv.org/html/2408.12263v1)       |
| Truncated       | False       |
| Word Count       | 8798       |