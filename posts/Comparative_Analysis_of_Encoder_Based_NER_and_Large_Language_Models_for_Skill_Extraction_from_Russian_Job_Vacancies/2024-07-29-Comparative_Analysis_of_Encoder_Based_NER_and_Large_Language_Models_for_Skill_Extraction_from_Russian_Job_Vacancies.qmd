
---
title: "Comparative Analysis of Encoder-Based NER and Large Language Models for Skill Extraction from Russian Job Vacancies"
id: "2407.19816v1"
description: "Traditional NER models, like DeepPavlov RuBERT, outperform LLMs in extracting skills from Russian job vacancies, aiding job seekers and employers."
author: Nikita Matkin, Aleksei Smirnov, Mikhail Usanin, Egor Ivanov, Kirill Sobyanin, Sofiia Paklina, Petr Parshakov
date: "2024-07-29"
image: "https://browse.arxiv.org/html/2407.19816v1/extracted/5760770/f1_size.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.19816v1/extracted/5760770/f1_size.png)

### Summary:

This study compares traditional Named Entity Recognition (NER) methods based on encoders with Large Language Models (LLMs) for extracting skills from Russian job vacancies. Using a labeled dataset of 4,000 job vacancies for training and 1,472 for testing, the performance of both approaches is evaluated. Results indicate that traditional NER models, especially DeepPavlov RuBERT NER tuned, outperform LLMs across various metrics including accuracy, precision, recall, and inference time. The findings suggest that traditional NER models provide more effective and efficient solutions for skill extraction, enhancing job requirement clarity and aiding job seekers in aligning their qualifications with employer expectations.

### Major Findings:

1. Traditional NER models, particularly DeepPavlov RuBERT NER tuned, outperform LLMs across various metrics, including accuracy, precision, recall, and inference time.
2. The highest F1 score of 0.81 was achieved by the DeepPavlov RuBERT NER tuned model, indicating its high performance in balancing precision and recall.
3. The highest accuracy of 0.73 was also achieved by the DeepPavlov RuBERT NER tuned model, with a precision of 0.96 and a recall of 0.73.

### Analysis and Critique:

* The study's focus on Russian job vacancies may limit the generalizability of the findings to other languages or regions.
* The dataset, though substantial, may not fully represent the diversity of Russian job descriptions across different industries and regions.
* The study did not benchmark models against human performance, and the rapid evolution of LLMs may alter future comparative performance landscapes.
* The research did not explore practical deployment considerations like infrastructure and scalability.
* Further optimization of training and fine-tuning processes could enhance the results.
* The evaluation of inference costs, conducted exclusively for LLMs, reveals significant differences in the financial efficiency of these models.
* The study highlights the continued relevance and effectiveness of traditional NER models in specific NLP tasks, while also pointing to areas where LLMs can be further optimized to meet the demands of structured information extraction in diverse linguistic contexts.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.19816v1](https://arxiv.org/abs/2407.19816v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.19816v1](https://browse.arxiv.org/html/2407.19816v1)       |
| Truncated       | False       |
| Word Count       | 4205       |