
---
title: "CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion"
id: "2402.05889v1"
description: "CREMA framework efficiently integrates multiple modalities for video reasoning, outperforming strong multimodal models with fewer parameters."
author: Shoubin Yu, Jaehong Yoon, Mohit Bansal
date: "2024-02-08"
image: "../../img/2402.05889v1/image_1.png"
categories: ['education', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.05889v1/image_1.png)

### Summary:
- The article introduces the CREMA framework, an efficient and modular modality-fusion framework for multimodal video reasoning, addressing the limitations of existing approaches.
- The CREMA framework includes a query transformer, parameter-efficient modules, a fusion module, and a multimodal Q-Former for response generation, achieving better performance with 96% fewer trainable parameters.
- The self-gated multimodal query fusion module in the CREMA-espresso approach prevents query token size growth, leading to superior performance in multimodal reasoning tasks.

### Major Findings:
1. The CREMA framework achieves better performance with 96% fewer trainable parameters than strong multimodal models.
2. The self-gated multimodal query fusion module in the CREMA-espresso approach reduces computational costs and improves performance in multimodal reasoning tasks.
3. Adding new modalities, such as video RGB frames, improves the accuracy of both easy and hard subsets in video reasoning problems, with a more significant improvement in the hard subset.

### Analysis and Critique:
- The CREMA framework introduces a novel approach to integrating various sensory representations, providing a lightweight, universal module capable of integrating various sensory representations.
- The self-gated multimodal query fusion module demonstrates competitive performance and efficiency in handling multiple modalities, emphasizing its significance in multimodal reasoning tasks.
- The effectiveness of the self-gated fusion module in improving the accuracy of multimodal video reasoning tasks has implications for the development of more robust and efficient multimodal frameworks for video understanding and reasoning.
- The additional experiments and analysis support the efficacy of the CREMA framework in enhancing multimodal reasoning and question-answering capabilities, demonstrating its adaptability and robustness.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.05889v1](https://arxiv.org/abs/2402.05889v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.05889v1](https://browse.arxiv.org/html/2402.05889v1)       |
| Truncated       | True       |
| Word Count       | 19641       |