
---
title: "Exploiting LLM Quantization"
id: "2405.18137v1"
description: "Quantization of LLMs can create malicious models that appear benign in full-precision, posing a security risk."
author: Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, Martin Vechev
date: "2024-05-28"
image: "../../img/2405.18137v1/image_1.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](../../img/2405.18137v1/image_1.png)

**Summary:**

This paper explores the security implications of quantization methods used to reduce the memory usage of large language models (LLMs). The authors reveal that widely used quantization methods can be exploited to produce harmful quantized LLMs, even when the full-precision counterpart appears benign. This could potentially trick users into deploying malicious quantized models. The authors demonstrate this threat using a three-staged attack framework: (i) obtaining a malicious LLM through fine-tuning on an adversarial task, (ii) quantizing the malicious model and calculating constraints that characterize all full-precision models that map to the same quantized model, and (iii) using projected gradient descent to tune out the poisoned behavior from the full-precision model while ensuring that its weights satisfy the constraints computed in step (ii). This procedure results in an LLM that exhibits benign behavior in full precision but follows the adversarial behavior when quantized. The authors experimentally demonstrate the feasibility and severity of such an attack across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack.

**Major Findings:**

1. Widely used quantization methods can be exploited to produce harmful quantized LLMs, even when the full-precision counterpart appears benign.
2. A three-staged attack framework can be used to produce an LLM that exhibits benign behavior in full precision but follows the adversarial behavior when quantized.
3. The attack can be demonstrated across three diverse scenarios: vulnerable code generation, content injection, and over-refusal attack.

**Analysis and Critique:**

The paper presents a novel and significant contribution to the field of LLM security. The authors' findings highlight the potential risks associated with using quantization methods to reduce the memory usage of LLMs. The three-staged attack framework proposed by the authors is well-structured and effectively demonstrates the feasibility and severity of the attack. However, the paper does not discuss potential countermeasures or defenses against such attacks, which could be a valuable addition to the research. Additionally, the paper does not provide a comprehensive evaluation of the attack's effectiveness across different types of LLMs and quantization methods, which could be a direction for future work.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.18137v1](https://arxiv.org/abs/2405.18137v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.18137v1](https://browse.arxiv.org/html/2405.18137v1)       |
| Truncated       | False       |
| Word Count       | 14784       |