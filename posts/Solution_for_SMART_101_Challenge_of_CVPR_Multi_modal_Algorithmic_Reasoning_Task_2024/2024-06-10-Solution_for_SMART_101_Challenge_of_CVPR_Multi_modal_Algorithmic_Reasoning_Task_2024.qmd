
---
title: "Solution for SMART-101 Challenge of CVPR Multi-modal Algorithmic Reasoning Task 2024"
id: "2406.05963v1"
description: "Team HYU_MLLAB_KT solves SMART-101 CVPR 2024 challenge with LLM and object detection, achieving 29.5 accuracy on test set and 27.1 WOSA on challenge set."
author: Jinwoo Ahn, Junhyeok Park, Min-Jun Kim, Kang-Hyeon Kim, So-Yeong Sohn, Yun-Ji Lee, Du-Seong Chang, Yu-Jung Heo, Eun-Sol Kim
date: "2024-06-10"
image: "https://browse.arxiv.org/html/2406.05963v1/extracted/5655160/figures/fig_main_arch.png"
categories: ['hci', 'education', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.05963v1/extracted/5655160/figures/fig_main_arch.png)

# Summary:

**Summary:**
The paper presents the solution of HYU_MLLAB_KT Team to the Multimodal Algorithmic Reasoning Task: SMART-101 CVPR 2024 Challenge. The team proposes two main ideas to tackle the problem. First, they utilize the reasoning ability of a large-scale language model (LLM) by grounding visual cues (images) in the text modality. They generate highly detailed text captions that describe the context of the image and use these captions as input for the LLM. Second, they utilize an object detection algorithm to ensure complex diagrammatic visual patterns are not overlooked in the captioning process. They employ the SAM algorithm to capture the complex visual features and use this information as input for the LLM. The team achieved an option selection accuracy of 29.5 on the test set and a weighted option selection accuracy (WOSA) of 27.1 on the challenge set.

## Major Findings:
1. The team proposes a new instruction-tuned vision-language model with two novel ideas: grounding visual cues in the text modality and utilizing an object detection algorithm to capture complex diagrammatic visual patterns.
2. The team achieves a 27.11 WOSA score on the challenge split and qualitatively validates the effectiveness of their proposed approach.
3. The team utilizes the Segmentation Anything Model (SAM) algorithm to capture the complex visual features and uses this information as input for the LLM.

## Analysis and Critique:
- The paper does not provide a detailed analysis of the performance of the proposed method compared to other state-of-the-art methods.
- The paper does not discuss the limitations of the proposed method or any potential biases that were apparent while reviewing the text.
- The paper does not discuss any methodological issues, conflicting evidence, or areas that require further research or clarification.
- The paper does not provide a detailed analysis of the performance of the proposed method on different types of puzzles.
- The paper does not discuss the generalizability of the proposed method to other types of multimodal reasoning tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.05963v1](https://arxiv.org/abs/2406.05963v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.05963v1](https://browse.arxiv.org/html/2406.05963v1)       |
| Truncated       | False       |
| Word Count       | 3407       |