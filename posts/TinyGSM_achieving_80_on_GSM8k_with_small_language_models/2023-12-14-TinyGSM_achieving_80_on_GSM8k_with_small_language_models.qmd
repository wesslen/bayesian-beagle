
---
title: "TinyGSM: achieving >80% on GSM8k with small language models"
id: "2312.09241v1"
description: "Small-scale models can solve grade school math with high accuracy using high-quality datasets and verifiers."
author: Bingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward, Yi Zhang
date: "2023-12-14"
image: "../../../bayesian-beagle.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
The article discusses the development of TinyGSM, a synthetic dataset of grade school math problems paired with Python solutions, generated by GPT-3.5. The study aims to explore the potential of small language models (SLMs) in solving mathematical reasoning problems. The authors find that by finetuning a 1.3B generation model and a 1.3B verifier model on TinyGSM, they can achieve 81.5% accuracy on the GSM8K benchmark, outperforming existing models that are orders of magnitude larger. The study demonstrates that high-quality datasets, such as TinyGSM, and the use of a verifier are key components for enhancing the performance of small models.

### **Major Findings:**
1. Small-scale models, such as TinyGSM, can achieve high accuracy on mathematical reasoning problems, surpassing larger models.
2. The use of a verifier, which selects the final outputs from multiple candidate generations, significantly improves the performance of small-scale models.
3. The study highlights the importance of high-quality datasets for small language models to acquire mathematical reasoning.

### **Analysis and Critique:**
The article presents a compelling case for the potential of small language models in solving mathematical reasoning problems. However, the study's focus on synthetic data and the use of a verifier raises questions about the generalizability of the findings to real-world scenarios. Additionally, the article does not address potential biases or limitations of using synthetic data generated by GPT-3.5. Further research is needed to validate the effectiveness of TinyGSM in real-world applications and to address potential ethical concerns related to the use of synthetic data. Additionally, the study could benefit from a more in-depth discussion of the implications of the findings for the field of natural language processing and machine learning.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2312.09241v1](https://arxiv.org/abs/2312.09241v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.09241v1](https://browse.arxiv.org/html/2312.09241v1)       |
| Truncated       | False       |
| Word Count       | 13135       |