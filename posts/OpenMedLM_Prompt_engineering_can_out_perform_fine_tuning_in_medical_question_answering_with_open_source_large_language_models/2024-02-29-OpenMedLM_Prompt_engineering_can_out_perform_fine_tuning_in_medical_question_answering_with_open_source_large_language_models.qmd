
---
title: "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models"
id: "2402.19371v1"
description: "OpenMedLM: Open-source LLMs Achieve State-of-the-Art Results in Medical Benchmarks."
author: Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, Ritankar Das
date: "2024-02-29"
image: "../../img/2402.19371v1/image_1.png"
categories: ['production', 'education', 'architectures', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.19371v1/image_1.png)

### **Summary:**

- The article presents OpenMedLM, a prompting platform that delivers state-of-the-art performance for open-source large language models (LLMs) on medical benchmarks.
- OpenMedLM utilizes various prompting strategies, including zero-shot, few-shot, chain-of-thought, and ensemble/self-consistency voting, to optimize the performance of open-source foundation models.
- The platform demonstrates that open-source models can provide transparency and compliance required in healthcare, surpassing previous best-performing models that relied on extensive fine-tuning and specialized medical data.

### **Major Findings:**

1. OpenMedLM achieves state-of-the-art results on three common medical LLM benchmarks, outperforming previous models that used computationally costly extensive fine-tuning.
2. The model displays the first results

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19371v1](https://arxiv.org/abs/2402.19371v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19371v1](https://browse.arxiv.org/html/2402.19371v1)       |
| Truncated       | False       |
| Word Count       | 12122       |