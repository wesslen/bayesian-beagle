
---
title: "The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale"
id: "2406.17557v1"
description: "TL;DR: FineWeb, a 15-trillion token dataset, improves LLM performance; FineWeb-Edu boosts knowledge and reasoning tasks."
author: Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf
date: "2024-06-25"
image: "https://browse.arxiv.org/html/2406.17557v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17557v1/x1.png)

### Summary:
The FineWeb datasets are a collection of large-scale pretraining datasets designed to produce better-performing large language models (LLMs) than other open pretraining datasets. The authors introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots, and FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. The authors carefully document and ablate all design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. They demonstrate that models trained on FineWeb perform better than those trained on other public web-based pre-training datasets. Additionally, models trained on FineWeb-Edu exhibit significantly better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. The authors release their data curation codebase and all models trained during their ablation experiments.

### Major Findings:
1. The FineWeb dataset, derived from 96 Common Crawl snapshots, produces better-performing LLMs than other open pretraining datasets.
2. The authors carefully document and ablate all design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies.
3. Models trained on FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb, exhibit significantly better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC.

### Analysis and Critique:
The authors provide a comprehensive and well-documented approach to curating large-scale pretraining datasets for LLMs. The introduction of FineWeb and FineWeb-Edu, along with the release of their data curation codebase and models, is a significant contribution to the field. However, the lack of access to high-quality large-scale pretraining datasets and the lack of information about their curation has led to concerns of a growing gap between proprietary and public knowledge. The authors' work represents a significant step towards improving public knowledge and resources for curating LLM pre-training datasets.

One potential limitation of the study is the reliance on Common Crawl data, which may not capture the full diversity of language use and may introduce biases. Additionally, the evaluation of the models is limited to academic benchmarks without

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17557v1](https://arxiv.org/abs/2406.17557v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17557v1](https://browse.arxiv.org/html/2406.17557v1)       |
| Truncated       | False       |
| Word Count       | 10755       |