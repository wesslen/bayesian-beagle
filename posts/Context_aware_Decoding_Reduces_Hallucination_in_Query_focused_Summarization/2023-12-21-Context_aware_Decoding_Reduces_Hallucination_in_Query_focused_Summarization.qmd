
---
title: "Context-aware Decoding Reduces Hallucination in Query-focused Summarization"
description: "Query-focused summarization (QFS) benefits from new decoding techniques, improving quality but with increased complexity and reduced speed."
author: authors
date: "2023-12-21"
image: "../../../bayesian-beagle.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](None)

### Major Takeaways

1. **Query-focused summarization (QFS)** aims to provide a summary of a single document/multiple documents that satisfy the information needs of a given query. The dominant QFS pipeline consists of a retriever (sparse or dense retrieval) and a generator based on large language models (LLM).

2. The deployment of LLMs in QFS potentially leads to **hallucination**, where the generated summary contains information contradicting the source documents/evidence, impacting the quality of the generated summary.

3. The paper focuses on **Context-aware Decoding (CAD)** as a decoding method to improve QFS quality and reduce hallucination. Through experiments, it is shown that CAD improves QFS quality by reducing factual mistakes/hallucinations while mostly retaining the match of lexical patterns, with the caveat of increased inference-time FLOPs and reduced decoding speed.

### Introduction
- QFS is important for real-world applications like abstractive snippet generation and augmented generation.
- Mainstream search engines still use extractive snippets due to problems with deploying generative models.
- Research interest has grown in developing decoding methods to improve generation quality and reduce hallucination.

### Background
- **Context-aware Decoding (CAD)** leverages pointwise mutual information and proposes a product-of-experts enhancement to make generation more conditioned on the input evidence.
- The paper explains the computational cost and trade-offs involved in using CAD.
  - It presents the FLOPs per token in the forward pass and the impact on decoding speed.

### Experiments
- The paper conducts experiments on QFS and news summarization datasets with different choices of language models.
- It uses various language models, including pre-trained and instruction finetuned models.
- The hyperparameter settings for decoding are also detailed, along with the specific prompting templates used.

### Results and Analysis
- CAD's effectiveness in QFS and news summarization is evaluated using metrics like ROUGE F1, BERTScore-precision, and FactKB.
- The paper discusses the choice of hyperparameter **α** and its impact on model performance.
  - There's a trade-off between FactKB score and ROUGE score as α increases.

### Related Work
- The paper discusses other research on hallucination in natural language generation and decoding methods to improve generation quality.

### Conclusion and Limitations
- The reproducibility study shows that CAD improves QFS and news summarization quality but comes with additional computational complexity and reduced decoding speed.
- The paper acknowledges limitations like the limited bandwidth and resources for experimenting with larger language models.

### Critique
- The paper could benefit from a more in-depth discussion of the potential implications and limitations of using CAD in real-world applications.
- The trade-offs between improved quality and increased computational cost could be further explored, offering more nuanced insights.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| HTML     |        |
| Truncated       | False       |
| Word Count       | 6395       |