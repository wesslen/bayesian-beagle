[
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Investigates the fragility of state-of-the-art (SOTA) open-source LLMs under simple optimization-free attacks called priming attacks.\nThese attacks bypass alignment from safety training and can improve the attack success rate on harmful behaviors by up to 3.3Ã—.\n\n\n\n\n\nDiscusses the need for safety training for LLMs and previous research on circumventing safety alignment in open-source LLMs.\nIntroduces the concept of allowing unrestricted inputs for open-source LLMs.\n\n\n\n\n\nDescribes the methodology for few-shot priming attacks involving prompting a non-safety-trained helper LLM and evaluates the attack success rate (ASR) on harmful behaviors.\nPresents results showing that priming attacks outperform baselines for all models.\n\n\n\n\n\nSummarizes the findings and highlights the fragility of current LLM safety measures under practical assumptions, raising concerns for the future of open-sourcing LLMs.\n\n\n\n\n\nDetails on the experimental setup, few-shot prompt format, Llama Guard task instructions, manual evaluation benchmark, manual evaluation vs.Â Llama Guard, and runtime comparison.\n\n\n\n\n\n\n\nLink\nhttps://browse.arxiv.org/html/2312.12321v1\n\n\nTruncated\nFalse\n\n\nWord Count\n4388"
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#abstract",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#abstract",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Investigates the fragility of state-of-the-art (SOTA) open-source LLMs under simple optimization-free attacks called priming attacks.\nThese attacks bypass alignment from safety training and can improve the attack success rate on harmful behaviors by up to 3.3Ã—."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#introduction",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#introduction",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Discusses the need for safety training for LLMs and previous research on circumventing safety alignment in open-source LLMs.\nIntroduces the concept of allowing unrestricted inputs for open-source LLMs."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#methodology-results",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#methodology-results",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Describes the methodology for few-shot priming attacks involving prompting a non-safety-trained helper LLM and evaluates the attack success rate (ASR) on harmful behaviors.\nPresents results showing that priming attacks outperform baselines for all models."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#conclusion",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#conclusion",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Summarizes the findings and highlights the fragility of current LLM safety measures under practical assumptions, raising concerns for the future of open-sourcing LLMs."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#appendices",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#appendices",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Details on the experimental setup, few-shot prompt format, Llama Guard task instructions, manual evaluation benchmark, manual evaluation vs.Â Llama Guard, and runtime comparison."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#appendix",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#appendix",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Link\nhttps://browse.arxiv.org/html/2312.12321v1\n\n\nTruncated\nFalse\n\n\nWord Count\n4388"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bayesian beagle",
    "section": "",
    "text": "Iâ€™m a Bayesian beagle who has curated reading though LLM generation."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryanâ€™s LLM Blog ðŸ¤–",
    "section": "",
    "text": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n\n\nsecurity\n\n\nopen-source\n\n\n\nLLMs are vulnerable to priming attacks that bypass safety training, increasing attack success rates on harmful behaviors.\n\n\n\ngpt-3.5-turbo-1106\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]