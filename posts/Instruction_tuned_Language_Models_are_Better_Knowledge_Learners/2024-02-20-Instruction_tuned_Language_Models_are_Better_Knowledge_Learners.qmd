
---
title: "Instruction-tuned Language Models are Better Knowledge Learners"
id: "2402.12847v1"
description: "Pre-instruction-tuning (PIT) improves large language model (LLM) knowledge absorption by 17.8%."
author: Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Victoria Lin, Wen-tau Yih, Srinivasan Iyer
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.12847v1/x2.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12847v1/x2.png)

### **Summary:**
- Large language models (LLMs) store vast amounts of factual knowledge in their parameters through large-scale pre-training, but this knowledge can become outdated as the world evolves.
- Continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs is a widely used practice to update the factual knowledge stored in LLMs.
- However, LLMs trained with this practice struggle to answer questions about documents they have been trained on, even though the perplexity of documents is minimized.
- Pre-instruction-tuning (PIT) is proposed as a method that instruction-tunes on questions prior to training on documents, significantly enhancing the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.

### **Major Findings:**
1. Continued pre-training on new documents followed by instruction-tuning is a widely used practice to update the factual knowledge stored in LLMs.
2. LLMs trained with this practice struggle to answer questions about documents they have been trained on, even though the perplexity of documents is minimized.
3. Pre-instruction-tuning (PIT) significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.

### **Analysis and Critique:**
- The study provides valuable insights into the limitations of standard instruction-tuning and the effectiveness of pre-instruction-tuning in enhancing the ability of LLMs to absorb knowledge from new documents.
- The findings are based on experiments and ablation studies, providing a comprehensive understanding of the proposed method.
- The limitations of the study include the scope being limited to Wikipedia data, which may restrict the adaptability of the trained models to other sources. Further exploration of pre-instruction-tuning with different types of data for enhancing other skills is suggested for future studies.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.12847v1](https://arxiv.org/abs/2402.12847v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12847v1](https://browse.arxiv.org/html/2402.12847v1)       |
| Truncated       | False       |
| Word Count       | 7579       |