
---
title: "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline"
id: "2402.13823v1"
description: "TL;DR: This article provides knowledge and guidelines for using Large Language Models in NLP for RE."
author: Andreas Vogelsang, Jannik Fischbach
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13823v1/x1.png"
categories: ['education', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13823v1/x1.png)

### **Summary:**
- Large Language Models (LLMs) have revolutionized how we can automate Requirements Engineering (RE) tasks and improve quality.
- LLMs are successful in RE because they do not need extensive datasets to be trained.
- The chapter provides a fundamental understanding of LLMs and how to use them for RE tasks.

### **Major Findings:**
1. LLMs have improved the quality of RE tasks over traditional classification algorithms.
2. LLMs do not require extensive datasets for training, making them suitable for RE tasks.
3. The chapter provides a detailed guideline for using LLMs for RE tasks.

### **Analysis and Critique:**
- The chapter provides a comprehensive overview of using LLMs for RE tasks, but it does not address potential biases in the LLMs.
- The fine-tuning process for LLMs may lead to overfitting and bias amplification, which are not adequately addressed in the chapter.
- The chapter acknowledges the challenges of fine-tuning LLMs but does not provide detailed solutions for mitigating these challenges.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13823v1](https://arxiv.org/abs/2402.13823v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13823v1](https://browse.arxiv.org/html/2402.13823v1)       |
| Truncated       | False       |
| Word Count       | 7217       |