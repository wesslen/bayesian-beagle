
---
title: "Are Large Language Models Chameleons?"
id: "2405.19323v1"
description: "LLMs exhibit biases influenced by prompts, resembling cultural, age, and gender biases; their imitation abilities are approximate."
author: Mingmeng Geng, Sihong He, Roberto Trotta
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.19323v1/extracted/5630151/freehms_men_p09.png"
categories: ['hci', 'prompt-engineering', 'education', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19323v1/extracted/5630151/freehms_men_p09.png)

### Summary:

The study investigates the ability of large language models (LLMs) to mimic human opinions and biases by comparing their responses to subjective questions with real survey data from the European Social Survey (ESS). The authors find that LLMs exhibit significant cultural, age, and gender biases, and that the effect of prompts on bias and variability is fundamental. They propose methods for measuring the difference between LLMs and survey data, such as calculating weighted means and a new measure inspired by Jaccard similarity. The authors conclude that it is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior.

### Major Findings:

1. LLMs exhibit significant cultural, age, and gender biases when responding to subjective questions.
2. The effect of prompts on bias and variability is fundamental in LLMs.
3. Methods for measuring the difference between LLMs and survey data, such as calculating weighted means and a new measure inspired by Jaccard similarity, are proposed.
4. It is important to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior.

### Analysis and Critique:

1. The study highlights the importance of considering cultural, age, and gender biases in LLMs when using them to model human opinions and behavior.
2. The proposed methods for measuring the difference between LLMs and survey data provide a useful framework for evaluating the accuracy of LLMs in mimicking human opinions.
3. The study emphasizes the need to analyze the robustness and variability of prompts before using LLMs to model individual decisions or collective behavior.
4. However, the study does not address the potential limitations of using LLMs to model human opinions and behavior, such as their inability to capture the complexity and nuance of human thought and emotion.
5. The study also does not discuss the potential ethical implications of using LLMs to model human opinions and behavior, such as the risk of perpetuating existing biases and stereotypes.
6. Future research should address these limitations and explore the potential ethical implications of using LLMs to model human opinions and behavior.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19323v1](https://arxiv.org/abs/2405.19323v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19323v1](https://browse.arxiv.org/html/2405.19323v1)       |
| Truncated       | False       |
| Word Count       | 6778       |