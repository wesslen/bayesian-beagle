
---
title: "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline"
id: "2402.13823v2"
description: "LLMs for NLP in RE need basic knowledge and a usage guideline."
author: Andreas Vogelsang, Jannik Fischbach
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.13823v2/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13823v2/x1.png)

### **Summary:**
- Large Language Models (LLMs) have revolutionized how we can automate Requirements Engineering (RE) tasks and improve quality.
- LLMs are successful in RE because they do not need extensive datasets to be trained.
- The chapter provides a detailed guideline for using LLMs for RE tasks, including understanding and defining the NLP problem, fundamentals of LLMs, and different ways to use LLMs in RE.

### **Major Findings:**
1. LLMs have revolutionized the automation of RE tasks and improved the quality of results.
2. LLMs do not require extensive datasets for training, making them suitable for RE tasks with limited data availability.
3. The chapter provides a systematic guideline for using LLMs in RE, including domain adaptation, supervised fine-tuning, and different usage modes for encoder-only, decoder-only, and encoder-decoder LLMs.

### **Analysis and Critique:**
- The chapter provides a comprehensive overview of using LLMs for RE tasks, but it does not extensively address potential biases and ethical considerations associated with LLMs.
- The fine-tuning process for LLMs may lead to overfitting, bias amplification, and hyperparameter tuning challenges, which are not extensively discussed in the chapter.
- The chapter acknowledges that fine-tuning LLMs may not always be possible or useful, but it does not provide alternative approaches for RE tasks in such scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.13823v2](https://arxiv.org/abs/2402.13823v2)        |
| HTML     | [https://browse.arxiv.org/html/2402.13823v2](https://browse.arxiv.org/html/2402.13823v2)       |
| Truncated       | False       |
| Word Count       | 7202       |