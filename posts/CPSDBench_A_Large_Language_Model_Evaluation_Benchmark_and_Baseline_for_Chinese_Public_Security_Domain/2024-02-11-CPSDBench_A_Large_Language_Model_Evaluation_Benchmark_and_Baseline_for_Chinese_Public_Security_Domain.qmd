
---
title: "CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain"
id: "2402.07234v1"
description: "TL;DR: Study creates CPSDbench to evaluate LLMs in Chinese public security tasks."
author: Xin Tong, Bo Jin, Zhi Lin, Binjun Wang, Ting Yu
date: "2024-02-11"
image: "../../../bayesian-beagle.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
- Large Language Models (LLMs) have shown potential in various domains, including public security.
- The study aims to construct a specialized evaluation benchmark, CPSDBench, tailored to the Chinese public security domain.
- CPSDBench integrates datasets related to public security from real-world scenarios and introduces innovative evaluation metrics.

### **Major Findings:**
1. **Constructed a Dataset for Public Security Tasks:**
   - Developed CPSDBench, a specialized evaluation benchmark dataset designed explicitly for public security tasks.
   - Assembled a comprehensive set of test datasets from four dimensions: text classification, information extraction, question answering, and text generation.

2. **Selection and Design of Appropriate Evaluation Metrics:**
   - Meticulously selected and designed a series of evaluation metrics, including an innovative two-stage evaluation metric for information extraction tasks.
   - Evaluated nine mainstream LLMs, providing valuable insights into their performance in public security tasks.

3. **Evaluation of Mainstream Open-source and Proprietary LLMs:**
   - Showcased the performance of each model in public security tasks and revealed their respective strengths and limitations.
   - Offered guidance for future research and development in the public security domain.

### **Analysis and Critique:**
- The study highlights the strengths and limitations of mainstream LLMs in public security tasks.
- Identified challenges faced by LLMs in handling sensitive data, output format, understanding instructions, and content generation.
- Future research aims to collect a broader array of public security-related datasets and optimize the evaluation metric system to provide deeper insights and guidance for the application of LLMs in the public security domain.

The article provides a comprehensive overview of the development of CPSDBench and its evaluation of mainstream LLMs in the Chinese public security domain. It effectively highlights the potential and challenges of LLMs in addressing public security tasks. However, the study could benefit from a more detailed discussion of the limitations and biases of the evaluation metrics used, as well as a deeper analysis of the impact of language specificity and parameter scale on model performance. Additionally, further exploration of the challenges faced by LLMs in handling sensitive data and adversarial samples would enhance the study's insights.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.07234v1](https://arxiv.org/abs/2402.07234v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07234v1](https://browse.arxiv.org/html/2402.07234v1)       |
| Truncated       | False       |
| Word Count       | 6339       |