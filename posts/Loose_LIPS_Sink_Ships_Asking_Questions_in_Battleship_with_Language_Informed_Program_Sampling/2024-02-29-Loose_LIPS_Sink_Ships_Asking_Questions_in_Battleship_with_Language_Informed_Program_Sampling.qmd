
---
title: "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling"
id: "2402.19471v1"
description: "Model generates informative questions in Battleship, mirroring human performance; LLMs struggle with grounding questions in board state."
author: Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum
date: "2024-02-29"
image: "../../img/2402.19471v1/image_1.png"
categories: ['production', 'education', 'programming', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.19471v1/image_1.png)

### Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling

**Summary:**
- The study explores how people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources in the context of the game Battleship.
- A language-informed program sampling (LIPS) model is proposed, which uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain.
- The LIPS model outperforms LLM-only baselines and illustrates how Bayesian models of question-asking can leverage the statistics of language to capture human priors.

**Major Findings:**
1. The LIPS model, with a surprisingly modest resource budget, yields informative questions that mirror human performance across varied Battleship board scenarios.
2. LLM-only baselines struggle to ground questions in the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19471v1](https://arxiv.org/abs/2402.19471v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19471v1](https://browse.arxiv.org/html/2402.19471v1)       |
| Truncated       | False       |
| Word Count       | 18405       |