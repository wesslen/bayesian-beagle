
---
title: "FFN: a Fine-grained Chinese-English Financial Domain Parallel Corpus"
id: "2406.18856v1"
description: "LLMs' financial translation quality is evaluated, revealing room for improvement and optimization."
author: Yuxin Fu, Shijing Si, Leyi Mai, Xi-ang Li
date: "2024-06-27"
image: "../../../bayesian-beagle.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The authors of this study focus on the application of Large Language Models (LLMs) in the financial domain, specifically for Chinese-English translation.
- They constructed a fine-grained Chinese-English parallel corpus of financial news called FFN, consisting of 1,013 main texts and 809 titles, all manually corrected.
- The translation quality of two LLMs, ChatGPT and ERNIE-bot, was measured using BLEU, TER, and chrF scores. An OpenNMT model was also trained based on the dataset for comparison.
- The study aims to highlight the need for optimizing LLMs within the specific field of financial translation to ensure accuracy and quality.

### Major Findings:
1. The authors built a parallel dataset of English-Chinese news translation in the finance domain, including main texts and titles.
2. They evaluated the performance of ChatGPT and ERNIE-bot in translation and compared them with DeepL and Google, finding some unexpected feedback.
3. The authors trained an OpenNMT model based on the dataset to evaluate its performance.
4. They provided a quantitative and qualitative analysis to reveal problems when prompting for machine translation, offering insights for future study.

### Analysis and Critique:
- The study provides a valuable contribution to the field by focusing on the application of LLMs in the financial domain, which has been largely underexplored.
- The construction of the FFN corpus is a significant step towards improving the quality of Chinese-English translation in the financial domain.
- However, the study could have benefited from a more detailed analysis of the unexpected feedback from the LLMs and a comparison with other translation models.
- The authors could have also discussed potential limitations of their study, such as the size of the dataset and the generalizability of the findings to other language pairs and domains.
- Future research could explore the application of LLMs in other domains and language pairs, as well as the development of more sophisticated evaluation metrics for machine translation.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.18856v1](https://arxiv.org/abs/2406.18856v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.18856v1](https://browse.arxiv.org/html/2406.18856v1)       |
| Truncated       | False       |
| Word Count       | 848       |