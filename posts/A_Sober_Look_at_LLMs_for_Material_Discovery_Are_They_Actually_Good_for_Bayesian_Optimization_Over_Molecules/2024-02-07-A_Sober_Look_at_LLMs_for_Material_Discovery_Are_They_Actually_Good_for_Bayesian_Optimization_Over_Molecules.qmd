
---
title: "A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?"
id: "2402.05015v1"
description: "LLMs can accelerate Bayesian optimization in molecular space with domain-specific data."
author: Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Al√°n Aspuru-Guzik, Geoff Pleiss
date: "2024-02-07"
image: "../../../bayesian-beagle.png"
categories: ['production', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The article explores the use of large language models (LLMs) in Bayesian optimization (BO) for material discovery, highlighting the challenges in material discovery and the role of BO in leveraging prior knowledge for efficient exploration of a large molecular space. It introduces the concept of LLMs as fixed feature extractors and discusses the use of parameter-efficient finetuning methods and Bayesian neural networks to obtain the posterior of the LLM surrogate. The authors present their contributions, including the study of LLMs for material discovery, the development of software for principled BO with LLMs, and insights on the usefulness of LLMs for scientific discovery.
- The use of parameter-efficient fine-tuning (PEFT) methods to optimize LLMs for BO in molecular discovery is discussed, comparing the performance of general-purpose LLMs and chemistry-specific LLMs as feature extractors in BO. The impact of prompting on BO performance and the effectiveness of finetuned LLM surrogates with fixed-feature surrogates are also explored.
- The article also delves into the use of LLMs as surrogate models for BO over molecules, comparing finetuned LLM surrogates to fixed-feature surrogates and highlighting the challenges of using LLMs due to their computational expense and memory limitations. It discusses related work on the usage of LLMs for BO and the application of generative models for continuous optimization of molecules.

### Major Findings:
1. LLMs can be effectively optimized for BO in molecular discovery using parameter-efficient fine-tuning methods, with chemistry-specific LLMs showing promising performance as feature extractors.
2. The choice of prompt and molecular representation significantly influences the performance of LLM-based Bayesian optimization, emphasizing the importance of these factors in the optimization process.
3. The computational expense and memory limitations of LLMs present challenges in their application as surrogate models for BO over molecules, but their potential for aiding scientific discovery in chemistry is evident.

### Analysis and Critique:
- The article provides valuable insights into the practical application of LLMs in molecular discovery and Bayesian optimization, shedding light on the challenges, advantages, and potential of LLM-based optimization methods.
- However, the computational expense and memory limitations of LLMs pose significant practical challenges, and further research is needed to address these limitations and optimize the efficiency of LLM-based optimization methods.
- The study's focus on the application of LLMs in material discovery and the development of software for principled BO with LLMs is commendable, but additional research is required to fully understand the implications and limitations of LLM-based optimization in scientific applications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.05015v1](https://arxiv.org/abs/2402.05015v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.05015v1](https://browse.arxiv.org/html/2402.05015v1)       |
| Truncated       | True       |
| Word Count       | 19253       |