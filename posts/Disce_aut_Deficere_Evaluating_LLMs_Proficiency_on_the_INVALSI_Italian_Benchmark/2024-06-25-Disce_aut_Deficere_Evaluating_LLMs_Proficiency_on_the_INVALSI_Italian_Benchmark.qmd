
---
title: "Disce aut Deficere: Evaluating LLMs Proficiency on the INVALSI Italian Benchmark"
id: "2406.17535v1"
description: "TL;DR: We adapt INVALSI tests to evaluate LLMs in Italian, comparing them to human performance and inviting further model submissions."
author: Fabio Mercorio, Mario Mezzanzanica, Daniele Potert√¨, Antonio Serino, Andrea Seveso
date: "2024-06-25"
image: "../../https://browse.arxiv.org/html/2406.17535v1/extracted/5687658/img/grade_model_performance.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../https://browse.arxiv.org/html/2406.17535v1/extracted/5687658/img/grade_model_performance.png)

### Summary:

This study introduces a structured benchmark using the INVALSI tests, a set of well-established assessments designed to measure educational competencies across Italy, to evaluate the proficiency of Large Language Models (LLMs) in handling real-world, nuanced language tasks. The contributions of this work are threefold: (1) adapting the INVALSI benchmark for automated LLM evaluation, (2) providing a detailed assessment of current LLMs, and (3) visually comparing the performance of these models against human results. The paper is structured as follows: Section 1 presents the introduction, Section 2 discusses related work, Section 3 details the data curation process for creating the benchmark, Section 4 displays the results of multiple models tested against this benchmark, Section 5 discusses these results and identifies limitations, and Section 6 concludes the paper and outlines proposals for future work.

### Major Findings:

1. Models perform better on tasks aimed at lower school grades than those designed for higher grades. The complexity of language and cognitive tasks in higher educational levels poses significant challenges for current language models. Models excel in text comprehension while reflecting on the Italian language is harder.
2. Larger models consistently outperform smaller ones, even those fine-tuned for the Italian language. This indicates that the inherent capabilities of larger models, possibly due to more extensive training data and more complex neural architectures, contribute to better handling of the nuances of language tasks.

### Analysis and Critique:

The study provides a valuable contribution to the field by establishing a structured benchmark for evaluating LLMs in the Italian language. However, there are some potential limitations and areas for improvement:

1. The study focuses on the Italian language, which may limit its applicability to other languages. Future research could explore adapting the benchmark to other languages and evaluating the performance of LLMs in those contexts.
2. The study does not explicitly address the potential biases in the INVALSI tests or the LLMs themselves. It is essential to consider the cultural and contextual relevance of the tests and the potential biases in the models when interpreting the results.
3. The study does not discuss the potential impact of the benchmark on the development and deployment of LL

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17535v1](https://arxiv.org/abs/2406.17535v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17535v1](https://browse.arxiv.org/html/2406.17535v1)       |
| Truncated       | False       |
| Word Count       | 8268       |