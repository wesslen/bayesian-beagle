
---
title: "Poisoned LangChain: Jailbreak LLMs by LangChain"
id: "2406.18122v1"
description: "Poisoned-LangChain: Novel method for indirect jailbreak attacks on LLMs, achieving 88.56%, 79.04%, and 82.69% success rates in three scenarios."
author: Ziqiu Wang, Jun Liu, Shengkai Zhang, Yang Yang
date: "2024-06-26"
image: "https://browse.arxiv.org/html/2406.18122v1/extracted/5692368/fig_top.png"
categories: ['security', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.18122v1/extracted/5692368/fig_top.png)

### Summary:

The paper introduces an innovative method for conducting indirect jailbreak attacks on large language models (LLMs) using LangChain, termed Poisoned LangChain (PLC). The PLC method leverages a poisoned external knowledge base to interact with LLMs, causing them to generate malicious non-compliant dialogues. The paper focuses on Chinese LLMs and demonstrates the effectiveness of PLC in executing jailbreak attacks on the latest versions of Chinese LLMs with high success rates.

### Major Findings:

1. The paper proposes a new method for indirect jailbreak attacks on LLMs using LangChain, called Poisoned LangChain (PLC), which utilizes a poisoned external knowledge base to interact with LLMs and generate malicious non-compliant dialogues.
2. The PLC method is designed by setting keyword triggers, crafting inducement prompts, and creating a specific toxic knowledge base tailored to circumvent scrutiny.
3. The paper demonstrates the effectiveness of PLC in executing jailbreak attacks on six different Chinese LLMs, achieving success rates of 88.56%, 79.04%, and 82.69% in three different scenarios.

### Analysis and Critique:

The paper presents a novel approach to conducting indirect jailbreak attacks on LLMs using LangChain, which has the potential to significantly enhance our ability to detect vulnerabilities in language models. However, the paper only focuses on Chinese LLMs, and it is unclear whether the proposed method would be effective on LLMs in other languages. Additionally, the paper does not discuss the ethical implications of using PLC to conduct jailbreak attacks on LLMs, which is an important consideration given the potential for misuse of this method. Finally, the paper does not provide a detailed analysis of the limitations of the proposed method or discuss potential countermeasures that could be used to defend against PLC attacks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.18122v1](https://arxiv.org/abs/2406.18122v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.18122v1](https://browse.arxiv.org/html/2406.18122v1)       |
| Truncated       | False       |
| Word Count       | 4003       |