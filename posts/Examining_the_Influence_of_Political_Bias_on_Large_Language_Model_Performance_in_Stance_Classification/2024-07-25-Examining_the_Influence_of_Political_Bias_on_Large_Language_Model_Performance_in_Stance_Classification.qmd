
---
title: "Examining the Influence of Political Bias on Large Language Model Performance in Stance Classification"
id: "2407.17688v1"
description: "LLMs show bias in stance classification, performing better on certain political stances, with accuracy decreasing when target ambiguity rises."
author: Lynnette Hui Xian Ng, Iain Cruickshank, Roy Ka-Wei Lee
date: "2024-07-25"
image: "https://browse.arxiv.org/html/2407.17688v1/extracted/5753667/curves_pic.png"
categories: ['prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.17688v1/extracted/5753667/curves_pic.png)

### Summary:

This study investigates the political biases of Large Language Models (LLMs) within the stance classification task. The authors utilize three datasets, seven LLMs, and four distinct prompting schemes to analyze the performance of LLMs on politically oriented statements and targets. The findings reveal a statistically significant difference in the performance of LLMs across various politically oriented stance classification tasks. This difference primarily manifests at the dataset level, with models and prompting schemes showing statistically similar performances across different stance classification datasets. Additionally, LLMs have poorer stance classification accuracy when there is greater ambiguity in the target the statement is directed towards.

### Major Findings:

1. A statistically significant difference in the performance of LLMs across various politically oriented stance classification tasks.
2. The difference in performance primarily manifests at the dataset level, with models and prompting schemes showing statistically similar performances across different stance classification datasets.
3. LLMs have poorer stance classification accuracy when there is greater ambiguity in the target the statement is directed towards.

### Analysis and Critique:

1. The study does not provide a clear definition of what constitutes a "politically oriented" statement or target, which may introduce subjectivity in the analysis.
2. The authors do not discuss the potential impact of the size and diversity of the training datasets on the observed political biases in LLMs.
3. The study does not explore the potential impact of different model architectures on the observed political biases in LLMs.
4. The authors do not discuss the potential implications of their findings for the development and deployment of LLMs in real-world applications.
5. The study does not provide a clear explanation of how the observed political biases in LLMs might be mitigated or addressed in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.17688v1](https://arxiv.org/abs/2407.17688v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.17688v1](https://browse.arxiv.org/html/2407.17688v1)       |
| Truncated       | False       |
| Word Count       | 7443       |