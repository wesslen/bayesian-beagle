
---
title: "Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks"
description: "Study evaluates prompting techniques for LLMs on math tasks. Findings show models struggle with elementary calculations and reasoning even with red teaming."
author: Aleksander Buszydlik, Karol Dobiczek, Michał Teodor Okoń, Konrad Skublicki, Philip Lippmann, Jie Yang
date: "2023-12-30"
image: "../../../bayesian-beagle.png"
categories: ['security', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](None)

### Red Teaming for Large Language Models At Scale: Tackling Hallucinations on Mathematics Tasks

#### Main Findings
1. Red teaming techniques **do not effectively** reduce hallucinations in gpt-3.5-turbo and gpt-4 models for elementary calculations and reasoning tasks.
2. Models achieve **low accuracy** ranging from 50-60% on harder calculations and puzzles, with some techniques slightly improving performance while others degrading it.
3. Providing **examples** improves model performance, suggesting some capacity for knowledge transfer between problems.

#### Introduction
- Red teaming aims to find backdoors in Large Language Models (LLMs) to elicit irresponsible responses and involves strategic prompting and querying to identify vulnerabilities.

#### Related Work
- Top-performing LLMs are not entirely safe and are prone to hallucinate content, posing significant threats.
- Previous research has found inconsistencies and amplification of hallucinations in LLMs when it comes to mathematical reasoning.

#### Methods
- Used the gpt-4 and gpt-3.5-turbo models and developed a Python framework for automatic red teaming at scale to assess their performance on school-level calculations and algebraic puzzles.

#### Results
- Models have low accuracy on harder calculations, with some red teaming techniques improving performance and others degrading it, while providing examples improves model performance on different metrics.

#### Discussion
- Models are not well-suited for mathematics tasks, and their numerical abilities mainly stem from memorization rather than the ability to follow simple algorithms. Examples may introduce noise or not be tailored well enough for the models.

#### Conclusions and Limitations
- Presented a Python framework for red teaming evaluations and highlighted the limitations of the study, including evaluating only one type of LLM and the lack of publicly available training data.

#### Critique
The paper provides insightful findings on the effectiveness of red teaming techniques and examples in mitigating hallucinations in LLMs on mathematical tasks. However, the study is limited by evaluating only two specific LLM models and may not consider all potential factors contributing to model behavior, such as fine-tuning methods or model architecture differences. Additionally, the study could benefit from a more comprehensive exploration of red teaming techniques and their impact on a broader range of LLMs. Overall, while the study provides valuable insights, further research is needed to fully understand and address the potential risks associated with LLM hallucinations.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| Abstract | [http://arxiv.org/abs/2401.00290v1](http://arxiv.org/abs/2401.00290v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.00290v1](https://browse.arxiv.org/html/2401.00290v1)       |
| Truncated       | False       |
| Word Count       | 7380       |