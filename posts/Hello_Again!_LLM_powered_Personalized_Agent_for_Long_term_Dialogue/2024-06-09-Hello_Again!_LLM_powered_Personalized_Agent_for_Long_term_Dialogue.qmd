
---
title: "Hello Again! LLM-powered Personalized Agent for Long-term Dialogue"
id: "2406.05925v1"
description: "LD-Agent: A framework for long-term dialogue systems with event memory, persona modeling, and response generation."
author: Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua
date: "2024-06-09"
image: "https://browse.arxiv.org/html/2406.05925v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.05925v1/x1.png)

### Summary:

The paper introduces a model-agnostic framework called Long-term Dialogue Agent (LD-Agent) for open-domain dialogue systems. The LD-Agent aims to address the real-world need for long-term companionship and personalized interactions with chatbots. The framework consists of three independently tunable modules: event perception, persona extraction, and response generation. The event memory module uses long and short-term memory banks to focus on historical and ongoing sessions, respectively, and a topic-based retrieval mechanism to enhance memory retrieval accuracy. The persona module conducts dynamic persona modeling for both users and agents. The effectiveness, generality, and cross-domain capabilities of LD-Agent are demonstrated across various benchmarks, models, and tasks.

### Major Findings:

1. The LD-Agent framework is model-agnostic, deployable in various real-world domains, and capable of autonomously integrating comprehensive data from both event memories and personas.
2. The event memory module ensures dialogue coherence across sessions, while the persona module ensures character consistency.
3. The LD-Agent framework introduces a disentangled, tunable approach for long-term dialogue to ensure the accuracy of each module, enabling it to adapt to various dialogue tasks through module re-training.

### Analysis and Critique:

1. The paper does not provide a detailed comparison with other existing frameworks for long-term dialogue systems, which could have helped to better understand the advantages and limitations of the proposed LD-Agent framework.
2. The paper does not discuss the potential challenges and limitations of the LD-Agent framework, such as the computational resources required for training and deploying the model, or the potential biases in the data used for training the model.
3. The paper does not provide a clear explanation of how the LD-Agent framework can be adapted to different domains and tasks, which could have helped to better understand the generalizability of the framework.
4. The paper does not discuss the potential ethical implications of using the LD-Agent framework for long-term dialogue systems, such as the potential for the model to perpetuate biases or to be used for malicious purposes.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.05925v1](https://arxiv.org/abs/2406.05925v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.05925v1](https://browse.arxiv.org/html/2406.05925v1)       |
| Truncated       | False       |
| Word Count       | 6818       |