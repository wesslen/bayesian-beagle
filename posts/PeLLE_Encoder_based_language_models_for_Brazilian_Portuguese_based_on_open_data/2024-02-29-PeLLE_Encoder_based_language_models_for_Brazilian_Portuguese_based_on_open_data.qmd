
---
title: "PeLLE: Encoder-based language models for Brazilian Portuguese based on open data"
id: "2402.19204v1"
description: "PeLLE models: Large PT-BR language models; size matters, but so does curated data."
author: Guilherme Lamartine de Mello, Marcelo Finger, and Felipe Serras, Miguel de Mello Carpi, Marcos Menon Jose, Pedro Henrique Domingues, Paulo Cavalim
date: "2024-02-29"
image: "../../../bayesian-beagle.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

- The paper introduces PeLLE, a family of Large Language Models (LLMs) for Brazilian Portuguese based on the RoBERTa architecture and trained on the curated, open Carolina corpus.
- PeLLE models are compared to existing multilingual and Portuguese-specific refined pretrained Transformer-based LLM encoders in various downstream tasks.
- The results suggest that larger models perform better in some tasks, while smaller-but-curated data in pretraining benefits others.

### Major Findings:
1. PeLLE models, when compared to existing multilingual and Portuguese-specific refined pretrained Transformer-based LLM encoders, show competitive performance in various downstream tasks.
2. Larger models generally perform better in regression tasks when all other factors are equalized.
3. The pure Portuguese-only pretrained model, pPeLLE

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19204v1](https://arxiv.org/abs/2402.19204v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19204v1](https://browse.arxiv.org/html/2402.19204v1)       |
| Truncated       | False       |
| Word Count       | 5656       |