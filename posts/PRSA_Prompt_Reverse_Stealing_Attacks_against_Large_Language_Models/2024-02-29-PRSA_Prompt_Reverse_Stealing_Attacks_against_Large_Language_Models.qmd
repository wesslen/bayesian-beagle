
---
title: "PRSA: Prompt Reverse Stealing Attacks against Large Language Models"
id: "2402.19200v1"
description: "Threat of Prompt Leakage in Large Language Models: A Novel Attack Framework (PRSA) Effectively Infers Target Prompts."
author: Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19200v1/extracted/5440247/figures/prompt_jailbreaking.png"
categories: ['production', 'robustness', 'security', 'prompt-engineering', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19200v1/extracted/5440247/figures/prompt_jailbreaking.png)

### **Summary:**

- Prompt Stealing Attacks against Large Language Models (LLMs) pose a risk to intellectual property rights due to the exposure of input-output pairs.
- PRSA, a novel attack framework, is proposed to infer target prompts by analyzing critical features of input-output pairs using a generative model.
- PRSA consists of two main phases: prompt mutation and prompt pruning.
- Prompt mutation uses a prompt attention algorithm based on differential feedback to optimize the generative model.
- Prompt pruning identifies and masks input-dependent words to enhance the surrogate prompt's generality.
- PRSA is effective in stealing prompts across various categories in the marketplace and LLM application platforms.

### Major Findings:

1. PRSA poses a severe threat in real-world scenarios, stealing prompts effectively across various categories in the marketplace and

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19200v1](https://arxiv.org/abs/2402.19200v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19200v1](https://browse.arxiv.org/html/2402.19200v1)       |
| Truncated       | False       |
| Word Count       | 12660       |