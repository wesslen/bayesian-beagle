
---
title: "NTSEBENCH: Cognitive Reasoning Benchmark for Vision Language Models"
id: "2407.10380v1"
description: "TL;DR: New dataset, NTSEBench, tests LLMs and VLMs on complex cognitive reasoning tasks, featuring 2,728 multiple-choice questions with 4,642 images."
author: Pranshu Pandya, Agney S Talwarr, Vatsal Gupta, Tushar Kataria, Vivek Gupta, Dan Roth
date: "2024-07-15"
image: "https://browse.arxiv.org/html/2407.10380v1/extracted/5731050/figures/Figure1.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.10380v1/extracted/5731050/figures/Figure1.png)

### Summary:

The paper introduces a new dataset, NTSEBench, designed to evaluate the cognitive multi-modal reasoning and problem-solving skills of large models. The dataset comprises 2,728 multiple-choice questions with 4,642 images across 26 categories, sourced from the NTSE examination conducted in India. The questions focus on visual and textual general aptitude, not relying on rote learning. The authors establish baselines on the dataset using state-of-the-art LLMs and VLMs and propose four distinct modeling strategies to handle different modalities (text and images) in the dataset instances.

### Major Findings:

1. The NTSEBench dataset is introduced, consisting of 2,728 multiple-choice questions with 4,642 images across 26 categories, sourced from the NTSE examination in India.
2. The dataset focuses on visual and textual general aptitude questions that do not rely on rote learning.
3. Baselines are established on the dataset using state-of-the-art LLMs and VLMs.
4. Four distinct modeling strategies are proposed to handle different modalities (text and images) in the dataset instances.

### Analysis and Critique:

1. The paper does not provide a detailed analysis of the performance of the proposed modeling strategies on the NTSEBench dataset.
2. The paper does not discuss the limitations of the proposed dataset or the potential biases that may be present in the data.
3. The paper does not provide a comparison of the proposed dataset with other existing datasets for evaluating the cognitive reasoning skills of large models.
4. The paper does not discuss the potential applications of the proposed dataset in real-world scenarios.
5. The paper does not provide a detailed discussion of the potential ethical implications of using the proposed dataset for evaluating the cognitive reasoning skills of large models.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.10380v1](https://arxiv.org/abs/2407.10380v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.10380v1](https://browse.arxiv.org/html/2407.10380v1)       |
| Truncated       | False       |
| Word Count       | 7146       |