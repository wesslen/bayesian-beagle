
---
title: "ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain"
id: "2401.04898v1"
description: "New Chinese evaluation benchmark ANGO introduces keypoint categorization and quantifiable difficulty levels for better model analysis."
author: ['Bingchao Wang']
date: "2024-01-10"
image: "https://browse.arxiv.org/html/2401.04898v1/x1.png"
categories: ['production', 'architectures', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.04898v1/x1.png)

### Major Takeaways

1. **ANGO** introduces a novel Chinese benchmark for evaluating large language models (LLMs) in the domain of natural language processing (NLP), aiming to address issues with existing evaluation datasets and provide more precise guidance for model training.
2. It proposes a **Keypoint categorization standard** for multi-choice questions, allowing questions to correspond to multiple keypoints, enhancing interpretability of evaluation results.
3. ANGO's innovative features pose a stronger challenge to models and reveal more details in evaluation results compared to existing benchmarks, providing a more comprehensive and accurate multi-level, multi-perspective performance results for participating models.

### Introduction
The paper highlights the recent advancements in NLP, particularly the development of LLMs and the Transformer architecture. It discusses the evolution of benchmarks used to evaluate LLMs, from focusing on Natural Language Understanding (NLU) tasks to more specialized benchmarks with multiple-choice question formats.

### Challenges
Existing multi-choice question benchmarks suffer from issues such as single-subject categorization, immeasurable difficulty, and challenging updates due to potential sampling biases. Traditional benchmarks struggle to provide reliable measurements for modeling abilities across diverse disciplines.

### Contributions
ANGO introduces a novel Chinese benchmark for evaluation, adopting Keypoint categorization, a quantifiable difficulty standard, and specific strategies for sampling, aiming to minimize data leakage impact and provide comprehensive and accurate evaluation results for participating models.

### Data
The paper exclusively sources data from the Administrative Proficiency Test (AAT) used in the Chinese civil service examination. Data preprocessing involves removing duplicates, eliminating records containing pictures, and extracting unique formulas using OCR models.

### Sample Strategy
The paper presents a few-shot history sampling strategy and test set sampling method to ensure minimal information loss and achieve a balanced distribution of records for evaluation.

### Evaluation
ANGO employs accuracy as the ultimate metric for assessing model performance and introduces new evaluation metrics, such as Human Hit and Human Value, to capture human-like behavior and thinking patterns in model outputs.

### Experiment
The paper details the objectives, models used for evaluation, and results, showcasing performance at different keypoint, difficulty, and question levels.

### Related Work
Comparisons are made to existing benchmarks in both English and Chinese language domains, highlighting the distinct features of ANGO.

### Conclusion
ANGO is positioned as a valuable resource for fostering innovation and progress in NLP, with the potential to provide profound understanding of model capabilities and guidance for model design and improvement. The unique features of ANGO enable developers to better evaluate and enhance their own models.

### Critique
The paper does not thoroughly address potential limitations or biases in the development and implementation of ANGO. It would be beneficial to provide a more detailed discussion on the generalizability and potential biases within the benchmark. Additionally, insights on the scalability and applicability of ANGO to different LLMs could further strengthen the paper.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [http://arxiv.org/abs/2401.04898v1](http://arxiv.org/abs/2401.04898v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.04898v1](https://browse.arxiv.org/html/2401.04898v1)       |
| Truncated       | False       |
| Word Count       | 6974       |