
---
title: "Explicit and Implicit Large Language Model Personas Generate Opinions but Fail to Replicate Deeper Perceptions and Biases"
id: "2406.14462v1"
description: "LLMs with personas struggle to replicate human biases, lacking intrinsic human cognition despite reflecting speech patterns."
author: Salvatore Giorgi, Tingting Liu, Ankit Aich, Kelsey Isman, Garrick Sherman, Zachary Fried, Jo√£o Sedoc, Lyle H. Ungar, Brenda Curtis
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.14462v1/x1.png"
categories: ['prompt-engineering', 'production', 'social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.14462v1/x1.png)

### Summary:

The paper examines the role of prompting large language models (LLMs) with human-like personas and asking the models to answer as if they were a specific human. The personas are created explicitly, with exact demographics, political beliefs, and lived experiences, or implicitly via names prevalent in specific populations. The LLM personas are then evaluated via a subjective annotation task and a belief generation task, both of which are known to vary across human factors. The results show that LLM personas show mixed results when reproducing known human biases, but generally fail to demonstrate implicit biases. The paper concludes that LLMs lack the intrinsic cognitive mechanisms of human thought, while capturing the statistical patterns of how people speak, which may restrict their effectiveness in complex social science applications.

### Major Findings:

1. LLM personas show mixed results when reproducing known human biases, but generally fail to demonstrate implicit biases.
2. LLMs lack the intrinsic cognitive mechanisms of human thought, while capturing the statistical patterns of how people speak.
3. The effectiveness of LLMs in complex social science applications may be restricted due to their lack of intrinsic cognitive mechanisms.

### Analysis and Critique:

* The paper provides a valuable contribution to the understanding of the limitations of LLMs in replicating human biases and thought processes.
* The use of both explicit and implicit personas to evaluate LLMs is a novel approach that provides a more comprehensive understanding of their capabilities.
* The paper could benefit from a more in-depth analysis of the implications of these findings for the use of LLMs in social science applications.
* The paper does not discuss the potential for LLMs to be trained to better replicate human biases and thought processes, which could be a valuable area for future research.
* The paper does not discuss the potential for LLMs to be used in conjunction with human annotators to improve the accuracy and reliability of annotations.
* The paper does not discuss the potential for LLMs to be used to identify and mitigate biases in human annotations.
* The paper does not discuss the potential for LLMs to be used to identify and mitigate biases in the training data used to train the models.
* The paper does not discuss the potential for LLMs to be used to identify and mitigate biases in

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.14462v1](https://arxiv.org/abs/2406.14462v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.14462v1](https://browse.arxiv.org/html/2406.14462v1)       |
| Truncated       | False       |
| Word Count       | 6689       |