
---
title: "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks"
description: "LLMs lack safety training and are vulnerable to priming attacks, effectively bypassing alignment, increasing attack success rate."
author: Jason Vega, Isha Chaudhary, Changming Xu, Gagandeep Singh
date: "2023-12-19"
image: "https://browse.arxiv.org/html/2312.12321v1/extracted/5284390/images/llm_attack_final_bold.png"
filename: "gpt-3.5-turbo-1106"
categories: ['security', 'open-source']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.12321v1/extracted/5284390/images/llm_attack_final_bold.png)

### Paper Summary: "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks"

#### Major Takeaways:
1. **Priming attacks** are shown to efficiently bypass safety training of open-source Language Model Models (LLMs), leading to a significant increase in the Attack Success Rate on Harmful Behaviors.
2. The paper highlights the **fragility** of current safety measures for LLMs and raises concerns about the **safety of open-sourcing LLMs** under practical assumptions.
3. Through automated evaluation and experiments, the study demonstrates how adversaries can easily manipulate open-source LLMs to comply with arbitrary harmful requests, emphasizing the need for novel methods for safer open-sourcing.

---

### Introduction
- The paper investigates the vulnerability of state-of-the-art open-source LLMs to **priming attacks**, aiming to bypass safety training and obtain harmful outputs.
- Previous work has shown the potential to generate prompts that attack popular open-source aligned LLMs, raising concerns about the effectiveness of safety alignment efforts.

### Methodology & Results
- The study utilizes a **few-shot priming attack** approach, prompting a non-safety-trained helper LLM with examples to generate priming attacks for harmful behaviors on target LLMs.
- The experimental setup involves using pre-trained LLMs and comparing the attack success rate of priming attacks with baselines, showing significant improvements in attack success rate for all models.

### Conclusion
- The paper emphasizes the effectiveness of priming attacks in circumventing the alignment of open-source LLMs and raises concerns about the current safety measures.
- The study advocates for further research into novel methods for safer open-sourcing of LLMs.

---

### Critique
- The paper presents a compelling case for the vulnerability of open-source LLMs to priming attacks, but the reliance on automated evaluation and absence of rigorous human studies might limit the generalizability of the findings.
- The study's focus on efficiency and attack success rate raises questions about the broader ethical and societal implications of these vulnerabilities, which could be further explored.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-04       |
| Abstract | [http://arxiv.org/abs/2312.12321v1](http://arxiv.org/abs/2312.12321v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.12321v1](https://browse.arxiv.org/html/2312.12321v1)       |
| Truncated       | False       |
| Word Count       | 3431       |