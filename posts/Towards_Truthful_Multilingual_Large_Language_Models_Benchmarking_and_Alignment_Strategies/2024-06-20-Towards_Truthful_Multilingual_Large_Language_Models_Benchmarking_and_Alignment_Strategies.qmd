
---
title: "Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies"
id: "2406.14434v1"
description: "Research proposes benchmark and method to improve truthfulness and reduce language disparity in multilingual large language models."
author: Weihao Liu, Ning Wu, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.14434v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.14434v1/x1.png)

### Summary:

The paper titled "Towards Truthful Multilingual Large Language Models: Benchmarking and Alignment Strategies" focuses on the development of multilingual large language models (MLLMs) that can serve users worldwide. The authors construct a benchmark for truthfulness evaluation in multilingual scenarios and explore ways to align facts across languages to enhance the truthfulness of MLLMs. They propose Fact-aware Multilingual Selective Synergy (FaMSS) to optimize data allocation across a large number of languages and different data types. The experimental results demonstrate that their approach can effectively reduce the multilingual representation disparity and enhance the multilingual capabilities of LLMs.

### Major Findings:

1. The authors construct MTruthfulQA, a novel benchmark designed to evaluate the truthfulness of LLMs in multilingual scenarios, encompassing nine languages with the same set of questions to ensure equitable evaluation of multilingual capabilities.
2. The authors introduce a practical method for multilingual truthfulness alignment called FaMSS, which significantly boosts the truthfulness of LLMs across multiple languages.
3. The authors propose a simple Language Bias Probe to detect biases between languages and devise effective strategies for data allocation.
4. The authors systematically investigate how FaMSS helps multilingual truthfulness transfer among different languages and conclude that it is better not to mix data of all languages into one huge pile.

### Analysis and Critique:

The paper presents a significant contribution to the development of truthful multilingual large language models by constructing a benchmark for truthfulness evaluation and proposing a method for multilingual truthfulness alignment. However, the paper does not discuss the limitations of the proposed approach or any potential biases that may have been introduced during the development of the benchmark or the alignment strategies. Additionally, the paper does not provide any information on the computational resources required to implement the proposed methods, which could be a potential limitation for researchers with limited resources. Furthermore, the paper does not discuss any potential ethical considerations that may arise from the use of large language models in multilingual scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.14434v1](https://arxiv.org/abs/2406.14434v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.14434v1](https://browse.arxiv.org/html/2406.14434v1)       |
| Truncated       | False       |
| Word Count       | 6080       |