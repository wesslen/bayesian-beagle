
---
title: "E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for Large Language Models"
id: "2401.15927v1"
description: "TL;DR: E-EVAL is a benchmark for Chinese K-12 education LLMs, showing strengths and limitations."
author: Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng, Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, Min Yang
date: "2024-01-29"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'education', 'production', 'architectures', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
The E-EVAL benchmark is a comprehensive evaluation tool designed for the Chinese K-12 education field, consisting of 4,351 multiple-choice questions across primary, middle, and high school levels. The experiment section details the testing of 15 advanced language models on the E-EVAL benchmark, with a focus on zero-shot, few-shot-answer-only, and few-shot-chain-of-thought evaluation methods. The performance of various language models in solving math problems and Chinese language questions is discussed, highlighting the surprising phenomenon of models struggling with elementary school-level questions despite being capable of solving complex high school math problems. The section also presents the accuracy of different models under zero-shot and five-shot conditions, as well as the impact of Chain-of-Thought prompting on model performance. Additionally, the section explores the latest advancements in large language models in the education domain, including ChatGPT, GPT 4.0, ERNIE-Bot, ERNIE-Bot 4.0, Chinese LLaMA-2, Chinese Alpaca-2, and EduChat, and their potential impact on educational technology and learning outcomes.

### Major Findings:
1. The performance of language models varies across different educational subjects and prompt scenarios.
2. Models struggle with elementary school-level questions despite being capable of solving complex high school math problems.
3. Advanced large language models specifically tailored for the education domain have the potential to revolutionize the way educational content is delivered and assessed.

### Analysis and Critique:
The E-EVAL benchmark and the experimental results provide valuable insights into the performance of large language models in the Chinese K-12 education domain. However, the study also reveals the limitations of language models in solving elementary school-level questions and the need for tailored benchmarks to accurately assess their performance. Additionally, the potential impact of advanced large language models on educational technology and learning outcomes is highlighted, suggesting a promising future for these models in the education domain. Further research is needed to address the shortcomings and limitations identified in the study and to enhance the performance of language models in educational applications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.15927v1](https://arxiv.org/abs/2401.15927v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.15927v1](https://browse.arxiv.org/html/2401.15927v1)       |
| Truncated       | True       |
| Word Count       | 21938       |