
---
title: "RL-GPT: Integrating Reinforcement Learning and Code-as-policy"
id: "2402.19299v1"
description: "RL-GPT Framework Boosts LLM's Tool Use: Efficiently Obtains Diamonds in Minecraft."
author: Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19299v1/x1.png"
categories: ['production', 'programming', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19299v1/x1.png)

### **Summary:**

- RL-GPT is a hierarchical framework that combines Large Language Models (LLMs) and Reinforcement Learning (RL) to improve the efficiency of LLMs in handling complex, embodied tasks.
- The framework consists of a slow agent and a fast agent. The slow agent decomposes tasks into sub-actions and determines which actions can be directly coded, while the fast agent writes code and instantiates RL configurations for low-level execution.
- RL-GPT outperforms traditional RL methods and existing GPT agents in Minecraft tasks, such as rapidly obtaining diamonds within a single day using an RTX3090 and achieving state-of-the-art performance across all designated MineDojo tasks.

### Major Findings:

1. **Hierarchical Framework:** RL-GPT introduces a two-level hierarchical framework that

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19299v1](https://arxiv.org/abs/2402.19299v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19299v1](https://browse.arxiv.org/html/2402.19299v1)       |
| Truncated       | False       |
| Word Count       | 6483       |