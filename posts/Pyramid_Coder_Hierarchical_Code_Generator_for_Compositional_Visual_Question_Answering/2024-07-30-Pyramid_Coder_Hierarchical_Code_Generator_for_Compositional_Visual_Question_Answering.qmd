
---
title: "Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering"
id: "2407.20563v1"
description: "PyramidCoder: A Prompting Framework for Programmatic VQA Models Improves Accuracy on GQA, VQAv2, and NLVR2 Datasets."
author: Ruoyue Shen, Nakamasa Inoue, Koichi Shinoda
date: "2024-07-30"
image: "https://browse.arxiv.org/html/2407.20563v1/x1.png"
categories: ['programming', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.20563v1/x1.png)

### Summary:

The paper introduces PyramidCoder, a novel code generation framework for Programmatic Visual Question Answering (PVQA) models. PyramidCoder consists of three modules: a query rephraser, a code generator, and an answer aggregator. The framework utilizes a single frozen Large Language Model (LLM) and pre-defined prompts at each level, eliminating the need for additional training and ensuring flexibility across various LLM architectures. The proposed method outperforms the state-of-the-art CodeVQA model by at least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the NLVR2 dataset.

### Major Findings:

1. PyramidCoder is a hierarchical code generation framework for PVQA models, consisting of three modules: a query rephraser, a code generator, and an answer aggregator.
2. The framework utilizes a single frozen LLM and pre-defined prompts at each level, eliminating the need for additional training and ensuring flexibility across various LLM architectures.
3. PyramidCoder outperforms the state-of-the-art CodeVQA model by at least 0.5% on the GQA dataset, 1.4% on the VQAv2 dataset, and 2.9% on the NLVR2 dataset.

### Analysis and Critique:

The paper presents a promising approach to improving the performance of PVQA models. The use of a single frozen LLM and pre-defined prompts at each level simplifies the training process and ensures flexibility across various LLM architectures. However, the paper does not provide a detailed analysis of the computational efficiency of the proposed method. Additionally, the paper does not discuss the potential limitations of the proposed method, such as the reliance on the quality of the pre-defined prompts and the potential for overfitting to specific datasets. Further research is needed to address these limitations and evaluate the generalizability of the proposed method to other tasks and datasets.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.20563v1](https://arxiv.org/abs/2407.20563v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.20563v1](https://browse.arxiv.org/html/2407.20563v1)       |
| Truncated       | False       |
| Word Count       | 4584       |