
---
title: "Are Large Language Models More Honest in Their Probabilistic or Verbalized Confidence?"
id: "2408.09773v1"
description: "LLMs' probabilistic perception of knowledge boundaries is more accurate but needs calibration. Both perceptions perform better on less frequent questions. LLMs struggle to express internal confidence in natural language."
author: Shiyu Ni, Keping Bi, Lulu Yu, Jiafeng Guo
date: "2024-08-19"
image: "https://browse.arxiv.org/html/2408.09773v1/extracted/5799495/figs/thre.png"
categories: ['social-sciences', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.09773v1/extracted/5799495/figs/thre.png)

# Summary:

**Summary:**
The paper investigates the perception of factual knowledge boundaries in large language models (LLMs) through probabilistic and verbalized confidence. The authors compare the strengths and weaknesses of these two perceptions, analyze their performance under varying question frequencies, and evaluate the correlation between LLMs' probabilistic and verbalized confidence. The study reveals that LLMs' probabilistic perception is generally more accurate than verbalized perception but requires an in-domain validation set to adjust the confidence threshold. Both perceptions perform better on less frequent questions, with probabilistic perception outperforming verbalized perception by a greater margin. However, LLMs struggle to accurately express their internal confidence in natural language.

## Major Findings:
1. LLMs' probabilistic perception of their knowledge boundaries is more accurate than their verbalized perception, but it necessitates the use of an in-domain dataset to determine an appropriate confidence threshold for binarizing continuous probabilistic confidence.
2. Both LLMs' probabilistic perception and verbalized perception of their knowledge boundaries perform better on less common questions, indicating that LLMs' perception levels decline on more familiar questions.
3. LLMs' verbalized confidence is positively correlated with their probabilistic confidence, but the correlation is weak and varies significantly across different datasets, making it challenging for LLMs to accurately express their internal confidence in natural language.

## Analysis and Critique:
- The study provides valuable insights into the perception of factual knowledge boundaries in LLMs, highlighting the differences and connections between probabilistic and verbalized confidence.
- The authors' comprehensive analysis and comparison of LLMs' perceptions under varying question frequencies and their correlation with QA performance offer a deeper understanding of LLMs' capabilities and limitations.
- However, the study could benefit from exploring additional factors that may influence LLMs' perception of their knowledge boundaries, such as model size, training data, and prompting techniques.
- Additionally, the study could investigate the potential applications of LLMs' perception of their knowledge boundaries in real-world scenarios, such as safety and healthcare, to further demonstrate their practical implications.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.09773v1](https://arxiv.org/abs/2408.09773v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.09773v1](https://browse.arxiv.org/html/2408.09773v1)       |
| Truncated       | False       |
| Word Count       | 4282       |