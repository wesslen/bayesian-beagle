
---
title: "Large Language Models in Student Assessment: Comparing ChatGPT and Human Graders"
id: "2406.16510v1"
description: "GPT-4 aligns with human mean scores but lacks adaptability in grading nuanced criteria, highlighting AI's limitations in higher education."
author: Magnus Lundgren
date: "2024-06-24"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

# Summary:

The study investigates the effectiveness of large language models (LLMs) as tools for grading master-level student essays in political science. The research compares the accuracy of grades suggested by the GPT-4 model with those awarded by university teachers using a sample of 60 essays. The results indicate that while GPT-4 aligns with human grading standards on mean scores, it exhibits a risk-averse grading pattern and its interrater reliability with human raters is low. Furthermore, modifications in the grading instructions (prompt engineering) do not significantly alter AI performance, suggesting that GPT-4 primarily assesses generic essay characteristics such as language quality rather than adapting to nuanced grading criteria.

# Major Findings:

1. GPT-4's grading closely aligns with human graders in terms of mean scores, but it exhibits a conservative grading pattern, primarily assigning grades within a narrower middle range.
2. GPT-4 demonstrates relatively low interrater reliability with human graders, as evidenced by a Cohen’s kappa of 0.18 and a percent agreement of 35%, indicating significant room for improvement in AI grading alignment with human judgment.
3. Adjustments to the grading instructions via prompt engineering do not significantly influence GPT-4’s performance, suggesting that the AI predominantly evaluates essays based on generic characteristics such as language quality and structural coherence, rather than adapting to the detailed and nuanced assessment criteria embedded within different prompts.

# Analysis and Critique:

1. The absence of a human-to-human comparison for the same set of essays limits the understanding of how GPT-4’s interrater reliability stacks up against typical human variance in grading.
2. The study's empirical findings contribute to a growing literature on using AI for grading and evaluation in higher education, highlighting the need for further development to enhance its adaptability and sensitivity to specific educational assessment requirements.
3. The research underscores the challenge AI presently faces in grading complex, lengthy essay materials compared to simpler, more deterministic tasks like exam questions.
4. The consistent performance of GPT-4 across different prompts reveals a limitation in its ability to differentiate

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.16510v1](https://arxiv.org/abs/2406.16510v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.16510v1](https://browse.arxiv.org/html/2406.16510v1)       |
| Truncated       | False       |
| Word Count       | 9017       |