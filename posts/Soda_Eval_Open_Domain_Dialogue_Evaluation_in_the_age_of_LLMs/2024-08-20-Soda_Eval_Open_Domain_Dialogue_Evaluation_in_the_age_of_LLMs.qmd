
---
title: "Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs"
id: "2408.10902v1"
description: "TL;DR: Soda-Eval dataset reveals dialogue evaluation challenges for LLMs, fine-tuning improves performance."
author: John Mendon√ßa, Isabel Trancoso, Alon Lavie
date: "2024-08-20"
image: "https://browse.arxiv.org/html/2408.10902v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.10902v1/x1.png)

### Summary:

The paper introduces Soda-Eval, an annotated dataset based on Soda, a GPT-3.5 generated dialogue dataset, containing over 120K turn-level assessments across 10K dialogues. The annotations were generated by GPT-4. The authors conducted a qualitative analysis of the dialogues in Soda, revealing that most issues pertain to a lack of coherence, commonsense knowledge, and repetitions, while generation is almost always fluent and relevant to the prior context. The authors also highlight the limitations of current dialogue evaluation practices, which are still highly dependent on human evaluation and use a limited number of outdated benchmark datasets.

### Major Findings:

1. The qualitative analysis of Soda reveals consistent issues with coherence and commonsense knowledge, but generally fluent and relevant responses.
2. Soda-Eval, a novel dialogue evaluation benchmark, contains over 120k turn-level assessments obtained by GPT-4, targeting various quality aspects and validated by human annotators.
3. The performance of several open-access instruction-tuned LLMs as dialogue evaluators was evaluated using Soda-Eval, demonstrating that finetuning these models improves their performance.

### Analysis and Critique:

1. The paper highlights the limitations of current dialogue evaluation practices, which are still highly dependent on human evaluation and use a limited number of outdated benchmark datasets.
2. The authors propose a solution to this problem by introducing Soda-Eval, a large-scale open-domain dialogue quality annotation dataset that targets the responses provided by a large language model.
3. The authors acknowledge the limitations of their approach, including the use of a single LLM for annotation and the potential for cultural bias in the assessment of response quality.
4. The paper does not address the potential for bias in the selection of the Soda dataset or the potential for overfitting to the specific language model used for annotation.
5. The paper does not discuss the potential for using Soda-Eval to evaluate other types of dialogue systems, such as task-oriented or conversational question-answering systems.
6. The paper does not provide a detailed analysis of the performance of the evaluated LLMs, making it difficult to compare

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.10902v1](https://arxiv.org/abs/2408.10902v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.10902v1](https://browse.arxiv.org/html/2408.10902v1)       |
| Truncated       | False       |
| Word Count       | 9986       |