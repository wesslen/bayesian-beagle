
---
title: "SLM as Guardian: Pioneering AI Safety with Small Language Models"
id: "2405.19795v1"
description: "TL;DR: Smaller LLM used for harmful query detection and safeguard response, outperforming larger models."
author: Ohjoon Kwon, Donghyeon Jeon, Nayoung Choi, Gyu-Hwung Cho, Changbong Kim, Hyunwoo Lee, Inho Kang, Sun Kim, Taiwoo Park
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.19795v1/extracted/5631553/figures/euthanize.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19795v1/extracted/5631553/figures/euthanize.png)

### Summary:

The paper proposes a novel approach to leverage a smaller language model (sLLM) for both harmful query detection and safeguard response generation in designing LLM-based systems with safety requirements. The authors introduce their safety requirements and the taxonomy of harmfulness categories, and then propose a multi-task learning mechanism fusing the two tasks into a single model. The effectiveness of the approach is demonstrated through both quantitative and qualitative measures, showing the possibility to simultaneously achieve training cost reduction and attain accuracy in safeguards that surpasses LLMs with small language models.

### Major Findings:

1. The proposed approach demonstrates the effectiveness of using a smaller language model for both harmful query detection and safeguard response generation, providing on par or surpassing harmful query detection and safeguard response performance compared to publicly available LLMs.
2. The paper reveals a detailed walkthrough of practical techniques and experimental findings for better reproducibility, including an in-depth analysis of experiments conducted in Korean, a language with limited resources, with an intention of establishing a foundational framework for safety research in other low-resource languages.
3. The study presents a comprehensive set of analysis and taxonomy of harmful queries, and manually develops curated evaluation datasets and Korean translations of existing benchmarks. This work will be publicly disclosed to facilitate more active follow-up research.

### Analysis and Critique:

1. The paper's focus on using a smaller language model for both harmful query detection and safeguard response generation is a novel approach that has the potential to reduce training costs and improve accuracy in safeguards. However, the paper does not provide a detailed comparison with other methods that use larger language models for the same tasks.
2. The paper's focus on Korean as a low-resource language is a valuable contribution to the field, as it provides a foundation for safety research in other low-resource languages. However, the paper does not provide a detailed analysis of the performance of the proposed approach in other low-resource languages.
3. The paper's use of a multi-task learning mechanism to fuse the two tasks of harmful query detection and safeguard response generation into a single model is a promising approach. However, the paper does not provide a detailed analysis of the performance of the multi-task learning mechanism compared to other methods that use separate

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19795v1](https://arxiv.org/abs/2405.19795v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19795v1](https://browse.arxiv.org/html/2405.19795v1)       |
| Truncated       | False       |
| Word Count       | 6681       |