
---
title: "Multilingual Instruction Tuning With Just a Pinch of Multilinguality"
id: "2401.01854v1"
description: "Multilingual instruction-tuning enhances LLMs to follow instructions across languages with minimal multilingual examples."
author: ['Uri Shaham', 'Jonathan Herzig', 'Roee Aharoni', 'Idan Szpektor', 'Reut Tsarfaty', 'Matan Eyal']
date: "2024-01-03"
image: "https://browse.arxiv.org/html/2401.01854v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.01854v1/x1.png)

### Major Takeaways

- Multilingual instruction tuning of a multilingual large language model (LLM) with a small set of multilingual examples can significantly improve multilingual instruction-following capabilities, even for languages unseen during tuning.
- Training on a mixture of languages can lead to models with comparable or superior performance in several languages compared to models tuned on a single language, despite using fewer examples in those languages.
- Adding just a few languages to the instruction tuning set can improve cross-lingual generalization for languages unseen during tuning.

### Introduction
The paper investigates the impact of multilinguality during instruction tuning of a multilingual LLM on instruction-following across languages. The authors address the need for these models to operate on a wide range of languages to be globally applicable.

### Experimental Setup
The study uses open-ended instructions and a modern multilingual pretrained LLM, and evaluates instruction-following abilities per language in a controlled setting using generations of monolingually tuned models as a baseline.

- **Data:** Datasets of high-quality open-ended instructions are used, with translations created for 11 diverse languages using the Google Translate API.
- **Evaluation Method:** A side-by-side automatic evaluation protocol is used, where an LLM assesses two responses for a single prompt to identify the superior one.

### How Much Multilinguality Is Needed For Multilingual Instruction Tuning?
The paper examines the impact of multilingual data during instruction tuning and finds that monolingual instruction tuning yields multilingual abilities. A small number of multilingual examples and languages can improve instruction-following and cross-lingual generalization.

### Potential Factors of Cross-Lingual Transfer
The authors explore the impact of language similarity and fraction of data in pretraining on cross-lingual transfer efficacy and find weak correlations.

### Related Work
The study relates to previous work on cross-lingual transfer and multilingual instruction tuning, emphasizing its findings in the context of massively multilingual instruction-following LLMs.

### Critique
The study's reliance on translated data introduces potential noise, limiting the generalizability of its findings. Additionally, the study's experiments encompass a limited number of languages and LLMs, opening up future research opportunities for scalability and generalization.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [http://arxiv.org/abs/2401.01854v1](http://arxiv.org/abs/2401.01854v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.01854v1](https://browse.arxiv.org/html/2401.01854v1)       |
| Truncated       | False       |
| Word Count       | 8421       |