
---
title: "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward"
id: "2406.07327v1"
description: "DPO in LLMs: Examining 3D-properties, issues, and solutions for better alignment with human preference."
author: Yuzi Yan, Yibo Miao, Jialian Li, Yipin Zhang, Jian Xie, Zhijie Deng, Dong Yan
date: "2024-06-11"
image: "https://browse.arxiv.org/html/2406.07327v1/extracted/5659467/figure/main_text/toy_model_diagram.png"
categories: ['architectures', 'social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.07327v1/extracted/5659467/figure/main_text/toy_model_diagram.png)

### Summary:

This paper examines the empirical efficacy of Direct Preference Optimization (DPO) and compares it to the RLHF-PPO method for aligning large language models (LLMs) with human preferences. The authors identify three 3D-properties of DPO's learning outcomes: a drastic drop in the likelihood of rejected responses, degradation into LLM unlearning, and the dispersion effect on unseen responses. These findings are supported by experiments with both a toy model and practical LLMs on tasks such as mathematical problem-solving and instruction following. The authors propose regularization methods to mitigate the issues caused by 3D-properties and improve the training stability and final performance of DPO. They also investigate the impact of the distribution of the paired preference data on DPO's effectiveness.

### Major Findings:

1. DPO exhibits 3D-properties in its learning outcomes, including a drastic drop in the likelihood of rejected responses, degradation into LLM unlearning, and the dispersion effect on unseen responses.
2. The authors propose regularization methods, such as adjusting positive and negative weights adaptively and incorporating SFT loss, to improve the stability of DPO and mitigate the issues caused by 3D-properties.
3. The distribution of the paired preference data significantly influences DPO's effectiveness, with on-policy DPO exhibiting the best performance.

### Analysis and Critique:

1. The paper provides a comprehensive examination of DPO's empirical efficacy and a systematic comparison with RLHF-PPO, which is valuable for understanding the strengths and weaknesses of both methods.
2. The identification of 3D-properties in DPO's learning outcomes is an important contribution, as it helps explain some of the challenges associated with using DPO for aligning LLMs with human preferences.
3. The proposed regularization methods are a promising approach to improving the stability of DPO and mitigating the issues caused by 3D-properties. However, further research is needed to evaluate their effectiveness in practice.
4. The investigation into the impact of the distribution of the paired preference data on DPO's effectiveness is an interesting direction for future research.
5. One limitation of the paper is that

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.07327v1](https://arxiv.org/abs/2406.07327v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07327v1](https://browse.arxiv.org/html/2406.07327v1)       |
| Truncated       | False       |
| Word Count       | 8028       |