
---
title: "Application of Large Language Models in Automated Question Generation: A Case Study on ChatGLM's Structured Questions for National Teacher Certification Exams"
id: "2408.09982v1"
description: "ChatGLM generates realistic exam questions for NTCE, but needs optimization for diverse rating criteria."
author: Yanxin Chen, Ling He
date: "2024-08-19"
image: "../../../bayesian-beagle.png"
categories: ['hci', 'prompt-engineering', 'social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

- The study explores the application potential of the large language model (LLM) ChatGLM in generating structured questions for National Teacher Certification Exams (NTCE).
- Through prompt engineering, ChatGLM generated a series of simulated questions, which were compared with questions recollected from past examinees.
- The study aimed to validate the application potential of LLMs in the simulation of question generation within the educational field.
- The research results indicate that ChatGLM-generated questions exhibit a high level of rationality, scientificity, and practicality, similar to real exam questions across most evaluation criteria.
- However, the study also reveals limitations in the model's consideration of various rating criteria when generating questions, suggesting the need for further optimization and adjustment.

### Major Findings:

1. ChatGLM-generated questions demonstrate high accuracy and reliability in simulating structured questions for teacher qualification exams, with close average scores and similar standard deviations to real exam questions in terms of the rationality of question design, balance of difficulty, logical coherence, and coverage of knowledge and abilities.
2. A significant difference (p < 0.05, 3.51 > 3.40) was observed under Evaluation Criterion 2 between real exam questions and those generated by ChatGLM, which may indicate that further optimization is needed in ChatGLM's question generation for certain evaluation criteria.
3. LLMs like ChatGLM show significant potential in the automated generation of structured questions for teacher qualification exams, providing effective preparation materials for candidates and serving as a new automated tool for educational assessment.

### Analysis and Critique:

- The study's methodology and results are well-structured and provide valuable insights into the potential of LLMs in generating structured questions for NTCE.
- The limitations of the study include the need for further optimization in ChatGLM's question generation for certain evaluation criteria and the model's performance being constrained by the quality and quantity of training data.
- The study could have benefited from a more in-depth analysis of the specific areas where ChatGLM underperformed in generating practical questions compared to real exam questions.
- Future research should continue to explore optimization strategies for the model to enhance its performance in generating complex and specialized questions, thereby better

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.09982v1](https://arxiv.org/abs/2408.09982v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.09982v1](https://browse.arxiv.org/html/2408.09982v1)       |
| Truncated       | False       |
| Word Count       | 8304       |