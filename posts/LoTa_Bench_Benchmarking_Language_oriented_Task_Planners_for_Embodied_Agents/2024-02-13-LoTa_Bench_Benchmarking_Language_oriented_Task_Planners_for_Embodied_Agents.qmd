
---
title: "LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents"
id: "2402.08178v1"
description: "Benchmark system evaluates language-oriented task planners for home-service embodied agents, accelerating development."
author: Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang
date: "2024-02-13"
image: "https://browse.arxiv.org/html/2402.08178v1/x1.png"
categories: ['prompt-engineering', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.08178v1/x1.png)

### Summary:
- The article introduces a benchmark system, LoTa-Bench, for evaluating language-oriented task planners for home-service embodied agents.
- The benchmark system is tested on two pairs of datasets and simulators: ALFRED and AI2-THOR, and an extension of Watch-And-Help and VirtualHome.
- The proposed benchmark system is expected to accelerate the development of language-oriented task planners.

### Major Findings:
1. Large language models (LLMs) have shown remarkable generalization capabilities through zero-shot or few-shot prompting, leading to a transformative impact on task planning.
2. The proposed benchmark system, LoTa-Bench, has demonstrated the effectiveness of LLM-based task planners for home-service agents.
3. The study found that the number of in-context examples has a significant impact on the performance of LLM-based task planners.

### Analysis and Critique:
- The study provides valuable insights into the performance of LLM-based task planners, but there are limitations in the evaluation frameworks for LLM-based task planning.
- The article highlights the need for further research to address the limitations and potential biases in the evaluation of LLM-based task planners.
- The study also emphasizes the importance of integrating context into planning and the need for visual understanding for low-level actions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-14       |
| Abstract | [https://arxiv.org/abs/2402.08178v1](https://arxiv.org/abs/2402.08178v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08178v1](https://browse.arxiv.org/html/2402.08178v1)       |
| Truncated       | False       |
| Word Count       | 9962       |