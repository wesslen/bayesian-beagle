
---
title: "Towards Reliable and Factual Response Generation: Detecting Unanswerable Questions in Information-Seeking Conversations"
id: "2401.11452v1"
description: "Approach uses AI to find and summarize relevant passages, improving answer accuracy and trust in conversational AI models."
author: ['Weronika ≈Åajewska', 'Krisztian Balog']
date: "2024-01-21"
image: "https://browse.arxiv.org/html/2401.11452v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.11452v1/x1.png)

### Summary:
The article discusses the challenge of hallucinations in generative AI models and presents an approach for detecting unanswerable questions in conversational information-seeking conversations. The proposed method employs a two-step process to automatically assess if the answer to the user's question is present in the corpus. It involves identifying relevant passages, utilizing a sentence-level classifier to detect the answer's presence, and aggregating predictions at different levels. The authors develop a dataset based on the TREC CAsT benchmark, including answerability labels on sentence, passage, and ranking levels. Their proposed method represents a strong baseline and outperforms a state-of-the-art language model on the answerability prediction task.

### Major Findings:
1. The proposed method employs a two-step process involving a sentence-level classifier to detect answer presence, with strong performance compared to a state-of-the-art language model.
2. The authors develop a dataset based on the TREC CAsT benchmark, providing answerability labels on sentence, passage, and ranking levels for training and evaluation.
3. Data augmentation from the SQuAD 2.0 dataset improves performance on the sentence level, but does not effectively translate to effective passage or ranking-level answerability prediction.

### Analysis and Critique:
The article presents a novel approach to address the challenge of unanswerable questions in conversational information seeking, offering valuable contributions to the field. However, it is important to note that the simplification of answerability as a binary concept may not fully capture the nuanced nature of answerability, which may necessitate a more detailed approach in future research. Additionally, while the proposed method outperformed a state-of-the-art language model, the limitations of using a small dataset for training and the potential biases introduced by the dataset collection should be carefully considered. Further research and evaluation, especially on real-world conversational data, are necessary to validate the scalability and generalizability of the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-31       |
| Abstract | [http://arxiv.org/abs/2401.11452v1](http://arxiv.org/abs/2401.11452v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.11452v1](https://browse.arxiv.org/html/2401.11452v1)       |
| Truncated       | False       |
| Word Count       | 4529       |