
---
title: "RepoMasterEval: Evaluating Code Completion via Real-World Repositories"
id: "2408.03519v1"
description: "RepoMasterEval: A novel benchmark for code completion models, tested on real-world Python and TypeScript repositories, aligns with practical scenarios and improves test accuracy."
author: Qinyun Wu, Chao Peng, Pengfei Gao, Ruida Hu, Haoyu Gan, Bo Jiang, Jinhe Tang, Zhiwen Deng, Zhanming Guan, Cuiyun Gao, Xia Liu, Ping Yang
date: "2024-08-07"
image: "https://browse.arxiv.org/html/2408.03519v1/x1.png"
categories: ['robustness', 'programming', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.03519v1/x1.png)

### Summary:
RepoMasterEval is a novel benchmark for evaluating code completion models, constructed from real-world Python and TypeScript repositories. The benchmark aims to address the limitations of existing benchmarks, which focus on simple code generation tasks and lack practical scenarios. RepoMasterEval generates test cases using mutation testing and manual crafting to improve test accuracy. The empirical evaluation on six state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark and that RepoMasterEval can report differences in model performance in real-world scenarios.

### Major Findings:
1. RepoMasterEval is a novel benchmark for evaluating code completion models, constructed from real-world Python and TypeScript repositories.
2. The benchmark employs mutation testing and manual test case crafting to improve test accuracy.
3. The empirical evaluation on six state-of-the-art models shows that test argumentation is critical in improving the accuracy of the benchmark.
4. RepoMasterEval can report differences in model performance in real-world scenarios.
5. The deployment of RepoMasterEval in a collaborated company for one month revealed that the benchmark is useful to give accurate feedback during model training and the score is in high correlation with the modelâ€™s performance in practice.

### Analysis and Critique:
RepoMasterEval is a promising benchmark for evaluating code completion models in real-world scenarios. However, there are some potential limitations and areas for improvement.

1. Limited to Python and TypeScript: The benchmark is currently limited to Python and TypeScript repositories. Expanding the benchmark to include more programming languages would increase its applicability and generalizability.
2. Limited to GitHub repositories: The benchmark is constructed from GitHub repositories, which may not fully represent the diversity of real-world coding tasks. Including repositories from other sources could improve the representativeness of the benchmark.
3. Manual test case crafting: The manual crafting of test cases may introduce biases and limit the scalability of the benchmark. Automated test case generation techniques could be explored to minimize bias and improve scalability.
4. Limited evaluation of model performance: The benchmark primarily evaluates model performance based on test case pass

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.03519v1](https://arxiv.org/abs/2408.03519v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.03519v1](https://browse.arxiv.org/html/2408.03519v1)       |
| Truncated       | False       |
| Word Count       | 7803       |