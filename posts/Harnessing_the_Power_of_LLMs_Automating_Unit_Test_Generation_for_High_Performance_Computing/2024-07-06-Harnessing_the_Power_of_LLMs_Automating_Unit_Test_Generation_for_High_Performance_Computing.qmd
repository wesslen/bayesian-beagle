
---
title: "Harnessing the Power of LLMs: Automating Unit Test Generation for High-Performance Computing"
id: "2407.05202v1"
description: "LLMs like Davinci and ChatGPT can generate syntactically correct unit tests for parallel and high-performance software, but may have limitations like repetitive assertions and blank test cases."
author: Rabimba Karanjai, Aftab Hussain, Md Rafiqul Islam Rabin, Lei Xu, Weidong Shi, Mohammad Amin Alipour
date: "2024-07-06"
image: "https://browse.arxiv.org/html/2407.05202v1/x2.png"
categories: ['robustness', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.05202v1/x2.png)

### Summary:

This study explores the capabilities of two well-known generative models, Davinci (text-davinci-002) and ChatGPT (gpt-3.5-turbo), in crafting unit testing cases for parallel and high-performance software. The research focuses on the unique features of these software, including complex logic and sophisticated parallel processing techniques. The study examines the effectiveness of LLMs in creating unit testing cases for C++ parallel programs and assesses their performance on extensive OpenMP/MPI projects. The findings indicate that LLMs can create unit testing cases that are mostly syntactically correct and offer substantial coverage, while exhibiting some limitations like repetitive assertions and blank test cases.

### Major Findings:

1. LLMs can create unit testing cases that are mostly syntactically correct and offer substantial coverage for parallel and high-performance software.
2. LLMs exhibit some limitations in generating unit testing cases, such as repetitive assertions and blank test cases.
3. The study highlights the potential benefits of using LLMs for generating C++ parallel program test cases, including improved coverage and reduced test smells.

### Analysis and Critique:

The study provides valuable insights into the potential of LLMs for generating unit testing cases for parallel and high-performance software. However, there are several limitations and areas for improvement. The research is based on a limited number of projects, which may not be representative of the entire field. Additionally, the study does not address the potential biases or limitations of the LLMs themselves, which could impact the quality and effectiveness of the generated test cases. Further research is needed to explore the generalizability of these findings and to address the methodological issues and potential biases identified in this study.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.05202v1](https://arxiv.org/abs/2407.05202v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.05202v1](https://browse.arxiv.org/html/2407.05202v1)       |
| Truncated       | False       |
| Word Count       | 10114       |