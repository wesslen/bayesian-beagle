
---
title: "Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study"
id: "2401.09637v1"
description: "Tool uses large language models to simplify clinical notes, benefiting patient understanding, but may introduce errors requiring human oversight."
author: Niklas Mannhardt, Elizabeth Bondi-Kelly, Barbara Lam, Chloe O'Connell, Mercy Asiedu, Hussein Mozannar, Monica Agrawal, Alejandro Buendia, Tatiana Urman, Irbaz B. Riaz, Catherine E. Ricciardi, Marzyeh Ghassemi, David Sontag
date: "2024-01-17"
image: "https://browse.arxiv.org/html/2401.09637v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.09637v1/x1.png)

**Summary of the Article:**

This mixed-methods study explores the impact of using large language models (LLMs) to assist patients in reading clinical notes, particularly focusing on breast cancer patients. The study examines the effects of LLM augmentations on patient understanding and the potential negative impacts of such augmentations. The authors developed a patient-facing tool to simplify, extract information, and add context to clinical notes using the LLM GPT-4. They conducted a web-based survey and in-depth interviews with breast cancer patients, as well as an error analysis of the augmentations.

### Major Findings:
1. Augmentations using LLMs were associated with a significant increase in action understanding scores among participants.
2. In-depth interviews with breast cancer patients showed positive responses to augmentations, particularly definitions, although concerns about relying on LLMs were expressed.
3. While augmentations improved some readability metrics, errors were found to be more common in real donated notes than synthetic notes, highlighting the importance of carefully written clinical notes.

### Analysis and Critique:
This study demonstrates the potential of LLMs to improve patients' experience with clinical notes by enhancing their understanding and confidence. However, it reveals concerns about the potential for misleading errors and the reliance on LLMs. The research also acknowledges the importance of having a human in the loop to correct potential model errors. The limitations of the study include the potential bias in the participant pool and the need for further research to evaluate the patient experience with real notes only. Additionally, the study highlights the importance of preserving the voices of both clinicians and patients while improving documentation and communication in healthcare. Further work is needed to reduce errors and potential biases introduced by LLMs, as well as to explore custom augmentations tailored to patient readiness and education levels.

This critical analysis raises questions about the potential limitations and biases introduced by LLMs, the need for careful implementation to avoid misleading errors, and the importance of preserving the authentic voices of clinicians and patients in healthcare documentation and communication. The study also emphasizes the ongoing need for further research to refine the use of LLMs in improving patient experience with clinical notes.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-19       |
| Abstract | [http://arxiv.org/abs/2401.09637v1](http://arxiv.org/abs/2401.09637v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.09637v1](https://browse.arxiv.org/html/2401.09637v1)       |
| Truncated       | False       |
| Word Count       | 10745       |