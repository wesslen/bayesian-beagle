
---
title: "Automated Adversarial Discovery for Safety Classifiers"
id: "2406.17104v1"
description: "Automated methods struggle to find diverse, successful attacks on safety classifiers, revealing a need for improved adversarial discovery techniques."
author: Yash Kumar Lal, Preethi Lahoti, Aradhana Sinha, Yao Qin, Ananth Balashankar
date: "2024-06-24"
image: "https://browse.arxiv.org/html/2406.17104v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17104v1/x1.png)

### Summary:

This paper focuses on the task of automated adversarial discovery for safety classifiers, which aims to find new attacks along previously unseen harm dimensions that expose new weaknesses in the classifier. The authors propose an evaluation framework that balances adversarial success and dimensional diversity to measure progress on this task. They benchmark various methods to generate adversarial attacks that belong to previously unseen dimensions and find that current methods produce dimensionally diverse and adversarial attacks only 5% of the time. This shows that the task is challenging and improving on it can positively impact the adversarial robustness of safety classifiers.

### Major Findings:

1. The authors formalize the task of automatically generating new dimensions of adversarial attacks against safety classifiers and propose an evaluation framework based on adversarial success and LLM-based dimensional diversity.
2. For toxic comment generation, the authors benchmark various methods to generate adversarial attacks that belong to previously unseen dimensions. At best, current methods produce dimensionally diverse and adversarial attacks 5% of the time.
3. The authors find that their task is challenging, and improving on it can positively impact the adversarial robustness of safety classifiers.

### Analysis and Critique:

1. The paper provides a comprehensive overview of the task of automated adversarial discovery for safety classifiers and proposes an evaluation framework that balances adversarial success and dimensional diversity.
2. The authors benchmark various methods to generate adversarial attacks that belong to previously unseen dimensions and find that current methods produce dimensionally diverse and adversarial attacks only 5% of the time. This highlights the need for further research in this area.
3. The paper does not discuss the limitations of the proposed evaluation framework or the potential biases that may be introduced by the use of LLMs for generating adversarial attacks.
4. The paper does not provide a detailed analysis of the strengths and weaknesses of the different methods used for generating adversarial attacks.
5. The paper does not discuss the potential ethical implications of using LLMs to generate adversarial attacks, such as the risk of generating harmful or offensive content.

Overall, the paper provides a valuable contribution to the field of automated adversarial discovery for safety classifiers. However, further research is needed to address the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17104v1](https://arxiv.org/abs/2406.17104v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17104v1](https://browse.arxiv.org/html/2406.17104v1)       |
| Truncated       | False       |
| Word Count       | 6260       |