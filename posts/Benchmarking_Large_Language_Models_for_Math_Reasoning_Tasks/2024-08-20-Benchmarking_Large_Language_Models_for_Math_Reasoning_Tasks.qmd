
---
title: "Benchmarking Large Language Models for Math Reasoning Tasks"
id: "2408.10839v1"
description: "Larger LLMs excel in math reasoning, while smaller models benefit from specific prompting strategies. Benchmarking code is open-sourced."
author: Kathrin Seßler, Yao Rong, Emek Gözlüklü, Enkelejda Kasneci
date: "2024-08-20"
image: "https://browse.arxiv.org/html/2408.10839v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.10839v1/x1.png)

### Summary:

This study presents a benchmark that compares seven state-of-the-art in-context learning algorithms for mathematical problem solving across five widely used mathematical datasets on four powerful foundation models. The goal is to explore the trade-off between efficiency and performance, highlighting the practical applications of LLMs for mathematical reasoning. The results indicate that larger foundation models like GPT-4o and LLaMA 3-70B can solve mathematical reasoning independently from the concrete prompting strategy, while for smaller models the in-context learning approach significantly influences the performance. Moreover, the optimal prompt depends on the chosen foundation model.

### Major Findings:

1. Larger foundation models like GPT-4o and LLaMA 3-70B can solve mathematical reasoning independently from the concrete prompting strategy.
2. For smaller models, the in-context learning approach significantly influences the performance.
3. The optimal prompt depends on the chosen foundation model.

### Analysis and Critique:

The study provides a comprehensive comparison of different in-context learning algorithms for mathematical problem solving. However, the following limitations and potential areas for improvement should be considered:

1. The study focuses on four foundation models, and while they are powerful, there may be other models that could provide different results.
2. The benchmark does not consider the impact of different prompting strategies on the performance of the models.
3. The study does not address the potential biases or limitations of the foundation models themselves, which could impact the results.
4. The study does not discuss the potential for overfitting to specific datasets or the generalizability of the results to other mathematical problem-solving tasks.
5. The study does not explore the potential for combining different in-context learning algorithms or foundation models to improve performance.

Overall, the study provides valuable insights into the performance of different in-context learning algorithms for mathematical problem solving. However, further research is needed to address the limitations and explore the potential for improving performance through different prompting strategies, foundation models, and combinations of algorithms.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.10839v1](https://arxiv.org/abs/2408.10839v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.10839v1](https://browse.arxiv.org/html/2408.10839v1)       |
| Truncated       | False       |
| Word Count       | 8337       |