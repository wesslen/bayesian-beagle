
---
title: "Measuring Impacts of Poisoning on Model Parameters and Neuron Activations: A Case Study of Poisoning CodeBERT"
id: "2402.12936v1"
description: "TL;DR: Analyzing model parameters to detect backdoor signals in code models."
author: Aftab Hussain, Md Rafiqul Islam Rabin, Navid Ayoobi, Mohammad Amin Alipour
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.12936v1/extracted/5420119/results/distribution_codebert-base_layer_11_weight.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12936v1/extracted/5420119/results/distribution_codebert-base_layer_11_weight.png)

### **Summary:**
- Large language models (LLMs) have revolutionized software development practices, but concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans.
- This paper focuses on analyzing the model parameters to detect potential backdoor signals in code models, specifically examining attention weights and biases, activation values, and context embeddings of the clean and poisoned CodeBERT models.
- The results suggest noticeable patterns in activation values and context embeddings of poisoned samples for the poisoned CodeBERT model, but attention weights and biases do not show significant differences.

### Major Findings:
1. Large language models (LLMs) have remarkable capabilities in software development practices, but concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans.
2. The analysis of the model parameters of the clean and poisoned CodeBERT models revealed noticeable patterns in activation values and context embeddings of poisoned samples for the poisoned CodeBERT model, but no significant differences in attention weights and biases.
3. The study contributes to ongoing efforts in white-box detection of backdoor signals in LLMs of code through the analysis of parameters and activations.

### Analysis and Critique:
- The study provides valuable insights into the potential presence of backdoor signals in large language models of code, but it is limited to a specific model (CodeBERT) and task (defect detection).
- The findings are based on a case study and may not be generalizable to other large language models or tasks.
- Further research is needed to extend the analysis to a broader spectrum of models and tasks to draw more general conclusions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.12936v1](https://arxiv.org/abs/2402.12936v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12936v1](https://browse.arxiv.org/html/2402.12936v1)       |
| Truncated       | False       |
| Word Count       | 3476       |