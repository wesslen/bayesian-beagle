
---
title: "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation"
id: "2402.13211v1"
description: "ESConv dataset reveals LLMs struggle with emotional support, need external assistance for improvement."
author: Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13211v1/extracted/5420974/figure/llms_motivation.png"
categories: ['social-sciences', 'hci', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13211v1/extracted/5420974/figure/llms_motivation.png)

### Summary:
The article investigates the ability of large language models (LLMs) to provide emotional support in daily conversations. The study focuses on the impact of inherent preference in LLMs on providing emotional support and explores methods to mitigate this preference bias. The findings emphasize that low preference for specific strategies hinders the progress of emotional support, external assistance helps reduce preference bias, and LLMs alone cannot become good emotional supporters. The study concludes that mitigating preference bias is crucial for decreasing the proportion of poor-quality responses and, consequently, for effective emotional support.

### Major Findings:
1. LLMs exhibit challenges in selecting the correct strategy and a notable preference for a specific strategy.
2. Low preference for specific strategies hinders the progress of emotional support.
3. External assistance helps reduce preference bias.

### Analysis and Critique:
- The study provides valuable insights into the challenges and potential solutions for LLMs to serve as proficient emotional supporters.
- The findings are based on a comprehensive analysis of LLMs' proficiency and preference for different strategies, providing a nuanced understanding of their performance.
- The study, however, has limitations in terms of the potential biases induced by few-shot learning and the impact of the number of examples on preference bias.
- The article also highlights the ethical considerations and potential risks associated with using LLMs as emotional support systems, emphasizing the need for further research to construct safer systems.

Overall, the article provides a comprehensive analysis of LLMs' performance in providing emotional support and offers valuable insights for future research in this domain. The critical analysis raises awareness of the limitations and ethical considerations associated with using LLMs for emotional support.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.13211v1](https://arxiv.org/abs/2402.13211v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13211v1](https://browse.arxiv.org/html/2402.13211v1)       |
| Truncated       | False       |
| Word Count       | 10983       |