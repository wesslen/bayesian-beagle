
---
title: "Can GPT Redefine Medical Understanding? Evaluating GPT on Biomedical Machine Reading Comprehension"
id: "2405.18682v1"
description: "LLMs, like GPT, excel in closed-book biomedical MRC with a novel prompting method, outperforming supervised models and setting new SoTA results."
author: Shubham Vatsal, Ayush Singh
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.18682v1/extracted/5627950/IRAG.png"
categories: ['prompt-engineering', 'social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.18682v1/extracted/5627950/IRAG.png)

### Summary:

This study evaluates the performance of GPT, a large language model (LLM), on four closed-book biomedical machine reading comprehension (MRC) benchmarks. The authors experiment with different prompting techniques, including a novel method called Implicit Retrieval Augmented Generation (RAG), which alleviates the need for vector databases to retrieve important chunks in traditional RAG setups. The results show that the new prompting technique achieves the best performance in two out of four datasets and ranks second in the rest. The study also demonstrates that modern-day LLMs like GPT can outperform supervised models, leading to new state-of-the-art (SoTA) results on two of the benchmarks.

### Major Findings:

1. GPT, when evaluated on four closed-book biomedical MRC benchmarks, achieves new SoTA results using different prompting techniques, including the novel Implicit RAG method.
2. The Implicit RAG technique outperforms traditional RAG setups by eliminating the need for vector databases to retrieve important chunks, emphasizing that LLMs are capable of performing retrieval in one go.
3. Modern-day LLMs like GPT can outperform supervised models, even in a zero-shot setting, leading to new SoTA results on two of the benchmarks.

### Analysis and Critique:

1. The study focuses on evaluating GPT on closed-book biomedical MRC benchmarks, which is a valuable contribution to the field. However, it would be interesting to see a comparison between GPT's performance on closed-book and open-book settings.
2. The authors introduce a novel prompting technique, Implicit RAG, which shows promising results. However, further research is needed to understand its limitations and potential applications in other domains.
3. The study compares GPT's performance to supervised models, but it does not provide a detailed comparison with other LLMs. It would be beneficial to see how GPT compares to other state-of-the-art LLMs in the biomedical domain.
4. The authors mention that machine evaluation is a good measure of performance, but it falls short when evaluating artificially generated text. They report qualitative preference metrics by human experts on

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.18682v1](https://arxiv.org/abs/2405.18682v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.18682v1](https://browse.arxiv.org/html/2405.18682v1)       |
| Truncated       | False       |
| Word Count       | 6162       |