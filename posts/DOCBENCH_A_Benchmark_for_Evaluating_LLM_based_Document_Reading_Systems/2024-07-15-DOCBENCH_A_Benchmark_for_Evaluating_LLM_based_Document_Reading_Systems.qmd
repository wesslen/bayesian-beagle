
---
title: "DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems"
id: "2407.10701v1"
description: "TL;DR: DocBench is a new benchmark for evaluating LLM-based document reading systems, featuring 229 real documents and 1,102 questions across five domains."
author: Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, Dong Yu
date: "2024-07-15"
image: "https://browse.arxiv.org/html/2407.10701v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.10701v1/x1.png)

### Summary:

The paper introduces DocBench, a new benchmark designed to evaluate LLM-based document reading systems. The benchmark includes 229 real documents and 1,102 questions, spanning across five different domains and four major types of questions. The authors evaluate both proprietary LLM-based systems and a parse-then-read pipeline employing open-source LLMs. The evaluations reveal noticeable gaps between existing LLM-based document reading systems and human performance, underscoring the challenges of developing proficient systems.

### Major Findings:

1. DocBench is a novel benchmark specifically designed to evaluate LLM-based document reading systems, featuring 229 real-world documents and 1,102 questions spanning 5 diverse domains: Academia, Finance, Government, Laws, and News.
2. The benchmark involves 4 question categories, including text-only, multi-modal (i.e., tables and figures), meta-data, and unanswerable, ensuring comprehensive coverage of various document reading capabilities.
3. The evaluations reveal noticeable gaps between existing LLM-based document reading systems and human performance, underscoring the challenges of developing proficient systems.

### Analysis and Critique:

1. The paper does not provide a detailed comparison of the performance of different LLM-based document reading systems, making it difficult to identify the strengths and weaknesses of each system.
2. The paper does not discuss the potential limitations of the DocBench benchmark, such as the representativeness of the documents and questions included in the benchmark.
3. The paper does not discuss the potential biases in the evaluation process, such as the potential biases of the human annotators and the potential biases of the evaluation metrics used.
4. The paper does not discuss the potential implications of the findings for the development of LLM-based document reading systems, such as the potential directions for future research and development.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.10701v1](https://arxiv.org/abs/2407.10701v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.10701v1](https://browse.arxiv.org/html/2407.10701v1)       |
| Truncated       | False       |
| Word Count       | 5979       |