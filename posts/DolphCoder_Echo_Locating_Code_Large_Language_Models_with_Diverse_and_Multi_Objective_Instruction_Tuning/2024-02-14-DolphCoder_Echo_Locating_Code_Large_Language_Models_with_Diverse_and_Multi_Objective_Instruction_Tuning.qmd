
---
title: "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning"
id: "2402.09136v1"
description: "DolphCoder improves code generation with diverse instructions and self-evaluation, outperforming benchmarks."
author: Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, Weiran Xu
date: "2024-02-14"
image: "../../img/2402.09136v1/image_1.png"
categories: ['programming', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.09136v1/image_1.png)

### **Summary:**
- Code Large Language Models (Code LLMs) have shown exceptional performance in code-related tasks.
- In this paper, the authors introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation, which achieves superior performance on the HumanEval and MBPP benchmarks.
- The key findings are that augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs, and improving one’s ability to evaluate the correctness of code solutions also enhances their ability to create it.

### Major Findings:
1. Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs.
2. Improving one’s ability to evaluate the correctness of code solutions also enhances their ability to create it.

### Analysis and Critique:
- The article provides valuable insights into the improvement of code generation performance through diverse instruction tuning and self-evaluating models.
- However, the reliance on GPT-4 for code evaluation raises concerns about the accuracy of the evaluation, as GPT-4 may not be able to perfectly perform code evaluation.
- The limitations of the study include the lack of exploration on larger foundation models and the need for more precise and open-source evaluation models.
- Future work should focus on improving automatic code evaluation and exploring the ethical and societal implications of using large language models for code generation.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.09136v1](https://arxiv.org/abs/2402.09136v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09136v1](https://browse.arxiv.org/html/2402.09136v1)       |
| Truncated       | False       |
| Word Count       | 14947       |