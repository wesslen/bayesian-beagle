
---
title: "Can ChatGPT Rival Neural Machine Translation? A Comparative Study"
id: "2401.05176v1"
description: "Comparison of ChatGPT and NMT in translating Chinese diplomatic texts, showing potential for ChatGPT with proper prompts."
author: Zhaokun Jiang, Ziyin Zhang
date: "2024-01-10"
image: "https://browse.arxiv.org/html/2401.05176v1/extracted/5340164/figures/error-penalty.png"
categories: ['architectures', 'social-sciences', 'prompt-engineering', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.05176v1/extracted/5340164/figures/error-penalty.png)

### Major Takeaways:

1. **ChatGPT** exhibits strong capabilities in translating Chinese diplomatic texts into English, particularly excelling under human evaluation and semantic-aware automatic evaluation.

2. Providing **example or contextual information** to ChatGPT notably improves its translation quality, highlighting the significance of tailored prompts.

3. **Automated metrics** fail to fully distinguish high-quality and lower-quality translations.

### Introduction
- Neural machine translation (NMT) has been extensively studied and has shown satisfying quality in various text types.
- Large language models (LLMs) like **ChatGPT** are revolutionizing translation technology, with recent studies showing their potential to surpass mainstream NMT engines.

### Related Work
- Prompt engineering has been explored to improve LLM translation performance, while previous research has emphasized both automated metrics and human evaluation in translation quality assessment (TQA).
- Comparative studies of LLM translations and NMT have shown LLMs' strong capacity in translating high-resource languages and specific text types, but competence in translating middle and low-resource languages is not fully known.

### Methodology
- A **corpus** of Chinese diplomatic texts translated into English was used, with ChatGPT and NMT systems (Microsoft Translate, Google Translate, and DeepL) evaluated using four automated metrics and human evaluation based on error-typology and analytic rubrics score.

### Results and Analysis
- **Automated metrics** demonstrated ChatGPT's strong semantic understanding and capability despite deviations from reference translations, while **human evaluation** indicated its variability under different prompting conditions and its superiority over NMT systems.
-**Correlation** between automated metrics and human evaluation was weak and non-significant, suggesting the divergence in translation quality assessment methods.

### Conclusion
- Limitations of traditional metrics for translation quality assessment were highlighted, emphasizing the need for more nuanced evaluation metrics that consider cultural aspects and contextual appropriateness.
- Tailoring prompts to guide the generation process and enhance the translation quality of LLMs like ChatGPT was deemed crucial based on the study's findings.

### Critique
The paper provides valuable insights into the translation capabilities of ChatGPT and NMT systems but has limitations such as reliance on publicly available datasets and small sample sizes for human evaluation, which may not fully capture the diversity of translation challenges. Additionally, the study offers prompts to ChatGPT without exploring the potential biases introduced, and the paper could benefit from a more detailed discussion on how to overcome the limitations of automated metrics for translation quality assessment.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-11       |
| Abstract | [http://arxiv.org/abs/2401.05176v1](http://arxiv.org/abs/2401.05176v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.05176v1](https://browse.arxiv.org/html/2401.05176v1)       |
| Truncated       | False       |
| Word Count       | 8993       |