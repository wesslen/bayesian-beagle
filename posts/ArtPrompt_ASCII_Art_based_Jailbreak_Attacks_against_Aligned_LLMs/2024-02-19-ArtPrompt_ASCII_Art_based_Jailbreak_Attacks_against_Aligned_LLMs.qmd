
---
title: "ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs"
id: "2402.11753v1"
description: "LLMs vulnerable to ASCII art-based jailbreak attack, bypassing safety measures."
author: Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran
date: "2024-02-19"
image: "../../img/2402.11753v1/image_1.png"
categories: ['robustness', 'prompt-engineering', 'security']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11753v1/image_1.png)

### Summary:
The article discusses the development of the ArtPrompt jailbreak attack, which exploits vulnerabilities in large language models (LLMs) to bypass safety measures and provoke unintended and unsafe behaviors. The attack leverages the weaknesses of LLMs in recognizing certain words, substituting them with ASCII art to increase the probability of bypassing safety measures. The authors propose a benchmark, Vision-in-Text Challenge (VITC), to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. Experimental evaluations demonstrate that ArtPrompt is effective against various victim LLMs, outperforming existing jailbreak attacks in terms of helpful rate, harmfulness score, and attack success rate. The section also provides detailed setups for baseline jailbreak attacks, defense settings, and experimental results on font recognition by different LLMs.

### Major Findings:
1. The ArtPrompt jailbreak attack effectively bypasses safety measures and induces undesired behaviors from LLMs.
2. Experimental evaluations demonstrate the superiority of ArtPrompt over other jailbreak attacks in terms of helpful rate, harmfulness score, and attack success rate.
3. The choice of font and arrangement of ASCII art significantly impact the effectiveness of the ArtPrompt attack.

### Analysis and Critique:
- The vulnerabilities of LLMs exposed by the ArtPrompt attack have significant implications for the safety and security of LLMs in real-world applications.
- The study underscores the need for robust defenses to mitigate the impact of jailbreak attacks on LLMs.
- The findings contribute to understanding the vulnerabilities and defenses of large language models against jailbreaking attacks.
- The ethical statement emphasizes the potential misuse of the vulnerabilities and the responsible dissemination of the code and prompts used in the experiments to the community for further research and red-teaming of LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.11753v1](https://arxiv.org/abs/2402.11753v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11753v1](https://browse.arxiv.org/html/2402.11753v1)       |
| Truncated       | True       |
| Word Count       | 17374       |