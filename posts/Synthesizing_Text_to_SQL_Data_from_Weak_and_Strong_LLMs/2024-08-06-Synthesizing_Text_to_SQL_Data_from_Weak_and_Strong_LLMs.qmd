
---
title: "Synthesizing Text-to-SQL Data from Weak and Strong LLMs"
id: "2408.03256v1"
description: "Synthetic data approach bridges gap between open-source and closed-source LLMs in text-to-SQL tasks, introducing Sense, a specialized model with state-of-the-art results."
author: Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, Chang Zhou
date: "2024-08-06"
image: "https://browse.arxiv.org/html/2408.03256v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.03256v1/x1.png)

### Summary:

The paper introduces a synthetic data approach that combines strong data generated by larger, more potent models with weak data produced by smaller, less well-aligned models. This approach aims to improve domain generalization in text-to-SQL models and investigate the potential of weak data supervision through preference learning. The synthetic data approach is also used for instruction tuning on open-source LLMs, resulting in a specialized text-to-SQL model called Sense. The effectiveness of Sense is demonstrated by achieving state-of-the-art results on the SPIDER and BIRD benchmarks, thereby mitigating the performance disparity between open-source models and methods derived from closed-source models.

### Major Findings:

1. The paper proposes a synthetic data approach that uses strong models to generate strong data to enhance data diversity and employs weak models to generate weak data combined with an executor to learn from feedback.
2. Extensive experiments show the effectiveness of Sense, achieving state-of-the-art performance, even competing with methods based on GPT-4.
3. The paper contributes to the advancement of the text-to-SQL community by making the data and models publicly available.

### Analysis and Critique:

1. The paper does not provide a detailed comparison of the performance of Sense with other state-of-the-art text-to-SQL models.
2. The paper does not discuss the limitations of the proposed approach, such as the potential for overfitting to the synthetic data or the need for large amounts of data to train the weak models.
3. The paper does not provide a detailed analysis of the impact of the synthetic data on the performance of the text-to-SQL models.
4. The paper does not discuss the potential for bias in the synthetic data, which could impact the performance of the text-to-SQL models.
5. The paper does not provide a detailed analysis of the computational cost of generating the synthetic data and training the text-to-SQL models.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.03256v1](https://arxiv.org/abs/2408.03256v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.03256v1](https://browse.arxiv.org/html/2408.03256v1)       |
| Truncated       | False       |
| Word Count       | 5683       |