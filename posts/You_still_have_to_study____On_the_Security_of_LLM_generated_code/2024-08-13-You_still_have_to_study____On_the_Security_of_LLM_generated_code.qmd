
---
title: "You still have to study -- On the Security of LLM generated code"
id: "2408.07106v1"
description: "AI-generated code often insecure; improves with better prompts and manual guidance."
author: Stefan Goetz, Andreas Schaad
date: "2024-08-13"
image: "../../img/2408.07106v1/image_1.png"
categories: ['security', 'education', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](../../img/2408.07106v1/image_1.png)

Summary:
This paper analyzes the security of code generated by four major Large Language Models (LLMs) in Python and JavaScript, using the MITRE CWE catalogue as a security definition. The results show that different prompting techniques can lead to up to 65% of the generated code being deemed insecure by a trained security engineer. However, with increasing manual guidance from a skilled engineer, almost all analyzed LLMs can generate code that is close to 100% secure.

Major Findings:
1. The study found that using different prompting techniques, some LLMs initially generate up to 65% code which is deemed insecure by a trained security engineer.
2. However, almost all analyzed LLMs will eventually generate code that is close to 100% secure with increasing manual guidance of a skilled engineer.
3. The study also found that the design of a case study showing an interactive, multi-user application accessible over a REST API, and the implementation of this application in Python and JavaScript using different prompting techniques for the current versions of ChatGPT, Copilot, CodeLLama, and CodeWhisperer Large Language Models (LLMs) can be used to evaluate the security of LLM-generated code.

Analysis and Critique:
The paper provides a comprehensive analysis of the security of LLM-generated code, using a well-structured and coherent approach. The use of the MITRE CWE catalogue as a security definition is a strength of the study, as it provides a standardized framework for evaluating the security of the generated code. However, the study does not provide a detailed analysis of the limitations or potential biases of the LLMs used in the study. Additionally, the study does not discuss the potential impact of the quality of the training data on the security of the generated code. Further research is needed to address these limitations and to evaluate the security of LLM-generated code in other programming languages and contexts.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.07106v1](https://arxiv.org/abs/2408.07106v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.07106v1](https://browse.arxiv.org/html/2408.07106v1)       |
| Truncated       | False       |
| Word Count       | 13606       |