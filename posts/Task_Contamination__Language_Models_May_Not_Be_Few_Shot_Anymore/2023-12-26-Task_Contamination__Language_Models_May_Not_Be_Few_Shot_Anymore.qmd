
---
title: "Task Contamination: Language Models May Not Be Few-Shot Anymore"
description: "Large language models (LLMs) excel in zero-shot and few-shot tasks, but their success may be affected by task contamination. This paper investigates the impact of task contamination on LLMs' performance over time."
author: "gpt-3.5-turbo-1106"
date: "2023-12-26"
link: "https://browse.arxiv.org/html/2312.16337v1"
image: "https://browse.arxiv.org/html/2312.16337v1/x1.png"
categories: ['prompt engineering']
file-modified: 2024-01-02
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.16337v1/x1.png)

# Task Contamination: Language Models May Not Be Few-Shot Anymore

## Major Findings
- Large language models (LLMs) demonstrate **inflated performance** in zero-shot or few-shot evaluation due to **task contamination**.
- Closed-source models may not be trustworthy baselines in these settings, especially those including **instruction fine-tuning** or **reinforcement learning with human feedback (RLHF)**.
- Models show little to no statistically significant improvements over **majority baselines** for tasks without demonstrated possibility of task contamination.
  
## Introduction
Large language models (LLMs), such as GPT-3 series models, have garnered attention for their impressive performance in zero-shot and few-shot settings. However, concerns about **data contamination** have been raised, particularly related to **task contamination** â€“ the inclusion of task training examples in the pre-training data, thereby affecting the zero-shot and few-shot performance.

## Overview
- Four methods of measuring task contamination:
  - **Training data inspection**: Search through the training data to find task training examples.
  - **Task example extraction**: Extract task examples from an existing model.
  - **Membership inference**: Check if the model generated content for an input instance exactly matches the original dataset.
  - **Chronological analysis**: Measure performance on a dataset with a known release date and check for evidence of contamination.

## Models and Datasets
- Experimented with 12 models, including both proprietary GPT-3 series models and open models with free access to their weights.
- Datasets were divided into pre-2021 and post-2021 categories for analyzing zero-shot or few-shot performance difference.

## Chronological Analysis
- Analyzed performance on datasets released before and after the model training data collection date.
- GPT-3 series models demonstrated a significant increase in performance on earlier datasets, indicating possible task contamination.

## Training Data Inspection
- Conducted on two instruction fine-tuned open LLMs (Alpaca and Vicuna) for various classification tasks.
- Performance improved for models with more task-specific training examples, indicating **contaminated performance**.

## Task Example Extraction
- Attempted to extract task examples from the LLM.
- GPT-3 series models, starting from davinci-001, were able to generate task-specific training examples, providing evidence of task contamination.

## LLM Performance on Tasks With No Contamination
- Rarely show statistically significant improvements over majority baselines for tasks without demonstrated possibility of task contamination.

## Membership Inference
- Strongly indicates increased contamination is related to increased performance for the semantic parsing task.

## Critique
- **Low recall** for methods detecting task contamination.
- Difficulty in analyzing task contamination especially for models without instruction tuning.

In conclusion, the paper reveals evidence of task contamination for LLMs and recommends additional research on this issue.



## Appendix

|          |          |
|----------|----------|
| Date Generated     | 2024-01-02       |
| HTML     | [https://browse.arxiv.org/html/2312.16337v1](https://browse.arxiv.org/html/2312.16337v1)       |
| Truncated       | False       |
| Word Count       | 5492       |