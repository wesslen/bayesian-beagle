
---
title: "RePLan: Robotic Replanning with Perception and Language Models"
id: "2401.04157v1"
description: "Advancements in language models help robots plan and execute tasks, with a new framework enabling real-time replanning for long-horizon tasks."
author: ['Marta Skreta', 'Zihan Zhou', 'Jia Lin Yuan', 'Kourosh Darvish', 'Al√°n Aspuru-Guzik', 'Animesh Garg']
date: "2024-01-08"
image: "https://browse.arxiv.org/html/2401.04157v1/x1.png"
categories: ['prompt-engineering', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.04157v1/x1.png)

### Summary of "RePLan: Robotic Replanning with Perception and Language Models"

#### **Key Findings**
1. **Advancements in large language models (LLMs) have enabled robots to successfully carry out open-ended tasks**. The authors note that traditional methods rely on extensive domain knowledge and complex reward engineering, while Large Language Models (LLMs) show considerable promise in robot planning.
2. **Vision Language Models (VLMs) prove to be crucial in interpreting the environment and facilitating ongoing task updates based on real-time observations**. The integration of visual cues with linguistic context enables robots to better interpret their surrounding environment and adapt to unforeseen obstacles.
3. **RePLan, a novel framework that utilizes LLMs and VLMs, has shown significant success in enabling robotic systems to autonomously adapt to unforeseen obstacles while accomplishing open-ended, long-horizon goals**. The study conducted using RePLan across four environments containing seven long-horizon tasks demonstrated its effectiveness in successfully tackling multi-stage tasks, with a notable 4x improvement over the current leading method.

---

### **Introduction**

- Designing embodied agents to execute multi-stage, long-horizon tasks is challenging, requiring manipulation skills, perceptive reasoning, and high-level planning with minimal human intervention.

### **Robot Control with Physically Grounded Language Models**

- Language models have shown promise in robot planning. However, they lack physical grounding, while Vision Language Models (VLMs) combine visual and linguistic context to enable robots to interpret their surroundings accurately.

### **Long-horizon Robot Planning**

- Traditional methods such as Task and Motion Planning (TAMP) and learning approaches like Hierarchical Reinforcement Learning (HRL) and Imitation Learning (IL) necessitate substantial domain expertise and large datasets for task learning. Large Language Models (LLMs) have potential in robot planning but face challenges in reasoning over extended periods without considering important details.

### **Language to Reward Shaping**

- Directly inferring rewards from natural language inputs using language-driven reward-shaping approaches has shown utility in various domains, including negotiation and gaming, facilitating desired behavior learning through reinforcement learning.

### **RePLan: Model Structure and Details**

- RePLan comprises five modules: a High-Level LLM Planner, a VLM Perceiver, a Low-Level LLM Planner, a Motion Controller, and an LLM Verifier. These modules collaborate to enable the robot to adapt and replan based on feedback from the environment.
- The High-Level Planner generates subtasks, the Perceiver provides physical grounding, the Low-Level Planner converts high-level tasks to low-level rewards, the Motion Controller instructs the robot, and the Verifier ensures the correctness of the plans.

### **Experiments**

- The study included seven long-horizon tasks across four distinct environments, each testing the robot's ability to adapt to unforeseen obstacles and accomplish open-ended goals.
- RePLan demonstrated a significant 4x improvement over the current leading method, achieving successful adaptation in almost 90% of the tested tasks.

### **Error Cases and Additional Experiments**

- The study presented real-world scenarios, providing insights into error cases and additional experiments, such as VLM ablation and GPT-4V experiments, highlighting the method's strengths and limitations.

---

### **Critique**
The paper provides valuable insights into the utilization of language and vision models for robotic planning. However, it would benefit from a more detailed comparison with existing methods and a comprehensive discussion of the potential limitations and challenges associated with the proposed framework. Additionally, while the experiments are comprehensive, the real-world applicability of RePLan in varied environments and scenarios could be further explored.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [http://arxiv.org/abs/2401.04157v1](http://arxiv.org/abs/2401.04157v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.04157v1](https://browse.arxiv.org/html/2401.04157v1)       |
| Truncated       | False       |
| Word Count       | 12338       |