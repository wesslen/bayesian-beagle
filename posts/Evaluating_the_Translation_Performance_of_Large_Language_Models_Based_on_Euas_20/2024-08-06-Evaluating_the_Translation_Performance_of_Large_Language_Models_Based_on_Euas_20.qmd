
---
title: "Evaluating the Translation Performance of Large Language Models Based on Euas-20"
id: "2408.03119v1"
description: "TL;DR: We introduce dataset Euas-20 to evaluate LLMs' translation performance and abilities."
author: Yan Huang, Wei Liu
date: "2024-08-06"
image: "https://browse.arxiv.org/html/2408.03119v1/extracted/5776875/prompt1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.03119v1/extracted/5776875/prompt1.png)

### Summary:

- The paper evaluates the translation performance of large language models (LLMs) using a dataset called Euas-20, which covers 20 representative languages.
- The study focuses on Chinese and English and compares the translation performance of nine popular LLMs: falcon7b, mistral-7b, Llama-2-7b-hf, bloom-7b1, bloomz-7b1-mt, Meta-Llama-3-8B, mpt-7b, vicuna-7b, and gemma-7b.
- The evaluation metrics used are BLEU and COMET, which measure translation accuracy and human judgments of translation quality, respectively.
- The results show that the translation ability of LLMs has improved significantly, with Llama-3-8B outperforming other models. However, the translation performance of LLMs varies across languages, with better performance on high-resource languages and poorer performance on low- and medium-resource languages.
- The study also finds that a high-quality and diverse corpus can significantly improve the translation performance of LLMs, and that multi-language and multi-domain training data can enhance the model's generalization ability and effectiveness in different languages and domains.

### Major Findings:

1. The translation ability of LLMs has improved significantly, with Llama-3-8B outperforming other models.
2. The translation performance of LLMs varies across languages, with better performance on high-resource languages and poorer performance on low- and medium-resource languages.
3. A high-quality and diverse corpus can significantly improve the translation performance of LLMs, and multi-language and multi-domain training data can enhance the model's generalization ability and effectiveness in different languages and domains.

### Analysis and Critique:

- The study provides a comprehensive evaluation of the translation performance of LLMs using a diverse dataset and multiple evaluation metrics.
- However, the study only evaluates nine LLMs, and there may be other models that perform better or worse than those evaluated.
- The study also does not consider the impact of model size on translation performance, which could be an important factor to consider.
- Additionally, the study does not address the issue of hallucinations in the translation of LL

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.03119v1](https://arxiv.org/abs/2408.03119v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.03119v1](https://browse.arxiv.org/html/2408.03119v1)       |
| Truncated       | False       |
| Word Count       | 5165       |