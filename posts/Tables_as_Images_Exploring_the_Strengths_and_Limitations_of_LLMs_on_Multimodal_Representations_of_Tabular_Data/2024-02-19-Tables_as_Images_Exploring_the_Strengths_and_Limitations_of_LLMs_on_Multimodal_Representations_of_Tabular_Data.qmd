
---
title: "Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data"
id: "2402.12424v1"
description: "Comparing LLMs on tabular data with different prompts and formats for effective use."
author: Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, Rada Mihalcea
date: "2024-02-19"
image: "https://browse.arxiv.org/html/2402.12424v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12424v1/x1.png)

### Summary:
- The paper investigates the effectiveness of various Large Language Models (LLMs) in interpreting tabular data through different prompting strategies and data formats.
- The analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking, comparing five text-based and three image-based table representations.
- The study provides insights into the effective use of LLMs on table-related tasks.

### Major Findings:
1. LLMs maintain decent performance when using image-based table representations, with image-based representations sometimes leading to better performance.
2. There are nuances in the prompting design for table-related tasks, with expert prompting working the best when the LLM is an "expert."
3. Different table representations do not affect the performance of GPT models much, but bracket representation can help LLMs better understand tables.

### Analysis and Critique:
- The study reveals that LLMs are not good at arithmetic reasoning, suggesting the need for further research in this area.
- Significant performance gaps exist between open-source LLaMa-2 models and closed-source GPT-4 models, indicating the need to close the gap between open-source and closed-source LLMs.
- The paper does not exhaust every possible text representation, image representation of tables, or every possible LLM, and does not have access to the closed-source LLMs behind their API.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.12424v1](https://arxiv.org/abs/2402.12424v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12424v1](https://browse.arxiv.org/html/2402.12424v1)       |
| Truncated       | False       |
| Word Count       | 7298       |