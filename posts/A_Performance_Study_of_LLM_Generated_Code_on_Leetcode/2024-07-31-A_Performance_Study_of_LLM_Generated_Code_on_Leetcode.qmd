
---
title: "A Performance Study of LLM-Generated Code on Leetcode"
id: "2407.21579v1"
description: "TL;DR: LLMs generate code that is often more efficient than human-written code, despite model variations."
author: Tristan Coignion, Clément Quinton, Romain Rouvoy
date: "2024-07-31"
image: "https://browse.arxiv.org/html/2407.21579v1/extracted/5766329/pass_at_1_by_difficulty_and_dataset.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.21579v1/extracted/5766329/pass_at_1_by_difficulty_and_dataset.png)

**Summary:**

This study evaluates the efficiency of code generation by Large Language Models (LLMs) and measures their performance against human-crafted solutions using a dataset from Leetcode. The research compares 18 LLMs, considering factors such as model temperature and success rate, and their impact on code performance. The study introduces a novel method for measuring and comparing the speed of LLM-generated code, revealing that LLMs produce code with comparable performance, irrespective of the adopted LLM. The paper also finds that LLMs are capable of generating code that is, on average, more efficient than the code written by humans. The authors further discuss the use of Leetcode as a benchmarking dataset, the limitations imposed by potential data contamination, and the platform’s measurement reliability.

**Major Findings:**

1. LLMs produce code with comparable performance, irrespective of the adopted LLM.
2. LLMs are capable of generating code that is, on average, more efficient than the code written by humans.
3. The study introduces a novel method for measuring and comparing the speed of LLM-generated code.

**Analysis and Critique:**

The study provides valuable insights into the performance of LLMs in generating code. However, there are some potential limitations and areas for improvement. The authors acknowledge the issue of data contamination, which can impact the reliability of the results. Additionally, the use of Leetcode as a benchmarking dataset may not fully represent the complexity and diversity of real-world coding tasks. The authors could have explored other benchmarking datasets or considered a more diverse range of coding tasks to provide a more comprehensive evaluation of LLMs. Furthermore, the study does not discuss the potential impact of different model architectures or training methodologies on the performance of LLMs in generating code. Future research could investigate these factors to gain a deeper understanding of LLM performance in code generation.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.21579v1](https://arxiv.org/abs/2407.21579v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.21579v1](https://browse.arxiv.org/html/2407.21579v1)       |
| Truncated       | False       |
| Word Count       | 11134       |