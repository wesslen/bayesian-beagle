
---
title: "Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries"
id: "2402.13043v1"
description: "TL;DR: Few-shot DST with LLM uses conversation summarization for effective conversation retrieval and improved performance."
author: Seanie Lee, Jianpeng Chen, Joris Driesen, Alexandru Coca, Anders Johannsen
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13043v1/x1.png"
categories: ['architectures', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13043v1/x1.png)

### Summary:
The article discusses the challenges of few-shot dialogue state tracking (DST) with Large Language Models (LLM) and proposes a conversation retrieval approach based on text summaries of the conversations. The authors adopt a LLM-based conversation summarizer for query and key generation, enabling effective maximum inner product search. To avoid extra inference costs, they further distill a light-weight conversation encoder. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.

### Major Findings:
1. Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning.
2. The LLM-based conversation summarizer is adopted for query and key generation, enabling effective maximum inner product search.
3. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.

### Analysis and Critique:
- The proposed approach significantly improves the efficiency and performance of few-shot DST, outperforming previous LLM-based DST baselines.
- The article effectively addresses the challenges of few-shot DST with LLM and provides a practical solution for conversation retrieval based on text summaries.
- The human evaluation of the conversation summarizer indicates that 90.3% of the generated summaries are consistent with the given prompt, demonstrating the reliability of the summarization approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.13043v1](https://arxiv.org/abs/2402.13043v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13043v1](https://browse.arxiv.org/html/2402.13043v1)       |
| Truncated       | False       |
| Word Count       | 6225       |