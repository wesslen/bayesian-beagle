
---
title: "Multimodal Reasoning with Multimodal Knowledge Graph"
id: "2406.02030v1"
description: "MR-MKG method enhances LLMs' multimodal reasoning, outperforming SOTA models with fewer trainable parameters."
author: Junlin Lee, Yequan Wang, Jing Li, Min Zhang
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02030v1/x1.png"
categories: ['education', 'hci', 'robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02030v1/x1.png)

### Summary:

The paper proposes a method called Multimodal Reasoning with Multimodal Knowledge Graph (MR-MKG) to enhance the multimodal reasoning capabilities of large language models (LLMs) by leveraging multimodal knowledge graphs (MMKGs). The method employs a relation graph attention network for encoding MMKGs and a cross-modal alignment module for optimizing image-text alignment. A MMKG-grounded dataset is constructed for pretraining LLMs to equip them with initial expertise in multimodal reasoning. The method achieves superior performance while training on only a small fraction of parameters, approximately 2.25% of the LLM’s parameter size. Experimental results on multimodal question answering and multimodal analogy reasoning tasks demonstrate that the MR-MKG method outperforms previous state-of-the-art models.

### Major Findings:

1. The MR-MKG method effectively processes and utilizes knowledge from MMKGs for multimodal reasoning, outperforming previous state-of-the-art models with a 1.95% increase in accuracy and a 10.4% improvement in the Hits@1 metric.
2. The method trains only a small fraction of the parameters, approximately 2.25% of the LLM’s parameter size, while achieving superior performance.
3. The method is evaluated on various LLM sizes and training configurations, demonstrating its effectiveness in enhancing multimodal reasoning capabilities.

### Analysis and Critique:

1. The paper does not provide a detailed comparison with other methods that use MMKGs for multimodal reasoning, making it difficult to assess the method's novelty and advantages.
2. The method's performance on other tasks and datasets is not evaluated, limiting the generalizability of the findings.
3. The paper does not discuss the potential limitations and challenges of using MMKGs for multimodal reasoning, such as the quality and completeness of the MMKGs and the computational cost of processing them.
4. The paper does not provide a detailed analysis of the method's performance on different types of multimodal reasoning tasks, such as visual

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02030v1](https://arxiv.org/abs/2406.02030v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02030v1](https://browse.arxiv.org/html/2406.02030v1)       |
| Truncated       | False       |
| Word Count       | 9298       |