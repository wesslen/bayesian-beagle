
---
title: "SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity"
id: "2401.17072v1"
description: "SemScore metric outperforms others in evaluating instruction-tuned LLMs."
author: Ansar Aynetdinov, Alan Akbik
date: "2024-01-30"
image: "../../../bayesian-beagle.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
The article proposes a new evaluation metric called SEMSCORE for instruction-tuned Large Language Models (LLMs) based on semantic textual similarity (STS). The authors compare model outputs of 12 instruction-tuned LLMs using 8 widely-used evaluation metrics for text generation. They find that SEMSCORE outperforms all other evaluation metrics in terms of correlation to human evaluation, indicating its utility for the evaluation of instruction-tuned LLMs.

### Major Findings:
1. The proposed SEMSCORE metric outperforms all other, in many cases more complex, evaluation metrics in terms of correlation to human evaluation.
2. Instruction-tuning has enabled large language models to produce fitting natural language responses to natural language instructions.
3. Traditional metrics like BLEU or ROUGE are based on N-gram overlaps and generally require more than one gold response, whereas instruction-tuning datasets usually contain only one target response for a given instruction.

### Analysis and Critique:
The proposed SEMSCORE metric shows strong correlation to human judgment, indicating its usefulness for automated evaluation. However, the article acknowledges limitations, such as the dependence on an underlying transformer model and the requirement for at least one gold-standard target output for evaluation. Additionally, the small size of the evaluation dataset and its lack of focus on traditional NLP tasks are recognized as limitations. The article also discusses the potential biases and limitations of LLM-based metrics, raising concerns about reproducibility and access to proprietary models. Overall, while SEMSCORE shows promise, the article recognizes the need for further research and improvement in automated evaluation approaches for instruction-tuned LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2401.17072v1](https://arxiv.org/abs/2401.17072v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.17072v1](https://browse.arxiv.org/html/2401.17072v1)       |
| Truncated       | False       |
| Word Count       | 10170       |