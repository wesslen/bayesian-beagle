
---
title: "HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment"
id: "2406.14021v1"
description: "HIGHT: New method improves graph-language alignment in LLMs, reducing hallucination and enhancing performance in molecule-language tasks."
author: Yongqiang Chen, Quanming Yao, Juzheng Zhang, James Cheng, Yatao Bian
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.14021v1/x1.png"
categories: ['robustness', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.14021v1/x1.png)

### Summary:

The paper introduces a novel strategy called HIerarchical GrapH Tokenization (HIGHT) to address the issue of subpar graph-language alignment and severe hallucination in generated outputs caused by neglecting the hierarchical information in graph tokenization. HIGHT employs a hierarchical graph tokenizer that extracts and encodes the hierarchy of node, motif, and graph levels of informative tokens to improve the graph perception of LLMs. It also adopts an augmented graph-language supervised fine-tuning dataset, enriched with the hierarchical graph information, to further enhance the graph-language alignment. Extensive experiments on molecule-centric benchmarks confirm the effectiveness of HIGHT in reducing hallucination and improving various molecule-language downstream tasks.

### Major Findings:

1. The paper establishes a simple benchmark showing that neglecting the hierarchical information in graph tokenization leads to subpar graph-language alignment and severe hallucination in generated outputs.
2. The proposed HIGHT strategy employs a hierarchical graph tokenizer and an augmented graph-language supervised fine-tuning dataset to improve the graph perception of LLMs and enhance the graph-language alignment.
3. Extensive experiments on molecule-centric benchmarks confirm the effectiveness of HIGHT in reducing hallucination and improving various molecule-language downstream tasks.

### Analysis and Critique:

The paper presents a well-structured and coherent summary of the proposed HIGHT strategy and its effectiveness in improving graph-language alignment. The use of a hierarchical graph tokenizer and an augmented graph-language supervised fine-tuning dataset is a novel approach to addressing the issue of subpar graph-language alignment and severe hallucination in generated outputs. However, the paper does not discuss any potential limitations, unanswered questions, or conflicting evidence that may arise while reviewing the text. Additionally, the paper does not provide any information on the methodology used for the experiments or the evaluation metrics used to measure the effectiveness of HIGHT. Further research is needed to validate the proposed approach and address any potential limitations or shortcomings.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.14021v1](https://arxiv.org/abs/2406.14021v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.14021v1](https://browse.arxiv.org/html/2406.14021v1)       |
| Truncated       | False       |
| Word Count       | 11102       |