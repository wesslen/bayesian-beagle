
---
title: "Mixed Distillation Helps Smaller Language Model Better Reasoning"
id: "2312.10730v1"
description: "Smaller models gain LLM capabilities through Mixed Distillation, outperforming LLMs in reasoning accuracy."
author: Li Chenglin, Chen Qianglong, Wang Caiyu, Zhang Yin
date: "2023-12-17"
image: "https://browse.arxiv.org/html/2312.10730v1/x1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.10730v1/x1.png)

### Major Takeaways
1. **Mixed Distillation Framework:** The paper introduces a Mixed Distillation framework that leverages the Program-of-Thought (PoT) and Chain-of-Thought (CoT) capabilities within large language models (LLMs) to distill these capabilities into smaller models, leading to better reasoning performance.
2. **Enhanced Reasoning Abilities:** The experiment results indicate that the Mixed Distillation framework significantly improves reasoning capabilities of smaller models, surpassing the performance of LLMs in certain reasoning tasks and outperforming previous distillation methods.
3. **PoT Supervisory Signal:** The paper highlights the effectiveness of using PoT as a supervisory signal for distillation, previously overlooked, to enhance the reasoning capabilities of smaller models.

### Introduction
- LLMs possess strong reasoning capabilities, and prior research has focused on improving smaller models through knowledge distillation from LLMs to reduce computational resource cost.
- The Mixed Distillation framework leverages PoT and CoT capabilities within LLMs to distill these capabilities into smaller models, enhancing their reasoning performance.

### Related Work
- Program-of-thought prompting, multi-path reasoning through PoT and CoT, and knowledge distillation from LLMs are discussed as related work, demonstrating the significance of reasoning capabilities and distillation methods in the field.

### Approach
- The Mixed Distillation framework involves two main steps: extraction of multiple reasoning paths from LLMs and reasoning paths-based training for smaller models.

### Experiments
- The experiments demonstrate the effectiveness of PoT as a supervisory signal and the synergistic benefits of mixed distillation, showcasing notable enhancements in reasoning capabilities across mathematical reasoning datasets and StrategyQA dataset.
- Generalization of the mixed distillation framework is evaluated through model comparison, training set size, and out-of-distribution evaluation, highlighting the efficacy of the framework across different scenarios.

### Case Study
- A case study is presented to illustrate the effectiveness of mixed distillation in addressing reasoning tasks and to showcase the enhancement in performance compared to CoT and PoT distillation methods.

### Critique
The paper presents a comprehensive framework and experimental results, but potential limitations such as the need for further validation on diverse datasets and additional comparison with state-of-the-art approaches could strengthen the study's robustness. Additionally, addressing potential biases introduced through the experimental setup and model selection would further enhance the paper's credibility.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-10       |
| Abstract | [http://arxiv.org/abs/2312.10730v1](http://arxiv.org/abs/2312.10730v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.10730v1](https://browse.arxiv.org/html/2312.10730v1)       |
| Truncated       | False       |
| Word Count       | 6198       |