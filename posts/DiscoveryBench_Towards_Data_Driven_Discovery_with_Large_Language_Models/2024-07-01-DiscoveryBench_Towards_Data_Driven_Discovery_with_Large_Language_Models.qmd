
---
title: "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models"
id: "2407.01725v1"
description: "LLMs struggle with autonomous data-driven discovery, scoring only 25% on the DiscoveryBench benchmark."
author: Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, Peter Clark
date: "2024-07-01"
image: "https://browse.arxiv.org/html/2407.01725v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.01725v1/x1.png)

# Summary:

- The paper presents DiscoveryBench, a comprehensive benchmark for evaluating the ability of large language models (LLMs) to automate the search and verification of hypotheses from a set of provided datasets.
- The benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers.
- The benchmark also includes 903 synthetic tasks to conduct controlled evaluations across task complexity.
- The structured formalism of data-driven discovery in DiscoveryBench enables a facet-based evaluation that provides useful insights into different failure modes.
- The paper evaluates several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and finds that even the best system scores only 25%.
- The benchmark illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.

# Major Findings:

1. DiscoveryBench is the first comprehensive benchmark to formalize the multi-step process of data-driven hypothesis search and verification, covering many real-world discovery tasks plus additional synthetic tasks.
2. The benchmark provides a pragmatic formalism for data-driven discovery, flexible enough to characterize many real-world tasks while constrained enough to allow for rigorous, reproducible evaluation.
3. The evaluation of state-of-the-art LLM-based reasoning methods on DiscoveryBench shows that performance peaks at 25%, demonstrating the challenging nature of the task.

# Analysis and Critique:

- The paper provides a valuable resource for the community to make progress on autonomous, data-driven discovery.
- However, the paper does not discuss the limitations of the benchmark or the potential biases that may have been introduced during the data collection process.
- Additionally, the paper does not provide a detailed analysis of the performance of different LLM-based reasoning frameworks on the benchmark, which could be useful for identifying the strengths and weaknesses of different approaches.
- Finally, the paper does not discuss the potential applications of the benchmark beyond evaluating LLMs, such as its use in developing new data-driven discovery methods or in education and training.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.01725v1](https://arxiv.org/abs/2407.01725v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.01725v1](https://browse.arxiv.org/html/2407.01725v1)       |
| Truncated       | False       |
| Word Count       | 11425       |