
---
title: "Questionnaires for Everyone: Streamlining Cross-Cultural Questionnaire Adaptation with GPT-Based Translation Quality Evaluation"
id: "2407.20608v1"
description: "Tool expedites questionnaire translation with AI, yielding quality similar to conventional methods, promoting equitable research."
author: Otso Haavisto, Robin Welsch
date: "2024-07-30"
image: "https://browse.arxiv.org/html/2407.20608v1/extracted/5757120/proto-zoomout-overview-2.png"
categories: ['hci', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.20608v1/extracted/5757120/proto-zoomout-overview-2.png)

### Summary:

The article presents a prototype tool that aims to expedite the questionnaire translation process for cross-cultural research. The tool incorporates forward-backward translation using DeepL and GPT-4-generated translation quality evaluations and improvement suggestions. Two online studies were conducted to evaluate the tool's effectiveness, with participants translating questionnaires from English to either German or Portuguese. The results indicate that integrating LLM-generated translation quality evaluations and suggestions for improvement can help users independently attain results similar to those provided by conventional, non-NLP-supported translation methods.

### Major Findings:

1. The prototype tool, which uses DeepL for forward-backward translation and GPT-4 for translation quality evaluations and improvement suggestions, can help users achieve translation results similar to those attained with conventional, non-NLP-supported methods.
2. In both studies, participants reached equivalent translation quality to conventionally translated (i.e., not translated using NLP tools) versions of the questionnaires using machine translation and GPT-4-generated quality evaluations and suggestions for improvement.
3. Participants found the evaluations helpful and made use of the suggestions provided by the prototype, with over half of the participants in the latter study reporting having implemented at least some of the suggestions.

### Analysis and Critique:

1. The study's sample size is relatively small, with only 10 participants in the first study and 20 in the second. A larger sample size would provide more robust results and increase the generalizability of the findings.
2. The study does not address the potential limitations of using LLMs for translation quality evaluations, such as their inability to capture cultural nuances or context-specific requirements.
3. The study does not discuss the potential biases that may be introduced by using LLMs for translation quality evaluations, such as the potential for the models to perpetuate existing biases in the data they were trained on.
4. The study does not address the potential ethical implications of using LLMs for translation quality evaluations, such as the potential for the models to be used to manipulate or misrepresent the meaning of translated texts.
5. The study does not discuss the potential impact of using LLMs for translation quality evaluations on the job market for human translators, such as the potential for

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.20608v1](https://arxiv.org/abs/2407.20608v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.20608v1](https://browse.arxiv.org/html/2407.20608v1)       |
| Truncated       | False       |
| Word Count       | 5792       |