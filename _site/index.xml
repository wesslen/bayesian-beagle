<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Bayesian beagle</title>
<link>https://bayesian-beagle.netlify.app/index.html</link>
<atom:link href="https://bayesian-beagle.netlify.app/index.xml" rel="self" type="application/rss+xml"/>
<description>Quarto blog of LLM-generated summaries of Arxiv preprints</description>
<generator>quarto-1.3.450</generator>
<lastBuildDate>Thu, 29 Feb 2024 05:00:00 GMT</lastBuildDate>
<item>
  <title>Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment</title>
  <dc:creator>Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Controllable_Preference_Optimization_Toward_Controllable_Multi_Objective_Alignment/2024-02-29-Controllable_Preference_Optimization_Toward_Controllable_Multi_Objective_Alignment.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Controllable_Preference_Optimization_Toward_Controllable_Multi_Objective_Alignment/https:/browse.arxiv.org/html/2402.19085v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>Controllable Preference Optimization (CPO) is a method for aligning large language models (LLMs) with human preferences and values, specifically targeting helpful, honest, and harmless LLMs.</li>
<li>The “3H” principle can lead to trade-offs between objectives, known as the “alignment tax,” where improving one alignment objective might negatively impact others.</li>
<li>CPO introduces controllable preference supervised fine-tuning (CPSFT) and controllable direct preference optimization (CDPO) to optimize LLMs based on explicit preference conditions, mitigating the alignment tax and achieving Pareto improvements in multi-objective alignment.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li><strong>Controllable Preference Optimization</strong>: CPO, consisting of CPSFT and CDPO, explicitly specifies preference scores for different objectives, guiding the model to generate responses that meet the requirements, sur</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19085v1">https://arxiv.org/abs/2402.19085v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19085v1">https://browse.arxiv.org/html/2402.19085v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>6069</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>social-sciences</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Controllable_Preference_Optimization_Toward_Controllable_Multi_Objective_Alignment/2024-02-29-Controllable_Preference_Optimization_Toward_Controllable_Multi_Objective_Alignment.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19085v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Compositional API Recommendation for Library-Oriented Code Generation</title>
  <dc:creator>Zexiong Ma, Shengnan An, Bing Xie, Zeqi Lin</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Compositional_API_Recommendation_for_Library_Oriented_Code_Generation/2024-02-29-Compositional_API_Recommendation_for_Library_Oriented_Code_Generation.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Compositional_API_Recommendation_for_Library_Oriented_Code_Generation/https:/browse.arxiv.org/html/2402.19431v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>Large language models (LLMs) have achieved exceptional performance in code generation, but the performance remains unsatisfactory in generating library-oriented code, especially for libraries not present in the training data of LLMs.</li>
<li>A new approach, CAPIR (Compositional API Recommendation), is proposed to address the challenge of granularity inconsistency between developmental requirements and API recommendation.</li>
<li>CAPIR employs an LLM-based Decomposer, an embedding-based Retriever, and an LLM-based Reranker to break down coarse-grained tasks, identify relevant APIs, and filter out redundant APIs.</li>
<li>Two new benchmarks, RAPID and LOCG, are presented to facilitate the evaluation of API recommendation methods on coarse-grained requirements.</li>
<li>Experimental results show that CAPIR outperforms existing baselines in API recommendation and library</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19431v1">https://arxiv.org/abs/2402.19431v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19431v1">https://browse.arxiv.org/html/2402.19431v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>11838</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>architectures</category>
  <category>programming</category>
  <category>recommender</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Compositional_API_Recommendation_for_Library_Oriented_Code_Generation/2024-02-29-Compositional_API_Recommendation_for_Library_Oriented_Code_Generation.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19431v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study</title>
  <dc:creator>Prottay Kumar Adhikary, Aseem Srivastava, Shivani Kumar, Salam Michael Singh, Puneet Manuja, Jini K Gopinath, Vijay Krishnan, Swati Kedia, Koushik Sinha Deb, Tanmoy Chakraborty</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Exploring_the_Efficacy_of_Large_Language_Models_in_Summarizing_Mental_Health_Counseling_Sessions_A_Benchmark_Study/2024-02-29-Exploring_the_Efficacy_of_Large_Language_Models_in_Summarizing_Mental_Health_Counseling_Sessions_A_Benchmark_Study.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/img/2402.19052v1/image_1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary"><strong>Summary:</strong></h3>
<ul>
<li>The article explores the effectiveness of Large Language Models (LLMs) in summarizing mental health counseling sessions through aspect-based summarization.</li>
<li>A new dataset, MentalCLOUDS, is introduced, which consists of 191 counseling sessions with summaries focused on three distinct counseling components.</li>
<li>Eleven state-of-the-art LLMs are assessed for their ability to address the task of component-guided summarization in counseling.</li>
<li>Findings suggest that task-specific LLMs, such as MentalLlama, Mistral, and MentalBART, perform better in terms of standard quantitative metrics and expert evaluation.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings"><strong>Major Findings:</strong></h3>
<ol type="1">
<li>Task-specific LLMs, MentalLlama, Mistral, and MentalBART, outperform other models in summarizing mental health counseling sessions based on standard quantitative</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19052v1">https://arxiv.org/abs/2402.19052v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19052v1">https://browse.arxiv.org/html/2402.19052v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>16555</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>social-sciences</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Exploring_the_Efficacy_of_Large_Language_Models_in_Summarizing_Mental_Health_Counseling_Sessions_A_Benchmark_Study/2024-02-29-Exploring_the_Efficacy_of_Large_Language_Models_in_Summarizing_Mental_Health_Counseling_Sessions_A_Benchmark_Study.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://bayesian-beagle.netlify.app/img/2402.19052v1/image_1.png" medium="image" type="image/png" height="115" width="144"/>
</item>
<item>
  <title>GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers</title>
  <dc:creator>Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/GSM_Plus_A_Comprehensive_Benchmark_for_Evaluating_the_Robustness_of_LLMs_as_Mathematical_Problem_Solvers/2024-02-29-GSM_Plus_A_Comprehensive_Benchmark_for_Evaluating_the_Robustness_of_LLMs_as_Mathematical_Problem_Solvers.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/GSM_Plus_A_Comprehensive_Benchmark_for_Evaluating_the_Robustness_of_LLMs_as_Mathematical_Problem_Solvers/https:/browse.arxiv.org/html/2402.19255v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>Large language models (LLMs) have shown impressive performance in mathematical reasoning benchmarks, but there are debates about their understanding and application of mathematical knowledge.</li>
<li>A new benchmark, GSM-Plus, is introduced to evaluate the robustness of LLMs’ math reasoning capability by testing various question variations.</li>
<li>The experiments on 25 LLMs and 4 prompting techniques show that LLMs’ performances are not robust, with mistakes made even on previously solved problems with new statements or altered targets.</li>
<li>An iterative method generating and verifying intermediate thoughts based on reasoning goals and calculation results is explored to achieve more robust performance.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li><strong>Performance varies among LLMs:</strong> Different LLMs exhibit varying levels of math reasoning abilities, with even top-performing models making mistakes on altered problems.</li>
<li><strong>Non-robust performance:</strong> LLMs struggle with new statements or altered targets, even</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19255v1">https://arxiv.org/abs/2402.19255v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19255v1">https://browse.arxiv.org/html/2402.19255v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>2949</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>education</category>
  <category>robustness</category>
  <category>security</category>
  <category>prompt-engineering</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/GSM_Plus_A_Comprehensive_Benchmark_for_Evaluating_the_Robustness_of_LLMs_as_Mathematical_Problem_Solvers/2024-02-29-GSM_Plus_A_Comprehensive_Benchmark_for_Evaluating_the_Robustness_of_LLMs_as_Mathematical_Problem_Solvers.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19255v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Aligning Language Models for Versatile Text-based Item Retrieval</title>
  <dc:creator>Yuxuan Lei, Jianxun Lian, Jing Yao, Mingqi Wu, Defu Lian, Xing Xie</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Aligning_Language_Models_for_Versatile_Text_based_Item_Retrieval/2024-02-29-Aligning_Language_Models_for_Versatile_Text_based_Item_Retrieval.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Aligning_Language_Models_for_Versatile_Text_based_Item_Retrieval/https:/browse.arxiv.org/html/2402.18899v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>The paper “Aligning Language Models for Versatile Text-based Item Retrieval” discusses the limitations of general-purpose text embeddings for item retrieval tasks and proposes a specialized fine-tuning dataset for improving language models’ performance in this area.</li>
<li>The authors create a dataset with 10 tasks tailored to unlocking models’ representation ability for item retrieval, demonstrating significant improvements in various retrieval tasks after fine-tuning embedding models on the dataset.</li>
<li>The refined model is applied in a conversational setting, enhancing the capabilities of LLM-based Recommender Agents like Chat-Rec.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li><strong>General-purpose text embedding models fall short in achieving satisfactory zero-shot performance for specific tasks like item retrieval.</strong></li>
<li>**A specialized fine-tuning dataset, encompassing 10 distinct types of tasks,</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.18899v1">https://arxiv.org/abs/2402.18899v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.18899v1">https://browse.arxiv.org/html/2402.18899v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>3406</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>architectures</category>
  <category>recommender</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Aligning_Language_Models_for_Versatile_Text_based_Item_Retrieval/2024-02-29-Aligning_Language_Models_for_Versatile_Text_based_Item_Retrieval.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.18899v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Watermark Stealing in Large Language Models</title>
  <dc:creator>Nikola Jovanović, Robin Staab, Martin Vechev</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Watermark_Stealing_in_Large_Language_Models/2024-02-29-Watermark_Stealing_in_Large_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/../bayesian-beagle.png" class="img-fluid"></p>
<section id="watermark-stealing-in-large-language-models" class="level3">
<h3 class="anchored" data-anchor-id="watermark-stealing-in-large-language-models">Watermark Stealing in Large Language Models</h3>
<p><strong>Summary:</strong></p>
<ul>
<li>LLM watermarking has been proposed as a promising way to detect AI-generated content.</li>
<li>However, the authors identify a fundamental vulnerability called watermark stealing (WS), where querying the API of a watermarked LLM can reverse-engineer the watermark, enabling spoofing and scrubbing attacks.</li>
<li>The authors propose an automated WS algorithm and demonstrate that, for under $50, an attacker can successfully spoof and scrub state-of-the-art watermarked schemes with an average success rate of over 80%.</li>
</ul>
<p><strong>Major Findings:</strong></p>
<ol type="1">
<li>Watermark stealing (WS) is a fundamental vulnerability in LLM watermarking schemes.</li>
<li>An attacker can use WS to launch both spoofing and scrubbing attacks on state-of-the-art watermarked</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19361v1">https://arxiv.org/abs/2402.19361v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19361v1">https://browse.arxiv.org/html/2402.19361v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>21936</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>robustness</category>
  <category>security</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Watermark_Stealing_in_Large_Language_Models/2024-02-29-Watermark_Stealing_in_Large_Language_Models.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://bayesian-beagle.netlify.app/bayesian-beagle.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling</title>
  <dc:creator>Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Loose_LIPS_Sink_Ships_Asking_Questions_in_Battleship_with_Language_Informed_Program_Sampling/2024-02-29-Loose_LIPS_Sink_Ships_Asking_Questions_in_Battleship_with_Language_Informed_Program_Sampling.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/img/2402.19471v1/image_1.png" class="img-fluid"></p>
<section id="loose-lips-sink-ships-asking-questions-in-battleship-with-language-informed-program-sampling" class="level3">
<h3 class="anchored" data-anchor-id="loose-lips-sink-ships-asking-questions-in-battleship-with-language-informed-program-sampling">Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling</h3>
<p><strong>Summary:</strong> - The study explores how people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources in the context of the game Battleship. - A language-informed program sampling (LIPS) model is proposed, which uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. - The LIPS model outperforms LLM-only baselines and illustrates how Bayesian models of question-asking can leverage the statistics of language to capture human priors.</p>
<p><strong>Major Findings:</strong> 1. The LIPS model, with a surprisingly modest resource budget, yields informative questions that mirror human performance across varied Battleship board scenarios. 2. LLM-only baselines struggle to ground questions in the</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19471v1">https://arxiv.org/abs/2402.19471v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19471v1">https://browse.arxiv.org/html/2402.19471v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>18405</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>education</category>
  <category>programming</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Loose_LIPS_Sink_Ships_Asking_Questions_in_Battleship_with_Language_Informed_Program_Sampling/2024-02-29-Loose_LIPS_Sink_Ships_Asking_Questions_in_Battleship_with_Language_Informed_Program_Sampling.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://bayesian-beagle.netlify.app/img/2402.19471v1/image_1.png" medium="image" type="image/png" height="147" width="144"/>
</item>
<item>
  <title>Let LLMs Take on the Latest Challenges! A Chinese Dynamic Question Answering Benchmark</title>
  <dc:creator>Zhikun Xu, Yinghui Li, Ruixue Ding, Xinyu Wang, Boli Chen, Yong Jiang, Xiaodong Deng, Jianxin Ma, Hai-Tao Zheng, Wenlian Lu, Pengjun Xie, Chang Zhou, Fei Huang</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Let_LLMs_Take_on_the_Latest_Challenges!_A_Chinese_Dynamic_Question_Answering_Benchmark/2024-02-29-Let_LLMs_Take_on_the_Latest_Challenges!_A_Chinese_Dynamic_Question_Answering_Benchmark.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Let_LLMs_Take_on_the_Latest_Challenges!_A_Chinese_Dynamic_Question_Answering_Benchmark/https:/browse.arxiv.org/html/2402.19248v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>The paper introduces CDQA, a Chinese Dynamic QA benchmark, to evaluate the ability of Large Language Models (LLMs) in answering questions related to the latest news on the Chinese Internet.</li>
<li>The dataset is collected through a pipeline that combines humans and models, and is classified according to the frequency of answer changes.</li>
<li>Mainstream and advanced Chinese LLMs have been evaluated on CDQA, revealing that the benchmark is challenging and worthy of further study.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li><strong>CDQA</strong>: The paper presents CDQA, a Chinese Dynamic QA benchmark designed to evaluate the capability of LLMs in answering questions related to the latest news on the Chinese Internet. The dataset is collected through a pipeline that combines humans and models and is classified according to the frequency of answer changes.</li>
<li><strong>Evaluation of LLMs</strong>: Several mainstream and advanced Chinese LLMs have been</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19248v1">https://arxiv.org/abs/2402.19248v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19248v1">https://browse.arxiv.org/html/2402.19248v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>6054</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Let_LLMs_Take_on_the_Latest_Challenges!_A_Chinese_Dynamic_Question_Answering_Benchmark/2024-02-29-Let_LLMs_Take_on_the_Latest_Challenges!_A_Chinese_Dynamic_Question_Answering_Benchmark.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19248v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines</title>
  <dc:creator>Lijia Ma, Xingchen Xu, Yong Tan</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Crafting_Knowledge_Exploring_the_Creative_Mechanisms_of_Chat_Based_Search_Engines/2024-02-29-Crafting_Knowledge_Exploring_the_Creative_Mechanisms_of_Chat_Based_Search_Engines.html</link>
  <description><![CDATA[ 



<p><img src="https:/browse.arxiv.org/html/2402.19421v1/extracted/5413983/img/new_bing_example.png" class="img-fluid"></p>
<section id="research-summary" class="level3">
<h3 class="anchored" data-anchor-id="research-summary">Research Summary</h3>
<p><strong>Summary:</strong></p>
<ul>
<li>Chat-based search engines, like Bing Chat and Bard, are a new category of chatbots that combine search engine capabilities with large language models (LLMs) to deliver responses in natural language.</li>
<li>These chat-based search engines demonstrate human-like metacognitive skills, including the acquisition of new knowledge and the demonstration of creativity.</li>
<li>The “cognitive” process through which chatbots discern pertinent information and formulate final responses remains largely inscrutable due to the complexity of the underlying LLMs.</li>
<li>The visibility of a website in chat-based search engines is crucial, as it influences decision-making processes and the distribution of welfare among stakeholders.</li>
<li>The study aims to investigate the criteria for citation in chat-based search engines and compare them with the ranking criteria of conventional search engines.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings</h3>
<ol type="1">
<li>**Chat-based</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19421v1">https://arxiv.org/abs/2402.19421v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19421v1">https://browse.arxiv.org/html/2402.19421v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>11501</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>hci</category>
  <category>production</category>
  <category>social-sciences</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Crafting_Knowledge_Exploring_the_Creative_Mechanisms_of_Chat_Based_Search_Engines/2024-02-29-Crafting_Knowledge_Exploring_the_Creative_Mechanisms_of_Chat_Based_Search_Engines.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://bayesian-beagle.netlify.app/https:/browse.arxiv.org/html/2402.19421v1/extracted/5413983/img/new_bing_example.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions</title>
  <dc:creator>Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Benchmarking_Large_Language_Models_on_Answering_and_Explaining_Challenging_Medical_Questions/2024-02-29-Benchmarking_Large_Language_Models_on_Answering_and_Explaining_Challenging_Medical_Questions.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Benchmarking_Large_Language_Models_on_Answering_and_Explaining_Challenging_Medical_Questions/https:/browse.arxiv.org/html/2402.18060v2/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>Large language models (LLMs) have shown impressive performance in answering medical questions, but they struggle with complex, real-world clinical cases.</li>
<li>To address this issue, two new datasets, JAMA Clinical Challenge and Medbullets, are constructed for multiple-choice question-answering tasks with expert-written explanations.</li>
<li>Four LLMs are evaluated on these datasets, and the results demonstrate that the new tasks are more challenging, with inconsistencies between automatic and human evaluations of model-generated explanations.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li>The two new datasets, JAMA Clinical Challenge and Medbullets, are designed to evaluate LLMs on more realistic and challenging clinical cases, with high-quality explanations provided by human experts.</li>
<li>Four LLMs are evaluated on these datasets, and the results show that the new tasks are more difficult, with lower scores compared to previous benchmarks.</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.18060v2">https://arxiv.org/abs/2402.18060v2</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.18060v2">https://browse.arxiv.org/html/2402.18060v2</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>7933</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>education</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Benchmarking_Large_Language_Models_on_Answering_and_Explaining_Challenging_Medical_Questions/2024-02-29-Benchmarking_Large_Language_Models_on_Answering_and_Explaining_Challenging_Medical_Questions.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.18060v2/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>On the Decision-Making Abilities in Role-Playing using Large Language Models</title>
  <dc:creator>Chenglei Shen, Guofu Xie, Xiao Zhang, Jun Xu</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/On_the_Decision_Making_Abilities_in_Role_Playing_using_Large_Language_Models/2024-02-29-On_the_Decision_Making_Abilities_in_Role_Playing_using_Large_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/On_the_Decision_Making_Abilities_in_Role_Playing_using_Large_Language_Models/https:/browse.arxiv.org/html/2402.18807v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li>Large Language Models (LLMs) are increasingly used for role-playing tasks, with decision-making abilities shaping behavioral patterns.</li>
<li>This paper evaluates the decision-making abilities of LLMs post role-playing, providing metrics and guidance for enhancing their performance.</li>
<li>Four aspects of decision-making abilities are analyzed: adaptability, exploration-exploitation trade-off ability, reasoning ability, and safety.</li>
<li>Results show stable differences in decision-making abilities across distinct roles, indicating that LLMs can effectively impersonate varied roles with genuine sociological characteristics.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings</h3>
<ol type="1">
<li><strong>Adaptability</strong>: LLMs can simulate diverse user groups with varying interests over time, reflecting adaptability in user preferences.</li>
<li><strong>Exploration-Exploitation Trade-off Ability</strong>: LLMs show varying exploration and exploitation tendencies depending on the role, with type E,</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.18807v1">https://arxiv.org/abs/2402.18807v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.18807v1">https://browse.arxiv.org/html/2402.18807v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>6273</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>hci</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/On_the_Decision_Making_Abilities_in_Role_Playing_using_Large_Language_Models/2024-02-29-On_the_Decision_Making_Abilities_in_Role_Playing_using_Large_Language_Models.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.18807v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>How to Understand Support? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding</title>
  <dc:creator>Jiamin Luo, Jianing Zhao, Jingjing Wang, Guodong Zhou</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/How_to_Understand_Support_An_Implicit_enhanced_Causal_Inference_Approach_for_Weakly_supervised_Phrase_Grounding/2024-02-29-How_to_Understand_Support_An_Implicit_enhanced_Causal_Inference_Approach_for_Weakly_supervised_Phrase_Grounding.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/How_to_Understand_Support_An_Implicit_enhanced_Causal_Inference_Approach_for_Weakly_supervised_Phrase_Grounding/https:/browse.arxiv.org/html/2402.19116v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li>Weakly-supervised Phrase Grounding (WPG) is an emerging task in multimodal learning that involves inferring fine-grained phrase-region matching using coarse-grained sentence-image pairs for training.</li>
<li>Existing studies on WPG often overlook implicit phrase-region matching relations, which are crucial for evaluating a model’s ability to understand deep multimodal semantics.</li>
<li>This paper proposes an Implicit-Enhanced Causal Inference (IECI) approach that uses intervention and counterfactual techniques to address the challenges of modeling implicit relations and highlighting them beyond the explicit.</li>
<li>A high-quality implicit-enhanced dataset is annotated to evaluate IECI, and results show its advantages over state-of-the-art baselines.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings</h3>
<ol type="1">
<li><strong>Implicit Relations in WPG:</strong> The paper highlights the importance</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19116v1">https://arxiv.org/abs/2402.19116v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19116v1">https://browse.arxiv.org/html/2402.19116v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>7721</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/How_to_Understand_Support_An_Implicit_enhanced_Causal_Inference_Approach_for_Weakly_supervised_Phrase_Grounding/2024-02-29-How_to_Understand_Support_An_Implicit_enhanced_Causal_Inference_Approach_for_Weakly_supervised_Phrase_Grounding.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19116v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>RL-GPT: Integrating Reinforcement Learning and Code-as-policy</title>
  <dc:creator>Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, Jiaya Jia</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/RL_GPT_Integrating_Reinforcement_Learning_and_Code_as_policy/2024-02-29-RL_GPT_Integrating_Reinforcement_Learning_and_Code_as_policy.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/RL_GPT_Integrating_Reinforcement_Learning_and_Code_as_policy/https:/browse.arxiv.org/html/2402.19299v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary"><strong>Summary:</strong></h3>
<ul>
<li>RL-GPT is a hierarchical framework that combines Large Language Models (LLMs) and Reinforcement Learning (RL) to improve the efficiency of LLMs in handling complex, embodied tasks.</li>
<li>The framework consists of a slow agent and a fast agent. The slow agent decomposes tasks into sub-actions and determines which actions can be directly coded, while the fast agent writes code and instantiates RL configurations for low-level execution.</li>
<li>RL-GPT outperforms traditional RL methods and existing GPT agents in Minecraft tasks, such as rapidly obtaining diamonds within a single day using an RTX3090 and achieving state-of-the-art performance across all designated MineDojo tasks.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li><strong>Hierarchical Framework:</strong> RL-GPT introduces a two-level hierarchical framework that</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19299v1">https://arxiv.org/abs/2402.19299v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19299v1">https://browse.arxiv.org/html/2402.19299v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>6483</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>programming</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/RL_GPT_Integrating_Reinforcement_Learning_and_Code_as_policy/2024-02-29-RL_GPT_Integrating_Reinforcement_Learning_and_Code_as_policy.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19299v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL</title>
  <dc:creator>Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/ArCHer_Training_Language_Model_Agents_via_Hierarchical_Multi_Turn_RL/2024-02-29-ArCHer_Training_Language_Model_Agents_via_Hierarchical_Multi_Turn_RL.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/ArCHer_Training_Language_Model_Agents_via_Hierarchical_Multi_Turn_RL/https:/browse.arxiv.org/html/2402.19446v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>Large language models (LLMs) have the potential to address decision-making or “agent” problems that can be expressed in text or natural language.</li>
<li>Current RL methods for LLMs largely focus on single-turn reward maximization, which cannot effectively train LLMs to seek and incorporate information over multiple turns, perform credit assignment, or reason about their past actions.</li>
<li>The paper proposes an algorithmic framework, ArCHer, for developing multi-turn RL algorithms for fine-tuning LLMs, preserving the flexibility of existing single-turn RL methods while accommodating multiple turns, long horizons, and delayed rewards effectively.</li>
<li>ArCHer adopts a hierarchical RL approach, running two RL algorithms in parallel: a high-level off-policy RL algorithm that trains a value function to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19446v1">https://arxiv.org/abs/2402.19446v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19446v1">https://browse.arxiv.org/html/2402.19446v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>16897</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>hci</category>
  <category>production</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/ArCHer_Training_Language_Model_Agents_via_Hierarchical_Multi_Turn_RL/2024-02-29-ArCHer_Training_Language_Model_Agents_via_Hierarchical_Multi_Turn_RL.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19446v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts</title>
  <dc:creator>Hao Cheng, Erjia Xiao, Renjing Xu</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Typographic_Attacks_in_Large_Multimodal_Models_Can_be_Alleviated_by_More_Informative_Prompts/2024-02-29-Typographic_Attacks_in_Large_Multimodal_Models_Can_be_Alleviated_by_More_Informative_Prompts.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Typographic_Attacks_in_Large_Multimodal_Models_Can_be_Alleviated_by_More_Informative_Prompts/https:/browse.arxiv.org/html/2402.19150v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary"><strong>Summary:</strong></h3>
<ul>
<li>Large Multimodal Models (LMMs) incorporate pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) to perform various multimodal tasks in the joint space of vision and language.</li>
<li>A security vulnerability to LMMs is the Typographic Attack, which disrupts VLMs.</li>
<li>This study investigates the distractibility of LMMs by typography and proposes a prompt information enhancement method to mitigate the effects of typography.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings"><strong>Major Findings:</strong></h3>
<ol type="1">
<li>LMMs can partially distinguish visual contents and typos when confronted with typographic attacks, indicating that embeddings from vision encoders contain enough information to distinguish visual contents and typos in images.</li>
<li>By providing more informative texts to match images, CLIP’s performance of zero-shot classification on typo-ridden images can be</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19150v1">https://arxiv.org/abs/2402.19150v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19150v1">https://browse.arxiv.org/html/2402.19150v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>6409</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>security</category>
  <category>architectures</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Typographic_Attacks_in_Large_Multimodal_Models_Can_be_Alleviated_by_More_Informative_Prompts/2024-02-29-Typographic_Attacks_in_Large_Multimodal_Models_Can_be_Alleviated_by_More_Informative_Prompts.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19150v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy</title>
  <dc:creator>Philipp Schoenegger, Indre Tuminauskaite, Peter S. Park, Philip E. Tetlock</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Wisdom_of_the_Silicon_Crowd_LLM_Ensemble_Prediction_Capabilities_Match_Human_Crowd_Accuracy/2024-02-29-Wisdom_of_the_Silicon_Crowd_LLM_Ensemble_Prediction_Capabilities_Match_Human_Crowd_Accuracy.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Wisdom_of_the_Silicon_Crowd_LLM_Ensemble_Prediction_Capabilities_Match_Human_Crowd_Accuracy/https:/browse.arxiv.org/html/2402.19379v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<ul>
<li>Human forecasting accuracy relies on the ‘wisdom of the crowd’ effect, where predictions are improved by aggregating across a crowd of individual forecasters.</li>
<li>Frontier LLMs underperform compared to a human crowd in a probabilistic forecasting context and fail to clear simple benchmarks.</li>
<li>An LLM ensemble approach, consisting of a crowd of twelve LLMs, has been shown to outperform a simple no-information benchmark and is statistically equivalent to the human crowd in forecasting accuracy.</li>
<li>LLM predictions benefit from exposure to the median human prediction as information, improving accuracy by up to 28%.</li>
<li>The ‘wisdom of the crowd’ effect for LLMs, or the ‘wisdom of the silicon crowd,’ has been replicated, opening up potential applications throughout society.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings</h3>
<ol type="1">
<li>An LLM ensemble approach outperforms a simple no-information</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19379v1">https://arxiv.org/abs/2402.19379v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19379v1">https://browse.arxiv.org/html/2402.19379v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>9200</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>hci</category>
  <category>production</category>
  <category>social-sciences</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Wisdom_of_the_Silicon_Crowd_LLM_Ensemble_Prediction_Capabilities_Match_Human_Crowd_Accuracy/2024-02-29-Wisdom_of_the_Silicon_Crowd_LLM_Ensemble_Prediction_Capabilities_Match_Human_Crowd_Accuracy.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19379v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>On the Scaling Laws of Geographical Representation in Language Models</title>
  <dc:creator>Nathan Godey, Éric de la Clergerie, Benoît Sagot</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/On_the_Scaling_Laws_of_Geographical_Representation_in_Language_Models/2024-02-29-On_the_Scaling_Laws_of_Geographical_Representation_in_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/On_the_Scaling_Laws_of_Geographical_Representation_in_Language_Models/https:/browse.arxiv.org/html/2402.19406v1/x1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary:</h3>
<ul>
<li>Recent studies have shown that language models implicitly embed geographical information in their hidden representations.</li>
<li>This geographical knowledge can be observed even in tiny models and scales consistently as the model size increases.</li>
<li>Larger language models cannot mitigate the geographical bias inherited from the training data.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li><strong>Geographical knowledge in language models:</strong> Geographical knowledge is observable even in tiny models and scales consistently as the model size increases.</li>
<li><strong>Geographical bias in language models:</strong> Larger language models cannot mitigate the geographical bias inherited from the training data.</li>
<li><strong>Correlation between model performance and training data frequency:</strong> The performance of models in terms of geographical probing is correlated with the frequency of corresponding country names in the training data.</li>
</ol>
</section>
<section id="analysis-and-critique" class="level3">
<h3 class="anchored" data-anchor-id="analysis-and-critique">Analysis and Critique:</h3>
<ul>
<li>The study does not provide a clear definition of what constitutes a “tiny”</li>
</ul>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19406v1">https://arxiv.org/abs/2402.19406v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19406v1">https://browse.arxiv.org/html/2402.19406v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>3038</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>social-sciences</category>
  <guid>https://bayesian-beagle.netlify.app/posts/On_the_Scaling_Laws_of_Geographical_Representation_in_Language_Models/2024-02-29-On_the_Scaling_Laws_of_Geographical_Representation_in_Language_Models.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19406v1/x1.png" medium="image" type="image/png"/>
</item>
<item>
  <title>PRSA: Prompt Reverse Stealing Attacks against Large Language Models</title>
  <dc:creator>Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/PRSA_Prompt_Reverse_Stealing_Attacks_against_Large_Language_Models/2024-02-29-PRSA_Prompt_Reverse_Stealing_Attacks_against_Large_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/PRSA_Prompt_Reverse_Stealing_Attacks_against_Large_Language_Models/https:/browse.arxiv.org/html/2402.19200v1/extracted/5440247/figures/prompt_jailbreaking.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary"><strong>Summary:</strong></h3>
<ul>
<li>Prompt Stealing Attacks against Large Language Models (LLMs) pose a risk to intellectual property rights due to the exposure of input-output pairs.</li>
<li>PRSA, a novel attack framework, is proposed to infer target prompts by analyzing critical features of input-output pairs using a generative model.</li>
<li>PRSA consists of two main phases: prompt mutation and prompt pruning.</li>
<li>Prompt mutation uses a prompt attention algorithm based on differential feedback to optimize the generative model.</li>
<li>Prompt pruning identifies and masks input-dependent words to enhance the surrogate prompt’s generality.</li>
<li>PRSA is effective in stealing prompts across various categories in the marketplace and LLM application platforms.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings">Major Findings:</h3>
<ol type="1">
<li>PRSA poses a severe threat in real-world scenarios, stealing prompts effectively across various categories in the marketplace and</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19200v1">https://arxiv.org/abs/2402.19200v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19200v1">https://browse.arxiv.org/html/2402.19200v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>12660</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>robustness</category>
  <category>security</category>
  <category>prompt-engineering</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/PRSA_Prompt_Reverse_Stealing_Attacks_against_Large_Language_Models/2024-02-29-PRSA_Prompt_Reverse_Stealing_Attacks_against_Large_Language_Models.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19200v1/extracted/5440247/figures/prompt_jailbreaking.png" medium="image" type="image/png"/>
</item>
<item>
  <title>OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models</title>
  <dc:creator>Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, Ritankar Das</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/OpenMedLM_Prompt_engineering_can_out_perform_fine_tuning_in_medical_question_answering_with_open_source_large_language_models/2024-02-29-OpenMedLM_Prompt_engineering_can_out_perform_fine_tuning_in_medical_question_answering_with_open_source_large_language_models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/img/2402.19371v1/image_1.png" class="img-fluid"></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary"><strong>Summary:</strong></h3>
<ul>
<li>The article presents OpenMedLM, a prompting platform that delivers state-of-the-art performance for open-source large language models (LLMs) on medical benchmarks.</li>
<li>OpenMedLM utilizes various prompting strategies, including zero-shot, few-shot, chain-of-thought, and ensemble/self-consistency voting, to optimize the performance of open-source foundation models.</li>
<li>The platform demonstrates that open-source models can provide transparency and compliance required in healthcare, surpassing previous best-performing models that relied on extensive fine-tuning and specialized medical data.</li>
</ul>
</section>
<section id="major-findings" class="level3">
<h3 class="anchored" data-anchor-id="major-findings"><strong>Major Findings:</strong></h3>
<ol type="1">
<li>OpenMedLM achieves state-of-the-art results on three common medical LLM benchmarks, outperforming previous models that used computationally costly extensive fine-tuning.</li>
<li>The model displays the first results</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19371v1">https://arxiv.org/abs/2402.19371v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19371v1">https://browse.arxiv.org/html/2402.19371v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>12122</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>education</category>
  <category>architectures</category>
  <category>prompt-engineering</category>
  <guid>https://bayesian-beagle.netlify.app/posts/OpenMedLM_Prompt_engineering_can_out_perform_fine_tuning_in_medical_question_answering_with_open_source_large_language_models/2024-02-29-OpenMedLM_Prompt_engineering_can_out_perform_fine_tuning_in_medical_question_answering_with_open_source_large_language_models.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://bayesian-beagle.netlify.app/img/2402.19371v1/image_1.png" medium="image" type="image/png" height="77" width="144"/>
</item>
<item>
  <title>Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models</title>
  <dc:creator>Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao</dc:creator>
  <link>https://bayesian-beagle.netlify.app/posts/Towards_Tracing_Trustworthiness_Dynamics_Revisiting_Pre_training_Period_of_Large_Language_Models/2024-02-29-Towards_Tracing_Trustworthiness_Dynamics_Revisiting_Pre_training_Period_of_Large_Language_Models.html</link>
  <description><![CDATA[ 



<p><img src="https://bayesian-beagle.netlify.app/posts/Towards_Tracing_Trustworthiness_Dynamics_Revisiting_Pre_training_Period_of_Large_Language_Models/https:/browse.arxiv.org/html/2402.19465v1/x1.png" class="img-fluid"></p>
<section id="tracing-trustworthiness-dynamics-in-large-language-models-pre-training" class="level3">
<h3 class="anchored" data-anchor-id="tracing-trustworthiness-dynamics-in-large-language-models-pre-training">Tracing Trustworthiness Dynamics in Large Language Models Pre-training</h3>
<p><strong>Summary:</strong></p>
<ul>
<li>This study explores the trustworthiness of large language models (LLMs) during the pre-training phase, focusing on five dimensions: reliability, privacy, toxicity, fairness, and robustness.</li>
<li>Linear probing is applied to LLMs, revealing that they can distinguish concepts in each trustworthiness dimension early in pre-training.</li>
<li>Steering vectors are extracted from LLMs’ pre-training checkpoints to enhance their trustworthiness.</li>
<li>Mutual information probing during pre-training uncovers a two-phase phenomenon: fitting and compression.</li>
</ul>
<p><strong>Major Findings:</strong></p>
<ol type="1">
<li>LLMs in early pre-training can already distinguish concepts in trustworthiness dimensions through linear probing.</li>
<li>Steering vectors extracted from pre-training checkpoints can enhance the LLM’s trustworthiness</li>
</ol>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>accounts/fireworks/models/mixtral-8x7b-instruct</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-03-13</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="https://arxiv.org/abs/2402.19465v1">https://arxiv.org/abs/2402.19465v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2402.19465v1">https://browse.arxiv.org/html/2402.19465v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>False</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>10553</td>
</tr>
</tbody>
</table>


</section>

 ]]></description>
  <category>production</category>
  <category>architectures</category>
  <guid>https://bayesian-beagle.netlify.app/posts/Towards_Tracing_Trustworthiness_Dynamics_Revisiting_Pre_training_Period_of_Large_Language_Models/2024-02-29-Towards_Tracing_Trustworthiness_Dynamics_Revisiting_Pre_training_Period_of_Large_Language_Models.html</guid>
  <pubDate>Thu, 29 Feb 2024 05:00:00 GMT</pubDate>
  <media:content url="https://browse.arxiv.org/html/2402.19465v1/x1.png" medium="image" type="image/png"/>
</item>
</channel>
</rss>
