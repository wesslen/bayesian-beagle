
---
title: "Large Language Models Make Sample-Efficient Recommender Systems"
id: "2406.02368v1"
description: "LLMs improve recommender systems' efficiency, needing less training data for superior performance."
author: Jianghao Lin, Xinyi Dai, Rong Shan, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02368v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02368v1/x1.png)

### Summary:

- The paper explores the use of large language models (LLMs) in recommender systems (RSs) to improve their sample efficiency, which refers to the model's ability to achieve high performance with a limited amount of training data.
- The authors propose a framework called Laser, which validates the core viewpoint that LLMs can make RSs more sample-efficient.
- Laser operates in two ways: (1) LLMs themselves as sample-efficient recommenders and (2) LLMs as feature generators and encoders to make conventional RSs more sample-efficient.
- The authors claim that their proposed Laser framework requires only a small fraction of training samples to match or even surpass conventional RSs trained on the entire training set.

### Major Findings:

1. **LLMs as Sample-Efficient Recommenders:** The paper demonstrates that LLMs can be used as standalone recommenders, achieving high performance with limited training data.
2. **LLMs Enhancing Conventional RSs:** The authors show that LLMs can be used to generate and encode features, making conventional RSs more sample-efficient.
3. **Superior Sample Efficiency:** The proposed Laser framework is shown to require only a small fraction of training samples to match or even surpass conventional RSs trained on the entire training set.

### Analysis and Critique:

- The paper presents an innovative approach to improving the sample efficiency of RSs using LLMs. However, the authors do not discuss the potential limitations or biases that could arise from using LLMs in this context.
- The authors claim that their proposed Laser framework can match or even surpass conventional RSs with limited training data. However, they do not provide a detailed comparison of the performance of Laser with other state-of-the-art RSs.
- The paper does not discuss the potential challenges or risks associated with using LLMs in RSs, such as the risk of generating inappropriate or irrelevant recommendations.
- The authors do not provide a detailed analysis of the computational resources required to train and deploy LLMs in RSs, which could be a significant limitation for practical applications.
- The paper does not discuss the potential impact of the quality and diversity of the training data on the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-14       |
| Abstract | [https://arxiv.org/abs/2406.02368v1](https://arxiv.org/abs/2406.02368v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02368v1](https://browse.arxiv.org/html/2406.02368v1)       |
| Truncated       | False       |
| Word Count       | 3649       |