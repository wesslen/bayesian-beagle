
---
title: "The Calibration Gap between Model and Human Confidence in Large Language Models"
id: "2401.13835v1"
description: "Large language models need well-calibrated confidence to be trusted. User perception can be improved with tailored explanations."
author: ['Mark Steyvers', 'Heliodoro Tejeda', 'Aakriti Kumar', 'Catarina Belem', 'Sheer Karny', 'Xinyue Hu', 'Lukas Mayer', 'Padhraic Smyth']
date: "2024-01-24"
image: "https://browse.arxiv.org/html/2401.13835v1/extracted/5365928/exp_setup_high_level.png"
categories: ['social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.13835v1/extracted/5365928/exp_setup_high_level.png)

### Summary of the Article:
The article discusses the disparity between external human confidence in large language models (LLMs) and the internal confidence of the model. It examines user perception of LLM confidence and investigates the impact of tailored explanations on this perception. The findings underscore the need for transparent communication of confidence levels in LLMs, particularly in high-stakes applications.

### Major Findings:
1. The Calibration Gap:
    - Users tend to overestimate the model's confidence and accuracy when presented with default explanations, leading to a significant disparity between LLMs' internal confidence and human perception.
    - Modifying explanations based on model confidence significantly reduces this calibration gap, aligning human perception more closely with the model's actual confidence levels.

2. Effects of Modified Explanations:
    - Tailored explanations, expressing varying levels of uncertainty, have a strong influence on human confidence, leading to lower confidence levels when uncertainties are explicitly mentioned.
    - The modified explanations based on model confidence narrow the calibration and discrimination gaps, improving the alignment between human confidence and LLM accuracy.

3. User Expertise and Accuracy:
    - Participants lack specialized knowledge and had limited success in accurately answering questions independent of LLM's explanations.
    - Self-assessed expertise did not significantly impact the participants' ability to estimate LLM performance.

### Analysis and Critique:
The article effectively demonstrates the impact of tailored explanations on aligning user perception with LLM confidence levels. However, the focus on a specific type of question and a single dataset limits the generalizability of the findings. Additionally, the approach to modifying prompts based on internal uncertainty may need refinement for more efficient single-step execution. Furthermore, the study's emphasis on multiple-choice questions prompts the need for investigations across broader scenarios. Overall, the article's findings suggest the critical role of transparent communication in the interaction between LLMs and users. However, further research is necessary to address potential limitations and enhance the applicability of the study's conclusions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-29       |
| Abstract | [http://arxiv.org/abs/2401.13835v1](http://arxiv.org/abs/2401.13835v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.13835v1](https://browse.arxiv.org/html/2401.13835v1)       |
| Truncated       | False       |
| Word Count       | 11792       |