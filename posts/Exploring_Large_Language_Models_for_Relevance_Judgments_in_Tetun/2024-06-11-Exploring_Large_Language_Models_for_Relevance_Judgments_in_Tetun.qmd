
---
title: "Exploring Large Language Models for Relevance Judgments in Tetun"
id: "2406.07299v1"
description: "LLMs can automate relevance assessments in low-resource languages, with results similar to high-resource languages."
author: Gabriel de Jesus, Sérgio Nunes
date: "2024-06-11"
image: "../../../bayesian-beagle.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

This paper explores the feasibility of using large language models (LLMs) to automate relevance assessments, particularly within the context of low-resource languages. The study employs LLMs to automate relevance judgment tasks by providing a series of query-document pairs in Tetun as input text. The models are tasked with assigning relevance scores to each pair, which are then compared to those from human annotators to evaluate inter-annotator agreement levels. The results reveal that LLMs can be used in low-resource language scenarios to automate relevance judgment tasks, with an inter-annotator agreement of Cohen’s kappa score of 0.2634 when evaluated using the 70B variant of the LLaMA3 model.

### Major Findings:

1. LLMs can be used to automate relevance judgment tasks in low-resource languages, such as Tetun, with an inter-annotator agreement of Cohen’s kappa score of 0.2634 when evaluated using the 70B variant of the LLaMA3 model.
2. The study demonstrates that LLMs can achieve results comparable to traditional methods, with ongoing improvement in the quality of automated relevance judgment tasks as LLMs continue to evolve.
3. The use of LLMs for automated relevance judgments can provide cost-effective solutions with judgment agreement comparable to human assessors.

### Analysis and Critique:

While the study demonstrates the feasibility of using LLMs for automated relevance judgments in low-resource languages, there are some limitations and potential biases that should be considered. The study primarily focuses on the Tetun language, and the results may not be generalizable to other low-resource languages. Additionally, the study uses a limited number of query-document pairs, which may not fully capture the complexity and diversity of relevance judgments in low-resource languages.

Furthermore, the study does not address potential methodological issues, such as the impact of different LLM architectures or the use of different evaluation metrics. The study also does not discuss conflicting evidence or areas that require further research or clarification.

Overall, the study provides valuable insights into the use of LLMs for automated relevance judgments in low-resource languages. However, further

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.07299v1](https://arxiv.org/abs/2406.07299v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07299v1](https://browse.arxiv.org/html/2406.07299v1)       |
| Truncated       | False       |
| Word Count       | 3697       |