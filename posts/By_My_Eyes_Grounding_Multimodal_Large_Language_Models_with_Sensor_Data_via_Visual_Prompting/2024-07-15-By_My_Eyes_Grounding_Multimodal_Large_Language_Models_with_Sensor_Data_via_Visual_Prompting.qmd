
---
title: "By My Eyes: Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting"
id: "2407.10385v1"
description: "Visual prompts with MLLMs improve sensor data accuracy by 10% and reduce token costs by 15.8×\times×, outperforming text-based prompts."
author: Hyungjun Yoon, Biniyam Aschalew Tolera, Taesik Gong, Kimin Lee, Sung-Ju Lee
date: "2024-07-15"
image: "https://browse.arxiv.org/html/2407.10385v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.10385v1/x1.png)

### Summary:

The paper proposes a visual prompting approach for sensor data using multimodal large language models (MLLMs). The authors design a visual prompt that directs MLLMs to utilize visualized sensor data alongside the target sensory task descriptions. They also introduce a visualization generator that automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge. The proposed approach is evaluated on nine sensory tasks involving four sensing modalities, achieving an average of 10% higher accuracy than text-based prompts and reducing token costs by 15.8.

### Major Findings:

1. The proposed visual prompting approach for sensor data using MLLMs achieves an average of 10% higher accuracy than text-based prompts.
2. The visualization generator automates the creation of optimal visualizations tailored to a given sensory task, eliminating the need for prior task-specific knowledge.
3. The proposed approach reduces token costs by 15.8 compared to text-based prompts.

### Analysis and Critique:

1. The paper presents a novel approach to grounding MLLMs with sensor data by providing visualized sensor data as images, which improves performance and reduces costs compared to the text-based baseline.
2. The visualization generator is a significant contribution, as it enables MLLMs to independently generate optimal visualizations using tools available in public libraries.
3. The experiments conducted on nine different sensory tasks across four modalities demonstrate the broad applicability of the proposed approach.
4. However, the paper does not discuss the limitations of the proposed approach, such as the potential for overfitting to specific visualizations or the generalizability of the visualization generator to other sensing tasks.
5. The paper also does not provide a comparison with other state-of-the-art methods for grounding MLLMs with sensor data, which could have strengthened the evaluation of the proposed approach.
6. The paper could benefit from a more detailed analysis of the results, including error analysis and ablation studies, to better understand the strengths and weaknesses of the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.10385v1](https://arxiv.org/abs/2407.10385v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.10385v1](https://browse.arxiv.org/html/2407.10385v1)       |
| Truncated       | False       |
| Word Count       | 7689       |