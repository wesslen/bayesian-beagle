
---
title: "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models"
id: "2402.14007v1"
description: "Text watermarking lacks cross-lingual consistency, vulnerable to removal attack, defense method proposed. AUC improved."
author: Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.14007v1/x1.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14007v1/x1.png)

### **Summary:**
- The study introduces the concept of "cross-lingual consistency" in text watermarking for large language models (LLMs).
- Preliminary empirical results reveal that current text watermarking technologies lack consistency when texts are translated into various languages.
- The study proposes a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking and effectively remove watermarks.

### **Major Findings:**
1. Current text watermarking technologies lack consistency across languages.
2. The Cross-lingual Watermark Removal Attack (CWRA) can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss.
3. The study proposes a defense method that increases the AUC from 0.67 to 0.88 under CWRA.

### **Analysis and Critique:**
- The study provides valuable insights into the deficiencies of current text watermarking technologies in maintaining cross-lingual consistency.
- The proposed Cross-lingual Watermark Removal Attack (CWRA) poses a significant challenge to text watermarking by efficiently eliminating watermarks without compromising performance.
- The defense method proposed in the study shows promise in improving cross-lingual consistency, but it has limitations in terms of applicability and scope. Further research is needed to address these limitations and enhance the effectiveness of the defense method.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.14007v1](https://arxiv.org/abs/2402.14007v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14007v1](https://browse.arxiv.org/html/2402.14007v1)       |
| Truncated       | False       |
| Word Count       | 7230       |