
---
title: "Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach"
id: "2405.19648v1"
description: "TL;DR: New method detects LLM hallucinations using simple classifiers and four numerical features, outperforming current methods. Code: https://github.com/Baylor-AI/HalluDetect."
author: Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, Tomas Cerny
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.19648v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19648v1/x1.png)

### Summary:

The paper introduces a supervised learning approach to detect hallucinations in Large Language Models (LLMs) using two simple classifiers and four numerical features derived from tokens and vocabulary probabilities. The method outperforms state-of-the-art outcomes in multiple tasks across three different benchmarks. The authors also provide a comprehensive examination of the strengths and weaknesses of their approach, highlighting the significance of the features utilized and the LLM employed as an evaluator.

### Major Findings:

1. The proposed supervised learning approach using four features to detect hallucinations in conditional text generated by LLMs achieves success with two classifiers: Logistic Regression (LR) and a Simple Neural Network (SNN).
2. The performance of this approach is evaluated across three datasets, comparing it with state-of-the-art methods and highlighting its strengths and weaknesses.
3. The impact of using the same LLM-Generator vs. different LLMs as evaluators is explored, finding that alternative LLMs provide better indicators to identify hallucinations.
4. The difference in performance when using smaller LLM evaluators compared to bigger LLM evaluators like LLaMa-Chat-7b is analyzed.
5. A feature importance study with ablation and coefficients analysis is conducted to understand the significance of each feature in the detection of hallucinations.

### Analysis and Critique:

1. The paper presents a promising approach to detect hallucinations in LLMs, which could be valuable for ensuring the reliability of applications relying on LLM-generated content.
2. The use of two simple classifiers and four numerical features derived from tokens and vocabulary probabilities makes the method resource-efficient and easy to implement.
3. The evaluation of the approach across three different benchmarks and the comparison with state-of-the-art methods provide a comprehensive understanding of its strengths and weaknesses.
4. The exploration of the impact of using the same LLM-Generator vs. different LLMs as evaluators is an important contribution, as it highlights the potential benefits of using alternative models for evaluation purposes.
5. The feature importance study with ablation and coefficients analysis provides insights into the significance of each feature in the detection of hallucinations, which could be useful for future research in this area.
6.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19648v1](https://arxiv.org/abs/2405.19648v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19648v1](https://browse.arxiv.org/html/2405.19648v1)       |
| Truncated       | False       |
| Word Count       | 7103       |