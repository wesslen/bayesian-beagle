
---
title: "Soft Self-Consistency Improves Language Model Agents"
id: "2402.13212v1"
description: "Sampling and scoring improve language model generations; Soft Self-Consistency increases performance and efficiency."
author: Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13212v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13212v1/x1.png)

### Summary:
- Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer.
- Current "sample and select" methods such as self-consistency (SC) rely on majority voting to score answers, but this method is not effective for tasks with many distinct and valid answers.
- Soft Self-Consistency (Soft-SC) replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance.

### Major Findings:
1. Soft-SC outperforms SC with the same number of samples, e.g., by up to 3% on WebShop using CodeLlama-34B.
2. Soft-SC exhibits better sample efficiency, i.e., produces better performance than SC with fewer samples.
3. Soft-SC scales better with model size than SC, increasing performance by 5% on Bash as model size increases from 7B to 70B, as opposed to only 1% improvement by SC.

### Analysis and Critique:
- Soft-SC improves performance and efficiency, but it still requires multiple samples from an LLM, making it more costly than greedy decoding.
- The method may not be suitable for tasks with excessive diversity, as no majority will emerge, and it still requires further research to address these limitations.
- The potential for negative applications and malicious use of large language models is a concern, and the impact of Soft-SC on these applications should be carefully considered.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.13212v1](https://arxiv.org/abs/2402.13212v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13212v1](https://browse.arxiv.org/html/2402.13212v1)       |
| Truncated       | False       |
| Word Count       | 7396       |