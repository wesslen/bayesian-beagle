
---
title: "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models"
id: "2402.04614v1"
description: "LLMs generate self-explanations, but their faithfulness is questionable. Plausibility may compromise faithfulness."
author: Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju
date: "2024-02-07"
image: "https://browse.arxiv.org/html/2402.04614v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.04614v1/x1.png)

### **Summary:**
- Large Language Models (LLMs) generate self-explanations (SEs) that are conversational and plausible but lack understanding of their faithfulness.
- The dichotomy between faithfulness and plausibility in SEs generated by LLMs is discussed, highlighting the need for faithful explanations in high-stakes decision-making.
- The current trend towards increasing the plausibility of explanations may come at the cost of diminishing their faithfulness.

### **Major Findings:**
1. LLMs are adept at generating plausible explanations but lack faithfulness, raising concerns about their reliability and trustworthiness.
2. The lack of universally agreed-upon metrics to quantify the faithfulness of self-explanations is a significant challenge.
3. Different applications demand varying levels of faithfulness and plausibility in SEs provided by LLMs.

### **Analysis and Critique:**
- Evaluating the faithfulness of explanations is a non-trivial problem due to the lack of ground truth explanations, making assessments using saliency maps and other gradient-based methods nearly impossible.
- The overemphasis on plausibility over faithfulness in LLM explanations has led to misplaced trust, over-reliance, and security concerns in high-stakes applications.
- Different applications require varying levels of faithfulness and plausibility in SEs provided by LLMs, highlighting the importance of tailoring the explanation style to the application domain.
- The community should prioritize developing reliable metrics to characterize the faithfulness of explanations and pioneering novel strategies to generate more faithful SEs.

In conclusion, the review emphasizes the need for LLMs to provide explanations that are both plausible and faithful, addressing the challenges of understanding the reasoning processes of these complex models. The community is called upon to unite in addressing these challenges and to collaborate and innovate to ensure that the explanations provided by LLMs solve the dichotomy of plausibility and faithfulness, enhancing user trust, and advancing the frontiers of explainable AI.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.04614v1](https://arxiv.org/abs/2402.04614v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04614v1](https://browse.arxiv.org/html/2402.04614v1)       |
| Truncated       | False       |
| Word Count       | 7366       |