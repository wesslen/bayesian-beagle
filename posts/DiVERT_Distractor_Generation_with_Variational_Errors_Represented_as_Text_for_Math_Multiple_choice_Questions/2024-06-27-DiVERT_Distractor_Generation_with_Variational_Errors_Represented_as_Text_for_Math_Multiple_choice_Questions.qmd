
---
title: "DiVERT: Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions"
id: "2406.19356v1"
description: "DiVERT outperforms state-of-the-art distractor generation methods in math MCQs, using a 7B parameter LLM and producing human-like error labels."
author: Nigel Fernandez, Alexander Scarlatos, Simon Woodhead, Andrew Lan
date: "2024-06-27"
image: "https://browse.arxiv.org/html/2406.19356v1/x1.png"
categories: ['education', 'prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.19356v1/x1.png)

### Summary:

The paper introduces DiVERT, a novel variational approach for generating high-quality distractors in math multiple-choice questions (MCQs). The approach aims to learn an interpretable representation of errors behind distractors, which is crucial for both assessment and pedagogical value. DiVERT outperforms state-of-the-art approaches using GPT-o on downstream distractor generation and leads to error labels comparable in quality to human-authored ones.

### Major Findings:

1. DiVERT, a variational approach, learns an interpretable representation of errors behind distractors in math MCQs, outperforming state-of-the-art approaches on downstream distractor generation.
2. The approach uses a base open-source LLM with 7B parameters, demonstrating that high-quality distractors can be generated without relying on large language models.
3. Human evaluation with math educators shows that DiVERT leads to error labels of comparable quality to human-authored ones.

### Analysis and Critique:

The paper presents a promising approach to generating high-quality distractors in math MCQs. The use of a variational approach to learn an interpretable representation of errors is a novel contribution. However, the paper does not discuss the limitations or potential biases of the approach. Additionally, the evaluation is primarily based on a single dataset, and the generalizability of the approach to other datasets or domains is not explored. Further research is needed to evaluate the approach's performance in different contexts and to identify potential limitations or areas for improvement.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.19356v1](https://arxiv.org/abs/2406.19356v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.19356v1](https://browse.arxiv.org/html/2406.19356v1)       |
| Truncated       | False       |
| Word Count       | 9499       |