
---
title: "SeaLLMs 3: Open Foundation and Chat Multilingual Large Language Models for Southeast Asian Languages"
id: "2407.19672v1"
description: "SeaLLMs 3: Cost-effective, versatile model for Southeast Asian languages, prioritizing safety and inclusivity."
author: Wenxuan Zhang, Hou Pong Chan, Yiran Zhao, Mahani Aljunied, Jianyu Wang, Chaoqun Liu, Yue Deng, Zhiqiang Hu, Weiwen Xu, Yew Ken Chia, Xin Li, Lidong Bing
date: "2024-07-29"
image: "https://browse.arxiv.org/html/2407.19672v1/x1.png"
categories: ['social-sciences', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.19672v1/x1.png)

### Summary:

SeaLLMs 3 is a large language model designed for Southeast Asian languages, addressing the lack of inclusivity and equitable distribution of AI advancements across diverse linguistic and cultural communities. The model covers a comprehensive range of languages spoken in the region, including English, Chinese, Indonesian, Vietnamese, Thai, Tagalog, Malay, Burmese, Khmer, Lao, Tamil, and Javanese.

SeaLLMs 3 employs efficient language enhancement techniques and a specially constructed instruction tuning dataset, significantly reducing training costs while maintaining high performance and versatility. The model excels in tasks such as world knowledge, mathematical reasoning, translation, and instruction following, achieving state-of-the-art performance among similarly sized models.

The model's development prioritized safety and reliability, addressing both general and culture-specific considerations and incorporating mechanisms to reduce hallucinations. The model is trained to be aware of its knowledge boundary and refuse what it does not know, with a novel benchmark, SeaRefuse, introduced to evaluate this capability.

### Major Findings:

1. **Inclusive AI for Southeast Asian Languages**: SeaLLMs 3 is designed to bridge the gap in AI development for Southeast Asian languages, providing advanced large language model capabilities to underserved linguistic and cultural communities.
2. **Efficient Language Enhancement**: The model employs efficient language enhancement techniques, training language-specific neurons only based on a foundation model, significantly reducing overall training cost.
3. **High Performance and Versatility**: SeaLLMs 3 achieves state-of-the-art performance among models with similar sizes, excelling across a diverse array of tasks such as world knowledge, mathematical reasoning, translation, and instruction following.

### Analysis and Critique:

While SeaLLMs 3 represents a significant advancement in the development of large language models for Southeast Asian languages, there are potential areas for improvement and further research.

1. **Limited Language Coverage**: While the model covers a comprehensive range of languages spoken in Southeast Asia, there are still many languages in the region that are not included. Future iterations could aim to expand the model's language coverage.
2. **Data Availability and Quality

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.19672v1](https://arxiv.org/abs/2407.19672v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.19672v1](https://browse.arxiv.org/html/2407.19672v1)       |
| Truncated       | False       |
| Word Count       | 5720       |