
---
title: "Efficient Training of Large Language Models on Distributed Infrastructures: A Survey"
id: "2407.20018v1"
description: "TL;DR: Survey explores advancements in training systems for LLMs, including infrastructure, parallelism, optimizations, and reliability, with a focus on optical computing."
author: Jiangfei Duan, Shuo Zhang, Zerui Wang, Lijuan Jiang, Wenwen Qu, Qinghao Hu, Guoteng Wang, Qizhen Weng, Hang Yan, Xingcheng Zhang, Xipeng Qiu, Dahua Lin, Yonggang Wen, Xin Jin, Tianwei Zhang, Peng Sun
date: "2024-07-29"
image: "https://browse.arxiv.org/html/2407.20018v1/x1.png"
categories: ['programming', 'architectures', 'production', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.20018v1/x1.png)

**Summary:**

The paper provides a comprehensive overview of the challenges and advancements in training large language models (LLMs) on distributed infrastructures. It discusses various AI accelerators, network infrastructure, resource scheduling, and parallelism schemes for LLM training. The paper also covers heterogeneity in LLM training, low-bit fixed point training, memory optimization techniques, in-network aggregation, and checkpoint-free recovery methods. The survey aims to provide insights into improving LLM training systems and tackling ongoing challenges, such as scalability, efficiency, and reliability.

**Key Terms:**

* Large Language Models (LLMs)
* Distributed training
* Machine learning systems
* AI accelerators
* Networking
* Storage
* Scheduling
* Parallelism strategies
* Computation optimizations
* Communication optimizations
* Memory optimizations
* Reliability

**Major

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.20018v1](https://arxiv.org/abs/2407.20018v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.20018v1](https://browse.arxiv.org/html/2407.20018v1)       |
| Truncated       | True       |
| Word Count       | 37218       |