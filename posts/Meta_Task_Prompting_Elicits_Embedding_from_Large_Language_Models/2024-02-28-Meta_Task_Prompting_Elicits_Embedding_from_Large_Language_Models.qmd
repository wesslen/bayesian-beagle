
---
title: "Meta-Task Prompting Elicits Embedding from Large Language Models"
id: "2402.18458v1"
description: "MetaEOL is a new unsupervised embedding method for generating high-quality sentence embeddings from LLMs."
author: Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, Andrew Yates
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18458v1/extracted/5438050/Figures/Figure-1.png"
categories: ['production', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18458v1/extracted/5438050/Figures/Figure-1.png)

### **Summary:**
- MetaEOL introduces a new unsupervised embedding method for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering.
- The method leverages meta-task prompting to guide LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects.
- Comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models.

### Major Findings:
1. MetaEOL demonstrates competitive performance on STS benchmarks and excels in downstream tasks, surpassing contrastive-trained models.
2. Simply averaging embeddings from different meta-tasks without any training leads to general embeddings that are competitive to contrastive-trained models on STS tasks and can achieve the best average result in downstream tasks.
3. Incrementally integrating more meta-tasks yields consistent improvements across STS tasks, showcasing high generalities, and highlighting the significant impact of meta-task integration on overall performance.

### Analysis and Critique:
- The article provides a comprehensive and innovative approach to generating high-quality sentence embeddings from LLMs without the need for training. The method's performance on STS benchmarks and downstream tasks is impressive, surpassing contrastive-trained models.
- The article acknowledges limitations in terms of computational overhead and restricted evaluation benchmarks, providing avenues for future research and improvement.
- The experiments and findings are well-documented and provide valuable insights into the potential of MetaEOL for diverse sentence-centric scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-29       |
| Abstract | [https://arxiv.org/abs/2402.18458v1](https://arxiv.org/abs/2402.18458v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18458v1](https://browse.arxiv.org/html/2402.18458v1)       |
| Truncated       | False       |
| Word Count       | 5849       |