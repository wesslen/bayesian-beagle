
---
title: "RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models"
id: "2402.13463v1"
description: "LLMs struggle to accept and follow user feedback, prompting need for recall-and-repeat prompts."
author: Jianhao Yan, Yun Luo, Yue Zhang
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13463v1/extracted/5421565/editing.png"
categories: ['robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13463v1/extracted/5421565/editing.png)

### **Summary:**
- The paper introduces RefuteBench, a benchmark for evaluating large language models' (LLMs) ability to accept and follow refuting feedback.
- The evaluation covers tasks such as question answering, machine translation, and email writing to assess LLMs' responsiveness to refuting instructions.
- Findings indicate that LLMs are stubborn and tend to adhere to their internal knowledge, often failing to comply with user feedback.
- The paper proposes a recall-and-repeat prompts strategy to enhance the model's responsiveness to feedback.

### **Major Findings:**
1. LLMs exhibit a tendency to adhere to their pre-existing knowledge, with GPT-4 and Claude-2 showing the highest flexibility.
2. It is challenging for LLMs to apply feedback to generalization queries, leading to performance degradation of 10-20%.
3. LLMs gradually forget feedback and revert to their internal knowledge as the dialogue proceeds.

### **Analysis and Critique:**
- The paper provides a comprehensive evaluation of LLMs' responsiveness to refuting instructions, highlighting their stubbornness and resistance to user feedback.
- The proposed recall-and-repeat prompts strategy shows promising results in improving LLMs' response rate to refuting instructions.
- The study is limited to three specific tasks (question answering, machine translation, and email writing), and further research is needed to evaluate LLMs' responsiveness in other application domains.
- The correlation between feedback acceptance and response rate suggests that LLMs' ability to accept feedback positively impacts their performance in subsequent queries.
- The findings have implications for improving LLMs' responsiveness to user feedback and addressing their stubbornness in following refuting instructions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.13463v1](https://arxiv.org/abs/2402.13463v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13463v1](https://browse.arxiv.org/html/2402.13463v1)       |
| Truncated       | False       |
| Word Count       | 7146       |