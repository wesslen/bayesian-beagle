<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jiaqi Wang">
<meta name="author" content="Zihao Wu">
<meta name="author" content="Yiwei Li">
<meta name="author" content="Hanqi Jiang">
<meta name="author" content="Peng Shu">
<meta name="author" content="Enze Shi">
<meta name="author" content="Huawen Hu">
<meta name="author" content="Chong Ma">
<meta name="author" content="Yiheng Liu">
<meta name="author" content="Xuhui Wang">
<meta name="author" content="Yincheng Yao">
<meta name="author" content="Xuan Liu">
<meta name="author" content="Huaqin Zhao">
<meta name="author" content="Zhengliang Liu">
<meta name="author" content="Haixing Dai">
<meta name="author" content="Lin Zhao">
<meta name="author" content="Bao Ge">
<meta name="author" content="Xiang Li">
<meta name="author" content="Tianming Liu">
<meta name="author" content="Shu Zhang">
<meta name="dcterms.date" content="2024-01-09">
<meta name="description" content="Large language models (LLMs) integrate with robots for task planning, with a focus on multimodal LLMs for enhanced performance.">

<title>Bayesian beagle - Large Language Models for Robotics: Opportunities, Challenges, and Perspectives</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Bayesian beagle - Large Language Models for Robotics: Opportunities, Challenges, and Perspectives">
<meta property="og:description" content="Large language models (LLMs) integrate with robots for task planning, with a focus on multimodal LLMs for enhanced performance.">
<meta property="og:image" content="https://browse.arxiv.org/html/2401.04334v1/x1.png">
<meta property="og:site-name" content="Bayesian beagle">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../icon.jpg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Bayesian beagle</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Bayesian beagle</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/wesslen/bayesian-beagle" rel="" target=""><i class="bi bi-github" role="img" aria-label="github">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Large Language Models for Robotics: Opportunities, Challenges, and Perspectives</h1>
  <div class="quarto-categories">
    <div class="quarto-category">social-sciences</div>
    <div class="quarto-category">hci</div>
  </div>
  </div>

<div>
  <div class="description">
    Large language models (LLMs) integrate with robots for task planning, with a focus on multimodal LLMs for enhanced performance.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Jiaqi Wang </p>
             <p>Zihao Wu </p>
             <p>Yiwei Li </p>
             <p>Hanqi Jiang </p>
             <p>Peng Shu </p>
             <p>Enze Shi </p>
             <p>Huawen Hu </p>
             <p>Chong Ma </p>
             <p>Yiheng Liu </p>
             <p>Xuhui Wang </p>
             <p>Yincheng Yao </p>
             <p>Xuan Liu </p>
             <p>Huaqin Zhao </p>
             <p>Zhengliang Liu </p>
             <p>Haixing Dai </p>
             <p>Lin Zhao </p>
             <p>Bao Ge </p>
             <p>Xiang Li </p>
             <p>Tianming Liu </p>
             <p>Shu Zhang </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 9, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><img src="https://browse.arxiv.org/html/2401.04334v1/x1.png" class="img-fluid"></p>
<section id="large-language-models-for-robotics-opportunities-challenges-and-perspectives" class="level1">
<h1>Large Language Models for Robotics: Opportunities, Challenges, and Perspectives</h1>
<section id="takeaways" class="level2">
<h2 class="anchored" data-anchor-id="takeaways">Takeaways:</h2>
<ol type="1">
<li><strong>LLMs</strong> have been increasingly integrated into robotic task planning due to their advanced reasoning and language comprehension capabilities.</li>
<li>The integration of <strong>multimodal GPT-4V</strong> has shown promise in enhancing robot performance in embodied tasks, as demonstrated by diverse datasets.</li>
<li>LLM-centric embodied intelligence holds potential for various applications, such as precision agriculture, healthcare, and brain-computer interfaces.</li>
</ol>
</section>
<section id="i-introduction" class="level2">
<h2 class="anchored" data-anchor-id="i-introduction">I Introduction</h2>
<ul>
<li>Large pre-trained models have demonstrated remarkable capabilities across complex tasks in various domains.</li>
<li>The utilization of <strong>instruction tuning</strong> and <strong>alignment tuning</strong> has become the primary approach to adapt LLMs for specific objectives.</li>
</ul>
</section>
<section id="ii-related-work" class="level2">
<h2 class="anchored" data-anchor-id="ii-related-work">II Related Work</h2>
<section id="ii-a-llm-for-robotics" class="level3">
<h3 class="anchored" data-anchor-id="ii-a-llm-for-robotics">II-A LLM for Robotics</h3>
<ul>
<li>LLMs exhibit exceptional natural language understanding and commonsense reasoning capabilities, contributing to enhanced comprehension and execution for robots.</li>
</ul>
</section>
<section id="ii-b-multimodal-task-planning-with-llms" class="level3">
<h3 class="anchored" data-anchor-id="ii-b-multimodal-task-planning-with-llms">II-B Multimodal Task Planning with LLMs</h3>
<ul>
<li>Multimodal LLMs excel in interpreting and correlating multiple data streams, broadening their role from language processing to more integrative functions.</li>
</ul>
</section>
</section>
<section id="iii-scope-of-robotic-tasks" class="level2">
<h2 class="anchored" data-anchor-id="iii-scope-of-robotic-tasks">III Scope of Robotic Tasks</h2>
<section id="iii-a-planning" class="level3">
<h3 class="anchored" data-anchor-id="iii-a-planning">III-A Planning</h3>
<section id="iii-a1-natural-language-understanding" class="level4">
<h4 class="anchored" data-anchor-id="iii-a1-natural-language-understanding">III-A1 Natural Language Understanding</h4>
<ul>
<li>LLMs excel in interpreting natural language instructions and integrating multimodal information to create actionable guidance for virtual agents.</li>
</ul>
</section>
<section id="iii-a2-complex-task-reasoning-and-decision-making" class="level4">
<h4 class="anchored" data-anchor-id="iii-a2-complex-task-reasoning-and-decision-making">III-A2 Complex Task Reasoning and Decision-making</h4>
<ul>
<li>LLMs advance complex task reasoning and decision-making through reinforcement learning and collaboration with other modalities.</li>
</ul>
</section>
<section id="iii-a3-human-robot-interaction" class="level4">
<h4 class="anchored" data-anchor-id="iii-a3-human-robot-interaction">III-A3 Human-robot interaction</h4>
<ul>
<li>Integration of reinforcement learning with human feedback enables robots to continuously improve their task execution.</li>
</ul>
</section>
</section>
<section id="iii-b-manipulation" class="level3">
<h3 class="anchored" data-anchor-id="iii-b-manipulation">III-B Manipulation</h3>
<section id="iii-b1-natural-language-understanding" class="level4">
<h4 class="anchored" data-anchor-id="iii-b1-natural-language-understanding">III-B1 Natural Language Understanding</h4>
<ul>
<li>LLMs help robots make common-sense analyses and enhance adaptability to new scenarios, agents, and tasks.</li>
</ul>
</section>
<section id="iii-b2-interactive-strategies" class="level4">
<h4 class="anchored" data-anchor-id="iii-b2-interactive-strategies">III-B2 Interactive Strategies</h4>
<ul>
<li>The use of LLMs in robot control focuses on generating interactive reward codes and extracting operants and constraints from LLMs.</li>
</ul>
</section>
<section id="iii-b3-modular-approaches" class="level4">
<h4 class="anchored" data-anchor-id="iii-b3-modular-approaches">III-B3 Modular Approaches</h4>
<ul>
<li>Modular approaches enhance system flexibility and adaptability to new tasks and environments.</li>
</ul>
</section>
</section>
<section id="iii-c-reasoning" class="level3">
<h3 class="anchored" data-anchor-id="iii-c-reasoning">III-C Reasoning</h3>
<section id="iii-c1-natural-language-understanding" class="level4">
<h4 class="anchored" data-anchor-id="iii-c1-natural-language-understanding">III-C1 Natural Language Understanding</h4>
<ul>
<li>LLMs provide common sense insights crucial for various tasks, avoiding the need for costly data gathering and model training.</li>
</ul>
</section>
<section id="iii-c2-complex-task-reasoning-and-decision-making" class="level4">
<h4 class="anchored" data-anchor-id="iii-c2-complex-task-reasoning-and-decision-making">III-C2 Complex Task Reasoning and Decision-making</h4>
<ul>
<li>LLMs leverage high-level semantic knowledge to enhance task execution and demonstrate effective performance, even in tasks with intricate settings or specific requirements.</li>
</ul>
</section>
<section id="iii-c3-interactive-strategies" class="level4">
<h4 class="anchored" data-anchor-id="iii-c3-interactive-strategies">III-C3 Interactive Strategies</h4>
<ul>
<li>LLMs augment interactive multimodal perception and the development of advanced architectures and interaction patterns.</li>
</ul>
</section>
</section>
</section>
<section id="iv-gpt-4v-empowered-embodied-task-planning" class="level2">
<h2 class="anchored" data-anchor-id="iv-gpt-4v-empowered-embodied-task-planning">IV GPT-4V Empowered Embodied Task Planning</h2>
<ul>
<li><strong>GPT-4V</strong> has demonstrated impressive performance in multimodal task planning across diverse environments and scenarios.</li>
</ul>
</section>
<section id="v-experimental-results" class="level2">
<h2 class="anchored" data-anchor-id="v-experimental-results">V Experimental Results</h2>
<ul>
<li>The matching score for the generated task plans consistently reflects a high level of agreement between the LLM-generated plans and the ground truth demonstrations.</li>
</ul>
</section>
<section id="vi-limitation-discussion-and-future-work" class="level2">
<h2 class="anchored" data-anchor-id="vi-limitation-discussion-and-future-work">VI Limitation, Discussion and Future Work</h2>
<ul>
<li>Challenges include homogenous generated plans, the need for carefully crafted prompts, and the closed-source nature of the <strong>GPT-4V API</strong>.</li>
<li>Future work focuses on addressing these challenges and developing more robust AGI robotic systems.</li>
</ul>
</section>
<section id="vii-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="vii-conclusion">VII Conclusion</h2>
<ul>
<li>LLMs demonstrate impressive reasoning, language understanding, and multimodal processing abilities that can significantly enhance robotic comprehension and task execution.</li>
</ul>
</section>
<section id="critique" class="level2">
<h2 class="anchored" data-anchor-id="critique">Critique</h2>
<p>The paper provides a comprehensive overview and evaluation of LLMs and multimodal LLMs in robotic tasks. However, it would benefit from addressing potential biases in the evaluation process and considering the ethical implications of implementing advanced LLMs in robotics. Additionally, the critique would be enhanced by acknowledging potential limitations in the generalization of study results to real-world applications.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<table class="table">
<tbody>
<tr class="odd">
<td>Model</td>
<td>gpt-3.5-turbo-1106</td>
</tr>
<tr class="even">
<td>Date Generated</td>
<td>2024-01-29</td>
</tr>
<tr class="odd">
<td>Abstract</td>
<td><a href="http://arxiv.org/abs/2401.04334v1">http://arxiv.org/abs/2401.04334v1</a></td>
</tr>
<tr class="even">
<td>HTML</td>
<td><a href="https://browse.arxiv.org/html/2401.04334v1">https://browse.arxiv.org/html/2401.04334v1</a></td>
</tr>
<tr class="odd">
<td>Truncated</td>
<td>True</td>
</tr>
<tr class="even">
<td>Word Count</td>
<td>14055</td>
</tr>
</tbody>
</table>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="wesslen/bayesian-beagle" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/wesslen/bayesian-beagle/edit/main/posts/Large_Language_Models_for_Robotics_Opportunities_Challenges_and_Perspectives/2024-01-09-Large_Language_Models_for_Robotics_Opportunities_Challenges_and_Perspectives.qmd" class="toc-action">Edit this page</a></p></div></div></div></div></footer></body></html>