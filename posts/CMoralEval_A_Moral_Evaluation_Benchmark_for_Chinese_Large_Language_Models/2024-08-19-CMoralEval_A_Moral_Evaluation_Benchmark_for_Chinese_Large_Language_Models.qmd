
---
title: "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models"
id: "2408.09819v1"
description: "CMoralEval: A Chinese Morality Benchmark for LLMs (13 words)"
author: Linhao Yu, Yongqi Leng, Yufei Huang, Shang Wu, Haixin Liu, Xinmeng Ji, Jiahui Zhao, Jinwang Song, Tingting Cui, Xiaoqing Cheng, Tao Liu, Deyi Xiong
date: "2024-08-19"
image: "https://browse.arxiv.org/html/2408.09819v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.09819v1/x1.png)

### Summary:

- The paper introduces CMoralEval, a large benchmark dataset for evaluating the morality of Chinese large language models (LLMs).
- The dataset is derived from two sources: a Chinese TV program discussing moral norms and a collection of Chinese moral anomies from various newspapers and academic papers.
- CMoralEval includes both explicit moral scenarios (14,964 instances) and moral dilemma scenarios (15,424 instances), each with instances from different data sources.
- The dataset is categorized into five groups of morality: familial morality, social morality, professional ethics, Internet ethics, and personal morality.
- The paper presents extensive experiments with CMoralEval to examine a variety of Chinese LLMs, demonstrating that it is a challenging benchmark.

### Major Findings:

1. CMoralEval is the first Chinese dataset curated to evaluate Chinese LLMs on morality, covering two distinct scenarios designed to evaluate the performance of Chinese LLMs when confronted with various types of moral situations.
2. The paper develops a moral taxonomy and a set of fundamental moral principles that are rooted in traditional Chinese culture and consistent with contemporary societal norms.
3. The paper conducts extensive experiments on a wide range of Chinese LLMs under both zero- and few-shot settings, comprehensively evaluating Chinese LLMs in ethics and morality both horizontally and vertically across different models and model sizes.

### Analysis and Critique:

- The paper does not provide a detailed analysis of the performance of individual LLMs on the CMoralEval dataset, making it difficult to compare the strengths and weaknesses of different models.
- The paper does not discuss the potential limitations of the CMoralEval dataset, such as the generalizability of the moral scenarios and the potential for cultural bias in the dataset.
- The paper does not provide a clear definition of what constitutes a "correct" or "incorrect" response to a moral scenario, which could impact the validity of the evaluation results.
- The paper does not discuss the potential implications of using a Chinese-specific dataset for evaluating LLMs, such as the potential for cultural bias in the evaluation results.
- The paper does not discuss the potential for using the CMoralEval dataset to improve the moral reasoning capabilities of

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.09819v1](https://arxiv.org/abs/2408.09819v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.09819v1](https://browse.arxiv.org/html/2408.09819v1)       |
| Truncated       | False       |
| Word Count       | 8745       |