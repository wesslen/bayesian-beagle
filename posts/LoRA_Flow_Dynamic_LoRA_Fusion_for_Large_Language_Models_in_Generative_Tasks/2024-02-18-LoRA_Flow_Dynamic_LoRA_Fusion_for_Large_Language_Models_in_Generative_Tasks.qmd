
---
title: "LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks"
id: "2402.11455v1"
description: "LoRA-Flow uses dynamic weights to combine LoRAs for better performance in generative tasks."
author: Hanqing Wang, Bowen Ping, Shuo Wang, Xu Han, Yun Chen, Zhiyuan Liu, Maosong Sun
date: "2024-02-18"
image: "https://browse.arxiv.org/html/2402.11455v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.11455v1/x1.png)

### **Summary:**
- LoRA-Flow is a method that combines existing LoRAs with dynamic fusion weights to effectively control the influence of each LoRA across various generation steps.
- The method consistently outperforms baselines with task-level fusion weights across six generative tasks.
- The fusion weights exhibit significant variation across different tokens, suggesting the necessity of dynamic fusion weights for generative tasks.

### **Major Findings:**
1. LoRA-Flow outperforms baselines with task-level fusion weights across six generative tasks.
2. The fusion weights exhibit significant variation across different tokens, suggesting the necessity of dynamic fusion weights for generative tasks.
3. Layer-level fusion gates achieve the highest scores, surpassing both step-level and module-level gates.

### **Analysis and Critique:**
- LoRA-Flow outperforms baselines with task-level fusion weights, demonstrating the effectiveness of dynamic fusion weights.
- The method exhibits significant variation in fusion weights across different tokens, highlighting the necessity of dynamic fusion weights for generative tasks.
- The study is limited by the use of models with a maximum of 13b parameters, and further exploration of larger models is recommended.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.11455v1](https://arxiv.org/abs/2402.11455v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11455v1](https://browse.arxiv.org/html/2402.11455v1)       |
| Truncated       | False       |
| Word Count       | 6002       |