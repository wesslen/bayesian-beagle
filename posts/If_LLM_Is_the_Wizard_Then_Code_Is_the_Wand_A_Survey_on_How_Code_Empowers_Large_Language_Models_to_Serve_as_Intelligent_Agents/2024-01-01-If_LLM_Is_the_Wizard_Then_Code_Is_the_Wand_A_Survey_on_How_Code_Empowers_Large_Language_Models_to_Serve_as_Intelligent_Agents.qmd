
---
title: "If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"
id: "2401.00812v1"
description: "LLMs benefit from integrating code in training, enhancing code generation and reasoning ability for complex tasks."
author: ['Ke Yang', 'Jiateng Liu', 'John Wu', 'Chaoqi Yang', 'Yi R. Fung', 'Sha Li', 'Zixuan Huang', 'Xu Cao', 'Xingyao Wang', 'Yiquan Wang', 'Heng Ji', 'Chengxiang Zhai']
date: "2024-01-01"
image: "https://browse.arxiv.org/html/2401.00812v1/extracted/5323849/Images/1.png"
categories: ['prompt-engineering', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.00812v1/extracted/5323849/Images/1.png)

# Summary

## Main Findings
1. **Enhanced Capabilities**: The integration of code into large language models (LLMs) enhances their reasoning ability and programming skills, leading to improved performance as intelligent agents (IAs).
2. **Diverse Benefits**: Code empowers LLMs to serve as IAs by improving their decision-making, execution, and self-improvement capabilities through the use of code-centric paradigms.
3. **Integration with Functional Ends**: LLMs connected to various functional ends through code exhibit versatility, enabling them to handle complex tasks and plan and execute actions.

## Introduction
The paper presents a survey on the benefits of integrating code into LLMs and the emergence of LLMs as IAs. The code-centric paradigm enhances LLMs' reasoning, planning, execution, and self-improvement capabilities in various contexts.

## Preliminaries
- **Definition of Code**: Code is a formal language that is both machine-executable and human-interpretable, including pre-defined formal languages and human-readable programming languages.
- **LLM Code Training Methods**: LLMs undergo code training through standard language modeling objectives applied to code corpora, involving code pre-training and code fine-tuning methods.

## Code Pre-Training Boosts LLMs’ Performance
- **Strengthen LLMs’ Programming Skills**: LLMs trained with code exhibit strong code generation and evaluation abilities, paving the way for various applications in different fields.
- **Empower LLMs’ Complex Reasoning**: Code pre-training improves LLMs' chain-of-thought performance, enhancing their reasoning skills and enabling them to perform complex reasoning tasks.
- **Enable LLMs to Capture Structured Knowledge**: Code-LLMs unveil superior structural commonsense reasoning, allowing them to understand complex multimedia data and structured information.

## Code Connects LLMs to Other Functional Ends
- **Relate LLMs to Digital Ends**: LLMs linked to digital ends via a code-centric paradigm, aiding in leveraging textual and multimodal tools for improved performance in various tasks.
- **Relate LLMs to Physical Ends**: LLMs connected to physical ends, such as robotics and autonomous driving, demonstrating their potential in bridging the gap between physical worlds and AI.

## Code Provides LLM with an Executable Environment for Automated Feedback
- **Various Feedback from Code Execution**: Code execution environment provides versatile automated feedback, including simple correctness feedback, textual feedback, and feedback from external evaluation modules.
- **Methods for Enhancing LLM’s Performance with Feedback**: Feedback derived from code execution and external evaluation modules enhance LLMs through selection-based, prompting-based, and finetuning-based methods.

## Application: Code-empowered LLMs Facilitate Intelligent Agents
- **Decision Making**: Code-empowered LLMs enhance IAs' decision-making skills through better environment perception and improved planning capabilities.
- **Execution**: LLMs as IAs benefit from better action grounding and memory organization, leading to improved execution of complex tasks.
- **Self-improvement**: LLM-based IAs can self-improve through feedback derived from code execution and external evaluation modules.

## Challenges
1. The causality between code pre-training and LLMs’ reasoning enhancement.
2. Acquisition of reasoning beyond code.
3. Challenges of applying the code-centric paradigm.

# Critique
The paper effectively highlights the extensive benefits of integrating code into LLMs and the challenges and future research needs in this domain. However, the paper could benefit from more qualitative and quantitative evidence supporting the observed enhancements in LLMs' reasoning and decision-making capabilities as a result of code integration. Additionally, the specific practical challenges and limitations in implementing the code-centric paradigm could have been more thoroughly explored.

Overall, the paper provides a comprehensive overview of the impact of code on LLMs and its potential as a tool for enhancing the capabilities of intelligent agents.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [http://arxiv.org/abs/2401.00812v1](http://arxiv.org/abs/2401.00812v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.00812v1](https://browse.arxiv.org/html/2401.00812v1)       |
| Truncated       | True       |
| Word Count       | 17975       |