
---
title: "Defining Boundaries: A Spectrum of Task Feasibility for Large Language Models"
id: "2408.05873v1"
description: "LLMs struggle with infeasible tasks; this paper categorizes them, tests LLMs, and explores training enhancements to improve refusal capabilities."
author: Wenbo Zhang, Zihang Xu, Hengrui Cai
date: "2024-08-11"
image: "https://browse.arxiv.org/html/2408.05873v1/extracted/5784417/figures/example.png"
categories: ['architectures', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.05873v1/extracted/5784417/figures/example.png)

### Summary:

- The paper addresses the need for large language models (LLMs) to recognize and refuse infeasible tasks due to the required skills surpassing their capabilities.
- The authors provide formal definitions and categorizations of infeasible tasks for LLMs, covering a spectrum of related hallucinations.
- A new dataset is developed and benchmarked to test multiple LLMs' abilities on task feasibility.
- The potential of training enhancements to increase LLMs' refusal capabilities with fine-tuning is explored.
- Experiments validate the effectiveness of the proposed methods, offering promising directions for refining the operational boundaries of LLMs in real applications.

### Major Findings:

1. The paper introduces a systematic conceptualization of tasks that are infeasible for LLMs, providing a formal definition and categorization of these tasks.
2. A new dataset for task feasibility is established, comprising a diverse range of commonly posed infeasible and feasible tasks, and benchmarking multiple LLMs under the developed dataset.
3. Three strategies are proposed to enhance the refusal awareness of LLMs when faced with infeasible tasks, by constructing a refusal-augmented instruction tuning dataset.

### Analysis and Critique:

- The paper provides a comprehensive examination of LLMs' capabilities and limitations, addressing the need for LLMs to recognize and refuse infeasible tasks.
- The proposed dataset and benchmarking methods offer valuable insights for future research in this area.
- The training enhancements proposed to increase LLMs' refusal capabilities with fine-tuning are promising, but further research is needed to evaluate their effectiveness in real-world applications.
- The paper does not discuss potential biases or limitations in the proposed methods, which should be considered in future work.
- The paper does not address the potential impact of the proposed methods on the overall performance of LLMs, which is an important consideration for practical applications.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.05873v1](https://arxiv.org/abs/2408.05873v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.05873v1](https://browse.arxiv.org/html/2408.05873v1)       |
| Truncated       | False       |
| Word Count       | 6756       |