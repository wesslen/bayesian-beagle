
---
title: "AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability"
id: "2402.09404v1"
description: "AQA-Bench assesses language models' sequential reasoning in algorithmic contexts, revealing performance variations. Code available."
author: Siwei Yang, Bingchen Zhao, Cihang Xie
date: "2024-02-14"
image: "../../img/2402.09404v1/image_1.png"
categories: ['architectures', 'production', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.09404v1/image_1.png)

### Summary:
- The academic paper introduces AQA-Bench, an interactive benchmark designed to evaluate the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts.
- The investigation reveals several interesting findings, including the strong performance of closed-source models like GPT-4 and Gemini, the potential negative impact of naively providing interactive examples, and the limited number of predecessor steps that can boost small models' performance.
- The paper discusses the significance of the benchmark in advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning.

### Major Findings:
1. AQA-Bench demonstrates the strong performance of closed-source models like GPT-4 and Gemini in sequential reasoning abilities.
2. Naively providing interactive examples may have a potential negative impact on the performance of language models.
3. Limited predecessor steps can significantly boost the performance of small language models in sequential reasoning tasks.

### Analysis and Critique:
- The findings provide valuable insights into the performance of different LLMs and highlight the potential impact of interactive examples on few-shot performance.
- The results have implications for future research focused on evaluating and enhancing the sequential reasoning abilities of LLMs.
- The study challenges the assumption that larger model sizes consistently lead to improved performance in language models, emphasizing the importance of considering the specific application and task when evaluating model performance.
- The section also highlights the importance of evaluating GPT models on the same dataset multiple times to limit the impact of their randomness on the results and emphasizes the consistency of the models' performance under the HARD testing protocol.
- The evaluation results provide insights into the performance of different LLMs in sequential reasoning tasks across various environments, guiding the development and improvement of LLMs for enhanced sequential reasoning abilities.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.09404v1](https://arxiv.org/abs/2402.09404v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09404v1](https://browse.arxiv.org/html/2402.09404v1)       |
| Truncated       | True       |
| Word Count       | 22322       |