
---
title: "SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model"
id: "2401.09712v1"
description: "TL;DR: SkyEyeGPT is a new multi-modal language model designed for remote sensing data tasks, showing superior performance in vision-language understanding."
author: ['Yang Zhan', 'Zhitong Xiong', 'Yuan Yuan']
date: "2024-01-18"
image: "https://browse.arxiv.org/html/2401.09712v1/x1.png"
categories: ['prompt-engineering', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.09712v1/x1.png)

### Summary:

The article introduces SkyEyeGPT, a multi-modal large language model specifically designed for remote sensing (RS) vision-language understanding, addressing the lack of satisfactory performance in this domain. SkyEyeGPT demonstrates impressive performance on different RS vision-language tasks without requiring extra encoding modules. The model's architecture consists of a visual encoder, an alignment layer, and an LLM-based decoder for RS open-ended tasks. Additionally, the article presents a meticulous curation of an RS multi-modal instruction tuning dataset and a two-stage tuning method to enhance instruction-following and multi-turn dialogue ability.

### Major Findings:
1. **Unified Model for RS Vision-Language Tasks:**
   - SkyEyeGPT works surprisingly well on considerably different tasks without the need for extra encoding modules, demonstrating a unified and efficient model for RS vision-language tasks.
  
2. **RS Vision-Language Instruction Dataset:**
   - The construction of the SkyEye-968k dataset, including single-task image-text instruction and multi-task conversation instruction, fills the gap of the lack of large-scale RS multimodal instruction-following data.

3. **Superior Performance:**
   - SkyEyeGPT exhibits superior performance in image-level and region-level RS vision-language tasks, such as captioning, visual grounding, and VQA, outperforming existing models such as GPT-4V in some tests.

### Analysis and Critique:
The article offers valuable contributions to the field of multi-modal large language models for remote sensing vision-language tasks. However, some potential limitations and areas of further research include:
- Clear Evaluation Methods: The article highlights the challenge of evaluating model performance accurately, especially in image captioning. While introducing a novel evaluation method using ChatGPT, relying on a single evaluation metric could be limited. Additional robust evaluation methods may be required.
- Generalization and Modality Difference: The model's performance is affected by modality differences between satellite imagery and aerial images in different tasks. Addressing this difference and ensuring generalization across diverse RS imagery sources should be a focus of future research.

The article effectively presents SkyEyeGPT's architecture, dataset creation, and the model's performance on various RS vision-language tasks, but further investigation is needed to ensure its robustness and generalizability across different modalities within the RS domain.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [http://arxiv.org/abs/2401.09712v1](http://arxiv.org/abs/2401.09712v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.09712v1](https://browse.arxiv.org/html/2401.09712v1)       |
| Truncated       | False       |
| Word Count       | 8815       |