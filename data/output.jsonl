{"id": "2312.12321v1", "text": "# Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n## 1 Introduction\nThe surge in popularity of Large Language Models (LLMs) has necessitated significant safety training. Despite efforts, it is still possible to circumvent the alignment to obtain harmful outputs. This paper investigates a threat model for circumventing alignment of open-source LLMs using priming attacks.\n\n## 2 Methodology & Results\nThe authors build an efficient pipeline for automated evaluation of priming attacks against open-source LLMs and demonstrate that priming with slightly more prompt-dependent content can improve the attack success rate by up to 3.3 times. The experimental setup, few-shot priming attacks, and the results of the attacks on different model families and sizes are presented.\n\n## 3 Conclusion\nThe paper highlights the fragility of current LLM safety measures under practical assumptions and emphasizes the need for further research into safer open-sourcing methods.\n\n#### A Details on experimental setup\nThe experimental setup involved using the pre-trained Llama-2 model for few-shot prompting and evaluating the attack success rate on harmful behaviors.\n\n#### B Few-Shot Prompt for Generating Priming Attacks\nThe paper presents the prompt format for generating priming attacks, which includes examples of prompts and affirmative initial responses.\n\n#### C Llama Guard Task Instructions\nThe instructions for evaluating the safety of responses using Llama Guard are provided, including the unsafe content categories and the conversation format used for evaluation.\n\n#### D Manual Evaluation Benchmark\nThe manual evaluation benchmark is explained using examples of harmful and safe responses, along with the criteria for labeling responses as harmful.\n\n#### E Manual Evaluation vs. Llama Guard\nA comparison between manual evaluation and Llama Guard results is presented, highlighting discrepancies in the assessment of harmful content.\n\n#### F Runtime Comparison\nThe runtime comparison between Llama-2 and few-shot prompting techniques is discussed, indicating the speed of generating priming attacks and comparing it to optimization-based techniques.\n\n---\nThe paper explores the vulnerability of open-source LLMs to priming attacks, presents a successful attack pipeline, and discusses the implications of the findings. Additionally, it provides insight into the discrepancies between manual and automated evaluation of harmful content.", "meta": {"url": "https://browse.arxiv.org/html/2312.12321v1", "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks", "subtitle": "LLMs need safety training due to vulnerability to priming attacks, improving attack success rates on harmful behaviors.", "categories": ["security", "open-source"], "publish_date": "2023-12-19"}}
{"id": "2312.02102v2", "text": "# Mitigating Data Injection Attacks on Federated Learning\n\n## 1 Introduction\nThe increasing volume and variety of data have led to a need for data privacy and security in various industries. Federated learning, a collaborative approach for training machine learning models without sharing the raw data, has gained popularity in addressing these concerns. However, federated learning is susceptible to data injection attacks, where malicious agents manipulate the learning process. Detecting and mitigating these attacks are significant challenges in federated learning systems.\n\n## 2 Problem Formulation\n### 2.1 Federated Learning\nFederated learning involves learning a model using agents' private data by refining local model parameters and transmitting updates to a coordinating node to obtain a global model. The goal is to minimize an objective function using a gradient descent approach.\n\n### 2.2 Data Injection Attacks\nIn data injection attacks, malicious participants inject false data into the training process to manipulate the global model. Different attack schemes such as label flipping and constant output attacks can be employed to steer the model towards a false, predetermined performance.\n\n## 3 Attacker Detection and Avoidance\nA low-complexity metric is proposed to detect attackers by comparing the updates from edge agents over time. When an agent is suspected to be an attacker, its parameter updates are ignored for a certain period. The detection method is designed to operate continuously, and conditions are provided for identifying all malicious agents.\n\n## 4 Simulations\n### 4.1 Example 1: Constant-Output Attack\nSimulated attacks on decentralized learning show that the proposed detection algorithm effectively mitigates constant-output attacks by detecting and isolating attackers, leading to convergence to a truthful model.\n\n### 4.2 Example 2: Label-Flip Attack\nSimulated label-flip attacks demonstrate the effectiveness of the detection scheme in identifying and mitigating attacks, preventing the model from succumbing to false labeling.\n\n## 5 Conclusions\nThe paper presents a robust federated learning algorithm capable of operating in the presence of data injection attacks and provides conditions for proper identification of malicious agents. Simulations demonstrate the performance of the proposed technique in mitigating different types of attacks. Additional details and proofs of the proposed scheme will be presented in an extended version of the work.\n\nReferences are also included in the text.", "meta": {"url": "https://browse.arxiv.org/html/2312.02102v2", "title": "Mitigating Data Injection Attacks on Federated Learning", "subtitle": "Federated learning vulnerable to attacks. Paper proposes detection and mitigation technique using local scheme by coordinating node.", "categories": ["security"], "publish_date": "2023-12-04"}}
