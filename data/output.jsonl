{"id": "2312.12321v1", "text": "# Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n## 1 Introduction\nThe paper investigates the vulnerability of state-of-the-art open-source LLMs to priming attacks, which circumvent safety training and can be used to extract harmful outputs.\n\n## 2 Methodology & Results\nThe researchers use a few-shot prompt approach to generate priming attacks and automate the evaluation process using another LLM. The results show that the proposed priming attack outperforms baselines in terms of attack success rate on harmful behaviors.\n\n### A Details on experimental setup\nExperiments were conducted on a server with specific hardware specifications.\n\n### B Few-Shot Prompt for Generating Priming Attacks\nThe paper provides examples of few-shot prompts used for generating priming attacks.\n\n### C Llama Guard Task Instructions\nInstructions given to Llama Guard for evaluating the attacks are provided, along with specific unsafe content categories.\n\n### D Manual Evaluation Benchmark\nThe paper clarifies the manual evaluation benchmark for labeling harmful responses, providing examples of harmful and safe responses.\n\n### E Manual Evaluation vs. Llama Guard\nA comparison between manual evaluation and Llama Guard's evaluation indicates differences in the assessment of harmful responses.\n\n### F Runtime Comparison\nA comparison of runtimes for Llama-2 and few-shot prompting is provided, demonstrating the efficiency of the few-shot task.\n\n## 3 Conclusion\nThe study highlights the fragility of current LLM safety measures and raises concerns about future open-sourcing of LLMs, emphasizing the need for further research into safer open-sourcing methods.\n\n## References\nThe paper includes references to related works for further exploration.", "meta": {"url": "https://browse.arxiv.org/html/2312.12321v1", "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks", "subtitle": "Popular LLMs are vulnerable to priming attacks that bypass safety training, improving harmful behavior success rate. Source code available.", "categories": ["security", "open-source"], "publish_date": "2023-12-19", "model": "gpt-3.5-turbo-1106", "word_count": 10319, "is_truncated": false}}
{"id": "2312.02102v2", "text": "# Summary of \"Mitigating Data Injection Attacks on Federated Learning\"\n\n## Abstract\nThe paper addresses the vulnerability of federated learning to data injection attacks, where malicious agents manipulate the learning process to compromise the model. It proposes a novel technique to detect and mitigate data injection attacks, with a focus on constant-output and label-flip attacks. The proposed method involves comparing updates from participating agents over time and ignoring updates from suspicious agents. Simulations demonstrate the effectiveness of the technique in overcoming attacks.\n\n## 1 Introduction\nThe introduction highlights the increasing need for data privacy in various industries and introduces federated learning as a technique that allows collaborative model training while preserving data privacy. However, it also outlines the vulnerability of federated learning to various security threats, including data injection attacks.\n\n## 2 Problem Formulation\n### 2.1 Federated Learning\nThis section describes the process of federated learning, where agents refine their model parameters iteratively using their private datasets and share updates with a coordinating node.\n\n### 2.2 Data Injection Attacks\nThe paper discusses the potential threat of data injection attacks in federated learning, particularly focusing on scenarios where some participating agents are malicious. It presents scenarios of attacks such as constant-output and label-flip attacks.\n\n## 3 Attacker Detection and Avoidance\nThe proposed method for detecting data injection attacks involves comparing updates from agents over time and ignoring updates from suspicious agents based on a low-complexity metric. The method is designed to overcome false alarms and ensure convergence to a truthful model with a high probability.\n\n## 4 Simulations\nThe paper presents simulated attacks on decentralized learning and evaluates the performance of the detection algorithm. It provides examples of constant-output attacks and label-flip attacks, demonstrating the effectiveness of the proposed detection and mitigation technique.\n\n## 5 Conclusions\nThe paper concludes by summarizing the proposed robust federated learning algorithm and its ability to operate in the presence of data injection attacks. It also outlines future work to be conducted including detailed proofs, and bounds on detection probability and false alarm probability.\n\nThe paper reflects the growing concern and interest in addressing security threats in federated learning systems, and it provides a novel perspective on detecting and mitigating data injection attacks. The simulation results demonstrate the potential effectiveness of the proposed technique in overcoming such attacks.", "meta": {"url": "https://browse.arxiv.org/html/2312.02102v2", "title": "Mitigating Data Injection Attacks on Federated Learning", "subtitle": "Federated learning can be vulnerable to false data attacks, but this paper proposes a method to detect and mitigate such attacks.", "categories": ["security"], "publish_date": "2023-12-04", "model": "gpt-3.5-turbo-1106", "word_count": 13622, "is_truncated": false}}
{"id": "2312.08282v2", "text": "# Summary of \"Prompting LLMs with content plans to enhance the summarization of scientific articles\"\n\n## Introduction\nThe paper introduces novel prompting techniques to enhance the performance of automatic summarization systems for scientific articles. It discusses the challenges in summarizing scientific articles due to their length, complexity, and irregular organizational structures.\n\n## Related Work\nThe related work section discusses prior approaches to automatic summarization, including extractive and abstractive methods, as well as prior techniques such as planning with learned entity prompts and faceted summarization.\n\n## Methods\nThe paper proposes and evaluates three key dimensions for enhancing summarization:\n### Prompting Technique Dimension\nIt compares various approaches for generating prompts, including keywords, MeSH, KeyBERT, TF, and TF-IDF, to provide scientific summarizers with useful contextual information.\n### Model Dimension\nThe paper studies the integration of proposed prompting techniques with a range of current state-of-the-art transformer models for scientific summarization.\n### Input Text Dimension\nIt investigates different text input conditions to evaluate the impact of prompts when summarizing sections independently.\n\n## Results\nThe paper presents the results of experiments evaluating the proposed prompting techniques integrated with various state-of-the-art summarization models and input texts. It provides a detailed description of the dataset used, preprocessing, training methodology, evaluation protocol, and the results obtained.\n\n## Discussion\nThe discussion section analyzes the findings and explores the implications of the results, particularly in terms of the impact of prompting techniques on the performance of different summarization models.\n\n## Future Work\nThe paper proposes opportunities for future research, highlighting potential avenues for further exploration and improvement in the field of scientific article summarization.\n\n## Conclusion\nThe paper concludes with a summary of the key contributions and findings, emphasizing the potential of prompting techniques to assist smaller models in overcoming the limitations of less powerful summarization systems.\n\nIn addition, the authors encounter difficulties with HTML conversions and acknowledge issues with unsupported packages. The paper also provides author information and an abstract outlining the core contributions of the work.\n\nOverall, the paper introduces novel prompting techniques to improve the summarization of scientific articles and provides valuable insights into the potential impact of these techniques on automatic summarization systems.", "meta": {"url": "https://browse.arxiv.org/html/2312.08282v2", "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles", "subtitle": "New prompting techniques improve summarization systems for scientific articles, especially for smaller models summarizing sections separately.", "categories": ["prompt engineering"], "publish_date": "2023-12-13", "model": "gpt-3.5-turbo-1106", "word_count": 46040, "is_truncated": true}}
