
---
title: "Reverse Image Retrieval Cues Parametric Memory in Multimodal LLMs"
id: "2405.18740v1"
description: "RIR improves MLLMs' performance on knowledge-intensive tasks by 18-43%, providing visual and textual cues without direct answers."
author: Jialiang Xu, Michael Moor, Jure Leskovec
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.18740v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.18740v1/x1.png)

### Summary:

The paper explores the use of Reverse Image Retrieval (RIR) augmentation to improve the performance of state-of-the-art multimodal large language models (MLLMs) in knowledge-intensive visual question answering (VQA). The authors build a browser-based API to reverse image search the web and capture a screenshot of the multimodal search result, which is then provided as context to the MLLM. The experiments show that RIR robustly and drastically improves knowledge-intensive VQA of GPT-4V by 37-43%, GPT-4 Turbo by 25-27%, and GPT-4o by 18-20% in terms of open-ended VQA evaluation metrics. The authors also find that RIR's benefit does not primarily stem from providing the required knowledge to answer knowledge-intensive visual questions, but by improving the alignment of the visual question with the model's own world knowledge.

### Major Findings:

1. RIR robustly and drastically improves knowledge-intensive VQA of state-of-the-art MLLMs.
2. RIR's benefit does not primarily stem from providing the required knowledge to answer knowledge-intensive visual questions, but by improving the alignment of the visual question with the model's own world knowledge.
3. RIR helps more with objects and concepts that have less presence on the web.

### Analysis and Critique:

The paper presents an interesting approach to improving the performance of MLLMs in knowledge-intensive VQA. The use of RIR augmentation is a simple yet effective strategy that can be easily implemented. The experiments show promising results, with RIR improving the performance of state-of-the-art MLLMs by a significant margin. However, the paper does not provide a detailed analysis of the limitations of RIR augmentation, such as the potential for overfitting to the search results or the impact of the quality of the search results on the performance of the MLLMs. Additionally, the paper does not discuss the potential for RIR augmentation to introduce biases or inaccuracies in the search results, which could impact the performance of the MLLMs. Overall, the paper provides a valuable contribution to

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.18740v1](https://arxiv.org/abs/2405.18740v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.18740v1](https://browse.arxiv.org/html/2405.18740v1)       |
| Truncated       | False       |
| Word Count       | 8853       |