
---
title: "TinyLLM: Learning a Small Student from Multiple Large Language Models"
id: "2402.04616v1"
description: "TL;DR: TinyLLM uses knowledge distillation to teach small language models reasoning skills from large ones."
author: Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, Nitesh V. Chawla
date: "2024-02-07"
image: "../../img/2402.04616v1/image_1.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.04616v1/image_1.png)

### **Summary:**
- TinyLLM proposes a novel knowledge distillation paradigm to learn a small student LLM from multiple large teacher LLMs.
- The method involves encouraging the student LLM to understand the rationales behind answers and assimilate knowledge from various teacher LLMs.
- Extensive experiments on six datasets across two reasoning tasks demonstrate the superiority of TinyLLM.

### **Major Findings:**
1. TinyLLM encourages the student LLM to not only generate correct answers but also understand the rationales behind these answers.
2. The method involves learning from multiple teacher LLMs to inherit a broader range of skills and knowledge, leading to better generalization capabilities.
3. Extensive experiments show that TinyLLM can outperform large teacher LLMs significantly, despite having a considerably smaller model size.

### **Analysis and Critique:**
- **Limited Knowledge Diversity:** Existing research predominantly employs a single-teacher approach, limiting the learning scope of the student model to the knowledge derived from its own training and architecture designs.
- **Lack of Rich Contextual Information:** Current research primarily focuses on leveraging ground truth labels, which indicate the correct answer but do not provide insights into the reasoning and thought process behind that answer.
- **Parameter Sensitivity:** The optimal parameters for various datasets and tasks differ, indicating the model's adaptability across different choices of parameter values.

Overall, TinyLLM presents a promising approach to knowledge distillation, but it is important to address the limitations of limited knowledge diversity and lack of rich contextual information in future research. Additionally, further exploration of parameter sensitivity and its impact on model performance could provide valuable insights for optimizing the TinyLLM approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.04616v1](https://arxiv.org/abs/2402.04616v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04616v1](https://browse.arxiv.org/html/2402.04616v1)       |
| Truncated       | False       |
| Word Count       | 6334       |