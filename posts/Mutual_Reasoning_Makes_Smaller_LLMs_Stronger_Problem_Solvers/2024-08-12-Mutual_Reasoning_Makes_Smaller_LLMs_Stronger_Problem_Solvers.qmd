
---
title: "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers"
id: "2408.06195v1"
description: "rStar improves SLMs' reasoning without fine-tuning via self-play mutual generation-discrimination, boosting GSM8K accuracy significantly."
author: Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang
date: "2024-08-12"
image: "https://browse.arxiv.org/html/2408.06195v1/extracted/5786663/teaser.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.06195v1/extracted/5786663/teaser.png)

# Summary:

The paper introduces rStar, a self-play mutual reasoning approach that significantly improves the reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. A target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Another SLM, with similar capabilities, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA.

# Major Findings:

1. rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, and from 74.53% to 91.13% for LLaMA3-8B-Instruct.
2. rStar significantly enhances SLMsâ€™ reasoning capabilities, matching or even surpassing the accuracy achieved after fine-tuning.
3. rStar outperforms state-of-the-art baselines, including single-round inference techniques like few-shot CoT, multi-round prompting approaches such as self-consistency, and self-improvement techniques such as RAP, ToT, self-evaluation and self-verification.

# Analysis and Critique:

* The paper does not discuss the potential limitations or biases of the rStar approach.
* The paper does not provide a detailed comparison with other self-improvement techniques for SLMs.
* The paper does not discuss the potential impact of the rStar approach on the generalization capabilities of SLMs.
* The paper does not discuss the potential impact of the rStar approach on the interpretability of SLMs.
* The paper does not discuss the potential impact of the rStar approach on the fairness and robustness of SLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.06195v1](https://arxiv.org/abs/2408.06195v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.06195v1](https://browse.arxiv.org/html/2408.06195v1)       |
| Truncated       | False       |
| Word Count       | 7311       |