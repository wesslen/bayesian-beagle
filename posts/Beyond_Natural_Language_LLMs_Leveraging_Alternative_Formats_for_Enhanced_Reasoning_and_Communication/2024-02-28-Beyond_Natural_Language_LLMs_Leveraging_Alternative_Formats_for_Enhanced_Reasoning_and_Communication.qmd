
---
title: "Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication"
id: "2402.18439v1"
description: "TL;DR: Non-NL formats improve LLM reasoning efficiency and multi-agent communication."
author: Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18439v1/x1.png"
categories: ['production', 'hci', 'architectures', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18439v1/x1.png)

### Summary:
- The article challenges the default use of natural language (NL) by exploring the utility of non-NL formats in single-LLM reasoning and multi-agent communication.
- Allowing LLMs to autonomously select the most suitable format before reasoning or communicating leads to a 3.3 to 5.7% improvement in reasoning efficiency for different LLMs and up to a 72.7% reduction in token usage in multi-agent communication.
- LLMs can devise a format from limited task instructions and the devised format is effectively transferable across different LLMs.

### Major Findings:
1. LLMs can autonomously select the most suitable format before reasoning or communicating, leading to improved reasoning efficiency and reduced token usage in multi-agent communication.
2. LLMs can devise a format from limited task instructions and the devised format is effectively transferable across different LLMs.
3. The communication formats decided by LLMs exhibit notable parallels with established agent communication languages, suggesting a natural evolution towards efficient, structured communication in agent communication.

### Analysis and Critique:
- The article provides valuable insights into the potential of LLMs to utilize non-NL formats for reasoning and communication. However, the scope of alternative formats explored is still not exhaustive, and further research is needed to fully harness the capabilities of alternative formats.
- The generalization of chosen formats across tasks shows variability in effectiveness depending on the complexity of the task and the specific LLM used, highlighting the need for further exploration.
- While LLMs can emulate the formality of traditional ACL formats, the AutoForm approach optimizes communication by enhancing clarity and structure, yet concurrently reduces token usage.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-29       |
| Abstract | [https://arxiv.org/abs/2402.18439v1](https://arxiv.org/abs/2402.18439v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18439v1](https://browse.arxiv.org/html/2402.18439v1)       |
| Truncated       | False       |
| Word Count       | 7115       |