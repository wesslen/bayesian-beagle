
---
title: "A Survey on Human Preference Learning for Large Language Models"
id: "2406.11191v1"
description: "This survey explores human preference learning for LLMs, covering feedback sources, modeling, usage, and evaluation."
author: Ruili Jiang, Kehai Chen, Xuefeng Bai, Zhixuan He, Juntao Li, Muyun Yang, Tiejun Zhao, Liqiang Nie, Min Zhang
date: "2024-06-17"
image: "https://browse.arxiv.org/html/2406.11191v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.11191v1/x1.png)

### Summary:

The recent surge of versatile large language models (LLMs) has been made possible by aligning increasingly capable foundation models with human intentions through preference learning. This survey reviews the progress in exploring human preference learning for LLMs from a preference-centered perspective, covering the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. The survey categorizes human feedback according to data sources and formats, summarizes techniques for human preferences modeling, and presents various preference usage methods sorted by the objectives to utilize human preference signals. The survey also summarizes some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discusses outlooks on the human intention alignment for LLMs.

### Major Findings:

1. Human preference learning can effectively align LLMs with human intentions by optimizing LLMs according to feedback information on their outputs that reflects the preferences and thus specifies the intentions of humans.
2. The quality and scale of preference feedback are of great importance for human preference learning, while the sources of feedback collection can heavily influence them. Recent human preference learning methods collect preference feedback from not only humans but also simulations of humans, exploring the balance between high-quality and large-scale.
3. The feedback formats adopted in works on human preference learning broadly include relative relations that are natural for preference expression but less informative, and absolute properties that are more informative about human preferences but harder to collect. The combinations of different formats can further increase the information density of preference feedback.

### Analysis and Critique:

The survey provides a comprehensive review of the development timeline and recent advances in human preference learning for LLMs. However, it does not discuss the limitations and potential biases of the reviewed studies. Additionally, the survey does not provide a critical evaluation of the effectiveness and efficiency of the reviewed methods. Furthermore, the survey does not discuss the potential ethical implications of aligning LLMs with human intentions.

The survey also does not discuss the potential risks and challenges associated with the use of LLMs, such as the potential for LLMs to be used for malicious purposes or the potential for LLMs to perpetuate biases and stereotypes. Additionally, the survey does not discuss the potential for LLMs to be used to manipulate or deceive humans, or the potential for LLMs to be used to automate jobs

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2406.11191v1](https://arxiv.org/abs/2406.11191v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.11191v1](https://browse.arxiv.org/html/2406.11191v1)       |
| Truncated       | False       |
| Word Count       | 12223       |