
---
title: "Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning"
id: "2402.10110v1"
description: "Selective Reflection-Tuning improves LLM finetuning without new data, achieving superior performance."
author: Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou
date: "2024-02-15"
image: "https://browse.arxiv.org/html/2402.10110v1/extracted/5411213/Figures/reflection_main.png"
categories: ['architectures', 'education', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.10110v1/extracted/5411213/Figures/reflection_main.png)

### **Summary:**
- Selective Reflection-Tuning is a novel paradigm that synergizes a teacher LLM’s reflection and introspection for improving existing data quality with the data selection capability of the student LLM.
- The teacher-student collaboration produces high-quality and student-compatible instruction-response pairs, resulting in sample-efficient instruction tuning and LLMs of superior performance.
- The method is applied to Alpaca and WizardLM data and achieves much stronger and top-tier 7B and 13B LLMs.

### Major Findings:
1. The quality of instruction tuning data is paramount to the LLM being fine-tuned, and Selective Reflection-Tuning significantly improves the data quality and compatibility with the student model.
2. The method introduces a teacher-student collaboration pipeline, where the teacher model and student model cooperate to build a more coherent and model-compatible instruction tuning dataset.
3. The use of the IFD and r-IFD scores enables a comprehensive and nuanced assessment of the instruction-tuning process, ensuring the refined data aligns well with the student model’s capabilities and objectives.

### Analysis and Critique:
- The involvement of the student model makes it possible to build high-quality and student-compatible instruction-response data, but the main limitation is that the data samples selected by different student models are different, thus the statistics need to be calculated again for different student models.
- The method significantly outperforms existing open-source models, but the potential need for re-calculation for new models is a potential limitation.
- The method's effectiveness is validated through various evaluation metrics, including pair-wise comparison, Alpaca Eval Leaderboard, Open LLM Leaderboard, MT-Bench, and human study.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.10110v1](https://arxiv.org/abs/2402.10110v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.10110v1](https://browse.arxiv.org/html/2402.10110v1)       |
| Truncated       | False       |
| Word Count       | 8375       |