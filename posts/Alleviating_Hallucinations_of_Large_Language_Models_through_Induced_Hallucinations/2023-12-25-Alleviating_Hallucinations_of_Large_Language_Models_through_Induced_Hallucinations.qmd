
---
title: "Alleviating Hallucinations of Large Language Models through Induced Hallucinations"
description: "New decoding strategy reduces misinformation in large language models, improving factuality across various models and benchmarks."
author: Yue Zhang, Leyang Cui, Wei Bi, Shuming Shi
date: "2023-12-25"
image: "https://browse.arxiv.org/html/2312.15710v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.15710v1/x1.png)

### Summary

#### Major Findings
1. Large language models (LLMs) often generate inaccurate or fabricated information, known as "hallucinations."
2. The proposed Induce-then-Contrast Decoding (ICD) method effectively reduces hallucinations in LLMs across various model sizes and families.
3. Experimental results demonstrate that ICD significantly improves the truthfulness of LLMs on both discrimination-based and generation-based hallucination evaluation benchmarks.

#### Introduction
- Large language models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks but continue to generate inaccurate or fabricated information, termed as "hallucinations."
- Previous work suggests that the pre-training objective of existing LLMs may be a cause for hallucinations, and injecting more knowledge into LLMs through post-hoc supervised fine-tuning may inadvertently encourage hallucinations.

#### Induce-then-Contrast Decoding
- ICD is a lightweight decoding method that first constructs a factually weak LLM by inducing hallucinations from the original LLM and then eliminates the non-factual knowledge by contrastive decoding.
- Experimental results show that inducing hallucinations through fine-tuning or zero-shot prompting and penalizing them effectively guides LLMs to generate more factual content.

#### Experiments
- ICD significantly improves the truthfulness of LLMs on both the discrimination-based benchmark (TruthfulQA) and the generation-based benchmark (FActScore) compared to other decoding methods.
- Additional analysis shows the impact of task format, model sizes, data size for inducing hallucinations, and comparisons between real and synthetic data for inducing hallucinations.

### Critique
- The paper lacks a thorough discussion of potential ethical considerations and broader societal implications of mitigating hallucinations in LLMs.
- The evaluation setting could be expanded to cover a wider range of tasks and benchmark datasets for a more comprehensive assessment of the proposed ICD method.
- The authors could have provided more details about potential future directions and how they would address the limitations of the current study.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| Abstract | [http://arxiv.org/abs/2312.15710v1](http://arxiv.org/abs/2312.15710v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.15710v1](https://browse.arxiv.org/html/2312.15710v1)       |
| Truncated       | False       |
| Word Count       | 9227       |