
---
title: "De-amplifying Bias from Differential Privacy in Language Model Fine-tuning"
id: "2402.04489v1"
description: "DP amplifies bias in large language models, but CDA can mitigate it."
author: Sanjari Srivastava, Piotr Mardziel, Zhikhun Zhang, Archana Ahlawat, Anupam Datta, John C Mitchell
date: "2024-02-07"
image: "https://browse.arxiv.org/html/2402.04489v1/extracted/5394328/images/bias_increase_dark.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.04489v1/extracted/5394328/images/bias_increase_dark.png)

### Summary:
- Fairness and privacy are important values in machine learning (ML) models.
- Differential privacy (DP) amplifies gender, racial, and religious bias when fine-tuning large language models (LLMs).
- Counterfactual Data Augmentation (CDA) mitigates bias amplification by DP.

### Major Findings:
1. DP amplifies bias in language models, particularly for gender, race, and religion.
2. Disparity in convergence of gradients across sub-groups causes the amplification of bias by DP.
3. CDA effectively reduces the adverse impact of DP training on bias.

### Analysis and Critique:
- The study is limited to specific bias metrics and model settings, potentially overlooking other forms of bias.
- The study does not explore the interaction between DP and bias in larger LLMs.
- The study acknowledges limitations in applying CDA in cases where social group signifiers and biases are not clearly expressed in individual words or phrases.
- The study provides valuable insights into the impact of DP on fairness and bias in generative models, but further research is needed to address the limitations and extend the findings to larger and open models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-08       |
| Abstract | [https://arxiv.org/abs/2402.04489v1](https://arxiv.org/abs/2402.04489v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04489v1](https://browse.arxiv.org/html/2402.04489v1)       |
| Truncated       | False       |
| Word Count       | 10056       |