
---
title: "When Can LLMs Actually Correct Their Own Mistakes? A Critical Survey of Self-Correction of LLMs"
id: "2406.01297v1"
description: "Self-correction in LLMs: Successful with reliable external feedback or large-scale fine-tuning, but not with prompted LLMs in general tasks."
author: Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang
date: "2024-06-03"
image: "https://browse.arxiv.org/html/2406.01297v1/x1.png"
categories: ['social-sciences', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.01297v1/x1.png)

### Summary:

This paper critically surveys broad papers on self-correction in large language models (LLMs) and discusses the conditions required for successful self-correction. The authors find that prior studies often do not define their research questions in detail and involve impractical frameworks or unfair evaluations that over-evaluate self-correction. To tackle these issues, the authors categorize research questions in self-correction research and provide a checklist for designing appropriate experiments.

Their analysis shows that no prior work demonstrates successful self-correction with feedback from prompted LLMs in general tasks. However, self-correction works well in tasks that can use reliable external feedback, and large-scale fine-tuning enables self-correction.

### Major Findings:

1. No prior work shows reliable evidence of successful self-correction with in-context learning in general tasks.
2. In tasks with properties that are exceptionally favorable for self-correction (e.g., responses are decomposable), self-correction is effective even with in-context learning.
3. Self-correction is effective in tasks where reliable external feedback is available.
4. Fine-tuning is effective with large training data but unexplored for small training data.

### Analysis and Critique:

The paper provides a comprehensive analysis of self-correction in LLMs, highlighting the limitations and challenges in the field. The authors' critical survey and categorization of research questions offer a valuable framework for future research. However, the paper does not discuss the potential biases or conflicting evidence in the studies reviewed, which could be a limitation. Additionally, the paper does not provide a clear roadmap for future research or suggest specific methodologies to address the identified challenges.

Overall, the paper is a valuable contribution to the field of LLM self-correction, offering a critical analysis of existing research and highlighting the need for more rigorous and detailed research questions and experimental designs.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.01297v1](https://arxiv.org/abs/2406.01297v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.01297v1](https://browse.arxiv.org/html/2406.01297v1)       |
| Truncated       | False       |
| Word Count       | 10352       |