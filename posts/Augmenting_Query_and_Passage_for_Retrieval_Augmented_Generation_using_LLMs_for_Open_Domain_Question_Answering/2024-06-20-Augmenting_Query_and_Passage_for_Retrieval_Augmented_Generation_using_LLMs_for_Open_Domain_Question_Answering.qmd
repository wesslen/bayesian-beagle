
---
title: "Augmenting Query and Passage for Retrieval-Augmented Generation using LLMs for Open-Domain Question Answering"
id: "2406.14277v1"
description: "TL;DR: Improving open-domain QA by augmenting questions and passages with LLMs."
author: Minsang Kim, Cheoneum Park, Seungjun Baek
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.14277v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.14277v1/x1.png)

### Summary:

- The paper proposes a method called question and passage augmentation via LLMs for open-domain QA.
- The method decomposes the original questions into multiple-step sub-questions to make the query more specific.
- It also augments the retrieved passages with self-generated passages by LLMs to guide the answer extraction.
- The proposed scheme outperforms previous state-of-the-art and achieves significant performance gain over existing RAG methods.

### Major Findings:

1. The proposed method improves retrieval performance by making the query more specific.
2. Augmenting the retrieved passages with self-generated passages by LLMs helps in guiding the answer extraction.
3. The proposed scheme outperforms previous state-of-the-art and achieves significant performance gain over existing RAG methods.

### Analysis and Critique:

- The paper does not discuss the limitations or potential biases of the proposed method.
- The method heavily relies on the quality of contexts provided by retrieved passages, which may not always be accurate or relevant.
- The paper does not provide any comparison with other methods that use different types of LLMs or retrievers.
- The paper does not discuss the scalability or generalizability of the proposed method to other domains or tasks.
- The paper does not provide any real-world use cases or applications of the proposed method.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.14277v1](https://arxiv.org/abs/2406.14277v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.14277v1](https://browse.arxiv.org/html/2406.14277v1)       |
| Truncated       | False       |
| Word Count       | 6421       |