
---
title: "Improving Small Language Models' Mathematical Reasoning via Mix Thoughts Distillation"
id: "2401.11864v1"
description: "TL;DR: New methods compress large language models into smaller ones without losing performance."
author: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang
date: "2024-01-22"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The article introduces novel techniques, including Equation-of-Thought Distillation (EoTD) and Mix Thoughts Distillation (MTD), to enhance the mathematical reasoning capabilities of Small Language Models (SLMs). EoTD prompts Large Language Models (LLMs) to generate equations in response to questions, which are then processed by an equation solver. The resulting EoTD dataset is used to fine-tune SLMs, boosting their reasoning abilities. MTD further enhances SLM reasoning by creating a reasoning dataset with multiple thought processes. Experimental findings demonstrate that EoTD significantly boosts the reasoning abilities of SLMs, while MTD enables these models to achieve state-of-the-art reasoning performance. Additionally, the article discusses the process of data generation from LLMs for Chain-of-Thought Distillation (CoTD), which involves creating a dataset using in-context learning strategies to elicit the generation of rationales for mathematical reasoning.

### Major Findings:
1. Equation-of-Thought Distillation (EoTD) significantly boosts the reasoning abilities of Small Language Models (SLMs).
2. Mix Thoughts Distillation (MTD) enables SLMs to achieve state-of-the-art reasoning performance.
3. The process of data generation from Large Language Models (LLMs) for Chain-of-Thought Distillation (CoTD) ensures the quality of the reasoning dataset, contributing to the improved performance of fine-tuned Single Language Models (SLMs).

### Analysis and Critique:
The introduction of EoTD and MTD represents a significant advancement in enhancing the mathematical reasoning capabilities of SLMs. By leveraging the strengths of different thought processes and creating a diverse fine-tuning dataset, the authors demonstrate improved reasoning performance in SLMs. Additionally, the validation and filtering process for data generation ensures the accuracy and dependability of the training data, directly impacting the performance of the SLMs. However, further research is needed to explore the broader implications and potential limitations of these techniques, particularly in real-world applications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.11864v1](https://arxiv.org/abs/2401.11864v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.11864v1](https://browse.arxiv.org/html/2401.11864v1)       |
| Truncated       | True       |
| Word Count       | 16335       |