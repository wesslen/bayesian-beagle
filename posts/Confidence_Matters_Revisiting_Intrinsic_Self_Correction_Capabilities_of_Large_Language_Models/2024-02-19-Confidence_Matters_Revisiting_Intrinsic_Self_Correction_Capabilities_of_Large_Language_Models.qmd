
---
title: "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models"
id: "2402.12563v1"
description: "LLMs self-correct with confidence using IoE prompting for improved accuracy. Code available."
author: Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang
date: "2024-02-19"
image: "https://browse.arxiv.org/html/2402.12563v1/x2.png"
categories: ['robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12563v1/x2.png)

### Summary:
- The article presents a comprehensive investigation into the intrinsic self-correction of Large Language Models (LLMs), focusing on the role of "confidence" in the self-correction process.
- The study experimentally observed that LLMs possess the capability to understand and assess their "confidence" in their own responses, and introduced an "If-or-Else" (IoE) prompting framework to guide LLMs in assessing their own "confidence".
- The IoE-based Prompt was found to achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers.

### Major Findings:
1. LLMs can efficiently and effectively assess confidence of their own responses.
2. Understanding confidence enhances self-correction, making this process more adaptive and preventing over-criticism.

### Analysis and Critique:
- The study has limitations in terms of the models and benchmarks used, as well as the focus on English datasets exclusively.
- Potential risks associated with the prompting methodology include susceptibility to exploitation by malicious attackers, leading to the generation of toxic or harmful text.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.12563v1](https://arxiv.org/abs/2402.12563v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12563v1](https://browse.arxiv.org/html/2402.12563v1)       |
| Truncated       | False       |
| Word Count       | 10224       |