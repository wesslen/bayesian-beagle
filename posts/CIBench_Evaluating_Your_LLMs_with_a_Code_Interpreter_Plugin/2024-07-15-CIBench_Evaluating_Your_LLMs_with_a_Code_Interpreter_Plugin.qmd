
---
title: "CIBench: Evaluating Your LLMs with a Code Interpreter Plugin"
id: "2407.10499v1"
description: "CIBench evaluates LLMs' code interpreter use for data science, with/without human help, offering insights for future LLM development."
author: Songyang Zhang, Chuyu Zhang, Yingfan Hu, Haowen Shen, Kuikun Liu, Zerun Ma, Fengzhe Zhou, Wenwei Zhang, Xuming He, Dahua Lin, Kai Chen
date: "2024-07-15"
image: "https://browse.arxiv.org/html/2407.10499v1/x1.png"
categories: ['social-sciences', 'programming', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.10499v1/x1.png)

### Summary:

CIBench is an evaluation framework designed to assess the ability of Large Language Models (LLMs) to utilize code interpreters for data science tasks. The framework includes an evaluation dataset and two evaluation modes. The evaluation dataset is constructed using an LLM-human cooperative approach and simulates an authentic workflow by leveraging consecutive and interactive IPython sessions. The two evaluation modes assess LLMs’ ability with and without human assistance. The study focuses on assessing the proficiency of LLMs in leveraging code interpreters to address data science problems across several distinct domains, like data analysis, visualization, and machine learning. The evaluation framework aims to provide a thorough evaluation of LLMs’ ability to use code interpreters and address the shortcomings of existing benchmarks.

### Major Findings:

1. CIBench is an evaluation framework that includes a benchmark with consecutive and diverse tasks, along with comprehensive assessment protocols. The benchmark employs a distinctive LLM-human cooperative approach and simulates authentic workflow scenarios using interactive IPython sessions with sequential, interconnected questions focused on popular Python modules such as Matplotlib, Pandas, and PyTorch.
2. The evaluation framework includes two distinct evaluation modes: the end-to-end mode and the oracle mode. In the end-to-end mode, LLMs are tasked with a holistic problem-solving process where they must reason through given instructions and generate corresponding code. In the oracle mode, the LLM is provided with the correct code snippet when it fails, mimicking human guidance and equipping the model to use this accurate example for tackling subsequent tasks in the same context.
3. The study conducts extensive experiments and analysis using 19 LLMs. The results indicate that open-sourced LLMs struggle to utilize PyTorch- and TensorFlow-like modules, and the best-open-sourced LLMs lag behind GPT-4 by 10.0%. The contributions of the study are three-fold: building a new benchmark for agents with code interpreters using an LLM-human cooperative method, devising unique assessment strategies involving both end-to-end and oracle modes, and conducting thorough experiments with 24 LLMs to analyze their performance on the benchmark.

###

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.10499v1](https://arxiv.org/abs/2407.10499v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.10499v1](https://browse.arxiv.org/html/2407.10499v1)       |
| Truncated       | False       |
| Word Count       | 6515       |