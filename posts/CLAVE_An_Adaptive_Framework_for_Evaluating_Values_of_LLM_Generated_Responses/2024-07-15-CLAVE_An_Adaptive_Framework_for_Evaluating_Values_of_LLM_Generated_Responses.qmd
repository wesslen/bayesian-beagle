
---
title: "CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses"
id: "2407.10725v1"
description: "CLAVE framework uses dual-model approach for adaptable, generalizable LLM value evaluation, benchmarked on ValEval dataset."
author: Jing Yao, Xiaoyuan Yi, Xing Xie
date: "2024-07-15"
image: "https://browse.arxiv.org/html/2407.10725v1/x1.png"
categories: ['architectures', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.10725v1/x1.png)

### Summary:

The paper introduces CLAVE, a novel framework for evaluating the values of Large Language Models (LLMs) generated responses. The framework integrates two complementary LLMs: a large one for extracting high-level value concepts from a few human labels and a smaller one fine-tuned on these concepts to better align with human value understanding. This dual-model approach enables calibration with any value system using 100 human-labeled samples per value. The paper also presents ValEval, a comprehensive dataset comprising 13k+ text, value, label tuples across diverse domains and three major value systems. The authors benchmark the capabilities of 12+ popular LLM evaluators and analyze their strengths and weaknesses. The findings reveal that combining fine-tuned small models and prompt-based large ones serves as a superior balance in value evaluation.

### Major Findings:

1. The CLAVE framework integrates two complementary LLMs, a large one for extracting high-level value concepts and a smaller one fine-tuned on these concepts, to better align with human value understanding.
2. The dual-model approach enables calibration with any value system using 100 human-labeled samples per value.
3. The paper presents ValEval, a comprehensive dataset comprising 13k+ text, value, label tuples across diverse domains and three major value systems.
4. The authors benchmark the capabilities of 12+ popular LLM evaluators and analyze their strengths and weaknesses.
5. The findings reveal that combining fine-tuned small models and prompt-based large ones serves as a superior balance in value evaluation.

### Analysis and Critique:

The paper presents a novel framework, CLAVE, for evaluating the values of LLMs generated responses. The framework addresses the challenges of adaptability and generalizability in LLM-based evaluators by integrating two complementary LLMs. The dual-model approach enables calibration with any value system using a relatively small number of human-labeled samples. The paper also presents a comprehensive dataset, ValEval, which covers diverse domains and three major value systems. The authors benchmark the capabilities of 12+ popular LLM evaluators and provide a detailed analysis of their strengths and weaknesses.

However

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.10725v1](https://arxiv.org/abs/2407.10725v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.10725v1](https://browse.arxiv.org/html/2407.10725v1)       |
| Truncated       | False       |
| Word Count       | 9188       |