
---
title: "Unsupervised Evaluation of Code LLMs with Round-Trip Correctness"
id: "2402.08699v1"
description: "New evaluation method RTC expands LLM testing to real-world software domains without human curation."
author: Miltiadis Allamanis, Sheena Panthaplackel, Pengcheng Yin
date: "2024-02-13"
image: "https://browse.arxiv.org/html/2402.08699v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.08699v1/x1.png)

### Summary:
- The article introduces round-trip correctness (RTC) as an alternative evaluation method for code large language models (LLMs).
- RTC allows for the evaluation of code LLMs on a broader spectrum of real-world software domains without the need for costly human curation.
- The authors show how to employ RTC to evaluate code synthesis and editing, and demonstrate that RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing for expansion to a much broader set of domains and tasks.

### Major Findings:
1. RTC strongly correlates with model performance on existing narrow-domain code synthesis benchmarks while allowing for expansion to a much broader set of domains and tasks.
2. RTC complements existing narrow-domain benchmarks and allows for the evaluation of code LLMs without human-provided annotations.
3. RTC can be used to measure an LLMâ€™s performance over a wide range of real-life software domains and complements existing narrow-domain benchmarks.

### Analysis and Critique:
- The article provides a comprehensive evaluation of the RTC method and its application to code synthesis and editing. However, it is important to note that RTC is not without limitations, such as the quality of the similarity function and the assumption of "reasonably" trained and instruction-tuned LLMs.
- The authors also highlight the need for additional benchmarks beyond narrow-domain ones and suggest that RTC can be a reliable metric for evaluating LLMs.
- The article provides a critical analysis of existing supervised code evaluation metrics and highlights the potential of RTC as a more reliable evaluation metric for code LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.08699v1](https://arxiv.org/abs/2402.08699v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08699v1](https://browse.arxiv.org/html/2402.08699v1)       |
| Truncated       | False       |
| Word Count       | 7313       |