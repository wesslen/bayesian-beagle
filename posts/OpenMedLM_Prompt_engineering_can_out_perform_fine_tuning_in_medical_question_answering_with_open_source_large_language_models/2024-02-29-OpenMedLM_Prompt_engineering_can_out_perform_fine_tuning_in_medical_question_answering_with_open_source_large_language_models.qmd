
---
title: "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models"
id: "2402.19371v1"
description: "OpenMedLM: Open-source LLMs Achieve State-of-the-Art Results in Medical Benchmarks."
author: Jenish Maharjan, Anurag Garikipati, Navan Preet Singh, Leo Cyrus, Mayank Sharma, Madalina Ciobanu, Gina Barnes, Rahul Thapa, Qingqing Mao, Ritankar Das
date: "2024-02-29"
image: "../../img/2402.19371v1/image_1.png"
categories: ['production', 'education', 'architectures', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.19371v1/image_1.png)

### **Summary:**

- The article presents OpenMedLM, a prompting platform that delivers state-of-the-art performance for open-source large language models (LLMs) on medical benchmarks.
- OpenMedLM utilizes a series of prompting strategies, including zero-shot, few-shot, chain-of-thought, and ensemble/self-consistency voting, to optimize the performance of open-source foundation LLMs on medical tasks.
- The authors demonstrate that OpenMedLM outperforms previous best performing open-source models that leveraged computationally costly extensive fine-tuning.

### **Major Findings:**

1. OpenMedLM delivers state-of-the-art results on three common medical LLM benchmarks, surpassing the previous best performing open-source models without the need for specialized fine-tuning.
2. The model achieves a 72.6% accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and establishes itself as the first open-source LLM to surpass 80% accuracy on the MMLU medical-subset benchmark.
3. The study highlights medical-specific emergent properties in open-source LLMs and showcases the benefits of further leveraging prompt engineering to improve the performance of accessible LLMs for medical applications.

### **Analysis and Critique:**

- The article focuses on open-source LLMs, which have inherent advantages in terms of transparency and compliance, making them suitable for healthcare applications. However, the study could benefit from a more thorough discussion on the limitations of open-source models, such as potential biases in training data and the challenges of maintaining and updating these models.
- While the study demonstrates the effectiveness of prompt engineering techniques, it would be beneficial to compare these methods with fine-tuning approaches to provide a more comprehensive understanding of their relative merits.
- The authors mention the potential for catastrophic forgetting in performance optimization methods like full-parameter fine-tuning. A more detailed exploration of this phenomenon and its implications for medical applications could provide valuable insights for researchers and practitioners in the field.
- The study could also include an analysis of the computational costs associated with different prompt engineering techniques and fine-tuning methods to help

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.19371v1](https://arxiv.org/abs/2402.19371v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19371v1](https://browse.arxiv.org/html/2402.19371v1)       |
| Truncated       | False       |
| Word Count       | 12122       |