
---
title: "Large Language Models as Evaluators for Recommendation Explanations"
id: "2406.03248v2"
description: "LLMs, like GPT4, can accurately evaluate recommendation explanations with proper prompts and settings, offering a cost-effective solution."
author: Xiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, Min Zhang
date: "2024-06-06"
image: "https://browse.arxiv.org/html/2406.03248v2/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.03248v2/x1.png)

### Summary:
- The study investigates the use of Large Language Models (LLMs) as evaluators for recommendation explanations, a challenging and unresolved issue in the field of explainable recommendations.
- The authors utilize real user feedback, third-party annotations, and LLM evaluations to measure the correlation between evaluator labels and user-provided ground truth.
- The experiments reveal that LLMs, such as GPT4, can provide comparable evaluations with appropriate prompts and settings.
- The study also explores combining human labels with the LLM evaluation process and utilizing ensembles of multiple heterogeneous LLM evaluators to enhance the accuracy and stability of evaluations.
- The findings suggest that utilizing LLMs as evaluators can be an accurate, reproducible, and cost-effective solution for evaluating recommendation explanation texts.

### Major Findings:
1. Certain zero-shot LLMs, such as GPT4, can attain evaluation accuracy comparable to or better than traditional methods, with performance varying across different aspects.
2. The effectiveness of one-shot learning depends on backbone LLMs. Particularly, personalized cases can assist GPT4 in learning user scoring bias.
3. Ensembling the scoring of multiple heterogeneous LLMs can improve the accuracy and stability of evaluation.

### Analysis and Critique:
- The study provides a comprehensive evaluation of LLMs as evaluators for recommendation explanations, addressing a significant gap in the field.
- The use of real user feedback as ground truth and the application of a 3-level meta-evaluation strategy contribute to the robustness of the findings.
- However, the study is limited to text-based explanations, and future research should consider unified evaluation protocols that encompass a broader range of explanation formats.
- Additionally, the development of novel methodologies to further enhance the evaluation accuracy of LLMs is an important area for future investigation.
- The study's findings have implications for the advancement of the area of explainable recommendation, as LLM-based evaluators can be applied to new datasets with few limitations and offer a cost-effective solution compared to traditional methods.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.03248v2](https://arxiv.org/abs/2406.03248v2)        |
| HTML     | [https://browse.arxiv.org/html/2406.03248v2](https://browse.arxiv.org/html/2406.03248v2)       |
| Truncated       | False       |
| Word Count       | 7752       |