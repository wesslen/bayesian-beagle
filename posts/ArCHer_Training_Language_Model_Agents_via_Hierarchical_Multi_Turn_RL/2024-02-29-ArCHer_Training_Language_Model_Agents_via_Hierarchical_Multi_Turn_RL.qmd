
---
title: "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL"
id: "2402.19446v1"
description: "ArCHer: Hierarchical RL for Efficient Multi-turn Decision-Making with LLMs

This paper proposes a new algorithmic framework, ArCHer, for developing multi-turn reinforcement learning (RL) algorithms for fine-tuning large language models (LLMs) in goal-directed decision-making tasks. ArCHer adopts a hierarchical RL approach, running two RL algorithms in parallel: a high-level off-policy RL algorithm that trains a value function to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token-by-token policy within each utterance or turn. The paper finds that ArCHer significantly improves efficiency and performance on multi-turn tasks, with a sample efficiency of about 100x over existing on-policy methods, and benefits from scaling up model capacity. The project page and code can be found at <https://yifeizhou02.github.io/archer.io/> and <https://github.com/YifeiZhou02/ArCHer>, respectively."
author: Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19446v1/x1.png"
categories: ['hci', 'production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19446v1/x1.png)

### ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL

**Summary:**

- Large language models (LLMs) have the potential to address a wide variety of decision-making or "agent" problems that can be expressed in text or natural language.
- Current RL methods for LLMs largely focus on single-turn reward maximization, which cannot effectively train LLMs to seek and incorporate information over multiple turns, perform credit assignment, or reason about their past actions.
- The proposed framework, ArCHer, adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy RL algorithm that trains a value function to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token-by-token policy within each utterance or turn.
- Empirically, ArCHer significantly improves efficiency and performance on multi-turn tasks, attaining sample efficiency of about 100x over existing on-policy methods.

**Major Findings:**

1. ArCHer, a hierarchical RL framework, is designed for multi-turn RL algorithms to train LLMs directly for long-term objective maximization.
2. ArCHer runs two RL algorithms in parallel: a high-level off-policy RL algorithm for value function aggregation and a low-level RL algorithm for token-by-token policy training.
3. ArCHer significantly improves efficiency and performance on multi-turn tasks, attaining sample efficiency of about 100x over existing on-policy methods.

**Analysis and Critique:**

- ArCHer's hierarchical RL approach effectively addresses the challenges of multi-turn RL for LLMs, but the paper could provide more details on the specific RL algorithms used in the high-level and low-level components.
- The paper demonstrates ArCHer's performance on multi-turn tasks, but it would be beneficial to see how it performs on a wider range of decision-making or "agent" problems.
- The paper could also discuss potential limitations and future research directions, such as exploring other hierarchical RL approaches, addressing the cold-start problem, and incorporating more sophisticated reward modeling techniques.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.19446v1](https://arxiv.org/abs/2402.19446v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19446v1](https://browse.arxiv.org/html/2402.19446v1)       |
| Truncated       | False       |
| Word Count       | 16897       |