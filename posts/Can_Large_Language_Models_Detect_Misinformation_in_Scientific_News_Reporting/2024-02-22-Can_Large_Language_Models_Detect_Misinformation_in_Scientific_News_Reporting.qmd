
---
title: "Can Large Language Models Detect Misinformation in Scientific News Reporting?"
id: "2402.14268v1"
description: "Detecting misinformation in scientific reporting using large language models and prompt engineering strategies."
author: Yupeng Cao, Aishwarya Muralidharan Nair, Elyon Eyimife, Nastaran Jamalipour Soofi, K. P. Subbalakshmi, John R. Wullert II, Chumki Basu, David Shallcross
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14268v1/extracted/5419300/image/dataset_pipeline.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14268v1/extracted/5419300/image/dataset_pipeline.png)

### Summary:
The article explores the use of large language models (LLMs) to detect misinformation in scientific news reporting. The authors propose three architectures using LLMs to automatically detect false representations of scientific findings in the popular press. They also define dimensions of scientific validity and test different prompting strategies to enhance the LLMs' ability to make accurate predictions. The study includes the creation of a novel dataset, SciNews, containing human-written and LLM-generated news articles paired with related scientific articles. The results show that it is more challenging to identify LLM-generated scientific misinformation compared to human-authored misinformation. The study also highlights the importance of prompt engineering and the potential misuse of LLMs.

### Major Findings:
1. LLMs struggle to detect LLM-generated scientific misinformation compared to human-authored misinformation.
2. The SIf architecture yields the highest accuracy in processing human-authored articles, indicating the potential to develop more flexible and generalized scientific misinformation detection models.
3. The CoT prompting strategy generally outperforms the zero-shot approach, suggesting that the defined dimensions of scientific validity effectively aid LLMs in making more accurate predictions.

### Analysis and Critique:
- The study provides valuable insights into the challenges and potential of using LLMs to detect scientific misinformation.
- The results highlight the need for further research to address the complexities of detecting LLM-generated misinformation and the potential risks associated with LLMs.
- The study could benefit from a more in-depth discussion of the ethical implications and potential biases associated with the use of LLMs in detecting scientific misinformation. Additionally, further exploration of the explainability of LLMs' decision-making processes would enhance the study's impact.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.14268v1](https://arxiv.org/abs/2402.14268v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14268v1](https://browse.arxiv.org/html/2402.14268v1)       |
| Truncated       | False       |
| Word Count       | 9361       |