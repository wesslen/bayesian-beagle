
---
title: "Can Large Language Models Understand Symbolic Graphics Programs?"
id: "2408.08313v1"
description: "LLMs can understand symbolic graphics programs by answering questions about visual content, which they may imagine from the programs. A new benchmark evaluates LLMs' visual reasoning abilities, and Symbolic Instruction Tuning improves their understanding with a small dataset."
author: Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Sch√∂lkopf
date: "2024-08-15"
image: "https://browse.arxiv.org/html/2408.08313v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.08313v1/x1.png)

**Summary:**

This paper explores the ability of large language models (LLMs) to understand symbolic graphics programs, which are a popular representation for generating visual data. The authors propose a new task of symbolic graphics program understanding and introduce a generic yet scalable benchmark creation pipeline for this task. They build a large benchmark, SGP-Bench, for comprehensively evaluating LLM's semantic understanding and consistency of symbolic graphics programs. The benchmark includes two types of symbolic graphics programs: SVG for 2D vector graphics and CAD for 2D/3D objects. To improve the symbolic program understanding, the authors collect an instruction-following dataset and propose a new finetuning method, called symbolic instruction tuning. They also introduce a symbolic MNIST dataset, where the symbolic problem understanding can be extremely challenging, and show that symbolic instruction tuning can also improve generic instruction following performance.

**Major Findings:**

1. The authors propose a new task of symbolic graphics program understanding and introduce a generic yet scalable benchmark creation pipeline for this task.
2. They build a large benchmark, SGP-Bench, for comprehensively evaluating LLM's semantic understanding and consistency of symbolic graphics programs.
3. The authors collect an instruction-following dataset and propose a new finetuning method, called symbolic instruction tuning, to improve the symbolic program understanding.
4. They introduce a symbolic MNIST dataset, where the symbolic problem understanding can be extremely challenging, and show that symbolic instruction tuning can also improve generic instruction following performance.

**Analysis and Critique:**

The paper presents an interesting and novel task of symbolic graphics program understanding and introduces a large benchmark, SGP-Bench, for evaluating LLMs on this task. The authors also propose a new finetuning method, symbolic instruction tuning, to improve the symbolic program understanding. However, the paper does not provide a detailed analysis of the limitations and potential biases of the proposed benchmark and finetuning method. Additionally, the paper does not discuss the potential applications and implications of the proposed task and benchmark. It would be interesting to see how the proposed task and benchmark can be used to improve the performance of LLMs on other tasks, such as visual

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.08313v1](https://arxiv.org/abs/2408.08313v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.08313v1](https://browse.arxiv.org/html/2408.08313v1)       |
| Truncated       | False       |
| Word Count       | 13856       |