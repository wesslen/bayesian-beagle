
---
title: "Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion Related to Harms of Misinformation"
id: "2401.16558v1"
description: "Fact-checkers prioritize limited resources, AI reflects gender differences in misinformation opinions."
author: Terrence Neumann, Sooyong Lee, Maria De-Arteaga, Sina Fazelpour, Matthew Lease
date: "2024-01-29"
image: "https://browse.arxiv.org/html/2401.16558v1/x1.png"
categories: ['robustness', 'social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.16558v1/x1.png)

### Summary:
The article investigates the potential implications of using large language models (LLMs) to facilitate the prioritization of fact-checking claims. The study focuses on gender differences in opinion related to the harms of misinformation and presents the TopicMisinfo dataset, containing fact-checked claims from diverse topics and human annotations. The findings reveal that LLMs reflect gender differences in opinion but also amplify the extent of these differences, particularly in contentious topics. Additionally, LLM responses tend to be more aligned with men's views when prompted neutrally, especially on topics related to abortion. The study raises concerns about the accuracy and fairness of using LLMs for claim prioritization in fact-checking pipelines.

### Major Findings:
1. LLMs reflect gender differences in opinion but also amplify the extent of these differences.
2. Neutrally-prompted LLM responses are more aligned with men's views, particularly on topics related to abortion.
3. The study raises concerns about the accuracy and fairness of using LLMs for claim prioritization in fact-checking pipelines.

### Analysis and Critique:
The study provides valuable insights into the potential implications of using LLMs for claim prioritization in fact-checking pipelines. However, the findings are limited by the homogeneity of the crowd-worker pool and the focus on a single LLM, which may limit the generalizability of the results. Additionally, the study's focus on the United States may not fully capture the diverse cultural contexts relevant to the global population. Further research is needed to explore a broader array of gender perspectives and assess a range of LLMs to understand their behaviors and biases more comprehensively. Overall, the study highlights the need for a conscientious approach to the development and use of LLMs in fact-checking pipelines to ensure accuracy, fairness, and representation of diverse societal perspectives.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2401.16558v1](https://arxiv.org/abs/2401.16558v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.16558v1](https://browse.arxiv.org/html/2401.16558v1)       |
| Truncated       | False       |
| Word Count       | 9112       |