
---
title: "Calibrating Reasoning in Language Models with Internal Consistency"
id: "2405.18711v1"
description: "LLMs' reasoning can be improved by up-weighting paths with high internal consistency, enhancing reliability and performance."
author: Zhihui Xie, Jizhou Guo, Tong Yu, Shuai Li
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.18711v1/x1.png"
categories: ['prompt-engineering', 'robustness', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.18711v1/x1.png)

### Summary:

This study investigates the internal representations of large language models (LLMs) in the context of chain-of-thought (CoT) reasoning. The authors find that while CoT reasoning improves answer accuracy, it also introduces inconsistencies between the model's internal representations in middle layers and those in final layers. To address this, the authors propose internal consistency as a measure of the model's confidence, which examines the agreement of latent predictions decoded from intermediate layers. Extensive empirical studies demonstrate that internal consistency effectively distinguishes between correct and incorrect reasoning paths and can be used to calibrate CoT reasoning, resulting in a significant boost in reasoning performance.

### Major Findings:

1. CoT reasoning improves answer accuracy but exacerbates inconsistency between hidden and stated reasoning in LLMs.
2. Internal consistency, a measure of the agreement of latent predictions decoded from intermediate layers, effectively distinguishes between correct and incorrect reasoning paths.
3. By up-weighting reasoning paths with high internal consistency, a significant improvement in reasoning performance can be achieved.

### Analysis and Critique:

The authors' approach to calibrating CoT reasoning using internal consistency is a promising method for improving the reliability of LLMs. However, the study is limited to decoder-only models, and the analysis focuses on vanilla CoT prompting. Further research is needed to explore the application of internal consistency to other prompting techniques and encoder-decoder models. Additionally, the potential negative societal impacts of enhanced reasoning abilities in LLMs, such as the generation of more convincing disinformation, should be considered and addressed through robust monitoring and access control mechanisms.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.18711v1](https://arxiv.org/abs/2405.18711v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.18711v1](https://browse.arxiv.org/html/2405.18711v1)       |
| Truncated       | False       |
| Word Count       | 8071       |