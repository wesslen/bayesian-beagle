
---
title: "DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories"
id: "2405.19856v1"
description: "TL;DR: DevEval, a new benchmark, better evaluates LLMs' coding abilities in real-world repositories, revealing their strengths and weaknesses."
author: Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Huanyu Liu, Hao Zhu, Lecheng Wang, Kaibo Liu, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming Zhang, Yuqi Zhu, Yihong Dong, Zhi Jin, Binhua Li, Fei Huang, Yongbin Li
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.19856v1/x1.png"
categories: ['prompt-engineering', 'programming', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19856v1/x1.png)

### Summary:

The paper introduces a new benchmark, DevEval, for evaluating the coding abilities of Large Language Models (LLMs) in real-world code repositories. DevEval is manually annotated by 13 developers and contains comprehensive annotations, including requirements, original repositories, reference code, and reference dependencies. The benchmark comprises 1,874 testing samples from 117 repositories, covering 10 popular domains. The authors evaluate 8 popular LLMs on DevEval and reveal their coding abilities in real-world code repositories. The highest Pass@1 of gpt-4-turbo only is 53.04% in their experiments. The paper also analyzes LLMs' failed cases and summarizes their shortcomings.

### Major Findings:

1. DevEval is a new benchmark that aligns with real-world code repositories, providing a more realistic evaluation scenario for LLMs.
2. DevEval contains 1,874 testing samples from 117 repositories, covering 10 popular domains, and is manually annotated by 13 developers.
3. The authors evaluate 8 popular LLMs on DevEval and reveal their coding abilities in real-world code repositories.
4. The highest Pass@1 of gpt-4-turbo only is 53.04% in their experiments.
5. The paper analyzes LLMs' failed cases and summarizes their shortcomings.

### Analysis and Critique:

The paper presents a valuable contribution to the field of LLMs by introducing a new benchmark, DevEval, that aligns with real-world code repositories. The benchmark is comprehensive and covers a wide range of domains, making it a useful tool for evaluating LLMs' coding abilities. However, the paper does not provide a detailed analysis of the shortcomings of LLMs, which could have been useful for future research. Additionally, the paper does not discuss the limitations of DevEval, such as its size or the potential for bias in the annotations. Overall, the paper provides a useful resource for researchers and practitioners interested in evaluating LLMs' coding abilities in real-world scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19856v1](https://arxiv.org/abs/2405.19856v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19856v1](https://browse.arxiv.org/html/2405.19856v1)       |
| Truncated       | False       |
| Word Count       | 6355       |