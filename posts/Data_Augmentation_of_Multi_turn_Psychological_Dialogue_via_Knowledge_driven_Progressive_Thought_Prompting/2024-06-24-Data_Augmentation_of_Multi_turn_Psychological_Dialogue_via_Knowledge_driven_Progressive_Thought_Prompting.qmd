
---
title: "Data Augmentation of Multi-turn Psychological Dialogue via Knowledge-driven Progressive Thought Prompting"
id: "2406.16567v1"
description: "New method for multi-turn dialogue data augmentation in psychology, using progressive thought and psychology knowledge generators, and a multi-turn dialogue generator."
author: Jiyue Jiang, Liheng Chen, Sheng Wang, Lingpeng Kong, Yu Li, Chuan Wu
date: "2024-06-24"
image: "https://browse.arxiv.org/html/2406.16567v1/x1.png"
categories: ['prompt-engineering', 'hci', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.16567v1/x1.png)

### Summary:

The paper proposes a novel method called Knowledge-driven Progressive Thought (KPT) prompting for multi-turn dialogue data augmentation in the psychology domain. The KPT method consists of three components: a progressive thought generator, a psychological knowledge generator, and a multi-turn dialogue generator. The progressive thought generator selects appropriate thoughts from a database to guide multi-turn dialogue generation and prevent semantic deviations. The psychological knowledge generator provides the necessary knowledge, while a penalty evaluation framework ensures dialogue quality. The multi-turn dialogue generator incorporates knowledge into the dialogue history, preventing information redundancy and ensuring high-quality generation.

### Major Findings:

1. The progressive thought generator effectively references contextual information and prevents semantic errors in dialogue generation.
2. The psychological knowledge generator supports the creation of psychological knowledge and prompts, enabling better generation of psychological dialogues.
3. The method leverages the powerful capabilities of LLMs in handling context, selecting and incorporating knowledge into the dialogue history, and preventing information redundancy.
4. Extensive experiments demonstrate the high quality of multi-turn dialogue generated by KPT on three datasets related to psychological dialogue, and the superiority of small models after training based on KPT augmented data.

### Analysis and Critique:

The proposed KPT method addresses the issue of multi-turn dialogue data augmentation for boosted performance in the psychology domain. The method effectively integrates a progressive thought generator, a psychology knowledge generator, and a multi-turn dialogue generator to guide LLM in generating multi-turn psychology-related dialogue. The method ensures the precision of multi-turn psychological dialogue generation by LLM through a meticulous professional evaluation.

However, the paper does not discuss the limitations or potential biases of the proposed method. It would be beneficial to explore the method's performance in handling longer dialogue history and domain-specific dialogue DA. Additionally, the paper does not provide a comparison with other multi-turn dialogue DA methods, which could further validate the proposed method's effectiveness.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.16567v1](https://arxiv.org/abs/2406.16567v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.16567v1](https://browse.arxiv.org/html/2406.16567v1)       |
| Truncated       | False       |
| Word Count       | 4719       |