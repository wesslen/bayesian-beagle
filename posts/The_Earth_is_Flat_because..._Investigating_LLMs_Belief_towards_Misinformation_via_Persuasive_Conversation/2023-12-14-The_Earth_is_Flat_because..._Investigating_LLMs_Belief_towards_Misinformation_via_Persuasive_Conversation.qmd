
---
title: "The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation"
id: "2312.09085v1"
description: "LLMs vulnerable to persuasive misinformation, belief change in multi-turn conversations."
author: Rongwu Xu, Brian S. Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu, Han Qiu
date: "2023-12-14"
image: "../../../bayesian-beagle.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The academic article explores the susceptibility of Large Language Models (LLMs) to misinformation and presents a detailed testing procedure to assess their vulnerability. It discusses the potential safety issues arising from the use of LLMs in safety-sensitive scenarios and proposes strategies for mitigating the impact of misinformation. Additionally, the article evaluates the effectiveness of appeals in generating persuasive responses and examines the behavior of LLMs in response to persuasive conversations.

### Major Findings:
1. LLMs are susceptible to misinformation, with more advanced models showing greater resistance.
2. Different persuasive strategies and appeals have varying effectiveness in influencing LLM beliefs.
3. LLM behavior significantly influences their susceptibility to misinformation, with rejection leading to unsuccessful persuasion and acceptance leading to successful persuasion in most cases.

### Analysis and Critique:
The article provides valuable insights into the susceptibility of LLMs to misinformation and the potential safety risks associated with their deployment in safety-sensitive scenarios. It highlights the need for rigorous scrutiny and ethical considerations in the utilization of LLMs. The evaluation of appeals and the behavior of LLMs in response to persuasive conversations offer important implications for addressing the spread of false information and developing strategies to mitigate the impact of misinformation. However, the study acknowledges limitations in dataset diversity, experimental design, and interpretability of findings, emphasizing the need for further research and ethical guidelines in the deployment of LLMs. Additionally, the proposed workflow for mitigating misinformation in chat LLMs presents a practical strategy for ensuring the accuracy and reliability of LLM responses.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2312.09085v1](https://arxiv.org/abs/2312.09085v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.09085v1](https://browse.arxiv.org/html/2312.09085v1)       |
| Truncated       | True       |
| Word Count       | 39171       |