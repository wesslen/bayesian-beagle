
---
title: "FENet: Focusing Enhanced Network for Lane Detection"
id: "2312.17163v1"
description: "Networks with Focusing Sampling improve lane detection for autonomous driving by capturing vital distant details. FENetV2 recommended for practical use."
author: Liman Wang, Hanyang Zhong
date: "2023-12-28"
image: "https://browse.arxiv.org/html/2312.17163v1/extracted/5320991/figure/Figure1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.17163v1/extracted/5320991/figure/Figure1.png)

### Major Findings

1. **FENet** introduces innovations such as **Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss** to address challenges in precise lane detection for autonomous driving, significantly boosting benchmark and practical lane recognition accuracy.

2. FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, while FENetV2 proves most reliable on the proposed Partial Field analysis. 

3. FENetV2 may underperform slightly in conventional metrics compared to FENetV1, but its specialization in distant lane regression makes it more suited for practical navigation.

### Methodology
- **Focusing Sampling**: Prioritizes small and distant lane details, addressing the deficiencies in understanding upcoming lanes. It emphasizes distant details while examining nearby points, transforming limitations of uniform sampling that risk losing critical data and semantics critical to lane prediction.
- **Positional Non-local Block and Position Enhanced FPN structure**: Introduces a structure assimilating non-local blocks to inject contextual clues into the network while preserving its depth and enhancing its multi-scale properties.
- **Lane Directional Intersection over Union (D-IoU) Module**: Ascertains directional discrepancies to improve accuracy, encoding distance and directional accuracy for precise lane alignment.
- **Augmenting Evaluation via Partial Field of View**: Proposes assessment through a Partial Field of View metric, subdividing the lower image half into fractional views after preprocessing cropping of irrelevant upper content.

### Experiment
- **Datasets**: Employs the CULane and LLAMAS datasets for evaluation.
- **Evaluation Metrics**: Uses F1 and mF1 to measure the model's performance.
- **Comparison with State-of-the-Art Results**: Performance on CULane and LLAMAS datasets, demonstrating the superiority of FENetV2 over existing methods for practical autonomous navigation. 

### Critique
The paper provides a comprehensive and detailed exploration of the **FENet** framework and its contributions to the field of lane detection for autonomous driving. However, the complexity of the proposed innovations may lead to challenges in implementation and practical real-world deployment. Additionally, the paper could benefit from more in-depth discussions on potential limitations and trade-offs associated with the proposed methodologies. Further validation on diverse real-world scenarios and conditions would enhance the robustness of the findings.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-05       |
| Abstract | [http://arxiv.org/abs/2312.17163v1](http://arxiv.org/abs/2312.17163v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.17163v1](https://browse.arxiv.org/html/2312.17163v1)       |
| Truncated       | False       |
| Word Count       | 6575       |