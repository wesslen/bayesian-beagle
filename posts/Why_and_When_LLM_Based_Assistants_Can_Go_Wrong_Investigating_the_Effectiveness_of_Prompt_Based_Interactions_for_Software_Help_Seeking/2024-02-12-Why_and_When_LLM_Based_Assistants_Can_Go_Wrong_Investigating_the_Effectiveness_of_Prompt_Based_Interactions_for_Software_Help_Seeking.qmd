
---
title: "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking"
id: "2402.08030v1"
description: "LLMs like ChatGPT mimic human-like interactions for software guidance, but users struggle to understand and evaluate their advice."
author: Anjali Khurana, Hari Subramonyam, Parmit K Chilana
date: "2024-02-12"
image: "https://browse.arxiv.org/html/2402.08030v1/extracted/5403405/task_powerpoint_5.png"
categories: ['prompt-engineering', 'programming', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.08030v1/extracted/5403405/task_powerpoint_5.png)

### **Summary:**
- The study investigated the effectiveness of Large Language Model (LLM)-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews.
- SoftAIBot, an LLM optimized for particular software contexts, outperformed the Baseline LLM in providing accurate and relevant assistance for software tasks.
- Users struggled to understand how the prompt’s text related to the LLM’s responses and often followed the LLM’s suggestions verbatim, even if they were incorrect.

### **Major Findings:**
1. SoftAIBot outperformed the Baseline LLM in providing accurate and relevant assistance for software tasks.
2. Users struggled to understand how the prompt’s text related to the LLM’s responses and often followed the LLM’s suggestions verbatim, even if they were incorrect.
3. Most users struggled to apply the LLM’s instructions to the software application, leading to low task completion rates.

### **Analysis and Critique:**
- The study highlights the need for incorporating explainable, context-aware cues into LLMs to help users understand prompt-based interactions and identify biases.
- Users exhibited unwarranted confidence in LLM-generated responses due to their human-like nature and consistent, contextual relevance, leading to overtrust and failure to recognize erroneous or hallucinated output.
- The study calls for more transparency in addressing users’ expectations and misconceptions about LLMs and emphasizes the importance of user training and education about AI-powered assistants.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.08030v1](https://arxiv.org/abs/2402.08030v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08030v1](https://browse.arxiv.org/html/2402.08030v1)       |
| Truncated       | False       |
| Word Count       | 13314       |