
---
title: "LLMs Meet Multimodal Generation and Editing: A Survey"
id: "2405.19334v1"
description: "This survey explores multimodal generation advancements in image, video, 3D, and audio, highlighting key technical components and datasets. It also discusses AI safety and emerging applications. [Link to related papers](https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation)."
author: Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.19334v1/x2.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19334v1/x2.png)

**Summary:**

The paper "LLMs Meet Multimodal Generation and Editing: A Survey" provides a comprehensive review of works involving Large Language Models (LLMs) in the generation of multiple modalities, including images, videos, 3D, and audio. The authors discuss the roles of LLMs in this process, such as evaluator, labeller, instruction processor, planner, provider of semantic guidance, or as backbone architectures. The paper also covers the safety issues in the AIGC era, emerging applications, and future prospects. The authors present the first systematic review of LLMs applied to the generation of multiple modalities and discuss the evolution of generative techniques through a comparative analysis of pre-LLM and post-LLM eras.

**Major Findings:**

1. The integration of MLLMs and the use of models like ImageBind have significant implications for the field of multimodal generation

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19334v1](https://arxiv.org/abs/2405.19334v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19334v1](https://browse.arxiv.org/html/2405.19334v1)       |
| Truncated       | True       |
| Word Count       | 34354       |