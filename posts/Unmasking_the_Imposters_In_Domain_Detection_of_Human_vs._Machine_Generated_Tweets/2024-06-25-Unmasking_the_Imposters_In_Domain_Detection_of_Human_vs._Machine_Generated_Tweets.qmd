
---
title: "Unmasking the Imposters: In-Domain Detection of Human vs. Machine-Generated Tweets"
id: "2406.17967v1"
description: "Uncensored, fine-tuned LLMs evade detection, raising concerns about misuse on social media."
author: Bryan E. Tuck, Rakesh M. Verma
date: "2024-06-25"
image: "https://browse.arxiv.org/html/2406.17967v1/extracted/5691672/machine_detect.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17967v1/extracted/5691672/machine_detect.png)

### Summary:

This study presents a methodology using Twitter datasets to examine the generative capabilities of four large language models (LLMs): Llama 3, Mistral, Qwen2, and GPT4o. The authors evaluate 7B and 8B parameter base-instruction models of the three open-source LLMs and validate the impact of further fine-tuning and "uncensored" versions. The findings show that "uncensored" models with additional in-domain fine-tuning dramatically reduce the effectiveness of automated detection methods. This research addresses a gap by exploring smaller open-source models and the effects of "uncensoring," providing insights into how fine-tuning and content moderation influence machine-generated text detection.

### Major Findings:

1. The study introduces a novel methodology that adapts publicly available Twitter datasets to examine the generative capabilities of four state-of-the-art LLMs, addressing a gap in previous research that primarily focused on OpenAIâ€™s GPT models.
2. The research conducts experiments with 7B and 8B parameter base-instruction models of four LLMs, including three open-source models (Llama 3, Mistral, and Qwen2) and GPT4o, validating the efficacy of fine-tuned and "uncensored" versions, providing insights into the impact of these factors on the detection of machine-generated text.
3. The findings reveal that "uncensored" models with additional in-domain fine-tuning substantially decrease the ability of automated detection methods, showcasing an absolute drop of 16.86% detection rate in the worst-case scenario. The authors provide nine benchmark detection sub-datasets and their complete methodology to facilitate future research.

### Analysis and Critique:

* The study focuses on Twitter data, which may not generalize to other social media platforms or domains outside social media. The unique characteristics of Twitter, such as the short text length, use of hashtags and mentions, and real-time nature of the platform, may influence the performance of the detection methods.
* The TweetEval dataset used for fine-tuning and evaluation may not fully capture the diversity of topics, opinions, and demographics on Twitter, potentially limiting the generaliz

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17967v1](https://arxiv.org/abs/2406.17967v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17967v1](https://browse.arxiv.org/html/2406.17967v1)       |
| Truncated       | False       |
| Word Count       | 6557       |