
---
title: "All Language Models Large and Small"
id: "2402.12061v1"
description: "TL;DR: LONDI framework uses large language models selectively, reducing computational costs by 30%."
author: Zhixun Chen, Yali Du, David Mguni
date: "2024-02-19"
image: "../../img/2402.12061v1/image_1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.12061v1/image_1.png)

### Summary:

- The Language Optimising Network Distribution (LONDI) framework aims to selectively employ large language models (LLMs) only where complex decision-making and reasoning are required while using low-resource LMs everywhere else.
- The LONDI framework consists of two language models, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn in which system states to call the LLM.
- The LONDI framework is tested in various tasks and is shown to significantly lower computational costs.
- The section discusses the solution to Switcher's problem and introduces the LONDI system, which aims to optimize the usage of the DEEPTHINK model while respecting a budget constraint on the number of allowed DEEPTHINK calls during training.
- The LONDI system outperforms other systems and effectively manages computational resources while achieving high performance.
- The section introduces the LONDI framework, which combines large language models (LLM) and language models (LM) to improve performance and reduce computational cost.
- The budget-oriented variant, LONDI-B, offers increased control and precision and showcases performance improvements and computational cost decreases.
- The section introduces the notation and proofs related to the contraction property of T and presents the three statements to be proven and then proceeds to prove them.

### Major Findings:

1. The LONDI framework significantly lowers computational costs while maintaining high performance.
2. LONDI outperforms other systems and effectively manages computational resources while achieving high performance.
3. The LONDI framework combines large language models to improve performance and reduce computational costs.

### Analysis and Critique:

- The LONDI framework demonstrates potential for practical applications by significantly reducing computational costs and preserving budgetary constraints on LLM calls.
- The experiments conducted demonstrate the adaptability and robustness of LONDI in different environments and with varying components.
- The section provides detailed proofs for Theorem 3, Proposition 1, and Theorem 2, contributing to the overall credibility and reliability of the research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.12061v1](https://arxiv.org/abs/2402.12061v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12061v1](https://browse.arxiv.org/html/2402.12061v1)       |
| Truncated       | True       |
| Word Count       | 16291       |