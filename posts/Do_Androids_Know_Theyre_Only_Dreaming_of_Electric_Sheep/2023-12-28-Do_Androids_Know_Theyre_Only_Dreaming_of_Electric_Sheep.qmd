
---
title: "Do Androids Know They're Only Dreaming of Electric Sheep?"
id: "2312.17249v1"
description: "Probes trained on language model representations detect hallucination behavior across tasks, but force-decoded states are not valid for organic hallucination detection. Detection varies by layer, state type, and task, with extrinsic hallucinations being more salient. Probing is a feasible alternative to evaluating language model hallucinations."
author: ['Sky CH-Wang', 'Benjamin Van Durme', 'Jason Eisner', 'Chris Kedzie']
date: "2023-12-28"
image: "https://browse.arxiv.org/html/2312.17249v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.17249v1/x1.png)

# Paper Summary

## Main Findings
- The study explores probes on a decoder-only transformer language model to detect **hallucinations** in multiple grounded generation tasks.
- Probes trained on the force-decoded states of synthetic hallucinations outperform contemporary baselines, showing that probing is a feasible and efficient alternative to language model hallucination evaluation when model states are available.
- The work presents a high-quality dataset of over 15k utterances with hallucination annotations for organic and synthetic output texts across three grounded generation tasks.

## Introduction
- The paper explores whether language models can detect hallucinations in their outputs and develops probes for this purpose.
- Previous work focused on creating secondary detection models trained on and applied to surface text, but ignored the information already computed during generation.
- The study aims to explore the degree to which probes on a decoder-only transformer language model can detect hallucinations in various grounded generation tasks.

## Related Work
- The study focuses on hallucinations in the setting of in-context generation where grounding knowledge sources are provided within the prompt.
- Hallucinations are classified as intrinsic, where generated responses directly contradict the knowledge sources, or extrinsic, where generated responses are neither entailed nor contradicted by the sources.
- Prior work uses various metrics and models such as Lexical metrics, NLI approaches, question-answer models, and transformer behavior prediction in small and large language models.

## Grounded Generation Tasks
- The study tests hallucination probes for autoregressive grounded generation in abstractive summarization, knowledge-grounded dialogue generation, and data-to-text.
- It collects hallucinations in two ways: from sampled responses generated from a large language model and by editing reference inputs or outputs to create discrepancies.
- The authors provide full details and examples for each task.

## Probing
- Probes are designed as tools to analyze a neural networkâ€™s internal representations using linear classifiers and attention-pooling probes.
- They are trained to discriminate between different types of inputs or outputs to detect hallucinations in the language model's generated responses.

## Experiments
- Results show that probes trained on organic hallucinations worked best on specific datasets.
- Probes achieve high F1 in the detection of synthetically created hallucinations across all tasks.
- The study demonstrates nuances in the saliency of hallucinatory behavior across model layers, hidden state types, model sizes, hallucination types, and contexts.

## Discussion
- The study points out the efficiency and access limitations of probing and highlights the need for labeled in-domain data for probe training.
- It emphasizes the need for better quality synthetic training data and discusses challenges in annotator disagreements, probe design, ecological validity, and the potential for mitigation of hallucinations in language models.

## Critique
This paper presents valuable insights into the detection of hallucinations in language model outputs. However, the study's generalization to out-of-domain tasks is limited, and the reliance on hidden states may pose challenges if LLMs move behind closed-source APIs. Additionally, the ecological validity of synthetic hallucinations and the annotation guidelines require further refinement to improve accuracy and reproducibility. Further exploration of more advanced probe architectures and mitigation strategies is also warranted for practical application.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-29       |
| Abstract | [http://arxiv.org/abs/2312.17249v1](http://arxiv.org/abs/2312.17249v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.17249v1](https://browse.arxiv.org/html/2312.17249v1)       |
| Truncated       | True       |
| Word Count       | 16039       |