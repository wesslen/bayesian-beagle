
---
title: "[Lions: 1] and [Tigers: 2] and [Bears: 3], Oh My! Literary Coreference Annotation with LLMs"
id: "2401.17922v1"
description: "Seq2seq systems solve coreference challenges in literary text with markdown-like annotations."
author: Rebecca M. M. Hicke, David Mimno
date: "2024-01-31"
image: "../../../bayesian-beagle.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:
The article discusses the use of generative large language models (LLMs) for coreference annotation in literary texts. The authors emphasize the difficulty of coreference annotation in fiction due to the nuanced understanding of text and the need for structured output. They propose the use of new language-model-based seq2seq systems to address these challenges. The study evaluates several trained models for coreference and presents a workflow for training new models. The authors also compare the performance of the LLMs with existing custom-built coreference systems and highlight the potential impact of this process on a broader class of markup.

### Major Findings:
1. Generative large language models (LLMs) have the capacity to solve the challenges of coreference annotation in literary texts by leveraging massive pretraining collections and billions of parameters.
2. The fine-tuned t5-3b model significantly outperforms a state-of-the-art neural model for literary coreference annotation, indicating the potential of LLMs for complex annotation tasks.
3. The study evaluates different sizes of LLMs and finds that larger models, such as t5-3b, achieve higher performance in replicating inputs and identifying complex entities.

### Analysis and Critique:
The article provides valuable insights into the potential of generative LLMs for coreference annotation in literary texts. However, it is important to note that the evaluation metrics used to measure the performance of the LLMs may underestimate their true performance. Additionally, the article acknowledges the limitations of the LLMs in replicating inputs and identifying certain complex entities. Further research is needed to explore the capabilities of LLMs for more complex annotations, such as identifying emotional states or power dynamics between characters. Overall, the study presents a promising approach for coreference annotation in literary texts, but it is essential to consider the limitations and challenges associated with the use of LLMs in this context.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-01       |
| Abstract | [https://arxiv.org/abs/2401.17922v1](https://arxiv.org/abs/2401.17922v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.17922v1](https://browse.arxiv.org/html/2401.17922v1)       |
| Truncated       | False       |
| Word Count       | 4197       |