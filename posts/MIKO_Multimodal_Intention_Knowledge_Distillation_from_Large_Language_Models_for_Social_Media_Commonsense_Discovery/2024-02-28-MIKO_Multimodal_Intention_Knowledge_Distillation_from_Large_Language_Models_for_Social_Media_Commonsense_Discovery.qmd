
---
title: "MIKO: Multimodal Intention Knowledge Distillation from Large Language Models for Social-Media Commonsense Discovery"
id: "2402.18169v1"
description: "TL;DR: Miko framework uses language and image models to uncover social media users' intentions."
author: Feihong Lu, Weiqi Wang, Yangyifei Luo, Ziqin Zhu, Qingyun Sun, Baixuan Xu, Haochen Shi, Shiqi Gao, Qian Li, Yangqiu Song, Jianxin Li
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18169v1/x1.png"
categories: ['social-sciences', 'production', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18169v1/x1.png)

### Summary:
- The article introduces Miko, a framework for extracting social intention knowledge from multimodal social media posts.
- Miko leverages a Large Language Model (LLM) and a Multimodal Large Language Model (MLLM) to interpret images and extract key information from text to generate intentions.
- The framework is evaluated intrinsically and extrinsically, demonstrating its effectiveness in generating high-quality intentions and improving sarcasm detection accuracy.

### Major Findings:
1. Miko uses MLLM to interpret images and LLM to extract key information from text to generate intentions.
2. The framework is capable of generating intentions that are highly plausible and typical to the userâ€™s original post.
3. Incorporating intentions in current methods leads to state-of-the-art performances in sarcasm detection tasks.

### Analysis and Critique:
- The article provides a comprehensive and well-structured framework for extracting social intention knowledge from social media posts.
- The intrinsic and extrinsic evaluations demonstrate the effectiveness of the framework in generating high-quality intentions and improving the accuracy of downstream tasks.
- The framework addresses the challenges of understanding implicit and commonsense intentions in social media posts, providing a valuable contribution to the field.
- The article could benefit from a more detailed discussion of potential limitations or future research directions for the Miko framework.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.18169v1](https://arxiv.org/abs/2402.18169v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18169v1](https://browse.arxiv.org/html/2402.18169v1)       |
| Truncated       | False       |
| Word Count       | 7809       |