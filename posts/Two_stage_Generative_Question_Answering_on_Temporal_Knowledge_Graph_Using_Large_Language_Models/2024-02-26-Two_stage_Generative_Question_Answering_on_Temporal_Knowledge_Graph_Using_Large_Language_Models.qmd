
---
title: "Two-stage Generative Question Answering on Temporal Knowledge Graph Using Large Language Models"
id: "2402.16568v1"
description: "GenTKGQA framework improves temporal knowledge graph question answering, outperforming state-of-the-art baselines, achieving 100% on simple questions."
author: Yifu Gao, Linbo Qiao, Zhigang Kan, Zhihua Wen, Yongquan He, Dongsheng Li
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16568v1/x2.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16568v1/x2.png)

### **Summary:**
- Temporal knowledge graph question answering (TKGQA) is a challenging task due to the temporal constraints hidden in questions and the dynamic structured knowledge sought for answers.
- Large language models (LLMs) have not been extensively explored for TKGQA, prompting the proposal of a novel generative temporal knowledge graph question answering framework, GenTKGQA.
- GenTKGQA guides LLMs to answer temporal questions through two phases: Subgraph Retrieval and Answer Generation.

### **Major Findings:**
1. GenTKGQA outperforms state-of-the-art baselines, achieving 100% on the metrics for the simple question type.
2. The model reduces the subgraph search space in both temporal and structural dimensions, improving reasoning on complex question types through instruction tuning.
3. The proposed virtual knowledge indicators bridge the links between pre-trained graph neural network signals of the temporal subgraph and text representations of the LLMs, enhancing reasoning ability for complex temporal questions.

### **Analysis and Critique:**
- The study demonstrates the effectiveness of GenTKGQA in addressing the TKGQA task, especially for complex question types. However, the limitations of the study include the single-hop inter-entity connections in the dataset, which may not fully represent real-world scenarios. Further exploration on multi-hop complex temporal problems over TKG is warranted. Additionally, the model's performance in generating multiple answers needs improvement to avoid irrelevant responses.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.16568v1](https://arxiv.org/abs/2402.16568v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16568v1](https://browse.arxiv.org/html/2402.16568v1)       |
| Truncated       | False       |
| Word Count       | 7074       |