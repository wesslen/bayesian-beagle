{"id": "2312.12321v1", "text": "# Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n## 1 Introduction\n\n- Autoregressive Large Language Models (LLMs) are widely used but require safety training for human alignment to prevent nefarious uses.\n- It's possible to bypass safety alignment and obtain harmful outputs from open-source LLMs through optimization-free attacks called **priming attacks**.\n\n## 2 Methodology & Results\n\n### A. Few-Shot Priming Attacks\n- Used a non-safety-trained LLM to generate priming attacks for harmful behaviors based on a new prompt format.\n- Showed that priming with slightly more prompt-dependent content can improve the attack success rate by up to 3.3\u00d7 compared to baseline attacks.\n\n### B. Experimental Setup\n- Used the pre-trained Llama-2 model for few-shot prompting with specific prompts and affirmative initial responses.\n- Evaluated the attack success rate using the SOTA response classification tool Llama Guard.\n\n### Results\n- Priming attacks outperformed baselines for all models, indicating the fragility of safety measures.\n- Manual evaluation indicated that Llama Guard might underestimate harmfulness.\n\n## 3 Conclusion\n\n- Highlighted the fragility of current LLM safety measures under increasingly practical assumptions.\n- Suggested the need for further research into novel methods for safer open-sourcing.\n\n## References\n- The paper provides a list of references for further exploration into the topic.\n\nFor more detailed information, refer to the original text.", "meta": {"url": "https://browse.arxiv.org/html/2312.12321v1", "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks", "subtitle": "LLMs vulnerable to simple attacks bypassing safety training, improving harmful behavior detection by up to 3.3x.", "categories": ["security", "open-source"], "publish_date": "2023-12-19", "model": "gpt-3.5-turbo-1106", "word_count": 10319, "is_truncated": false}}
{"id": "2312.02102v2", "text": "# Summary of \"Mitigating Data Injection Attacks on Federated Learning\"\n\n## Introduction\n\nData has become a crucial asset in various industries, emphasizing the need for privacy and security. Federated learning is a popular approach for training machine learning models collaboratively while preserving data privacy. However, federated learning is susceptible to various security threats, including data injection attacks, where malicious participants inject false data into the training process to manipulate the global model.\n\n## Problem Formulation\n\n### Federated Learning\n- Federated learning involves multiple agents refining model parameters using their private data to minimize an objective function using a gradient descent approach.\n- The agents exchange local model parameters with a coordinating node, and the goal is to minimize the objective function using a gradient descent approach.\n\n### Data Injection Attacks\n- Malicious agents inject false data into the training process to manipulate the global model, leading to suboptimal models.\n- Different attack schemes include label-flipping attacks, constant output attacks, and randomized attacks.\n\n## Attacker Detection and Avoidance\n\nThe coordinating agent uses a metric to compare updates received from edge agents and identifies malicious agents. A detection metric is proposed, and a low-complexity metric is computed over time to localize the attacker. If an agent is suspected to be malicious, its updates are ignored for a certain period. The proposed detection method allows for continuous operation, even during the convergence time of the joint model.\n\n## Simulations\n\n### Example 1: Constant-Output Attack\n- The simulation shows the impact of a constant-output attack by a single attacker on various network sizes with and without detection.\n- The proposed detection scheme successfully detects the attacker before it affects the network, allowing convergence of the model to a good model.\n\n### Example 2: Label-Flip Attack\n- The simulation illustrates the impact of a label-flip attack by a single attacker with and without detection.\n- With the proposed detection, the attacker is identified, and the average classification error is mitigated.\n\n## Conclusions\n\nThe paper presents a robust federated learning algorithm that can operate in the presence of data injection attacks. It provides conditions for identifying malicious agents and demonstrates the performance of the proposed technique on various data injection attacks.\n\n---\n\nThe paper addresses the significant challenge of mitigating data injection attacks on federated learning systems. It proposes a novel technique for detecting and mitigating such attacks, showcasing its effectiveness through simulations. The proposed detection and mitigation methods offer a promising approach to safeguard federated learning systems from malicious activities. The paper concludes by emphasizing the robustness and performance of the proposed technique in addressing data injection attacks.", "meta": {"url": "https://browse.arxiv.org/html/2312.02102v2", "title": "Mitigating Data Injection Attacks on Federated Learning", "subtitle": "TL;DR: Proposed method detects and mitigates false data injection attacks in federated learning systems.", "categories": ["security"], "publish_date": "2023-12-04", "model": "gpt-3.5-turbo-1106", "word_count": 13622, "is_truncated": false}}
{"id": "2312.08282v2", "text": "# Prompting LLMs with Content Plans to Enhance the Summarization of Scientific Articles\n\n## Abstract\nThis paper presents **novel prompting techniques** to improve the performance of automatic summarization systems for scientific articles. The authors feed summarizers with lists of key terms extracted from articles, such as author keywords or automatically generated keywords, to guide summarization systems. The techniques are tested with various summarization models and input texts, showing performance gains, especially for smaller models summarizing sections separately.\n\n## Introduction\n- **Automatic text summarization** aims to produce shortened versions of documents while retaining the most relevant information.\n- **Scientific articles are challenging** to summarize due to their length, complexity, and irregular organizational structures.\n\n## Related Work\n- Earlier approaches to automatic summarization relied on **extractive methods**, while current state-of-the-art systems are based on **abstractive summarization models** such as transformer architectures.\n- Prior work includes techniques like **planning with learned entity prompts** and **faceted summarization** to guide summarization systems.\n\n## Methods\n1. **Prompting Technique Dimension**: The paper evaluates five distinct prompting techniques for generating prompts to provide scientific summarizers with useful contextual information.\n2. **Model Dimension**: The authors study the effects of prompting techniques integrated with a range of current state-of-the-art transformer models for scientific summarization.\n3. **Input Text Dimension**: The study explores three main conditions for input texts to understand when contextual information from prompts provides significant gains.\n\n## Results\n- The authors use a dataset of open-access biomedical papers from PubMed Central for training and evaluation.\n- The experiments demonstrate consistent performance improvements from prompting techniques on smaller models, especially when summarizing sections independently.\n- Targeted confusion testing is conducted to isolate the benefits of prompting.\n\n## Discussion\nThe findings suggest that **prompting is an effective approach** to overcoming the limitations of smaller, less capable summarization systems. The study also underscores the potential of prompting as a promising technique to assist smaller models, particularly when computational resources are constrained.\n\n## Conclusion\nThe paper proposes **novel prompting techniques** to provide key term context and enhance scientific literature summarizers, presenting promising results for aiding smaller summarization models, particularly in the context of section-level summarization.\n\nThis summary is authored by Aldan Creo, Manuel Lama, and Juan C. Vidal from the University of Santiago de Compostela.", "meta": {"url": "https://browse.arxiv.org/html/2312.08282v2", "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles", "subtitle": "New prompting techniques improve scientific article summarization, especially for smaller models summarizing sections separately.", "categories": ["prompt engineering"], "publish_date": "2023-12-13", "model": "gpt-3.5-turbo-1106", "word_count": 46040, "is_truncated": true}}
