
---
title: "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers"
id: "2402.19255v1"
description: "LLMs' Math Reasoning Abilities Lack Robustness: Mistakes on Slightly Changed Questions in GSM-Plus Dataset."
author: Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19255v1/x1.png"
categories: ['production', 'education', 'robustness', 'security', 'prompt-engineering', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19255v1/x1.png)

### Summary:

- Large language models (LLMs) have shown impressive performance in mathematical reasoning benchmarks, but there are debates about their understanding and application of mathematical knowledge.
- A new benchmark, GSM-Plus, is introduced to evaluate the robustness of LLMs' math reasoning capability by testing various question variations.
- Experiments on 25 LLMs and 4 prompting techniques show that LLMs' performances are not robust, with mistakes made even on previously solved problems with new statements or altered targets.
- An iterative method generating and verifying intermediate thoughts based on reasoning goals and calculation results is explored for more robust performance.

### Major Findings:
1. **Performance varies among LLMs:** Different LLMs exhibit varying levels of math reasoning abilities, with some struggling with problems that have been solved in GSM8K when new statements are added or question targets are altered.
2. **Limited robustness:** Even when LLMs accurately solve GSM8K questions, they struggle with answering the variations in GSM-Plus, particularly in critical thinking, arithmetic variation, and distractor insertion.
3. **Prompting techniques' lack of robustness:** All investigated prompting techniques show a lack of robustness, especially for arithmetic variation and critical thinking. A compositional prompting method, however, demonstrates good performance on both GSM8K and GSM-Plus.

### Analysis and Critique:

- **Limited scope of evaluation:** The evaluation focuses on LLMs' performance on the GSM8K dataset and its variations, which may not generalize to other mathematical domains or problem types.
- **Annotation difficulties:** Underexplored perturbations, such as arithmetic variation and critical thinking, remain challenging due to annotation difficulties, potentially limiting the comprehensiveness of the evaluation.
- **Reliance on GPT-4:** The dataset construction relies on GPT-4's question-rewriting capabilities, which may not always be reliable, necessitating human revision and potentially introducing biases or errors.
- **Lack of interpretability:** The study does not provide insights into why LLMs struggle with specific question variations or perturbations, leaving open questions about the underlying causes of their limited robustness.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.19255v1](https://arxiv.org/abs/2402.19255v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19255v1](https://browse.arxiv.org/html/2402.19255v1)       |
| Truncated       | False       |
| Word Count       | 2949       |