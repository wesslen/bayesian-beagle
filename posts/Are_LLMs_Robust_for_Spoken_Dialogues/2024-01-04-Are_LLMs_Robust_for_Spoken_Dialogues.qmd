
---
title: "Are LLMs Robust for Spoken Dialogues?"
id: "2401.02297v1"
description: "Large language models perform well in written dialogue tasks but struggle with spoken interactions. Fine-tuning on spoken datasets improves performance."
author: Seyed Mahed Mousavi, Gabriel Roccabruna, Simone Alghisi, Massimo Rizzoli, Mirco Ravanelli, Giuseppe Riccardi
date: "2024-01-04"
image: "https://browse.arxiv.org/html/2401.02297v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.02297v1/x1.png)

### Are LLMs Robust for Spoken Dialogues?

#### Abstract
- Large Pre-Trained Language Models (LLMs) have shown excellent performance in task-oriented dialogues. However, their robustness to spoken interactions is unknown. This study evaluates LLMs' performance for spoken task-oriented dialogues and suggests that fine-tuning such models on a proper dataset of spoken TODs can result in a more robust performance.

#### Introduction
- Large Pre-Trained Language Models (LLMs) have outperformed other data-driven models in open-domain response generation and task-oriented dialogue modeling. However, their robustness to spoken dialogues is unknown due to the lack of proper datasets for spoken TODs.

#### Literature Review
- Various studies have explored the application of LLMs in Dialogue State Tracking and Response Generation, highlighting the importance of fine-tuning on proper datasets for robust performance. However, there is a lack of proper spoken dialogue datasets for evaluating LLMs' robustness to spoken interactions.

#### Approach
- The study transcribed a small number of spoken TODs and studied the transcription errors to simulate the same pattern in a larger dataset. It fine-tuned T5 and GPT-2 models for Dialogue State Tracking and Response Generation using a dataset of written TODs and its noise-injected version.
  
#### Evaluation
- The fine-tuned models' performance was evaluated on spoken test sets, indicating that fine-tuning on noisy TODs can improve the models' performance for spoken dialogues. The study involved both automatic evaluation and human evaluation, with mixed results that suggest the limitations and uninterpretability of automatic metrics.


### Major Takeaways

1. **LLMs' Robustness**: LLMs are not inherently robust to spoken noise, but fine-tuning on noisy TODs can lead to improved performance.
2. **Dataset Importance**: The lack of proper spoken dialogue datasets hinders the evaluation of LLMs' robustness to spoken interactions.
3. **Evaluation Challenges**: Automatic metrics and human evaluations showed mixed results, highlighting the limitations and uninterpretability of automatic metrics.

### Critique
- The study's reliance on automatic transcription and simulated noise may not fully capture the complexities and variations present in actual spoken dialogues.
- Additional human evaluations could provide deeper insights into the models' performance beyond automatic metrics.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-12       |
| Abstract | [http://arxiv.org/abs/2401.02297v1](http://arxiv.org/abs/2401.02297v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.02297v1](https://browse.arxiv.org/html/2401.02297v1)       |
| Truncated       | False       |
| Word Count       | 7839       |