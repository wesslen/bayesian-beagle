
---
title: "BertaQA: How Much Do Language Models Know About Local Culture?"
id: "2406.07302v1"
description: "LLMs struggle with local cultural knowledge but improve with continued pre-training in that language."
author: Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, Mikel Artetxe
date: "2024-06-11"
image: "../../../bayesian-beagle.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The paper introduces BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque, with a local subset containing questions relevant to the Basque culture and a global subset with questions of broader interest. The study aims to evaluate the performance of large language models (LLMs) on topics relevant to other cultures, whose presence on the web is not as prominent as global or anglocentric subjects.

### Major Findings:

1. State-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics.
2. Continued pre-training in Basque significantly improves the modelsâ€™ performance on Basque culture, even when queried in English.
3. This is the first solid evidence of knowledge transfer from a low-resource to a high-resource language.

### Analysis and Critique:

* The study reveals that some prior findings do not fully hold when reassessed on local topics.
* The evaluation of LLMs on global questions alone, as is commonly done, can show a distorted picture, as the trends can be radically different on local questions.
* The results prompt a reconsideration of some prior findings when reevaluated on local subjects, and demonstrate the complex interplay between language, knowledge, and culture.
* The paper does not discuss the potential limitations or biases in the dataset, which could impact the generalizability of the findings.
* The study does not provide a detailed analysis of the specific local cultural knowledge that the LLMs struggle with, which could be useful for future research.
* The paper does not discuss the potential implications of the findings for the development and deployment of LLMs in different cultural contexts.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.07302v1](https://arxiv.org/abs/2406.07302v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07302v1](https://browse.arxiv.org/html/2406.07302v1)       |
| Truncated       | False       |
| Word Count       | 5979       |