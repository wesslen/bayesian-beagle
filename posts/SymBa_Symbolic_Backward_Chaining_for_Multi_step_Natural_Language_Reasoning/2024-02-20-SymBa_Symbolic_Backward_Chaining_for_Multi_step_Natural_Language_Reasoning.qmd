
---
title: "SymBa: Symbolic Backward Chaining for Multi-step Natural Language Reasoning"
id: "2402.12806v1"
description: "TL;DR: Symbolic Backward Chaining improves multi-step reasoning with LLM integration."
author: Jinu Lee, Wonseok Hwang
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.12806v1/extracted/5419471/figures/figure_intro.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12806v1/extracted/5419471/figures/figure_intro.png)

In this markdown summary, we have organized the essential information from the academic article "Symbolic Backward Chaining for Multi-step Natural Language Reasoning" into structured headings and bullet points. We have also included a critical analysis of the article, highlighting potential problems and shortcomings identified while reading the text.

### Summary:
- Large Language Models (LLMs) have demonstrated remarkable reasoning ability in Chain-of-thought prompting, but faithful multi-step reasoning remains a challenge.
- Backward chaining offers unique beneficial properties compared to forward chaining, but current LLM-based implementations have limitations.
- The proposed method, \ours (Symbolic Backward Chaining), integrates a symbolic top-down solver and an LLM for natural language reasoning, achieving significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks.

### Major Findings:
1. Backward chaining offers unique beneficial properties compared to forward chaining.
2. The proposed method, \ours, achieves significant improvement in performance, proof faithfulness, and efficiency in diverse multi-step reasoning benchmarks.
3. Current LLM-based backward chaining implementations have limitations.

### Analysis and Critique:
- The proposed method, \ours, significantly improves performance and efficiency but still holds limitations inherited from LLMs, backward chaining, and symbolic reasoning.
- LLMs often produce counterfactual and inconsistent information, and backward chaining may require substantial computation in fact-driven tasks.
- Some reasoning problems may not be suitable to be formulated in logic programming notations used in this study.

The markdown summary effectively communicates the essential information from the academic article, providing a concise overview of the text and a critical analysis of its content.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.12806v1](https://arxiv.org/abs/2402.12806v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12806v1](https://browse.arxiv.org/html/2402.12806v1)       |
| Truncated       | False       |
| Word Count       | 13311       |