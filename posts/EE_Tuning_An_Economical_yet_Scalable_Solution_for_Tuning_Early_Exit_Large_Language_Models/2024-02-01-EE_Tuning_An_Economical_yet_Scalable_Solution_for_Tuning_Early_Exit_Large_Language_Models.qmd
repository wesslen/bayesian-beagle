
---
title: "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models"
id: "2402.00518v1"
description: "Tuning LLMs with Early-Exit Layers, Efficiently"
author: Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou
date: "2024-02-01"
image: "../../../bayesian-beagle.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**

- EE-Tuning is a method for converting a pre-trained LLM into an early-exit one, requiring minimum computational resources and preserving the full capability of the original LLM.
- EE-Tuning consists of a two-stage procedure: initializing early-exit layers with appropriate parameters and tuning these layers via backpropagation with certain training losses.
- The implementation of EE-Tuning is based on the EE-LLM framework, compatible with 3D parallelism for scalability.

### **Major Findings:**

1. EE-Tuning allows a pre-trained LLM to quickly acquire the ability of early exiting with a limited training budget, achieving 1.2× to 1.6× speedup on various downstream tasks while maintaining comparable or better benchmark scores.
2. The architecture and initialization of early-exit layers impact the training losses and downstream performance. Larger early-exit layers with more trainable parameters or located at deeper positions attain lower training losses, while MLP strikes the best overall balance between speed and quality.
3. Copy initialization leads to faster convergence and lower losses compared to random initialization, while the final inference quality and speedup are similar in both cases.

### **Analysis and Critique:**

- EE-Tuning faces limitations when the expressivity and adaptivity of early exits are constrained by the limited number of trainable parameters. Joint learning of both network backbone and early exits through full-parameter continued pre-training (CPT) or parameter-efficient fine-tuning like LoRA can potentially improve the tuned early-exit model.
- The current study uses the autoregressive language modeling loss on pre-training data as the training objective. Exploring other objectives, such as knowledge distillation or using texts generated by the original LLM as training data, might better inherit the knowledge and abilities of the original LLM.
- The experiments are limited to Llama 2-Chat models and a specific inference mechanism. Further investigation is needed to evaluate the performance of EE-Tuning with other model architectures, hyperparameters, and inference/decoding mechanisms.
- The lack of fine-tuning for alignment in early-exit models might affect their helpfulness and safety

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.00518v1](https://arxiv.org/abs/2402.00518v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00518v1](https://browse.arxiv.org/html/2402.00518v1)       |
| Truncated       | False       |
| Word Count       | 19822       |