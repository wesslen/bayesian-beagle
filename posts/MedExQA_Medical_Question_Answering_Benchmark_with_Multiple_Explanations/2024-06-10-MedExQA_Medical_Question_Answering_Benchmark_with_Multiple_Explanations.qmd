
---
title: "MedExQA: Medical Question Answering Benchmark with Multiple Explanations"
id: "2406.06331v1"
description: "MedExQA benchmark evaluates medical knowledge in LLMs via explanations, highlighting the need for explainability. New medical model, MedPhi-2, outperforms Llama2-based models in generating explanations."
author: Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Honghan Wu
date: "2024-06-10"
image: "https://browse.arxiv.org/html/2406.06331v1/Results/2.99_tSNE_3D_MedExQa_Questions_Answers_Explanations.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.06331v1/Results/2.99_tSNE_3D_MedExQa_Questions_Answers_Explanations.png)

### Summary:

MedExQA is a novel benchmark for medical question-answering that evaluates large language models' (LLMs) understanding of medical knowledge through explanations. The benchmark consists of five distinct medical specialties and incorporates multiple explanations for each question-answer pair. This addresses a major gap in current medical QA benchmarks, which lack comprehensive assessments of LLMs' ability to generate nuanced medical explanations. The paper introduces a new medical model, MedPhi-2, based on Phi-2 (2.7B), which outperformed medical LLMs based on Llama2-70B in generating explanations. The authors will share their benchmark datasets and the trained model.

### Major Findings:

1. MedExQA is a novel benchmark for medical question-answering that includes multiple explanations for each question-answer pair, addressing a major gap in current medical QA benchmarks.
2. The benchmark consists of five distinct medical specialties: biomedical engineering, clinical laboratory science, clinical psychology, occupational therapy, and speech language pathology.
3. The paper introduces a new medical model, MedPhi-2, based on Phi-2 (2.7B), which outperformed medical LLMs based on Llama2-70B in generating explanations.

### Analysis and Critique:

1. The paper highlights the importance of explainability in medical LLMs and proposes an effective methodology for evaluating models beyond classification accuracy.
2. The benchmark datasets and the trained model will be shared, which can facilitate further research in medical large language modeling.
3. The paper does not discuss the potential limitations or biases of the proposed benchmark or the new medical model, MedPhi-2.
4. The paper does not provide a detailed comparison of the performance of MedPhi-2 with other existing medical LLMs.
5. The paper does not discuss the potential applications or implications of the proposed benchmark and the new medical model in real-world medical scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.06331v1](https://arxiv.org/abs/2406.06331v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06331v1](https://browse.arxiv.org/html/2406.06331v1)       |
| Truncated       | False       |
| Word Count       | 7134       |