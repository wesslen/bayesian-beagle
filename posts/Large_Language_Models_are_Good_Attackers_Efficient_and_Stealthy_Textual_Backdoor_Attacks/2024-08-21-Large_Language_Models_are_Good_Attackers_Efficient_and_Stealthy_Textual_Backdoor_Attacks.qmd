
---
title: "Large Language Models are Good Attackers: Efficient and Stealthy Textual Backdoor Attacks"
id: "2408.11587v1"
description: "EST-Bad: Efficient, stealthy textual backdoor attack using LLMs."
author: Ziqiang Li, Yueqi Zeng, Pengfei Xia, Lei Liu, Zhangjie Fu, Bin Li
date: "2024-08-21"
image: "https://browse.arxiv.org/html/2408.11587v1/x1.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.11587v1/x1.png)

### Summary:

The paper introduces the Efficient and Stealthy Textual backdoor attack (EST-Bad) method, which aims to integrate the advantages of insertion-based and paraphrase-based triggers by leveraging Large Language Models (LLMs). EST-Bad encompasses three core strategies: optimizing the inherent flaw of models as the trigger, stealthily injecting triggers with LLMs, and meticulously selecting the most impactful samples for backdoor injection. The method demonstrates competitive attack performance while maintaining superior stealthiness compared to prior methods across various text classifier datasets.

### Major Findings:

1. EST-Bad introduces an optimized approach for identifying effective trigger words, achieving state-of-the-art poisoning effectiveness in textual backdoor attacks.
2. The method showcases how publicly accessible Large Language Models (LLMs) significantly improve the stealthiness of both clean-label and dirty-label backdoor attacks on text classifiers.
3. EST-Bad proposes a straightforward yet highly efficient sample selection strategy for textual backdoor attacks, enhancing attack efficiency and offering compatibility for integration into various other attack methodologies.

### Analysis and Critique:

The paper presents a comprehensive evaluation of EST-Bad, demonstrating its superiority over baseline attacks in terms of both effectiveness and stealthiness. However, the method's reliance on LLMs for trigger injection and sample selection may introduce potential limitations, such as the need for access to powerful language models and the computational resources required for training and inference. Additionally, the method's performance may be affected by the quality and diversity of the available LLMs, as well as the specific characteristics of the target text classifier datasets. Further research is needed to explore the method's generalizability and robustness across a wider range of datasets and model architectures.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.11587v1](https://arxiv.org/abs/2408.11587v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.11587v1](https://browse.arxiv.org/html/2408.11587v1)       |
| Truncated       | False       |
| Word Count       | 10651       |