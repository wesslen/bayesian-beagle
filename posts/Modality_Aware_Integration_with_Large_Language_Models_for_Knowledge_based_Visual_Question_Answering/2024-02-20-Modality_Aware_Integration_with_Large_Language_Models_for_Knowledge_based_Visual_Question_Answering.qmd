
---
title: "Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering"
id: "2402.12728v1"
description: "KVQA challenges addressed with modality-aware integration for image understanding and knowledge reasoning."
author: Junnan Dong, Qinggang Zhang, Huachi Zhou, Daochen Zha, Pai Zheng, Xiao Huang
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.12728v1/x1.png"
categories: ['education', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12728v1/x1.png)

### **Summary:**
- The article presents a novel modality-aware integration with large language models for knowledge-based visual question answering (KVQA).
- It addresses the challenges of leveraging large language models (LLMs) as an implicit knowledge source and aligning multiple knowledge sources for complex scenarios.
- The proposed framework, MAIL, leverages multimodal knowledge for image understanding and knowledge reasoning through a two-stage prompting strategy with LLMs and a tailored pseudo-siamese graph medium fusion.

### **Major Findings:**
1. The proposed MAIL framework outperforms traditional and LLM-enhanced baselines on two benchmark datasets, OK-VQA and FVQA, with 24 less computational resources.
2. The tailored pseudo-siamese graph medium fusion effectively integrates multimodal knowledge sources, balancing intra-modal processing and inter-modal exchange.
3. MAIL achieves faster inferential time compared to existing state-of-the-art baselines, making it resource-efficient.

### **Analysis and Critique:**
- The article effectively addresses the challenges of integrating large language models for knowledge-based visual question answering and presents a novel framework, MAIL, that outperforms existing baselines.
- The proposed framework demonstrates resource efficiency and faster inferential time, making it a promising solution for knowledge-based visual question answering.
- However, the article lacks a detailed discussion of potential limitations or areas for further research, which could provide a more comprehensive analysis of the proposed framework.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.12728v1](https://arxiv.org/abs/2402.12728v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12728v1](https://browse.arxiv.org/html/2402.12728v1)       |
| Truncated       | False       |
| Word Count       | 6813       |