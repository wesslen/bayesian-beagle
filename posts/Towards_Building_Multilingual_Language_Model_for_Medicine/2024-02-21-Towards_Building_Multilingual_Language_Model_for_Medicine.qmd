
---
title: "Towards Building Multilingual Language Model for Medicine"
id: "2402.13963v1"
description: "Developed open-source multilingual medical language model, MMedLM 2, outperforms other models, rivaling GPT-4."
author: Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13963v1/x1.png"
categories: ['production', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13963v1/x1.png)

In this academic article, the authors aim to develop an open-source, multilingual language model for medicine. They present three main contributions: the construction of a new multilingual medical corpus (MMedC), the proposal of a new multilingual medical multi-choice question-answering benchmark with rationale (MMedBench), and the assessment of popular, open-source large language models (LLMs) on their benchmark. The authors find that their final model, MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. They conclude by discussing the impact of multilingual medical LLMs on research and clinical practice, as well as the limitations and future work of their study.

### Summary:
- The authors aim to develop an open-source, multilingual language model for medicine.
- They construct a new multilingual medical corpus (MMedC) and propose a new multilingual medical multi-choice question-answering benchmark with rationale (MMedBench).
- The final model, MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench.

### Major Findings:
1. The authors construct a new multilingual medical corpus (MMedC) and propose a new multilingual medical multi-choice question-answering benchmark with rationale (MMedBench).
2. The final model, MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench.
3. The impact of multilingual medical LLMs on research and clinical practice is discussed.

### Analysis and Critique:
- The study provides valuable insights into the development of multilingual language models for medicine.
- The authors demonstrate the effectiveness of their final model, MMedLM 2, in achieving superior performance compared to other open-source models.
- The limitations of the study include the focus on only six languages and the need for further research to address the hallucination flaw in LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13963v1](https://arxiv.org/abs/2402.13963v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13963v1](https://browse.arxiv.org/html/2402.13963v1)       |
| Truncated       | False       |
| Word Count       | 13002       |