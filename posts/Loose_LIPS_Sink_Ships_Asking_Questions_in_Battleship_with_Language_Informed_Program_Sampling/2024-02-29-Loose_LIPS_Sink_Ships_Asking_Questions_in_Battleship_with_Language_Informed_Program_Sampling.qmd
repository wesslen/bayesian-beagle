
---
title: "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling"
id: "2402.19471v1"
description: "Model generates informative questions in Battleship, mirroring human performance; LLMs struggle with grounding questions in board state."
author: Gabriel Grand, Valerio Pepe, Jacob Andreas, Joshua B. Tenenbaum
date: "2024-02-29"
image: "../../img/2402.19471v1/image_1.png"
categories: ['production', 'education', 'programming', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.19471v1/image_1.png)

### Loose LIPS Sink Ships: A Summary

**Summary:**
- The study explores the process of generating informative questions in the game Battleship using a model called LIPS (Language-informed program sampling).
- LIPS combines a large language model (LLM) with a probabilistic program to generate questions based on the board state and evaluate their expected information gain.
- The authors compare LIPS with other question prior models, including a probabilistic context-free grammar (PCFG) and two LLMs (CodeLlama-7b and GPT-4).
- The results show that LIPS and LLMs approach human mean performance with a small number of samples, while the PCFG requires more samples to match human performance.
- LLMs struggle with grounding, producing many redundant or uninformative questions, especially when using visual prompts.

**Major Findings:**
1. LIPS, a model that combines LLMs with probabilistic programs, generates informative questions based on the board state and their expected information gain.
2. LLMs approach human mean performance with a small number of samples, while the PCFG requires more samples to match human performance.
3. LLMs struggle with grounding, producing many redundant or uninformative questions, especially when using visual prompts.

**Analysis and Critique:**
- The study focuses on the Battleship game, which may limit the generalizability of the findings to other contexts.
- The authors acknowledge the need for a labeled dataset containing many thousands of (s,x) pairs to achieve a good fit with the PCFG as an unconditional prior.
- The authors do not explore the potential impact of different LLM architectures or training procedures on question generation performance.
- The evaluation of different board formats is limited to three types, and the authors do not consider other possible formats or their potential impact on LLM performance.
- The authors do not address the potential impact of individual differences in human question-asking behavior on model performance.

Confidence: 85%

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.19471v1](https://arxiv.org/abs/2402.19471v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19471v1](https://browse.arxiv.org/html/2402.19471v1)       |
| Truncated       | False       |
| Word Count       | 18405       |