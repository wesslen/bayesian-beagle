
---
title: "CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?"
id: "2408.10718v1"
description: "[ABSTRACT] This paper presents a new method for detecting and classifying objects in images using deep learning techniques. The proposed method achieves state-of-the-art performance on several benchmark datasets, demonstrating its effectiveness and efficiency.

[TL;DR] New deep learning method for object detection and classification outperforms existing methods."
author: Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma
date: "2024-08-20"
image: "../../../bayesian-beagle.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The article introduces a novel benchmark, CodeJudge-Eval (CJ-Eval), to evaluate the code understanding abilities of large language models (LLMs) from the perspective of evaluating LLMs as code judges.
- Instead of requiring LLMs to generate code solutions for a given task, they are tasked with acting as code judges, determining whether a provided candidate solution is correct, whether it results in a Wrong Answer, a Time Limit Exceed, and so on.
- The CJ-Eval benchmark is constructed using problems from the APPS test set, with candidate code solutions generated by 16 different LLMs. The benchmark includes 1,860 solution codes and is structured into a multiple-choice format.
- The article evaluates 12 different proprietary and open-source LLMs on the CJ-Eval benchmark, with the results indicating that the benchmark is quite challenging. Proprietary LLMs like GPT-4o and Claude-3.5-Sonnet achieve significantly better performance than open-source models, but still only reach a macro F1 score of 50 on the easiest code judge tasks.

### Major Findings:
1. The CJ-Eval benchmark offers a new perspective for investigating the code understanding ability of both proprietary and open-source LLMs.
2. The benchmark is constructed using problems from the APPS test set, with candidate code solutions generated by 16 different LLMs.
3. The benchmark includes 1,860 solution codes and is structured into a multiple-choice format.
4. The results of the evaluation indicate that the benchmark is quite challenging, with proprietary LLMs like GPT-4o and Claude-3.5-Sonnet achieving significantly better performance than open-source models, but still only reaching a macro F1 score of 50 on the easiest code judge tasks.

### Analysis and Critique:
- The article provides a novel approach to evaluating the code understanding abilities of LLMs, which could be useful for improving the performance of these models in software development tasks.
- However, the article does not provide a detailed analysis of the limitations of the CJ-Eval benchmark, such as the potential for bias in the selection of problems and candidate code solutions, or the impact of

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.10718v1](https://arxiv.org/abs/2408.10718v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.10718v1](https://browse.arxiv.org/html/2408.10718v1)       |
| Truncated       | False       |
| Word Count       | 985       |