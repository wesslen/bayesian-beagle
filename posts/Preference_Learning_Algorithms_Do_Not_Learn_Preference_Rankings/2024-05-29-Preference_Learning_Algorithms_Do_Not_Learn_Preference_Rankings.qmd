
---
title: "Preference Learning Algorithms Do Not Learn Preference Rankings"
id: "2405.19534v1"
description: "Despite using preference learning, LLMs often struggle to rank outputs as humans do. This is due to an alignment gap and the DPO objective's limitations."
author: Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.19534v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19534v1/x1.png)

**Summary:**

Preference learning algorithms, such as RLHF and DPO, are used to align language models with human preferences. However, the inner workings of these algorithms are not well understood. This study investigates the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs. Surprisingly, the authors find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 70% on common preference datasets. The authors also derive the idealized ranking accuracy that a preference-tuned language model would achieve if it optimized the DPO or RLHF objective perfectly. They demonstrate that existing models exhibit a significant alignment gap, i.e., a gap between the observed and idealized ranking accuracies. This discrepancy is attributed to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model.

**Major Findings:**

1. Existing models do not achieve high ranking accuracies, with most achieving a ranking accuracy below 70% across a range of validation splits from commonly used preference datasets.
2. Existing models exhibit a significant alignment gap between the ranking accuracy they achieve and the accuracy achievable under idealized conditions.
3. Preference learning rarely corrects incorrect rankings, with even mild ranking errors in the reference model making it virtually impossible for DPO and its variants to correct the ranking.
4. Ranking accuracy and win rate are closely correlated when the model is close to the reference model, but become anti-correlated once the model has moved too far away.

**Analysis and Critique:**

The study highlights fundamental flaws in RLHF and DPO that prevent the preference-tuned model from achieving a high ranking accuracy even on the training dataset. The authors' findings suggest that the DPO objective is ill-formulated to induce a high ranking accuracy in practice. The study also reveals that the reference model conditions the optimization, whereby using a reference model with moderately incorrect likelihoods assigned to each continuation can effectively prevent DPO from learning the correct ranking. The authors' theoretical results allow for the formal identification of the points that will be hard to flip in their rankings. However, the study is limited by its focus on the optimization-

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19534v1](https://arxiv.org/abs/2405.19534v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19534v1](https://browse.arxiv.org/html/2405.19534v1)       |
| Truncated       | False       |
| Word Count       | 10665       |