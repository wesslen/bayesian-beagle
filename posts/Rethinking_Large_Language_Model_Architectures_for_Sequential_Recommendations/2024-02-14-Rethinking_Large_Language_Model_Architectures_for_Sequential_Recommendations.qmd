
---
title: "Rethinking Large Language Model Architectures for Sequential Recommendations"
id: "2402.09543v1"
description: "LLM-based Lite-LLM4Rec improves sequential recommendation efficiency and performance by 46.8%."
author: Hanbing Wang, Xiaorui Liu, Wenqi Fan, Xiangyu Zhao, Venkataramana Kini, Devendra Yadav, Fei Wang, Zhen Wen, Jiliang Tang, Hui Liu
date: "2024-02-14"
image: "https://browse.arxiv.org/html/2402.09543v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.09543v1/x1.png)

### **Summary:**
- Sequential recommendation has been adapted to the Large Language Model (LLM) paradigm to enjoy the power of LLMs.
- Lite-LLM4Rec is proposed to achieve efficient inference for the sequential recommendation task by streamlining existing LLM-based recommendation models.
- Lite-LLM4Rec introduces a hierarchical LLM structure tailored to efficiently handle the extensive contextual information associated with items, thereby reducing computational overhead while enjoying the capabilities of LLMs.

### **Major Findings:**
1. Lite-LLM4Rec achieves significant performance improvement and inference efficiency over existing LLM-based methods.
2. The beam search decoding process is unnecessary and resource-intensive for sequential recommendations.
3. Lite-LLM4Rec introduces a novel hierarchical LLM structure to efficiently process long context information in LLM-based recommendations.

### **Analysis and Critique:**
- The article effectively demonstrates the effectiveness of Lite-LLM4Rec in improving both performance and inference efficiency over existing LLM-based methods.
- The proposed model addresses the inefficiency of existing LLM-based recommendation algorithms by streamlining the architecture and introducing a hierarchical LLM structure.
- The article provides comprehensive experimental results and ablation studies to support the proposed model's effectiveness and efficiency.
- The limitations of the study are not explicitly addressed, and potential biases or shortcomings in the experimental design are not discussed. Further research on the impact of different backbones for Item LLMs and the influence of the item indexing methods on the training of the model could provide valuable insights.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.09543v1](https://arxiv.org/abs/2402.09543v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09543v1](https://browse.arxiv.org/html/2402.09543v1)       |
| Truncated       | False       |
| Word Count       | 8209       |