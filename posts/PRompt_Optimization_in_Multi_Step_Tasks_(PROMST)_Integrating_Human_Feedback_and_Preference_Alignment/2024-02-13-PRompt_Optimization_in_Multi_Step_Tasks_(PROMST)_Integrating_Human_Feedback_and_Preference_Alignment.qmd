
---
title: "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment"
id: "2402.08702v1"
description: "New LLM-driven prompt optimization framework outperforms human-engineered prompts for multi-step tasks."
author: Yongchao Chen, Jacob Arkin, Yilun Hao, Yang Zhang, Nicholas Roy, Chuchu Fan
date: "2024-02-13"
image: "../../img/2402.08702v1/image_1.png"
categories: ['prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.08702v1/image_1.png)

### Summary:
- The article introduces the PROMST framework for optimizing prompts for large language models (LLMs) in multi-step tasks, integrating human feedback and a score prediction model. It compares PROMST with other methods and evaluates its performance in eight representative multi-step tasks.
- The PromptLLM algorithm iteratively generates new prompts based on feedback and trajectory, using a score model to filter and select top prompts for evaluation. The section also presents experimental environments and results, highlighting the performance of the PROMST framework in optimizing prompts for various multi-step tasks.
- The article provides detailed information on types of human feedback, meta-prompts for LLMs, ablation experiments of score models, prompt score vs. token length and perplexity, component changes in each environment, the influence of score functions, and human prompts and discovered best prompts for GPT-3.5-0613 and GPT-4 in all eight multi-step tasks.

### Major Findings:
1. PROMST outperforms other methods, achieving a significant improvement over current best methods on GPT-3.5 and GPT-4 in multi-step tasks.
2. The PromptLLM algorithm effectively generates and evaluates prompts based on human feedback and trajectory, contributing to the success of the PROMST framework.
3. The detailed analysis of human feedback, score models, and discovered best prompts provides valuable insights for prompt optimization in multi-step tasks.

### Analysis and Critique:
- The article effectively introduces the PROMST framework and its significance in prompt optimization for LLM-driven multi-step tasks, showcasing its potential as a benchmark for automatic prompt optimization.
- The detailed information on human feedback, score models, and discovered best prompts provides valuable insights for prompt optimization in multi-step tasks, contributing to the advancement of research in this domain.
- The guidelines and rules for different multi-step tasks, such as central planner tasks, robot navigation, and logistics management, provide structured approaches for efficient task completion, emphasizing adaptive learning, error management, and strategic decision-making.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.08702v1](https://arxiv.org/abs/2402.08702v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08702v1](https://browse.arxiv.org/html/2402.08702v1)       |
| Truncated       | True       |
| Word Count       | 30906       |