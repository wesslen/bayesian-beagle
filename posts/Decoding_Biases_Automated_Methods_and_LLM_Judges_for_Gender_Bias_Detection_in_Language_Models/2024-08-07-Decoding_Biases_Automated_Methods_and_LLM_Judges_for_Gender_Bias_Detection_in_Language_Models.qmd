
---
title: "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models"
id: "2408.03907v1"
description: "LLMs can generate biased, harmful text; this work trains models to create adversarial prompts and evaluates LLM-based bias metrics, aligning with human judgment."
author: Shachi H Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, Ramesh Manuvinakurike, Nicole Beckage, Hsuan Su, Hung-yi Lee, Lama Nachman
date: "2024-08-07"
image: "https://browse.arxiv.org/html/2408.03907v1/extracted/5779743/figures/bias_detection_fig2.png"
categories: ['social-sciences', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.03907v1/extracted/5779743/figures/bias_detection_fig2.png)

### Summary:

The paper presents a method for detecting and measuring gender bias in Large Language Models (LLMs) using adversarial prompt generation and LLM-as-a-Judge paradigm. The authors define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association.

The authors use a 'Red-teaming language model' to generate a diverse set of adversarial prompts to evaluate the target language model's responses. They also finetune a 4-bit quantized Llama3 model for adversarial prompt generation using Low-Rank Adaptation (LoRA). The generated prompts are then evaluated by the evaluator from different aspects to capture bias, such as a sentiment analyzer, toxicity classifiers, or an LLM used as a judge.

The authors also present the LLM-as-a-Judge paradigm for identifying and measuring bias in responses generated by LLMs. They use GPT-4o to evaluate and score responses generated by target LLMs. The model is prompted to identify bias in an input-response pair in terms of 5 classes and generate a one-line explanation of the classification. The authors also calculate the difference in the LLM-as-a-Judge bias scores for male and female responses, then take the average of these differences to obtain the "LLM-judge Gap Score."

The authors conduct extensive human evaluations and demonstrate that the LLM-as-a-Judge metric most accurately aligns with human annotations for identifying and measuring bias. They focus on identifying gender bias, specifically binary (female/male) gender, however, this method is extensible to other protected attributes such as race, religion, age, and others.

### Major Findings:

1. The authors present a method for detecting and measuring gender bias in LLMs using adversarial prompt generation and LLM-as-a-Judge paradigm.
2. The authors define bias as the semantic variance in text generated by an LLM along with differential treatment towards stereotype identification, refusal, and correction of social group attributes when presented with prompts obtained by varying only the social group association

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.03907v1](https://arxiv.org/abs/2408.03907v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.03907v1](https://browse.arxiv.org/html/2408.03907v1)       |
| Truncated       | False       |
| Word Count       | 5205       |