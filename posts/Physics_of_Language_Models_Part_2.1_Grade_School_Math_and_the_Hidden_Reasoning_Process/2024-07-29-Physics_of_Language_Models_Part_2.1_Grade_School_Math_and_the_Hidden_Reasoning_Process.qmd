
---
title: "Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process"
id: "2407.20311v1"
description: "This paper explores how language models solve math problems, revealing hidden mechanisms and insights into their reasoning capabilities."
author: Tian Ye, Zicheng Xu, Yuanzhi Li, Zeyuan Allen-Zhu
date: "2024-07-29"
image: "../../img/2407.20311v1/image_1.png"
categories: ['hci', 'social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2407.20311v1/image_1.png)

**Summary:**
The paper investigates the performance of language models, specifically GPT2, in solving grade-school math problems. The study uses a synthetic dataset called iGSM to train and test the model, demonstrating that GPT2 can achieve high accuracy and learn to generate shortest solutions. The paper also introduces V-probing, a nearly-linear probing method, to analyze the model's performance on various tasks. The results indicate that the model can solve math problems like humans and even learn beyond human reasoning skills. The study also examines the reasoning mistakes made by the language models, finding that many are systematic and stem from errors in their mental process. The paper concludes by discussing the importance of the depth of the language model for its reasoning ability.

**Major Findings:**

1. GPT2 can achieve high accuracy in solving math problems and learn to generate shortest solutions.
2. The model can learn beyond human reasoning

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.20311v1](https://arxiv.org/abs/2407.20311v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.20311v1](https://browse.arxiv.org/html/2407.20311v1)       |
| Truncated       | True       |
| Word Count       | 38551       |