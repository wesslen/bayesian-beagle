
---
title: "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature"
id: "2408.03936v1"
description: "This study improves Portuguese NLP for specific domains like NCM, using a simplified RAFT technique and TeenyTineLLaMA, outperforming ChatGPT-4."
author: Vinícius Di Oliveira, Yuri Façanha Bezerra, Li Weigang, Pedro Carvalho Brom, Victor Rafael R. Celestino
date: "2024-08-07"
image: "https://browse.arxiv.org/html/2408.03936v1/extracted/5773706/SLIM-RAFT.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.03936v1/extracted/5773706/SLIM-RAFT.png)

# Summary:

- The study introduces the SLIM-RAFT model, a simplified version of the Retrieval-Augmented Fine-Tuning (RAFT) model, to address challenges associated with the Mercosur Common Nomenclature (NCM) code.
- The SLIM-RAFT model uses a significantly smaller language model source, TeenyTineLLaMA, and outperforms ChatGPT-4 in the proposed challenge.
- The study focuses on the NCM code system, which is used by all member countries of Mercosur and is derived from the Harmonized Commodity Description and Coding System (HS).
- The SLIM-RAFT model can be applied to any element description and multi-classification problems.

# Major Findings:

1. The SLIM-RAFT model outperforms ChatGPT-4 in the proposed challenge, achieving a score of 8.67/10 compared to ChatGPT-4's score of 4.5/10.
2. The SLIM-RAFT model uses a significantly smaller language model source, TeenyTineLLaMA, which has only 160 million parameters.
3. The SLIM-RAFT model can be applied to any element description and multi-classification problems.

# Analysis and Critique:

- The study does not provide a detailed comparison of the SLIM-RAFT model with other existing models for NCM code classification.
- The study does not discuss the limitations of the TeenyTineLLaMA model and how they may impact the performance of the SLIM-RAFT model.
- The study does not provide a detailed analysis of the results obtained from the comparative evaluations of the models.
- The study does not discuss the potential applications of the SLIM-RAFT model in other domains or industries.
- The study does not provide a detailed discussion of the methodology used to construct the SLIM-RAFT model.
- The study does not discuss the potential impact of the SLIM-RAFT model on the development of language models for non-English languages.
- The study does not discuss the potential implications of the SLIM-RAFT model for the development of language models for other specific domains.
- The study does not provide a detailed

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.03936v1](https://arxiv.org/abs/2408.03936v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.03936v1](https://browse.arxiv.org/html/2408.03936v1)       |
| Truncated       | False       |
| Word Count       | 8439       |