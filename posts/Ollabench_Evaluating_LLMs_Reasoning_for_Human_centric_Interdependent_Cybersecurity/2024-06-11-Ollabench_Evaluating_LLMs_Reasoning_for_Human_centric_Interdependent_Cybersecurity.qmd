
---
title: "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity"
id: "2406.06863v1"
description: "OllaBench evaluates LLMs for cybersecurity, revealing commercial models lead in accuracy but have room for improvement, while smaller open-weight models show promise."
author: Tam n. Nguyen
date: "2024-06-11"
image: "https://browse.arxiv.org/html/2406.06863v1/extracted/5657620/figures/hic.png"
categories: ['hci', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.06863v1/extracted/5657620/figures/hic.png)

### Summary:

The paper introduces OllaBench, a novel evaluation framework for assessing Large Language Models (LLMs) in the context of human-centric interdependent cybersecurity. OllaBench evaluates LLMs based on their accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions. The framework is built on a foundation of 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers. OllaBench was used to evaluate 21 LLMs, including both open-weight and commercial models from various organizations. The results reveal that while commercial LLMs have the highest overall accuracy scores, there is significant room for improvement. Smaller low-resolution open-weight LLMs are not far behind in performance, and there are significant differences in token efficiency and consistency among the evaluated models.

### Major Findings:

1. Commercial LLMs have the highest overall accuracy scores, but there is still room for improvement.
2. Smaller low-resolution open-weight LLMs are not far behind in performance compared to commercial LLMs.
3. There are significant differences in token efficiency and consistency among the evaluated models.

### Analysis and Critique:

OllaBench provides a valuable tool for researchers and solution developers in the field of human-centric interdependent cybersecurity. However, there are some potential limitations and areas for improvement:

1. The evaluation framework focuses primarily on accuracy, wastefulness, and consistency. While these are important metrics, other aspects such as fairness, transparency, and robustness should also be considered in future iterations.
2. The evaluation is based on a specific set of scenario-based questions. While these questions are designed to cover a wide range of information security compliance and non-compliance scenarios, they may not capture all possible situations that LLMs might encounter in real-world applications.
3. The evaluation does not consider the potential impact of different training data or model architectures on the performance of LLMs. Future work could explore how these factors influence the accuracy, wastefulness, and consistency of LLMs.
4. The evaluation does not account for the potential biases that may be present in the LLMs. Biases in LLMs can have significant implications for their performance and fairness, and should be addressed in future evalu

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.06863v1](https://arxiv.org/abs/2406.06863v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06863v1](https://browse.arxiv.org/html/2406.06863v1)       |
| Truncated       | False       |
| Word Count       | 7305       |