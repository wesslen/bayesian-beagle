
---
title: "Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback"
id: "2405.19686v1"
description: "KGT: A novel, efficient, and interpretable method for real-time personalization of LLMs using knowledge graphs, improving user experience and performance."
author: Jingwei Sun, Zhixu Du, Yiran Chen
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.19686v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19686v1/x1.png)

### Summary:

The paper proposes a novel approach called Knowledge Graph Tuning (KGT) for real-time large language model personalization based on human feedback. KGT leverages knowledge graphs (KGs) to extract personalized factual knowledge triples from users' queries and feedback, optimizing KGs without modifying the LLM parameters. This method improves computational and memory efficiency by avoiding back-propagation and ensures interpretability by making KG adjustments comprehensible to humans. Experiments with state-of-the-art LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves personalization performance while reducing latency and GPU memory costs.

### Major Findings:

1. KGT extracts personalized factual knowledge triples from users' queries and feedback, optimizing KGs without modifying the LLM parameters.
2. KGT improves computational and memory efficiency by avoiding back-propagation and ensures interpretability by making KG adjustments comprehensible to humans.
3. Experiments with state-of-the-art LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves personalization performance while reducing latency and GPU memory costs.

### Analysis and Critique:

1. The paper does not provide a detailed comparison of KGT with other existing methods for real-time large language model personalization.
2. The paper does not discuss the potential limitations of KGT, such as the need for high-quality KGs and the potential for overfitting to user feedback.
3. The paper does not provide a detailed analysis of the impact of KGT on the overall performance of LLMs in various NLP tasks.
4. The paper does not discuss the potential ethical implications of using KGT for personalizing LLMs, such as the risk of reinforcing biases in the training data.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-14       |
| Abstract | [https://arxiv.org/abs/2405.19686v1](https://arxiv.org/abs/2405.19686v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19686v1](https://browse.arxiv.org/html/2405.19686v1)       |
| Truncated       | False       |
| Word Count       | 6292       |