
---
title: "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning"
id: "2402.18865v1"
description: "I-LoRA balances plasticity and stability in continual fine-tuning of LLMs, with up to 11% performance gains."
author: Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, Wei Qin
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.18865v1/x1.png"
categories: ['programming', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18865v1/x1.png)

### Summary

- LLMs suffer from catastrophic forgetting when continuously fine-tuned on complex and diverse domain-specific downstream tasks.
- The paper investigates the geometric connections of different minima through the lens of mode connectivity.
- Experiments reveal the mode connectivity phenomenon in the LLMs continual learning scenario, which can balance plasticity and stability.
- A new method, Interpolation-based LoRA (I-LoRA), is proposed, demonstrating significant improvement over previous state-of-the-art approaches.

### Major Findings
1. **Catastrophic forgetting in LLMs:** Large language models (LLMs) experience a significant decrease in inference performance on historical tasks when continuously fine-tuned on complex and diverse domain-specific downstream tasks.
2. **Mode connectivity in LLMs:** The paper uncovers the mode connectivity phenomenon in the LLMs continual learning scenario, which can strike a balance between plasticity and stability.
3. **Interpolation-based LoRA (I-LoRA):** A new method is proposed, demonstrating significant improvement over previous state-of-the-art approaches in eight domain-specific CL benchmarks.

### Analysis and Critique
- The paper focuses on the geometric connections of different minima through the lens of mode connectivity, which is an interesting and novel approach to addressing catastrophic forgetting.
- The experiments are well-designed and provide strong evidence for the mode connectivity phenomenon in the LLMs continual learning scenario.
- The proposed I-LoRA method significantly improves over previous state-of-the-art approaches in eight domain-specific CL benchmarks.
- The paper could benefit from further exploration of the limitations and potential biases in the proposed method, as well as discussing areas that require further research or clarification.
- The authors might also consider comparing their method with other existing works that explore strategies like memory replay, regularization, and parameter isolation.
- The paper could provide more insights into the real-world applications of the proposed method and its potential impact on the industry.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.18865v1](https://arxiv.org/abs/2402.18865v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18865v1](https://browse.arxiv.org/html/2402.18865v1)       |
| Truncated       | False       |
| Word Count       | 6698       |