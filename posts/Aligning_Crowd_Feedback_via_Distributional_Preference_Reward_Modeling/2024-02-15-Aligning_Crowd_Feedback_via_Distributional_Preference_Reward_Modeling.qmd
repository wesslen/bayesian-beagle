
---
title: "Aligning Crowd Feedback via Distributional Preference Reward Modeling"
id: "2402.09764v1"
description: "TL;DR: DPRM aligns large language models with diverse human preferences using beta distribution and optimal transportation-based loss."
author: Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, Yong Liu
date: "2024-02-15"
image: "../../img/2402.09764v1/image_1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.09764v1/image_1.png)

### Summary:
- The article introduces the Distributional Preference Reward Model (DPRM) as a framework to align large language models with a diverse set of human preferences. It discusses the challenges of conventional reward modeling and proposes a novel approach to capture human preferences using a beta distribution. The section also outlines the three primary steps of the methodology, including collecting a diverse spectrum of human preferences, training the DPRM, and fine-tuning LLMs via PPO.
- The limitations of using binary ranking labels for human preferences are discussed, and the Distributional Preference Ranking Model (DPRM) is introduced to predict the distribution of human preferences for a given prompt-response pair. The DPRM uses optimal transport (OT) loss to capture subtle differences among label categories and is trained using the OT distance as the loss function. The section also presents theoretical insights, future research directions, experimental setups, and main results, demonstrating the effectiveness of the proposed approach.
- After updating the PPO, the well-trained DPRM is used to estimate and compare human preference distribution with and without RL fine-tuning. The results show that DPRMOT outperforms other baselines, receiving the highest percentage of favorable labels from a population of annotators. The proportion of 'not helpful and harmful' responses generated by DPRMOT is much lower than that of baselines. The proposed approaches consistently show improvements in aligning LLMs with a more diverse population preference. The win-rate evaluation results demonstrate that DPRMOT generates responses favored by most percentage of different personas, achieving approximately 70% win ratio across different LLM models.
- The section discusses the optimal transport distance between the original distribution and the smoothed distribution, presenting the formulas for the transport distance for indiscriminate label smoothing and targeted smoothing strategies, as well as the proof of Lemma 3.3 and Theorem 3.5. Additionally, it provides a proof for Proposition 3.4 and introduces a prompt to generate a human preference distribution dataset for reward training.
- The authors describe the process of obtaining the posterior preference distribution by instructing LLMAP I to simulate various personas, each with distinct feedback or preferences for prompt-response pairs. The detailed prompt for obtaining the posterior preference distribution is provided, along with the process of evaluating the quality of AI assistant's responses based on criteria such as "Helpfulness" and "Harmlessness". Additionally, the authors design prompts for LLMAP I to evaluate the quality of responses generated by different LLMs, creating a persona for LLMAP I and requesting the language model to evaluate different responses and select the best one.

### Major Findings:
1. The DPRM framework offers a promising approach to align large language models with a diverse set of human preferences, demonstrating superior performance in generating responses favored by a diverse population.
2. The use of optimal transport (OT) loss and RL fine-tuning shows significant improvements in capturing and predicting human preference distributions, leading to more accurate alignment of LLMs with diverse human perspectives.
3. The article provides a comprehensive methodology for evaluating and improving AI responses based on human preferences, laying the foundation for future research and development in this area.

### Analysis and Critique:
- The article presents a novel approach to addressing the limitations of conventional reward modeling, offering theoretical insights and empirical results to validate the effectiveness of the proposed methods. However, further research is needed to explore the scalability and generalizability of the DPRM framework across different language models and diverse populations.
- The use of optimal transport distance and label smoothing strategies provides a strong theoretical foundation for capturing and predicting human preference distributions. However, potential methodological issues and practical challenges in implementing these approaches should be further investigated to ensure their robustness and applicability in real-world scenarios.
- The methodology for obtaining posterior preference distribution and evaluating AI-generated responses based on specific criteria is well-documented and provides valuable insights into the evaluation process for AI-generated responses. However, the article could benefit from a more in-depth discussion of potential biases and limitations in the evaluation methodology, as well as the implications for real-world applications of AI assistants.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-16       |
| Abstract | [https://arxiv.org/abs/2402.09764v1](https://arxiv.org/abs/2402.09764v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09764v1](https://browse.arxiv.org/html/2402.09764v1)       |
| Truncated       | True       |
| Word Count       | 21359       |