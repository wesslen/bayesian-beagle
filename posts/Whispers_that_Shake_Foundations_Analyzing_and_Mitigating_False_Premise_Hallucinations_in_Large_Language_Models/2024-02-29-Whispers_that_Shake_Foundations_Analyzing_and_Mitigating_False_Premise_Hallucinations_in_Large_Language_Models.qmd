
---
title: "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models"
id: "2402.19103v1"
description: "Method to Reduce False Premise Hallucinations in Large Language Models. TL;DR: FAITH reduces false premise hallucinations in LLMs."
author: Hongbang Yuan, Pengfei Cao, Zhuoran Jin, Yubo Chen, Daojian Zeng, Kang Liu, Jun Zhao
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19103v1/x1.png"
categories: ['robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19103v1/x1.png)

### **Summary:**

- False premise hallucination is a significant issue in Large Language Models (LLMs) where models generate false text based on false premise questions.
- A study analyzes this issue and proposes a method called FAITH (False premise Attention head constraIning for miTigating Hallucinations) to mitigate it.
- FAITH constrains a small subset of attention heads, called false premise heads, that disturb the knowledge extraction process, leading to false premise hallucinations.
- Experiments show that constraining only a few attention heads in the model yields a significant increase in model performance.

### Major Findings:

1. **False Premise Hallucination:** A specific type of hallucination in LLMs where models generate false text based on false premise questions.
2. **False Premise Heads:** A small subset of attention heads in LLMs that

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19103v1](https://arxiv.org/abs/2402.19103v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19103v1](https://browse.arxiv.org/html/2402.19103v1)       |
| Truncated       | False       |
| Word Count       | 5838       |