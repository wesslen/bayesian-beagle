
---
title: "Transformers and Cortical Waves: Encoders for Pulling In Context Across Time"
id: "2401.14267v1"
description: "Transformers like ChatGPT use self-attention to learn long-range temporal dependencies in sequences. Cortical waves may implement similar encoding."
author: Lyle Muller, Patricia S. Churchland, Terrence J. Sejnowski
date: "2024-01-25"
image: "../../../bayesian-beagle.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
The article discusses the computational mechanisms underlying the performance of transformer networks, such as ChatGPT and other Large Language Models (LLMs). It suggests that the self-attention mechanism in transformers enhances temporal context by computing associations between pairs of words in the input sequence. The article also proposes that waves of neural activity in the cortex could implement a similar encoding principle, encapsulating recent input history into a single spatial pattern at each moment in time.

### Major Findings:
1. Transformer networks learn to predict long-range dependencies by concatenating input sequences into a long "encoding vector."
2. The article suggests that a computational role previously identified for cortical waves in sensory cortex may subserve the same underlying computational principle as the transformers' "encoding vector" to provide temporal context.
3. Self-attention in transformers assigns association strengths between pairs of words that can be far apart in a sequence. Self-attention could be implemented in brains by interacting waves in the cortex and basal ganglia over a wide range of time scales.

### Analysis and Critique:
The article provides a comprehensive overview of the computational principles underlying transformer networks and cortical waves. However, it raises questions about how sensory cortex could implement a similar computational principle while processing incoming inputs in real time. The article also discusses the potential limitations of the proposed computational roles for cortical waves and the need for further research to understand the complex interaction between spontaneous and stimulus-evoked waves in the visual cortex during normal perceptual processing in awake animals. Additionally, the article highlights the need for experimental predictions to test the proposed functions of cortical waves in providing temporal context for sequences of sensory inputs.

Overall, the article provides valuable insights into the potential similarities between transformers and waves in the cortex, shedding light on the function of waves in single regions of visual cortex and their computational advantages in predicting upcoming inputs and preparing behavioral responses. However, it also emphasizes the need for further research to fully understand the computational roles of waves in sensory processing.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.14267v1](https://arxiv.org/abs/2401.14267v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.14267v1](https://browse.arxiv.org/html/2401.14267v1)       |
| Truncated       | False       |
| Word Count       | 13118       |