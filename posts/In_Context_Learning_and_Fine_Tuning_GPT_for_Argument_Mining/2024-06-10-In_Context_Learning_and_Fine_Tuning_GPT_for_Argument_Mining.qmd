
---
title: "In-Context Learning and Fine-Tuning GPT for Argument Mining"
id: "2406.06699v1"
description: "GPT-4 and GPT-3.5 excel in Argument Type Classification using In-Context Learning and fine-tuning, respectively."
author: Jérémie Cabessa, Hugo Hernault, Umer Mushtaq
date: "2024-06-10"
image: "https://browse.arxiv.org/html/2406.06699v1/x1.png"
categories: ['prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.06699v1/x1.png)

### Summary:
- The article discusses the use of In-Context Learning (ICL) as a bridging paradigm between training-free and fine-tuning settings for Large Language Models (LLMs).
- The authors introduce an ICL strategy for Argument Type Classification (ATC), a crucial sub-task of Argument Mining (AM), which involves classifying argumentative units in text according to their argumentative roles.
- The ICL strategy combines NN-based examples selection and majority vote ensembling, and experiments with various prompting templates to reveal the proper contribution of different contextual elements.
- The study shows that GPT-4 is able to leverage relevant information from only a few demonstration examples to achieve competitive classification accuracy in the training-free ICL setting.
- In the fine-tuning setting, GPT-3.5 achieves state-of-the-art performance on ATC by incorporating well-crafted structural features given directly in textual form.
- The results emphasize the emergent ability of LLMs to grasp global discursive flow in raw text in both off-the-shelf and fine-tuned setups.

### Major Findings:
1. GPT-4 can leverage relevant information from only a few demonstration examples to achieve competitive classification accuracy in the training-free ICL setting.
2. GPT-3.5 achieves state-of-the-art performance on ATC in the fine-tuning setting by incorporating well-crafted structural features given directly in textual form.
3. The results highlight the emergent ability of LLMs to grasp global discursive flow in raw text in both off-the-shelf and fine-tuned setups.

### Analysis and Critique:
- The study provides a novel ICL strategy for ATC, which combines NN-based examples selection and majority vote ensembling.
- The results demonstrate the potential of LLMs to achieve competitive performance in ATC without requiring fine-tuning.
- However, the study does not provide a detailed comparison of the proposed ICL strategy with other existing methods for ATC.
- The study also does not discuss the limitations of the proposed ICL strategy, such as its dependence on the complexity of the LLM and the need for

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.06699v1](https://arxiv.org/abs/2406.06699v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06699v1](https://browse.arxiv.org/html/2406.06699v1)       |
| Truncated       | False       |
| Word Count       | 2590       |