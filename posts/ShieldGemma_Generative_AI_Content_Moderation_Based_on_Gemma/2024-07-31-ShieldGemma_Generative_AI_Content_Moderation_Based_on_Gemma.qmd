
---
title: "ShieldGemma: Generative AI Content Moderation Based on Gemma"
id: "2407.21772v1"
description: "ShieldGemma: LLM-based safety models outperform existing ones, improving content moderation for developers."
author: Wenjun Zeng, Yuchi Liu, Ryan Mullins, Ludovic Peran, Joe Fernandez, Hamza Harkous, Karthik Narasimhan, Drew Proud, Piyush Kumar, Bhaktipriya Radharapu, Olivia Sturman, Oscar Wahltinez
date: "2024-07-31"
image: "https://browse.arxiv.org/html/2407.21772v1/extracted/5738819/figures/dataCurationPipeline.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.21772v1/extracted/5738819/figures/dataCurationPipeline.png)

### Summary:

- The paper introduces ShieldGemma, a suite of LLM-based safety content moderation models built on Gemma2, which provides robust, state-of-the-art predictions of safety risks across key harm types.
- ShieldGemma outperforms existing models like Llama Guard and WildCard on both public and internal benchmarks.
- The paper also presents a novel LLM-based data curation pipeline, adaptable to various safety-related tasks and beyond, demonstrating strong generalization performance for models trained mainly on synthetic data.
- The authors address the limitations of existing solutions by introducing a spectrum of state-of-the-art content moderation models ranging from 2B to 27B parameters, tailored to accommodate various application requirements.
- The paper also introduces a novel methodology for generating high-quality, adversarial, diverse, and fair datasets, leveraging synthetic data generation techniques to reduce human annotation effort.

### Major Findings:

1. ShieldGemma outperforms existing models like Llama Guard and WildCard on both public and internal benchmarks, with a 10.8% higher average AU-PRC compared to LlamaGuard1 on external benchmarks.
2. The paper introduces a novel LLM-based data curation pipeline, adaptable to various safety-related tasks and beyond, demonstrating strong generalization performance for models trained mainly on synthetic data.
3. The authors address the limitations of existing solutions by introducing a spectrum of state-of-the-art content moderation models ranging from 2B to 27B parameters, tailored to accommodate various application requirements.

### Analysis and Critique:

- The paper provides a valuable resource to the research community, advancing LLM safety and enabling the creation of more effective content moderation solutions for developers.
- However, the paper does not discuss the potential for false positives or negatives in the model's predictions, which could impact the accuracy of content moderation.
- The paper also does not address the potential for the model to be biased towards certain types of content or users, which could impact the fairness of content moderation.
- The paper does not discuss the potential for the model to be used for malicious purposes, such as censorship or

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.21772v1](https://arxiv.org/abs/2407.21772v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.21772v1](https://browse.arxiv.org/html/2407.21772v1)       |
| Truncated       | False       |
| Word Count       | 5579       |