
---
title: "Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting"
id: "2401.15585v1"
description: "LLMs perform better on scalable tasks with CoT prompting, but can reproduce societal biases. CoT reduces bias."
author: Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki, Timothy Baldwin
date: "2024-01-28"
image: "https://browse.arxiv.org/html/2401.15585v1/extracted/5373213/abst.png"
categories: ['robustness', 'prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.15585v1/extracted/5373213/abst.png)

### **Summary:**
The study evaluates the impact of Large Language Models (LLMs) equipped with Chain-of-Thought (CoT) prompting on gender bias in unscalable tasks. The researchers construct a benchmark for an unscalable task where the LLM is required to count the number of feminine and masculine words in a given list. Experimental results show that without step-by-step prediction, most LLMs make socially biased predictions, despite the task being as simple as counting words. Interestingly, CoT prompting reduces this unconscious social bias in LLMs and encourages fair predictions.

### Major Findings:
1. Large language models (LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate incremental predictions even on unscalable tasks.
2. Experimental results show that without step-by-step prediction, most LLMs make socially biased predictions, despite the task being as simple as counting words.
3. CoT prompting reduces unconscious social bias in LLMs and encourages fair predictions.

### Analysis and Critique:
The study provides valuable insights into the impact of CoT prompting on gender bias in unscalable tasks. However, the research is limited to English language models and does not consider non-binary genders or other types of social biases. Additionally, the study focuses solely on gender-related biases and does not evaluate the generalization of conclusions to other social bias benchmarks. Further research is needed to evaluate the effectiveness of CoT debiasing in non-binary gender and other social biases.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2401.15585v1](https://arxiv.org/abs/2401.15585v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.15585v1](https://browse.arxiv.org/html/2401.15585v1)       |
| Truncated       | False       |
| Word Count       | 6157       |