
---
title: "Knowledge Graph Tuning: Real-time Large Language Model Personalization based on Human Feedback"
id: "2405.19686v1"
description: "KGT: A novel, efficient, and interpretable method for real-time LLM personalization using knowledge graphs."
author: Jingwei Sun, Zhixu Du, Yiran Chen
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.19686v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19686v1/x1.png)

### Summary:

The paper proposes a novel approach called Knowledge Graph Tuning (KGT) for real-time large language model personalization based on human feedback. KGT leverages knowledge graphs (KGs) to extract personalized factual knowledge triples from users' queries and feedback, optimizing KGs without modifying the LLM parameters. This approach improves computational and memory efficiency by avoiding back-propagation and ensures interpretability by making KG adjustments comprehensible to humans. Experiments with state-of-the-art LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves personalization performance while reducing latency and GPU memory costs.

### Major Findings:

1. KGT extracts personalized factual knowledge triples from users' queries and feedback, optimizing KGs without modifying the LLM parameters.
2. KGT improves computational and memory efficiency by avoiding back-propagation and ensures interpretability by making KG adjustments comprehensible to humans.
3. Experiments with state-of-the-art LLMs, including GPT-2, Llama2, and Llama3, show that KGT significantly improves personalization performance while reducing latency and GPU memory costs.

### Analysis and Critique:

1. The paper does not provide a detailed comparison of KGT with other existing methods for real-time large language model personalization.
2. The paper does not discuss the potential limitations or challenges of using KGT, such as the quality and completeness of the KGs used for personalization.
3. The paper does not provide a clear explanation of how KGT handles conflicting or inconsistent personalized knowledge triples extracted from users' queries and feedback.
4. The paper does not discuss the potential impact of KGT on the overall performance and accuracy of the LLMs, especially in the long term.
5. The paper does not provide a clear explanation of how KGT can be integrated with existing LLMs and how it can be scaled to handle a large number of users and personalized knowledge triples.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19686v1](https://arxiv.org/abs/2405.19686v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19686v1](https://browse.arxiv.org/html/2405.19686v1)       |
| Truncated       | False       |
| Word Count       | 6292       |