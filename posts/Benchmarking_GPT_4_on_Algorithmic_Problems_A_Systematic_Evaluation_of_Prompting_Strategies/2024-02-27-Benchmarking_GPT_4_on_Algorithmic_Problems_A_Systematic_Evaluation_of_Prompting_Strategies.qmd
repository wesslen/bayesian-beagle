
---
title: "Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies"
id: "2402.17396v1"
description: "LLMs excel in NLP tasks, but lack systematic generalization. GPT-4 outperforms GPT-3.5 and Neural Data Router."
author: Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17396v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17396v1/x1.png)

### **Summary:**
- Large Language Models (LLMs) have revolutionized Natural Language Processing by reusing knowledge acquired from text corpora on various tasks.
- However, LLMs lack systematic generalization, which limits their ability to extrapolate learned statistical regularities outside the training distribution.
- The study systematically benchmarks GPT-4, one of the most advanced LLMs, on three algorithmic tasks with varying complexity.
- The deployment of advanced prompting techniques allows GPT-4 to achieve superior accuracy on all tasks, demonstrating its strong baseline in challenging tasks requiring systematic generalization.

### **Major Findings:**
1. GPT-4 demonstrates superior accuracy on algorithmic tasks with advanced prompting techniques.
2. LLMs lack systematic generalization, but prompting methods can significantly improve their performance.
3. The Neural Data Router, a simpler neural architecture, competes with GPT-3.5 but is outperformed by GPT-4 with prompting techniques.

### **Analysis and Critique:**
- LLMs, including GPT-4, struggle with systematic generalization, limiting their performance on complex algorithmic tasks.
- The study highlights the effectiveness of prompting techniques in improving LLMs' performance, but their limitations for complex symbolic reasoning problems are evident.
- The Neural Data Router competes with GPT-3.5 but is outperformed by GPT-4 with prompting techniques, indicating the dominance of advanced LLMs in solving algorithmic tasks.
- Future research should focus on developing novel prompting techniques and ad-hoc Transformer variants to address the limitations of LLMs in systematic generalization.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.17396v1](https://arxiv.org/abs/2402.17396v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17396v1](https://browse.arxiv.org/html/2402.17396v1)       |
| Truncated       | False       |
| Word Count       | 6473       |