
---
title: "StarCoder 2 and The Stack v2: The Next Generation"
id: "2402.19173v1"
description: "StarCoder2 Outperforms Comparable Code LLMs; Matches or Surpasses Larger Models in Certain Tasks (13 words)"
author: Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19173v1/x1.png"
categories: ['production', 'programming', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19173v1/x1.png)

**Summary:**

- The BigCode project introduces StarCoder2, a Large Language Model for Code (Code LLM), trained on a dataset 4× larger than its predecessor.
- The Stack v2 dataset, built on Software Heritage's source code archive, includes 619 programming languages and high-quality data sources like GitHub pull requests and Kaggle notebooks.
- StarCoder2 models with 3B, 7B, and 15B parameters are trained, with the small model outperforming similar-sized Code LLMs and the large model surpassing competitors of comparable size.

**Major Findings:**

1. **Larger, more diverse training dataset**: The use of a larger and more diverse training dataset led to improved performance in StarCoder2 models compared to their predecessors.
2. **StarCoder2-15B performance**: The StarCoder

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19173v1](https://arxiv.org/abs/2402.19173v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19173v1](https://browse.arxiv.org/html/2402.19173v1)       |
| Truncated       | True       |
| Word Count       | 31564       |