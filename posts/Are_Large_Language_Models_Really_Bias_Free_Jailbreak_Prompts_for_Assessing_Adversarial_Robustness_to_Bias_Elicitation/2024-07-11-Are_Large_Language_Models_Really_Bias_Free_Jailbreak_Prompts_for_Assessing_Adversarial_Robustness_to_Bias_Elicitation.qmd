
---
title: "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation"
id: "2407.08441v1"
description: "LLMs, despite advancements, still exhibit biases from training data, impacting fairness and reliability. Prompt engineering can reveal hidden biases, emphasizing the need for improved mitigation techniques."
author: Riccardo Cantini, Giada Cosenza, Alessio Orsino, Domenico Talia
date: "2024-07-11"
image: "https://browse.arxiv.org/html/2407.08441v1/x1.png"
categories: ['security', 'social-sciences', 'robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.08441v1/x1.png)

### Summary:
- Large Language Models (LLMs) have shown remarkable computational power and linguistic capabilities, but they are prone to various biases stemming from their training data.
- These biases include selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age.
- This study explores the presence of these biases within the responses given by LLMs, analyzing their impact on fairness and reliability.
- The study also investigates how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation.
- Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes.

### Major Findings:
1. LLMs are prone to various biases stemming from their training data, including selection, linguistic, and confirmation biases, along with common stereotypes related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age.
2. Known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation.
3. Extensive experiments using the most widespread LLMs at different scales confirm that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes.

### Analysis and Critique:
- The study provides a comprehensive analysis of the biases present in LLMs and their impact on fairness and reliability.
- The use of jailbreak prompts to test the adversarial robustness of LLMs is a novel approach that can effectively reveal hidden biases.
- However, the study does not provide a clear solution to mitigate these biases, and further research is needed to develop effective bias mitigation techniques.
- The study also does not discuss the potential ethical implications of using LLMs with known biases in real-world applications.
- Additionally, the study does not consider the potential impact of

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.08441v1](https://arxiv.org/abs/2407.08441v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.08441v1](https://browse.arxiv.org/html/2407.08441v1)       |
| Truncated       | False       |
| Word Count       | 5194       |