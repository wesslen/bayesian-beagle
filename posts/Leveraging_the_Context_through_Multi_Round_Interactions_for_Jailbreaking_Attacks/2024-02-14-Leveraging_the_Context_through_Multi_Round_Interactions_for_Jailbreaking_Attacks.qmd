
---
title: "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks"
id: "2402.09177v1"
description: "LLMs vulnerable to Contextual Interaction Attack using prior context to extract harmful information."
author: Yixin Cheng, Markos Georgopoulos, Volkan Cevher, Grigorios G. Chrysos
date: "2024-02-14"
image: "../../img/2402.09177v1/image_1.png"
categories: ['security', 'prompt-engineering', 'robustness', 'architectures', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.09177v1/image_1.png)

### Summary:
- The article discusses the Contextual Interaction Attack, a new form of Jailbreaking attack that leverages the autoregressive nature of Large Language Models (LLMs) to elicit harmful information. It highlights the significance of the context vector in forging potent Jailbreaking attacks and demonstrates the efficacy and transferability of the attack across multiple state-of-the-art LLMs. It also presents an experiment to evaluate the effectiveness of the attack, comparing it with other jailbreaking methods and automated jailbreaking methods, as well as an ablation study to analyze the impact of different factors on the attack's success rate. The article also provides acknowledgments for the support and funding of the research, as well as a list of references for the academic paper. Additionally, it discusses examples of jailbreaking attacks on language models, ethical considerations and challenges of using language models to generate prompts for sensitive topics, and best practices for creating a diverse and inclusive workplace culture.

### Major Findings:
1. The Contextual Interaction Attack leverages the context vector in LLMs to elicit harmful information and demonstrates high success rates and strong transferability properties across different LLMs.
2. The Contextual Interaction Attack is superior to other jailbreaking methods and automated techniques, as demonstrated by comparison experiments and ablation studies.
3. The article provides insights into the potential misuse of language models for generating harmful and unethical content, highlighting the vulnerability of AI models to being manipulated for malicious purposes.

### Analysis and Critique:
- The article provides valuable insights into the development and evaluation of the Contextual Interaction Attack, as well as the ethical considerations and challenges of using language models to generate prompts for sensitive topics. It emphasizes the need for context and semantic consistency in generating successful attacks and the balance between security and the ability to generate relevant text. The selection of auxiliary LLMs has implications for the effectiveness of generating diverse prompts for jailbreaking attacks. However, further research is needed to address potential biases, methodological issues, and the impact of in-context learning and recency bias on LLM security.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.09177v1](https://arxiv.org/abs/2402.09177v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09177v1](https://browse.arxiv.org/html/2402.09177v1)       |
| Truncated       | True       |
| Word Count       | 24565       |