
---
title: "Learning to Plan for Retrieval-Augmented Large Language Models from Knowledge Graphs"
id: "2406.14282v1"
description: "TL;DR: Fine-tuning LLMs with KG-derived data enhances planning, improving complex QA task performance."
author: Junjie Wang, Mingyang Chen, Binbin Hu, Dan Yang, Ziqi Liu, Yue Shen, Peng Wei, Zhiqiang Zhang, Jinjie Gu, Jun Zhou, Jeff Z. Pan, Wen Zhang, Huajun Chen
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.14282v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.14282v1/x1.png)

### Summary:

The paper introduces a novel framework called Learning to Plan from Knowledge Graphs (LPKG) that enhances the planning ability of large language models (LLMs) using data constructed from knowledge graph (KG) patterns. The framework consists of three main steps: (1) constructing planning data from KGs, (2) fine-tuning LLMs based on the planning data, and (3) parsing and executing the plans to obtain the final answers. The authors also develop a comprehensive and challenging evaluation benchmark, CLQA-Wiki, to assess the performance of LLMs on complex question-answering (QA) tasks. The proposed framework outperforms popular baselines on multiple conventional complex QA benchmarks and verifies the effectiveness of KG-sourced planning data.

### Major Findings:

1. The LPKG framework enhances the planning ability of LLMs using data constructed from KG patterns, resulting in better final answers for complex QA tasks.
2. The CLQA-Wiki benchmark is a more comprehensive and challenging evaluation benchmark for complex QA tasks, covering multi-hop, comparison, intersection, and union types of questions.
3. The LPKG framework achieves better results than popular baselines on multiple conventional complex QA benchmarks, demonstrating the effectiveness of KG-sourced planning data.

### Analysis and Critique:

1. The paper presents a novel approach to enhancing the planning ability of LLMs using KG-sourced planning data, which is a significant contribution to the field.
2. The proposed CLQA-Wiki benchmark is a valuable addition to the existing complex QA benchmarks, as it covers a more comprehensive range of question types and allows for multiple correct answers.
3. The paper could benefit from a more detailed analysis of the limitations and potential biases of the proposed framework, as well as a discussion of the methodological issues and conflicting evidence in the field.
4. The paper could also benefit from a more thorough evaluation of the proposed framework on a wider range of complex QA tasks and datasets.
5. The paper could provide more insights into the potential applications and implications of the proposed framework in real-world scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.14282v1](https://arxiv.org/abs/2406.14282v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.14282v1](https://browse.arxiv.org/html/2406.14282v1)       |
| Truncated       | False       |
| Word Count       | 6692       |