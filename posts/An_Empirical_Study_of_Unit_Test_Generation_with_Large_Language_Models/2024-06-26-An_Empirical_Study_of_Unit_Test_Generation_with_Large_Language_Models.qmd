
---
title: "An Empirical Study of Unit Test Generation with Large Language Models"
id: "2406.18181v1"
description: "TL;DR: Study explores open-source LLMs for unit test generation, comparing them to commercial GPT-4 and traditional Evosuite, highlighting prompt factors and limitations."
author: Lin Yang, Chen Yang, Shutao Gao, Weijing Wang, Bo Wang, Qihao Zhu, Xiao Chu, Jianyi Zhou, Guangtai Liang, Qianxiang Wang, Junjie Chen
date: "2024-06-26"
image: "../../../bayesian-beagle.png"
categories: ['education', 'prompt-engineering', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

# Summary:

The paper presents an empirical study on the use of Large Language Models (LLMs) for unit test generation. The study is based on 17 Java projects, five widely-used open-source LLMs with different structures and parameter sizes, and comprehensive evaluation metrics. The findings highlight the significant influence of various prompt factors, show the performance of open-source LLMs compared to the commercial GPT-4 and the traditional Evosuite, and identify limitations in LLM-based unit test generation. The study also derives a series of implications for future research and practical use of LLM-based unit test generation.

# Major Findings:

1. The prompt design, including the description style and selected code features, is crucial to the effectiveness of LLMs in unit test generation. It is recommended to align the description style with the training data and choose code features considering the LLMs’ code comprehension ability and the space left for generating unit tests.
2. The conclusions drawn from open-source LLMs in other tasks do not necessarily generalize to unit test generation, including dominance relationships among studied LLMs. However, all studied LLMs, including the state-of-the-art GPT-4, underperform traditional Evosuite in terms of test coverage. This is primarily due to the large percentage of syntactically invalid unit tests generated by LLMs, a result of LLMs’ hallucination.
3. Directly adapting the Chain-of-Thoughts (CoT) and Retrieval Augmented Generation (RAG) methods for unit test generation does not improve effectiveness and may even reduce it in some cases. CoT is primarily limited by the LLMs’ code comprehension ability, while RAG is constrained by the significant gap between the retrieved unit tests and those that LLMs excel at generating. Special design for the use of ICL methods in unit test generation is required.
4. The defect detection ability of LLM-generated unit tests is limited, primarily due to their low validity. Although valid unit tests are generated by LLMs for many defects, a significant number of defects remain undetected, mainly because the tests fail to produce the specific inputs necessary to trigger these defects. Therefore, designing effective mutation strategies for the inputs within generated unit tests could further improve defect detection effectiveness.



## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.18181v1](https://arxiv.org/abs/2406.18181v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.18181v1](https://browse.arxiv.org/html/2406.18181v1)       |
| Truncated       | False       |
| Word Count       | 13168       |