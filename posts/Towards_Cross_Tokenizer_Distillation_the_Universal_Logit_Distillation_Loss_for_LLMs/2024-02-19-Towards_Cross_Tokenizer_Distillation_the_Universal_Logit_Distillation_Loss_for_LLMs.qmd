
---
title: "Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs"
id: "2402.12030v1"
description: "TL;DR: Universal Logit Distillation compresses knowledge from large language models for wider applicability."
author: Nicolas Boizard, Kevin El-Haddad, CÃ©line Hudelot, Pierre Colombo
date: "2024-02-19"
image: "https://browse.arxiv.org/html/2402.12030v1/extracted/5417308/tokenize-vocabularies-small.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12030v1/extracted/5417308/tokenize-vocabularies-small.png)

### **Summary:**
- Large language models (LLMs) are resource-intensive and impractical for many industrial use cases due to cost, latency, and hardware constraints.
- Knowledge distillation (KD) offers a solution by compressing knowledge from large models to smaller ones, but existing methods based on logits have limitations.
- The Universal Logit Distillation (ULD) loss, grounded in optimal transport, addresses these limitations and enables distillation across models with different architectures and tokenizers.

### Major Findings:
1. ULD loss effectively improves the performance of student models across various tasks and datasets, outperforming teacher-generated text distillation.
2. ULD loss achieves better overall results with only half of the training dataset or student size, while preventing overfitting.
3. ULD loss stabilizes the distillation process over training and mitigates overfitting issues, enabling effective training across multiple epochs.

### Analysis and Critique:
- The ULD loss demonstrates versatility and effectiveness in distilling knowledge from any decoder teacher model into any student model for LLM generative tasks.
- The study is limited to English language models, and the evaluation metrics may not fully capture the quality of generative model predictions.
- The work opens new perspectives for reducing the size, cost, and energy consumption of models at inference time, aligning with the desire for environmental sobriety.
- The study could be extended to non-English languages and beyond mono-task distillation to assess the transferability of generalist assistant abilities from larger to smaller models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.12030v1](https://arxiv.org/abs/2402.12030v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12030v1](https://browse.arxiv.org/html/2402.12030v1)       |
| Truncated       | False       |
| Word Count       | 7813       |