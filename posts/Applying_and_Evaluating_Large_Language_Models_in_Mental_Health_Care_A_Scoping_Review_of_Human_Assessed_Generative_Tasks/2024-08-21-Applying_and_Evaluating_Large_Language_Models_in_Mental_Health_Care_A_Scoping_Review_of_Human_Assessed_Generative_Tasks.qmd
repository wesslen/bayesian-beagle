
---
title: "Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks"
id: "2408.11288v1"
description: "LLMs show promise in mental health care, but require rigorous evaluation and ethical oversight for safe integration into clinical practice."
author: Yining Hua, Hongbin Na, Zehan Li, Fenglin Liu, Xiao Fang, David Clifton, John Torous
date: "2024-08-21"
image: "../../img/2408.11288v1/image_1.png"
categories: ['hci', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2408.11288v1/image_1.png)

**Summary:**

The article "Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks" by Yining Hua et al. presents a scoping review of the current applications of large language models (LLMs) in mental health care. The review focuses on studies where LLMs were tested with human participants in real-world scenarios. The authors identified 17 studies that met their inclusion criteria, covering applications such as clinical assistance, counseling, therapy, and emotional support. However, the evaluation methods used in these studies were often non-standardized, relying on ad-hoc scales that limit comparability and robustness. Privacy, safety, and fairness were also frequently underexplored. The reliance on proprietary models, such as OpenAI's GPT series, raises concerns about transparency and reproducibility. While LLMs show potential in expanding mental health care access, especially in underserved areas, the current evidence does not fully support their use as standalone interventions. More rigorous, standardized evaluations and ethical oversight are needed to ensure these tools can be safely and effectively integrated into clinical practice.

**Major Findings:**

1. Large language models (LLMs) have been applied in various mental health care settings, including clinical assistance, counseling, therapy, and emotional support.
2. The evaluation methods used in these studies are often non-standardized and rely on ad-hoc scales, limiting comparability and robustness.
3. Privacy, safety, and fairness are frequently underexplored in the evaluation of LLMs in mental health care.
4. The reliance on proprietary models, such as OpenAI's GPT series, raises concerns about transparency and reproducibility.
5. More rigorous, standardized evaluations and ethical oversight are needed to ensure LLMs can be safely and effectively integrated into clinical practice.

**Analysis and Critique:**

The article provides a comprehensive review of the current applications of LLMs in mental health care. The authors highlight the potential of LLMs in expanding mental health care access, especially in underserved areas. However, they also identify several limitations and challenges in the current evidence. The non-standardized evaluation methods and reliance on proprietary models limit the comparability

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.11288v1](https://arxiv.org/abs/2408.11288v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.11288v1](https://browse.arxiv.org/html/2408.11288v1)       |
| Truncated       | False       |
| Word Count       | 15816       |