
---
title: "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models"
id: "2408.10947v1"
description: "LLMs, like GPT-4 and Claude2, show potential as educators, excelling in generating relevant, diverse, and consistent educational questions across disciplines."
author: Yuyan Chen, Chenwei Wu, Songzhou Yan, Panjun Liu, Haoyu Zhou, Yanghua Xiao
date: "2024-08-20"
image: "https://browse.arxiv.org/html/2408.10947v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.10947v1/x1.png)

### Summary:

This paper introduces a benchmark, Dr.Academy, to evaluate the questioning capability of large language models (LLMs) in education. The study shifts the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. The benchmark utilizes Anderson and Krathwohl's taxonomy across general, monodisciplinary, and interdisciplinary domains. Four metrics, including relevance, coverage, representativeness, and consistency, are applied to evaluate the educational quality of LLMs' outputs. The results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses, while Claude2 appears more apt as an interdisciplinary teacher. The automatic scores align with human perspectives.

### Major Findings:

1. GPT-4 demonstrates significant potential in teaching general, humanities, and science courses.
2. Claude2 appears more apt as an interdisciplinary teacher.
3. The automatic scores align with human perspectives.

### Analysis and Critique:

While the study provides valuable insights into the potential of LLMs as teaching aids, it primarily focuses on the ability of LLMs to generate questions, which is just one aspect of teaching. Actual teaching involves more complex interactions, including providing feedback, adapting to studentsâ€™ needs, and fostering critical thinking, areas not fully captured by the current benchmark. Additionally, the approach relies heavily on textual content, which may not comprehensively represent the nuances of human teaching methods that include non-verbal cues and personalized interactions. Therefore, while the findings offer valuable insights, they should be viewed as a starting point for more in-depth research into the multifaceted nature of teaching and learning processes.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.10947v1](https://arxiv.org/abs/2408.10947v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.10947v1](https://browse.arxiv.org/html/2408.10947v1)       |
| Truncated       | False       |
| Word Count       | 7147       |