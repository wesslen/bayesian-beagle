
---
title: "Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models"
id: "2402.12276v1"
description: "Scale calibration in ranking systems is crucial for mirroring real-world value and boosting effectiveness. Neural rankers pose challenges."
author: Puxuan Yu, Daniel Cohen, Hemank Lamba, Joel Tetreault, Alex Jaimes
date: "2024-02-19"
image: "https://browse.arxiv.org/html/2402.12276v1/x1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12276v1/x1.png)

### **Summary:**
- The article discusses the process of scale calibration in ranking systems and the challenges of adjusting the scale for neural rankers.
- The study explores the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores.
- By employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty, the study carries out comprehensive tests on two major document ranking datasets.

### **Major Findings:**
1. The study reveals that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers.
2. NLE-based approaches consistently surpass the performance of neural models that process raw text queries and documents, across different training objectives.
3. The aggregate MC method substantially surpasses the use of only the most probable explanation in both ranking and scale calibration, across both literal and conditional explanation setups.

### **Analysis and Critique:**
- The study effectively demonstrates the effectiveness of NLEs generated by LLMs in enhancing the scale calibration of neural rankers, often maintaining or even boosting ranking performance in most scenarios.
- The study highlights the limitations of the expected calibration error (ECE) metric and proposes the adoption of class-balanced ECE (CB-ECE) to address the bias in using ECE for scale calibration data.
- A case study demonstrates the superior effectiveness of the NLE-based methods in assessing the relevance of a query to a specific document, attributing the improvement to the deeper contextual understanding provided by LLM-generated NLEs.
- Future research could focus on increasing the efficiency of NLE generation through techniques like distillation and enhancing the reliability of explanations to develop better calibrated rankers.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.12276v1](https://arxiv.org/abs/2402.12276v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12276v1](https://browse.arxiv.org/html/2402.12276v1)       |
| Truncated       | False       |
| Word Count       | 9445       |