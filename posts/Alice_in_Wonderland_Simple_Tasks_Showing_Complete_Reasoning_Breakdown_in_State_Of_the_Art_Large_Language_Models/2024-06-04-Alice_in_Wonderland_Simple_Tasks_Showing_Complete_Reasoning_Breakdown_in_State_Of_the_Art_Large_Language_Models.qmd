
---
title: "Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models"
id: "2406.02061v1"
description: "LLMs, despite high benchmark scores, fail at basic reasoning tasks and confidently provide nonsensical explanations."
author: Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02061v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02061v1/x1.png)

**Summary:**

The paper presents a study on the reasoning capabilities of state-of-the-art large language models (LLMs) using a simple, short, and conventional common-sense problem formulated in concise natural language. The problem, referred to as the "Alice in Wonderland" (AIW) problem, is easily solvable by humans but poses a significant challenge for LLMs. The study reveals a dramatic breakdown in the reasoning and functional capabilities of LLMs, including those that claim strong function and reasoning capabilities. The models not only fail to provide correct responses but also express strong overconfidence in their wrong solutions and provide nonsensical "reasoning"-like explanations to justify their failed responses. Various interventions, such as enhanced prompting or multi-step re-evaluation, fail to improve the models' performance. The study aims to stimulate an urgent re-assessment of the claimed capabilities of current-generation LLMs and emphasizes the need for common action to create standardized benchmarks that can detect such basic reasoning deficits.

**Major Findings:**

1. State-of-the-art LLMs, including those that claim strong reasoning capabilities, exhibit a dramatic breakdown in their ability to solve a simple, short, and conventional common-sense problem formulated in concise natural language.
2. The models not only fail to provide correct responses but also express strong overconfidence in their wrong solutions and provide nonsensical "reasoning"-like explanations to justify their failed responses.
3. Various interventions, such as enhanced prompting or multi-step re-evaluation, fail to improve the models' performance.

**Analysis and Critique:**

The study raises concerns about the claimed capabilities of current-generation LLMs and highlights the need for a re-assessment of their reasoning abilities. The dramatic breakdown in the models' performance on a simple common-sense problem suggests that current language model benchmarks, especially those aiming to measure reasoning capabilities, do not properly reflect such weaknesses. The study also questions the validity of standardized benchmarks that claim to test reasoning functions, as they may not accurately reflect the models' true reasoning skills. The lack of robustness to problem formulation variations and the inability to revise wrong solutions further emphasize the need for a more comprehensive evaluation of

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02061v1](https://arxiv.org/abs/2406.02061v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02061v1](https://browse.arxiv.org/html/2406.02061v1)       |
| Truncated       | False       |
| Word Count       | 15478       |