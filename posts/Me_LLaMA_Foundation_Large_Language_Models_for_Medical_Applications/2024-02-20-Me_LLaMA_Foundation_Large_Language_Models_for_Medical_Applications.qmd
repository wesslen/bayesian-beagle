
---
title: "Me LLaMA: Foundation Large Language Models for Medical Applications"
id: "2402.12749v1"
description: "Me LLaMA outperforms other medical LLMs in various tasks, making it ideal for medical AI."
author: Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, Huan He, Lucila Ohno-Machido, Yonghui Wu, Hua Xu, Jiang Bian
date: "2024-02-20"
image: "../../img/2402.12749v1/image_1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.12749v1/image_1.png)

### Summary:
The article introduces Me LLaMA, a family of large language models (LLMs) designed for medical applications. The study focuses on the development of Me LLaMA 13B and Me LLaMA 70B, which are foundation models trained on large domain-specific datasets. The models are evaluated using a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. The results show that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 6 out of 8 datasets and GPT-4 in 3 out of 8 datasets. Additionally, the study investigates the catastrophic forgetting problem and shows that Me LLaMA models outperform other medical LLMs in retaining knowledge across updates.

### Major Findings:
1. Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning.
2. Me LLaMA models outperform commercial giants like ChatGPT and GPT-4 in several datasets.
3. Me LLaMA models excel in mitigating the catastrophic forgetting problem, retaining knowledge across updates.

### Analysis and Critique:
- The study provides a comprehensive evaluation of Me LLaMA models, showcasing their superior performance in medical tasks.
- The investigation of the catastrophic forgetting problem highlights the robustness of Me LLaMA models in retaining knowledge.
- The study emphasizes the importance of balanced data sources and the effectiveness of instruction tuning in enhancing model performance.
- The article could benefit from a more detailed discussion of potential limitations and future research directions. Additionally, a critical analysis of the potential biases and ethical considerations of using large language models in medical applications would be valuable.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-21       |
| Abstract | [https://arxiv.org/abs/2402.12749v1](https://arxiv.org/abs/2402.12749v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12749v1](https://browse.arxiv.org/html/2402.12749v1)       |
| Truncated       | False       |
| Word Count       | 14536       |