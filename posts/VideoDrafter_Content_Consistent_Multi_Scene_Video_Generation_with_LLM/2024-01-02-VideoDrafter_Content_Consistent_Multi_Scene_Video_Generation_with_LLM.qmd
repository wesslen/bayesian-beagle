
---
title: "VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM"
id: "2401.01256v1"
description: "VideoDrafter uses language models to create consistent multi-scene videos, outperforming existing models in quality and consistency."
author: Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
date: "2024-01-02"
image: "https://browse.arxiv.org/html/2401.01256v1/x2.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.01256v1/x2.png)

### Major Takeaways

1. **VideoDrafter** is a novel framework for generating content-consistent multi-scene videos, leveraging Large Language Models (LLM) to convert input prompts into comprehensive multi-scene scripts and generating reference images to ensure consistency across scenes.

2. The use of **LLM** allows VideoDrafter to manage logical reasoning between scenes, and the generation of reference images ensures the consistent appearance of entities across a multi-scene video.

3. Extensive experiments show that VideoDrafter outperforms state-of-the-art video generation models in terms of visual quality, content consistency, and user preference.

### VideoDrafter Framework

- **Multi-Scene Video Script Generation**
  - Utilizes LLM to convert input prompts into a comprehensive multi-scene script, including descriptive prompts, foreground and background entities, and camera movement.
  - Identifies common entities across scenes and generates reference images for consistency.

- **Entity Reference Image Generation**
  - Generates reference images for each entity by feeding entity descriptions into a pre-trained Stable Diffusion model.

- **Video Scene Generation**
  - Utilizes two diffusion models, VideoDrafter-Img and VideoDrafter-Vid, to generate multi-scene videos.

### Related Work

- **Diffusion Probabilistic Models (DPM)** have led to significant improvements in generating high-fidelity images, and VideoDrafter extends this progress to multi-scene video generation.

- Previous approaches focused on single-scene videos, making the generation of multi-scene videos an underexplored problem.

### Experiments and Evaluations

- Trained and evaluated on large-scale datasets to demonstrate superior visual quality and content consistency compared to existing models.

- Extensive human evaluation shows the impact of LLM-generated video scripts and entity reference images in improving logical coherence and content consistency.

### Critique

The paper provides a comprehensive overview of the VideoDrafter framework and its performance compared to existing models. However, it would benefit from a more detailed discussion of potential limitations, such as computational efficiency, robustness to noisy or ambiguous prompts, and generalizability to different types of multi-scene videos. Additionally, the paper could address potential ethical considerations related to deepfake technology and the use of large language models for video generation.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-12       |
| Abstract | [http://arxiv.org/abs/2401.01256v1](http://arxiv.org/abs/2401.01256v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.01256v1](https://browse.arxiv.org/html/2401.01256v1)       |
| Truncated       | False       |
| Word Count       | 9002       |