
---
title: "Improving Multilingual Instruction Finetuning via Linguistically Natural and Diverse Datasets"
id: "2407.01853v1"
description: "New method for multilingual IFT datasets improves LLM performance in non-English contexts, boosting summarization by up to 17.57%."
author: Sathish Reddy Indurthi, Wenxuan Zhou, Shamil Chollampatt, Ravi Agrawal, Kaiqiang Song, Lingxiao Zhao, Chenguang Zhu
date: "2024-07-01"
image: "https://browse.arxiv.org/html/2407.01853v1/extracted/5703957/error_translation_example.png"
categories: ['education', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.01853v1/extracted/5703957/error_translation_example.png)

### Summary:

- The paper proposes a novel method for collecting multilingual Instruction Fine-Tuning (IFT) datasets that preserve linguistic naturalness and ensure prompt diversity.
- The approach leverages English-focused LLMs, monolingual corpora, and a scoring function to create high-quality, diversified IFT datasets in multiple languages.
- Experiments demonstrate that LLMs finetuned using these IFT datasets show notable improvements in both generative and discriminative tasks, indicating enhanced language comprehension by LLMs in non-English contexts.

### Major Findings:

1. On the multilingual summarization task, LLMs using the proposed IFT dataset achieved 17.57% and 15.23% improvements over LLMs fine-tuned with translation-based and template-based datasets, respectively.
2. The proposed method for creating IFT datasets addresses the challenges of translation and templated approaches, preserving the nuances of languages, avoiding errors, and creating a diverse set of IFT examples for multiple languages.
3. The method relies on English-focused LLMs to tap into their extensive capabilities and transfer these abilities across diverse linguistic contexts, while utilizing monolingual corpora to capture the unique linguistic and cultural nuances of each language.

### Analysis and Critique:

- The paper effectively addresses the issue of language imbalance in IFT datasets, which has led to suboptimal performance in non-English contexts.
- The proposed method for creating multilingual IFT datasets is a significant improvement over traditional methods, such as translating existing English IFT datasets or templating existing NLP datasets.
- The paper provides a well-structured and coherent summary of the proposed method, along with experimental results demonstrating its effectiveness.
- However, the paper does not discuss potential limitations or shortcomings of the proposed method, such as the potential for errors in the scoring function or the impact of the quality of the monolingual corpora used.
- Additionally, the paper does not provide a detailed comparison of the proposed method with other recent approaches for creating multilingual IFT datasets.
- Future research could address these limitations by conducting a more comprehensive evaluation of the proposed method and comparing it with other state-of-the-art approaches.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.01853v1](https://arxiv.org/abs/2407.01853v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.01853v1](https://browse.arxiv.org/html/2407.01853v1)       |
| Truncated       | False       |
| Word Count       | 6194       |