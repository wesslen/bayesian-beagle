
---
title: "Make Every Move Count: LLM-based High-Quality RTL Code Generation Using MCTS"
id: "2402.03289v1"
description: "New algorithm improves LLM code generation, addressing PPA-unawareness and achieving 31.8% area-delay product improvement."
author: Matthew DeLorenzo, Animesh Basak Chowdhury, Vasudev Gohil, Shailja Thakur, Ramesh Karri, Siddharth Garg, Jeyavijayan Rajendran
date: "2024-02-05"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'production', 'architectures', 'programming']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- Existing large language models (LLMs) for register transfer level code generation face challenges like compilation failures and sub-optimal power, performance, and area (PPA) efficiency.
- The authors present an automated transformer decoding algorithm that integrates Monte Carlo tree-search for lookahead, guiding the transformer to produce compilable, functionally correct, and PPA-optimized code.
- Empirical evaluation with a fine-tuned language model on RTL codesets shows that the proposed technique consistently generates functionally correct code compared to prompting-only methods and effectively addresses the PPA-unawareness drawback of naive large language models.

### Major Findings:
1. The proposed technique consistently generates functionally correct code compared to prompting-only methods.
2. The technique effectively addresses the PPA-unawareness drawback of naive large language models.
3. For the largest design generated by the state-of-the-art LLM (16-bit adder), the technique can achieve a 31.8% improvement in the area-delay product.

### Analysis and Critique:
- The proposed technique shows promise in addressing the limitations of existing LLMs for Verilog code generation.
- However, the time-intensive nature of the approach and the need for fine-tuning the LLM parameters for each new module are potential shortcomings.
- Balancing the trade-off between fine-tuning and the MCTS-based approach in terms of time and resources required for training vs. during inference for each new module is an area for future work.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.03289v1](https://arxiv.org/abs/2402.03289v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.03289v1](https://browse.arxiv.org/html/2402.03289v1)       |
| Truncated       | False       |
| Word Count       | 10707       |