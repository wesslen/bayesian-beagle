
---
title: "Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study"
id: "2401.12789v1"
description: "Non-autoregressive LM-fused ASR system improves speech recognition, achieving up to 10.8% WER improvement. Ablation study explores key parameters' impact."
author: ['W. Ronny Huang', 'Cyril Allauzen', 'Tongzhou Chen', 'Kilol Gupta', 'Ke Hu', 'James Qin', 'Yu Zhang', 'Yongqiang Wang', 'Shuo-Yiin Chang', 'Tara N. Sainath']
date: "2024-01-23"
image: "https://browse.arxiv.org/html/2401.12789v1/x1.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.12789v1/x1.png)

**Summary:**

In the article "Multilingual and Fully Non-Autoregressive ASR with Large Language Model Fusion: A Comprehensive Study," the authors address the latency issues associated with autoregressive nature in decoding within large language model (LLM) assisted automatic speech recognition (ASR) systems. They propose a non-autoregressive LLM-fused ASR system that leverages the Universal Speech Model (USM) and PaLM 2 language model, achieving significant average relative word error rate (WER) improvement of 10.8% on the FLEURS testset and 3.6% on YouTube captioning across all languages. The study also includes a comprehensive ablation study to analyze the impact of LLM size, context length, vocabulary size, and fusion methodology on ASR performance.

### Major Findings:
1. The non-autoregressive LLM-fused ASR system achieved an average relative WER improvement of 10.8% on the FLEURS testset and 3.6% on YouTube captioning across all languages.
2. The study analyzed factors such as LLM size, context length, vocabulary size, and fusion methodology, providing valuable insights into the factors influencing the effectiveness of large-scale LLM-fused speech recognition systems.
3. When examining the impact of LLM size, the study found that larger models can reduce the sensitivity to fusion weight, with optimal LM scoring weight shifting from 0.25 for a 128M LLM to 0.45 for a 340B LLM.

### Analysis and Critique:
The study effectively addresses the issue of latency in large language model assisted ASR systems by proposing a non-autoregressive LLM-fused ASR system. The comprehensive ablation study provides significant insights into the impact of various parameters on ASR efficacy, contributing valuable knowledge to the field of multilingual ASR. However, while the study highlights the improvements in WER, it would benefit from a more detailed discussion on the potential limitations and challenges associated with the proposed non-autoregressive LLM-fused ASR system. Furthermore, a critical analysis of the computational costs and hardware requirements for implementing the proposed system would provide a more holistic view of its practical implications. Additionally, the study could further address the generalizability of the findings to different types of speech data and potential user experience considerations.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [http://arxiv.org/abs/2401.12789v1](http://arxiv.org/abs/2401.12789v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.12789v1](https://browse.arxiv.org/html/2401.12789v1)       |
| Truncated       | False       |
| Word Count       | 3501       |