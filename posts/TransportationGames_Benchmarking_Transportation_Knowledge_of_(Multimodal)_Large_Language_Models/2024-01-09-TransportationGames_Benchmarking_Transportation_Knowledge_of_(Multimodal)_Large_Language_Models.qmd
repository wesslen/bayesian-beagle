
---
title: "TransportationGames: Benchmarking Transportation Knowledge of (Multimodal) Large Language Models"
id: "2401.04471v1"
description: "(TL;DR) Large language models (LLMs) excel in professional domains, but their performance in transportation tasks needs improvement, leading to the proposal of TransportationGames benchmark."
author: ['Xue Zhang', 'Xiangyu Shi', 'Xinyue Lou', 'Rui Qi', 'Yufeng Chen', 'Jinan Xu', 'Wenjuan Han']
date: "2024-01-09"
image: "https://browse.arxiv.org/html/2401.04471v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.04471v1/x1.png)

## Major Takeaways

1. **TransportationGames** is a comprehensive evaluation benchmark designed to assess the capabilities of (M)LLMs in executing transportation-related tasks. It categorizes these tasks into three skill levels based on widely recognized Bloom’s cognitive models: Transportation knowledge memorization, understanding, and applying.

2. Evaluation results show that while some models perform well in certain tasks, there is still much **room for improvement** overall. This suggests that (M)LLMs may not possess reliable transportation knowledge and struggle with transportation-related tasks.

3. The study not only identifies the performance of various (M)LLMs but also analyzes the key factors affecting model performance. It hopes that the release of TransportationGames can serve as a foundation for future research, thereby accelerating the implementation and application of (M)LLMs in the **transportation domain**.

## Introduction

- Large language models (LLMs) and multimodal large language models (MLLMs) have shown exceptional general capabilities and are increasingly being utilized across various professional domains.
- Evaluation benchmarks are crucial for assessing (M)LLMs and gaining insights into their strengths and weaknesses. Domain-specific benchmarks are especially important for driving practical progress and responsible implementation.
- There is a lack of systematic evaluation benchmarks for the transportation domain, prompting the introduction of TransportationGames to assess (M)LLMs in transportation-related tasks.

## Benchmark Construction

- TransportationGames is organized using the first three levels in **Bloom’s Taxonomy** to evaluate (M)LLMs. It includes 10 tasks based on diverse sub-domains in the transportation domain, employing multiple-choice, "True/False" judge, and text generation formats.
- The tasks are categorized into three skill levels: Transportation knowledge memorization, understanding, and applying, to offer a systematic outline of the skillset necessary for transportation-related tasks.

## Experiments

- The evaluation results of LLMs on the text-only dataset of TransportationGames show varying performance across different models. Similarly, MLLMs exhibit differing performance on the multimodal dataset. 

## Analysis

- The study observes that the format error rate of some models is zero, indicating excellent instruction-following ability. There is still much **room for improvement** for some tasks, especially in multimodal scenarios.
- The choice of BaseModel significantly affects model performance, and scaling up the model size can improve performance with similar BaseModels.

## Conclusion

- The release of TransportationGames serves as a foundation for future research and hopes to accelerate the implementation and application of (M)LLMs in the field of transportation.

## Critique

- **Data Leakage:** The study mentions the potential issue of data leakage as the data is collected from the internet. This could impact the fairness of the evaluation.
- **Model and Task Selection:** Due to time constraints, only a small portion of common models were tested. Additionally, the selection of evaluation tasks may not fully represent all aspects of the transportation domain.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [http://arxiv.org/abs/2401.04471v1](http://arxiv.org/abs/2401.04471v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.04471v1](https://browse.arxiv.org/html/2401.04471v1)       |
| Truncated       | False       |
| Word Count       | 7381       |