
---
title: "What's Wrong with Your Code Generated by Large Language Models? An Extensive Study"
id: "2407.06153v1"
description: "LLMs struggle with complex code, often producing shorter, more complicated code. A novel iterative method improves LLM-generated code, boosting passing rate by 29.2%."
author: Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang
date: "2024-07-08"
image: "https://browse.arxiv.org/html/2407.06153v1/x1.png"
categories: ['architectures', 'robustness', 'programming', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.06153v1/x1.png)

### Summary:

This paper presents a comprehensive empirical study on the effectiveness and limitations of code generation using large language models (LLMs). The study evaluates seven widely-used LLMs across three popular benchmarks and reveals that these models struggle to generate accurate code for more complex problems. The authors manually annotate bug types in the generated code, construct a taxonomy of these bugs, analyze their distributions, and summarize 14 findings that lead to the generation of erroneous code.

To evaluate the effectiveness of LLMs in real-world projects, the authors design a rigorous benchmark construction process to minimize data leakage and create a real-world project benchmark called RWPB. Additionally, the paper proposes a novel method that introduces self-critique, allowing LLMs to iteratively critique their generated codes and fix bugs.

### Major Findings:

1. LLMs face challenges in generating successful code for more complex problems and tend to produce code that is shorter yet more complicated compared to canonical solutions.
2. The study develops a taxonomy of bugs for incorrect codes, including three categories and 12 sub-categories, and analyzes the root cause for common bug types.
3. The authors propose a novel training-free iterative method that introduces self-critique, enabling LLMs to critique and correct their generated code based on bug types and compiler feedback.
4. Experimental results demonstrate that the proposed approach can significantly mitigate bugs and increase the passing rate by 29.2% after two iterations, indicating substantial potential for LLMs to handle more complex problems.

### Analysis and Critique:

The paper provides a thorough evaluation of LLMs in code generation and offers valuable insights into their limitations and potential areas for improvement. However, the study could benefit from a more in-depth analysis of the impact of different training methods and hyperparameters on the performance of LLMs in code generation tasks. Additionally, the authors could explore the potential of using more diverse and complex benchmarks to further evaluate the capabilities of LLMs in handling real-world code generation challenges.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.06153v1](https://arxiv.org/abs/2407.06153v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.06153v1](https://browse.arxiv.org/html/2407.06153v1)       |
| Truncated       | False       |
| Word Count       | 14163       |