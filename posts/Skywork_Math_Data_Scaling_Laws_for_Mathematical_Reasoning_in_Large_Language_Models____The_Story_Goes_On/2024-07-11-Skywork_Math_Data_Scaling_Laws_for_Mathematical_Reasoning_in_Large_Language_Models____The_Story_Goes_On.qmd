
---
title: "Skywork-Math: Data Scaling Laws for Mathematical Reasoning in Large Language Models -- The Story Goes On"
id: "2407.08348v1"
description: "TL;DR: Skywork-Math model outperforms early GPT-4 on math tasks, highlighting data scaling's impact on LLMs' math reasoning abilities."
author: Liang Zeng, Liangjun Zhong, Liang Zhao, Tianwen Wei, Liu Yang, Jujie He, Cheng Cheng, Rui Hu, Yang Liu, Shuicheng Yan, Han Fang, Yahui Zhou
date: "2024-07-11"
image: "https://browse.arxiv.org/html/2407.08348v1/x2.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.08348v1/x2.png)

### Summary:

The paper investigates the factors that enhance the mathematical reasoning capabilities of large language models (LLMs). The authors introduce the Skywork-Math model series, which are supervised fine-tuned (SFT) on common 7B LLMs using the proposed 2.5M-instance Skywork-MathQA dataset. The Skywork-Math 7B models achieve impressive accuracies on the competition-level MATH and GSM8K benchmarks, outperforming an early version of GPT-4 on MATH. The superior performance of Skywork-Math models is attributed to their novel two-stage data synthesis and model SFT pipelines, which include three different augmentation methods and a diverse seed problem set. The paper also provides practical takeaways to enhance math reasoning abilities in LLMs for research and industry applications.

### Major Findings:

1. The Skywork-Math model series, supervised fine-tuned on common 7B LLMs using the Skywork-MathQA dataset, achieve impressive accuracies on the MATH and GSM8K benchmarks, outperforming an early version of GPT-4 on MATH.
2. The superior performance of Skywork-Math models is attributed to their novel two-stage data synthesis and model SFT pipelines, which include three different augmentation methods and a diverse seed problem set.
3. The paper provides practical takeaways to enhance math reasoning abilities in LLMs for research and industry applications.

### Analysis and Critique:

The paper presents an interesting approach to enhancing the mathematical reasoning capabilities of LLMs by introducing the Skywork-Math model series and the Skywork-MathQA dataset. The results are impressive, with the Skywork-Math 7B models outperforming an early version of GPT-4 on the MATH benchmark. However, the paper does not provide a detailed comparison with other state-of-the-art models, which would have strengthened the findings. Additionally, the paper does not discuss the limitations or potential biases of the proposed approach, which is important for a comprehensive evaluation. Overall, the paper provides valuable insights into enhancing the mathematical reasoning abilities of LLMs, but further research is needed to fully understand the strengths and weaknesses of the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.08348v1](https://arxiv.org/abs/2407.08348v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.08348v1](https://browse.arxiv.org/html/2407.08348v1)       |
| Truncated       | False       |
| Word Count       | 12177       |