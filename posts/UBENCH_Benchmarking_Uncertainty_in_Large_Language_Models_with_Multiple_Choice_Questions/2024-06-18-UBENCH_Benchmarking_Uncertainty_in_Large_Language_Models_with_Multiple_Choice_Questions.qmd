
---
title: "UBENCH: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions"
id: "2406.12784v1"
description: "UBench is a new benchmark for evaluating LLM reliability, offering improved performance and resource efficiency. It finds GLM4 and GPT-4 as the most reliable LLMs."
author: Xunzhi Wang, Zhuowei Zhang, Qiongyu Li, Gaonan Chen, Mengting Hu, Zhiyu li, Bitong Luo, Hang Gao, Zhixin Han, Haotian Wang
date: "2024-06-18"
image: "https://browse.arxiv.org/html/2406.12784v1/x2.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.12784v1/x2.png)

# Summary:

**Summary:**
The paper introduces UBench, a new benchmark for evaluating the reliability of large language models (LLMs) using multiple-choice questions. UBench consists of 3,978 questions covering knowledge, language, understanding, and reasoning abilities. The proposed method outperforms other state-of-the-art uncertainty estimation methods while significantly reducing computational resources. The authors evaluate the reliability of 15 popular LLMs using UBench, finding GLM4 to be the most outstanding, followed by GPT-4. The paper also explores the impact of Chain-of-Thought prompts, role-playing prompts, option order, and temperature on LLM reliability.

**Major Findings:**
1. UBench achieves state-of-the-art performance in evaluating LLM reliability, with a single-sampling method that significantly saves computational resources compared to baseline methods.
2. GLM4 is the most reliable LLM, followed by GPT-4, based on UBench evaluations.
3. Chain-of-Thought prompts, role-playing prompts, option order, and temperature have varying effects on different LLMs, with some methods improving reliability while others decrease it.

**Analysis and Critique:**
- The paper provides a comprehensive evaluation of LLM reliability using UBench, which covers a wide range of abilities and tasks.
- The authors' findings on the varying effects of different methods on LLM reliability highlight the need for further research to understand the underlying mechanisms and develop more effective techniques.
- The paper does not discuss the limitations of UBench or potential biases in the evaluation process, which could be addressed in future work.
- The paper focuses on the reliability of LLMs, but other aspects of model performance, such as accuracy and fairness, are also important and should be considered in future evaluations.
- The paper does not provide a detailed comparison of UBench with other benchmarks, which could help to better understand its strengths and weaknesses.
- The paper does not discuss the potential applications of UBench in real-world scenarios, which could help to demonstrate its practical value.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.12784v1](https://arxiv.org/abs/2406.12784v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.12784v1](https://browse.arxiv.org/html/2406.12784v1)       |
| Truncated       | False       |
| Word Count       | 7284       |