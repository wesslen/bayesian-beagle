
---
title: "The Battle of LLMs: A Comparative Study in Conversational QA Tasks"
id: "2405.18344v1"
description: "TL;DR: This study compares and evaluates ChatGPT, GPT-4, Gemini, Mixtral, and Claude, highlighting their strengths, weaknesses, and potential for improvement."
author: Aryan Rangapur, Aman Rangapur
date: "2024-05-28"
image: "https://browse.arxiv.org/html/2405.18344v1/extracted/5627108/Figures/CoQAsunburst2.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.18344v1/extracted/5627108/Figures/CoQAsunburst2.png)

### Summary:

This research paper aims to explore the performance of ChatGPT, Gemini, Mixtral, and Claude in various domains, focusing on their potential use in conversational QA tasks. The study analyzes the accuracy and consistency of the model's responses to different datasets and investigates areas where the models may be prone to error. The research employs a pipeline that generates large-scale responses and compares the model's responses to existing QA corpora, calculating various scores like BLEU, ROUGE, etc. The results show that these language models can generate high-quality responses, but responses can be generic and irrelevant, reducing their usefulness for practical applications. The study also highlights the superior performance of GPT-4 and Claude in generating coherent and contextually relevant responses, making them promising candidates for conversational QA tasks.

### Major Findings:

1. ChatGPT, Gemini, Mixtral, and Claude can generate high-quality responses, with an average BLEU score of 0.79 and an average ROUGE-L score of 0.53.
2. GPT-4 and Claude outperform ChatGPT-3, Gemini, and Mixtral in terms of accuracy, relevance, and consistency, demonstrating significant improvements in generating coherent and contextually relevant responses.
3. GPT-4 and Claude also outperform ChatGPT-3, Gemini, and Mixtral in Chain of Thought evaluations, Zero Shot, and 3-shot learning scenarios, showcasing their superior performance across various scenarios.

### Analysis and Critique:

1. The study focuses on evaluating the performance of language models in conversational QA tasks, but it does not address other potential applications of these models, such as language translation, text summarization, or creative content generation.
2. The study does not provide a detailed comparison of the computational resources required to run these models, which could be an essential factor for practical applications.
3. The study does not discuss the ethical implications of using these models, such as potential biases or privacy concerns.
4. The study does not explore the potential of integrating these models with other AI technologies, such as computer vision or speech recognition, to create more advanced conversational agents.
5. The study does

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.18344v1](https://arxiv.org/abs/2405.18344v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.18344v1](https://browse.arxiv.org/html/2405.18344v1)       |
| Truncated       | False       |
| Word Count       | 4766       |