
---
title: "Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation"
id: "2402.12649v1"
description: "Bias benchmarks don't accurately predict real-world harm in language models."
author: Kristian Lum, Jacy Reese Anthis, Chirag Nagpal, Alexander D'Amour
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.12649v1/extracted/5418948/final_results_combined.png"
categories: ['robustness', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12649v1/extracted/5418948/final_results_combined.png)

### **Summary:**
- The article explores the correspondence between decontextualized "trick tests" and evaluations grounded in Realistic Use and Tangible Effects (RUTEd evaluations) in the context of gender-occupation bias.
- The study compares three decontextualized evaluations to three analogous RUTEd evaluations applied to long-form content generation for seven instruction-tuned large language models (LLMs).
- The findings reveal no correspondence between trick tests and RUTEd evaluations, suggesting that evaluations not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.

### **Major Findings:**
1. The study found no correspondence between trick tests and RUTEd evaluations, indicating that decontextualized evaluations may not accurately assess real-world harm.
2. Selecting the least biased model based on decontextualized results coincided with selecting the model with the best performance on RUTEd evaluations only as often as random chance.
3. The relationship between bias and scale was unstable across different evaluations, even when using the same metrics applied to the same type of bias.

### **Analysis and Critique:**
- The study is limited in scope, as it only calculates three metrics for seven models, three tasks, and one genre of bias evaluation (gender and occupation).
- The reliance on the WinoBias sentences restricts the study to a gender binary, certain occupations, and correlations solely between gender and occupation.
- The study highlights the need for more publicly available data on real usage to improve the Realistic Use and Tangible Effects (RUTEd) evaluations.
- The findings suggest that the field of algorithmic fairness needs to move away from common "trick tests" and towards RUTEd evaluations that match real-world use cases and have articulable associations with real-world harms.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.12649v1](https://arxiv.org/abs/2402.12649v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12649v1](https://browse.arxiv.org/html/2402.12649v1)       |
| Truncated       | False       |
| Word Count       | 6909       |