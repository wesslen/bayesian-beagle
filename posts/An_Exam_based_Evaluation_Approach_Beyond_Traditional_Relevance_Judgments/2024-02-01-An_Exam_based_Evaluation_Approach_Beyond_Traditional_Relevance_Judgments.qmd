
---
title: "An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments"
id: "2402.00309v1"
description: "IR evaluation based on answering key questions, not relevance judgments. New metric for evaluation."
author: Naghmeh Farzi, Laura Dietz
date: "2024-02-01"
image: "https://browse.arxiv.org/html/2402.00309v1/x1.png"
categories: ['prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.00309v1/x1.png)

### **Summary:**
- The article proposes an alternative paradigm for information retrieval (IR) evaluation that does not rely on traditional relevance judgments.
- Instead of relevance judgments, the proposed approach defines a text as relevant if it contains information that enables the answering of key questions.
- The proposed EXAM Answerability Metric is designed to evaluate IR systems based on their ability to provide topically relevant information.

### **Major Findings:**
1. Large Language Models (LLMs) are used to generate or retrieve responses for search queries, and there is a need for convincing evaluation metrics to assess the accuracy and completeness of the information content in responses.
2. The proposed EXAM Answerability Metric involves the development of an exam question bank and the grading of system responses using an LLM-based question answering system.
3. The proposed evaluation approach allows for the expansion of the exam question set post-hoc and facilitates the ongoing evaluation of future information systems.

### **Analysis and Critique:**
- The proposed approach offers a novel way to evaluate IR systems without relying on traditional relevance judgments, which can be beneficial in reducing assessment costs and obtaining reusable test collections.
- The use of LLMs for question answering and self-rating presents a modernized approach to evaluation, but the article acknowledges the need for further research to improve question bank generation and study the effects of this approach on the quality and cost of human judges.
- The article provides a detailed experimental evaluation of the proposed approach using TREC datasets, demonstrating strong correlations with official leaderboards and highlighting the potential of the EXAM evaluation metric for future IR evaluation tracks. However, the article also acknowledges the need for human-in-the-loop refinement of the exam question banks and the importance of further research in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.00309v1](https://arxiv.org/abs/2402.00309v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00309v1](https://browse.arxiv.org/html/2402.00309v1)       |
| Truncated       | False       |
| Word Count       | 7831       |