
---
title: "Learning to Poison Large Language Models During Instruction Tuning"
id: "2402.13459v1"
description: "LLMs vulnerable to data poisoning attacks, new approach for trigger learning, high success rate."
author: Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13459v1/extracted/5421549/example.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13459v1/extracted/5421549/example.png)

### **Summary:**
- Large Language Models (LLMs) are vulnerable to data poisoning attacks during the instruction tuning process, where backdoor triggers are inserted into the training data to manipulate model outputs for malicious purposes.
- A novel gradient-guided backdoor trigger learning approach is proposed to efficiently identify adversarial triggers, ensuring evasion of detection by conventional defenses while maintaining content integrity.
- Experimental validation across various LLMs and tasks demonstrates a high success rate in compromising model outputs, with only 1% of instruction tuning samples leading to a Performance Drop Rate (PDR) of around 80%.

### **Major Findings:**
1. LLMs are susceptible to data poisoning attacks during instruction tuning, compromising their integrity and functionality.
2. The proposed gradient-guided backdoor trigger learning approach efficiently identifies adversarial triggers, ensuring evasion of detection by conventional defenses while maintaining content integrity.
3. Experimental validation demonstrates a high success rate in compromising model outputs, with only 1% of instruction tuning samples leading to a Performance Drop Rate (PDR) of around 80%.

### **Analysis and Critique:**
- The study highlights the need for stronger defenses against data poisoning attacks, offering insights into safeguarding LLMs against these more sophisticated attacks.
- The proposed attack method demonstrates advanced properties, including transferability across different datasets and models, imperceptibility, and maintenance of semantic integrity and coherence of the original content.
- The study focuses on sentiment analysis and multi-class domain classification tasks, and further research is warranted to extend the approach to a wide range of downstream tasks and LLMs.
- The findings can be utilized to enhance the resilience of LLMs to such threats and guard against malicious uses.

The article provides valuable insights into the vulnerabilities of LLMs to data poisoning attacks during instruction tuning and proposes a novel approach to identify adversarial triggers. However, further research is needed to generalize the findings to a wider range of tasks and LLMs. Additionally, the study's focus on safeguarding LLMs against sophisticated attacks is crucial for ensuring the reliability and security of these models in language-based tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.13459v1](https://arxiv.org/abs/2402.13459v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13459v1](https://browse.arxiv.org/html/2402.13459v1)       |
| Truncated       | False       |
| Word Count       | 6272       |