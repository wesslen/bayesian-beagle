
---
title: "Open Generative Large Language Models for Galician"
id: "2406.13893v1"
description: "[TEXT] This study examines the relationship between social media use and mental health in adolescents. Results indicate a significant correlation between excessive social media use and increased symptoms of anxiety and depression.

[TL;DR] Excessive social media use linked to anxiety and depression in teens."
author: Pablo Gamallo, Pablo Rodríguez, Iria de-Dios-Flores, Susana Sotelo, Silvia Paniagua, Daniel Bardanca, José Ramom Pichel, Marcos Garcia
date: "2024-06-19"
image: "https://browse.arxiv.org/html/2406.13893v1/extracted/5679601/plot.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.13893v1/extracted/5679601/plot.png)

### Summary:

This article presents the creation of the first generative large language models (LLMs) for the Galician language, a Romance language spoken primarily in the autonomous community of Galicia. The models were developed using a strategy of continual pretraining, which involves leveraging the existing knowledge encapsulated within a fully-trained LLM and adjusting only the weights of the embedding layer. The Galician models were created by adapting trilingual LLMs of Catalan, Spanish, and English, which were, in turn, the result of another adaptation of foundational models with a majority presence of English. The two Galician LLMs were evaluated in two ways: a systematic qualitative human evaluation and a quantitative automatic evaluation on several tasks using common benchmarking datasets translated into Galician. The results of the evaluations indicate that the models are capable of generating high-quality and semantically coherent text in Galician, but that automatic evaluation through few-shot learning based on specific instruction tasks may not be suitable for architectural models with approximately 1 billion parameters that have not been instructed for those tasks.

### Major Findings:

1. The first generative LLMs for the Galician language were developed using a strategy of continual pretraining, which involves leveraging the existing knowledge encapsulated within a fully-trained LLM and adjusting only the weights of the embedding layer.
2. The Galician models were created by adapting trilingual LLMs of Catalan, Spanish, and English, which were, in turn, the result of another adaptation of foundational models with a majority presence of English.
3. The two Galician LLMs were evaluated in two ways: a systematic qualitative human evaluation and a quantitative automatic evaluation on several tasks using common benchmarking datasets translated into Galician.
4. The results of the evaluations indicate that the models are capable of generating high-quality and semantically coherent text in Galician, but that automatic evaluation through few-shot learning based on specific instruction tasks may not be suitable for architectural models with approximately 1 billion parameters that have not been instructed for those tasks.

### Analysis and Critique:

* The article does not provide a clear methodology for building a LLM adapted to a particular language, as each project works with different architectures, different base models, and a

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.13893v1](https://arxiv.org/abs/2406.13893v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.13893v1](https://browse.arxiv.org/html/2406.13893v1)       |
| Truncated       | False       |
| Word Count       | 6815       |