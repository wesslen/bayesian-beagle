
---
title: "GKT: A Novel Guidance-Based Knowledge Transfer Framework For Efficient Cloud-edge Collaboration LLM Deployment"
id: "2405.19635v1"
description: "GKT framework uses a teacher LLM to guide a student model, achieving 95% of ChatGPT's performance at 52% of the cost, with no fine-tuning required."
author: Yao Yao, Zuchao Li, Hai Zhao
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.19635v1/x1.png"
categories: ['prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19635v1/x1.png)

### Summary:

- The paper introduces a novel and intuitive Guidance-based Knowledge Transfer (GKT) framework to address the challenges of large language models (LLMs) inference, such as slow inference times and high resource demands.
- GKT leverages a larger LLM as a "teacher" to create guidance prompts, paired with a smaller "student" model to finalize responses, requiring no fine-tuning and allowing for extensive batch generation.
- The framework can be seamlessly integrated into cloud-edge collaboration architectures and is versatile enough for plug-and-play application across various models.
- When utilizing ChatGPT as the teacher model and Llama2-70B as the student model, GKT can achieve 95.00% of ChatGPTâ€™s performance at 52% of the cost.

### Major Findings:

1. GKT significantly enhances accuracy and processing speed on the GSM8K and CSQA datasets, surpassing the performance of using either the student or teacher models in isolation.
2. GKT reduces the burden of LLM inference, speeding up response generation, and does not require generating a dataset from the teacher model or fine-tuning the student model.
3. GKT can be seamlessly integrated into cloud-edge collaboration architectures, allowing for brief guidance prompts to be easily transmitted to mobile devices and significantly reducing data transmission costs.

### Analysis and Critique:

- The paper does not provide a detailed comparison of GKT with other knowledge transfer frameworks, such as knowledge distillation or speculative decoding, in terms of computational efficiency and resource utilization.
- The paper does not discuss the potential limitations of GKT, such as the need for a large teacher model to generate guidance prompts during inference, which may impact the overall efficiency of the framework.
- The paper does not explore the potential impact of different teacher and student model combinations on the performance of GKT, which could provide valuable insights for optimizing the framework.
- The paper does not provide a detailed analysis of the trade-offs between accuracy and computational efficiency in GKT, which could be important for practical applications of the framework.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19635v1](https://arxiv.org/abs/2405.19635v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19635v1](https://browse.arxiv.org/html/2405.19635v1)       |
| Truncated       | False       |
| Word Count       | 6248       |