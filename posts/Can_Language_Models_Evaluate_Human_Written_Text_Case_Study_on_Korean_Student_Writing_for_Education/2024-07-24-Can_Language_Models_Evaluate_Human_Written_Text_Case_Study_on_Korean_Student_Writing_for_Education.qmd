
---
title: "Can Language Models Evaluate Human Written Text? Case Study on Korean Student Writing for Education"
id: "2407.17022v1"
description: "LLMs can reliably assess grammar and fluency in human-written text, but struggle with other criteria and types of writing."
author: Seungyoon Kim, Seungone Kim
date: "2024-07-24"
image: "../../../bayesian-beagle.png"
categories: ['social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The paper explores the potential of large language models (LLMs) to evaluate human-written text, specifically focusing on Korean students' writing for educational purposes. The study collected 100 texts from 32 Korean students across 15 types of writing and employed GPT-4-Turbo to evaluate them using grammaticality, fluency, coherence, consistency, and relevance as criteria. The results indicate that LLM evaluators can reliably assess grammaticality and fluency, as well as more objective types of writing, though they struggle with other criteria and types of writing.

### Major Findings:

1. LLM evaluators can reliably assess grammaticality and fluency in human-written text.
2. LLM evaluators can effectively evaluate more objective types of writing, such as essays and reports.
3. LLM evaluators struggle with assessing coherence, consistency, and relevance in human-written text.

### Analysis and Critique:

- The study's findings suggest that LLMs can be a valuable tool for providing feedback on human-written text, particularly in educational settings. However, the limitations of LLMs in assessing certain criteria and types of writing highlight the need for further research and development in this area.
- The study's reliance on GPT-4-Turbo as the sole LLM evaluator may limit the generalizability of the findings. Future research should consider using a variety of LLMs to evaluate human-written text.
- The study's focus on Korean students' writing may limit the generalizability of the findings to other populations. Future research should consider evaluating human-written text from a diverse range of populations.
- The study's use of a single dataset may limit the generalizability of the findings. Future research should consider using multiple datasets to evaluate human-written text.
- The study's focus on a limited set of evaluation criteria may limit the comprehensiveness of the findings. Future research should consider using a broader range of evaluation criteria to assess human-written text.
- The study's reliance on student self-assessment of the evaluation results may introduce bias into the findings. Future research should consider using independent evaluators to assess the validity of the evaluation results.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.17022v1](https://arxiv.org/abs/2407.17022v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.17022v1](https://browse.arxiv.org/html/2407.17022v1)       |
| Truncated       | False       |
| Word Count       | 2789       |