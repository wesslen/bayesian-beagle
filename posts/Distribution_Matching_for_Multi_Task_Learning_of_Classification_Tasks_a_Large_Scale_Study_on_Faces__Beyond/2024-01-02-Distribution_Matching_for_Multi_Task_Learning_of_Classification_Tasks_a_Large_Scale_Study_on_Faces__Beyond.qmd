
---
title: "Distribution Matching for Multi-Task Learning of Classification Tasks: a Large-Scale Study on Faces & Beyond"
id: "2401.01219v1"
description: "Multi-Task Learning can be successful with little overlapping annotations and uneven data sizes, with performance improvements in multiple domains."
author: Dimitrios Kollias, Viktoriia Sharmanska, Stefanos Zafeiriou
date: "2024-01-02"
image: "../../../bayesian-beagle.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](None)

### Major Takeaways
1. **Challenging the Traditional Setup**: The paper challenges the traditional setup of multi-task learning (MTL) which relies on building a framework where learning is done based on the ground truth annotations with full or partial overlap across tasks. Instead, the paper shows that MTL can be successful with classification tasks with little or non-overlapping annotations, or when there is a big discrepancy in the size of labeled data per task.

2. **Task-Relatedness for Cou-Annotation and Co-Training**: The paper explores task-relatedness for co-annotation and co-training, proposing a novel approach where knowledge exchange is enabled between tasks via distribution

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-06       |
| Abstract | [http://arxiv.org/abs/2401.01219v1](http://arxiv.org/abs/2401.01219v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.01219v1](https://browse.arxiv.org/html/2401.01219v1)       |
| Truncated       | True       |
| Word Count       | 14703       |