
---
title: "M-QALM: A Benchmark to Assess Clinical Reading Comprehension and Knowledge Recall in Large Language Models via Question Answering"
id: "2406.03699v1"
description: "LLMs' success in healthcare tasks depends on recall, comprehension, and integration of knowledge, with instruction tuning and fine-tuning on medical datasets showing promise."
author: Anand Subramanian, Viktor Schlegel, Abhinav Ramesh Kashyap, Thanh-Tung Nguyen, Vijay Prakash Dwivedi, Stefan Winkler
date: "2024-06-06"
image: "../../../bayesian-beagle.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

**Summary:**

The paper introduces M-QALM, a benchmark for evaluating clinical reading comprehension and knowledge recall in large language models (LLMs) through question answering. The authors conduct a large-scale empirical study using 22 datasets in three generalist and three specialist biomedical sub-domains. They analyze the performance of 15 LLMs, focusing on factors such as instruction tuning, domain-adapted models, and fine-tuning on medical knowledge datasets. The results show that while recent domain-adapted models may lack adequate knowledge, fine-tuning on medical knowledge datasets shows encouraging results, even generalizing to unseen specialist sub-domains. The paper also includes a skill-oriented manual error analysis, revealing a significant gap between the models' capabilities to recall necessary knowledge and integrate it with the presented context.

**Major Findings:**

1. Fine-tuning on medical knowledge

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.03699v1](https://arxiv.org/abs/2406.03699v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.03699v1](https://browse.arxiv.org/html/2406.03699v1)       |
| Truncated       | True       |
| Word Count       | 32275       |