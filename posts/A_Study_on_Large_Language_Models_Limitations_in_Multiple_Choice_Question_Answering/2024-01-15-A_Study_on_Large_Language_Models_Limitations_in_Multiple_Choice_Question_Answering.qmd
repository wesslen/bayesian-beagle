
---
title: "A Study on Large Language Models' Limitations in Multiple-Choice Question Answering"
id: "2401.07955v1"
description: "Small open-source language models struggle with Multiple Choice Question tasks, requiring caution when using them."
author: Aisha Khatun, Daniel G. Brown
date: "2024-01-15"
image: "../../../bayesian-beagle.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:

The article investigates the performance of 26 small open-source language models in answering Multiple Choice Questions (MCQs). The study reveals that a majority of the models lack an understanding of the MCQ task and are dependent on the order of choices provided. The findings emphasize the importance of caution and testing task understanding before using MCQ to evaluate Large Language Models (LLMs) in any field. The article also provides a comprehensive list of the models used in the analysis and discusses a randomization experiment to analyze the models' responses to MCQ prompts.

### Major Findings:
1. Most small open-source language models lack an understanding of the MCQ task and are dependent on the order of choices provided.
2. Only a few Mistral-based models show better performance in answering MCQs.
3. The randomization experiment revealed that some models were choice order dependent, while others were choice order independent.

### Analysis and Critique:
The article's findings have significant implications for the use of LLMs in various fields, particularly in tasks such as political bias identification and misinformation detection. The study highlights the need for future model developments to address issues related to task understanding and choice order dependence. However, the article could benefit from a more in-depth discussion of potential biases in the models and the limitations of the randomization experiment. Further research is needed to explore the broader implications of the findings and to address the shortcomings of the current study.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2401.07955v1](https://arxiv.org/abs/2401.07955v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.07955v1](https://browse.arxiv.org/html/2401.07955v1)       |
| Truncated       | True       |
| Word Count       | 15014       |