
---
title: "Graph-enhanced Large Language Models in Asynchronous Plan Reasoning"
id: "2402.02805v1"
description: "LLMs struggle with asynchronous planning, but PLaG technique improves performance."
author: Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang, Anthony Cohn, Janet B. Pierrehumbert
date: "2024-02-05"
image: "../../img/2402.02805v1/image_1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.02805v1/image_1.png)

### Summary:
- The paper investigates the ability of large language models (LLMs) to reason about asynchronous plans, finding that LLMs perform poorly on the task without illustrations about the task-solving process. The proposed Plan Like a Graph (PLaG) method combines graphs with natural language prompts and achieves state-of-the-art results. However, LLMs still suffer from drastic degradation when task complexity increases, highlighting the limits of utilizing LLMs for simulating digital devices. The authors present a new benchmark, Asynchronous WikiHow (AsyncHow), to evaluate LLMs on asynchronous planning, showing that LLMs cannot efficiently execute asynchronous plans unless they are supplied with detailed solution illustrations. The proposed PLaG method consistently boosts model performance across all considered task complexities.

### Major Findings:
1. LLMs perform poorly on asynchronous planning tasks without detailed solution illustrations.
2. The proposed PLaG method consistently improves model performance across all task complexities.
3. LLMs suffer from drastic degradation in performance as task complexity increases.

### Analysis and Critique:
- The paper provides valuable insights into the limitations of LLMs in reasoning about asynchronous plans and the potential of the PLaG method to enhance model performance. However, the study highlights the challenges of utilizing LLMs for simulating digital devices, indicating the need for further research in this area. The introduction of the new benchmark, AsyncHow, and the detailed methodology for generating the benchmark contribute to advancing the field of machine learning. The section on academic partners, scientific artifacts, and data for good projects emphasizes the importance of collaboration and support in academic research. The formalism for asynchronous planning and the data generation process provide a comprehensive understanding of the methodology and approach used in the study. Additionally, the methodology for assigning topics and generating the dataset lays the foundation for subsequent experiments and analyses. The section on statistical significance and model performance data offers crucial insights into the research methodology and its implications. However, the study's limitations and the need for further research are also highlighted.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.02805v1](https://arxiv.org/abs/2402.02805v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.02805v1](https://browse.arxiv.org/html/2402.02805v1)       |
| Truncated       | True       |
| Word Count       | 24562       |