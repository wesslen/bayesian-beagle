
---
title: "Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation"
id: "2402.11894v1"
description: "Automating dataset updates for reliable and timely evaluation of Large Language Models. Mimicking and extending strategies."
author: Jiahao Ying, Yixin Cao, Bo Wang, Wei Tang, Yizhe Yang, Shuicheng Yan
date: "2024-02-19"
image: "../../img/2402.11894v1/image_1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11894v1/image_1.png)

### Summary:
- The article discusses the challenges faced by Large Language Models (LLMs) in evaluating datasets due to data leakage and the high cost of manual dataset curation. The authors propose to automate dataset updates for reliable and timely evaluation by generating unseen and high-quality testing samples based on existing ones to mitigate leakage issues. They introduce two strategies: mimicking and extending, which were verified through experiments to be effective in dealing with data leakage issues. The section also evaluates the performance of baseline models on original and mimicked datasets, the impact of cognitive levels and seed popularity on model performance, and the validation and evaluation process for the mimicked and extended benchmarks. Additionally, it provides a comprehensive evaluation of the responses to specific questions related to mathematical and statistical concepts.

### Major Findings:
1. The mimicking and extending strategies effectively mitigate data leakage issues and provide a balanced difficulty of datasets for fair and fine-grained analysis.
2. The impact of cognitive levels and seed popularity on model performance is significant, with more popular seeds leading to better model performance.
3. The human evaluation results validate the reliability and effectiveness of the framework in automatic question generation, demonstrating high scores for fluency, coherence, and answer accuracy.

### Analysis and Critique:
- The proposed strategies for automating dataset updates offer a practical solution with minimal human effort, but data leakage issues still impact model performance.
- The findings on the impact of cognitive levels and seed popularity on model performance contribute to a better understanding of how to control the difficulty of generated questions and ensure the quality of extended datasets.
- The high scores obtained in the human evaluation results validate the reliability and effectiveness of the framework in automatic question generation, underscoring the significance of the section in demonstrating the robustness and accuracy of the question generation process.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-20       |
| Abstract | [https://arxiv.org/abs/2402.11894v1](https://arxiv.org/abs/2402.11894v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11894v1](https://browse.arxiv.org/html/2402.11894v1)       |
| Truncated       | True       |
| Word Count       | 21484       |