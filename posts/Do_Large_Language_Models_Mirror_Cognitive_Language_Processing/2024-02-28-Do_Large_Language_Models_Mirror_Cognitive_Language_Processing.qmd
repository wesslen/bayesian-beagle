
---
title: "Do Large Language Models Mirror Cognitive Language Processing?"
id: "2402.18023v1"
description: "LLMs simulate cognitive language processing, with model scaling and alignment training improving similarity."
author: Yuqi Ren, Renren Jin, Tongxuan Zhang, Deyi Xiong
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18023v1/extracted/5436309/model.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18023v1/extracted/5436309/model.png)

### **Summary:**
- Large language models (LLMs) have demonstrated remarkable capabilities in text comprehension and logical reasoning, achieving or surpassing human-level performance in numerous cognition tasks.
- The study aims to evaluate how effectively LLMs simulate cognitive language processing by employing Representational Similarity Analysis (RSA) to measure the alignment between LLM representations and fMRI signals of the brain.
- Experimental results indicate that model scaling is positively correlated with LLM-brain similarity, and alignment training can significantly improve LLM-brain similarity. Additionally, the performance of a wide range of LLM evaluations is highly correlated with the LLM-brain similarity.

### **Major Findings:**
1. Model scaling is positively correlated with LLM-brain similarity.
2. Alignment training can significantly improve LLM-brain similarity.
3. The performance of a wide range of LLM evaluations is highly correlated with the LLM-brain similarity.

### **Analysis and Critique:**
- The study is limited to open-source LLMs and English language fMRI stimulus texts, limiting the generalization of the findings to other linguistic environments and closed-source LLMs.
- The observed discrepancies in LLM-brain similarity between native English speakers and bilingual speakers require further exploration to understand the underlying reasons or consequences of this phenomenon.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-29       |
| Abstract | [https://arxiv.org/abs/2402.18023v1](https://arxiv.org/abs/2402.18023v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18023v1](https://browse.arxiv.org/html/2402.18023v1)       |
| Truncated       | False       |
| Word Count       | 5798       |