
---
title: "Investigating Bias Representations in Llama 2 Chat via Activation Steering"
id: "2402.00402v1"
description: "Addressing societal bias in LLMs, using activation steering to mitigate gender bias. Bias persists post-RLHF."
author: Dawn Lu, Nina Rimsky
date: "2024-02-01"
image: "../../img/2402.00402v1/image_1.png"
categories: ['architectures', 'hci', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.00402v1/image_1.png)

### **Summary:**
- The study addresses societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model.
- Activation steering is employed to probe for and mitigate biases related to gender, race, and religion.
- Inherent gender bias in Llama 2 7B Chat is revealed, persisting even after Reinforcement Learning from Human Feedback (RLHF).

### Major Findings:
1. Inherent gender bias in Llama 2 7B Chat, persisting even after RLHF.
2. Predictable negative correlation between bias and the model’s tendency to refuse responses.
3. RLHF tends to increase the similarity in the model’s representation of different forms of societal biases.

### Analysis and Critique:
- The study highlights the importance of integrating a refusal vector when red-teaming LLMs for biased behaviors.
- RLHF seems to lead the model to more closely associate various forms of societal biases, raising questions about the model’s nuanced understanding of these concepts.
- The study provides valuable insights into effective red-teaming strategies for LLMs using activation steering, emphasizing the importance of integrating a refusal vector.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.00402v1](https://arxiv.org/abs/2402.00402v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00402v1](https://browse.arxiv.org/html/2402.00402v1)       |
| Truncated       | False       |
| Word Count       | 3670       |