
---
title: "Words Matter: Reducing Stigma in Online Conversations about Substance Use with Large Language Models"
id: "2408.07873v1"
description: "Study analyzes Reddit posts to develop a model for destigmatizing language towards substance users, promoting a supportive online environment."
author: Layla Bouzoubaa, Elham Aghakhani, Rezvaneh Rezapour
date: "2024-08-15"
image: "https://browse.arxiv.org/html/2408.07873v1/x1.png"
categories: ['social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.07873v1/x1.png)

### Summary:

This study investigates the manifestation of stigma towards people who use substances (PWUS) on social media platforms, particularly Reddit. The authors analyze over 1.2 million posts, identifying 3,207 that exhibit stigmatizing language. Using Informed and Stylized Large Language Models (LLMs), they develop a model for de-stigmatizing these expressions into empathetic language, resulting in 1,649 reformed phrase pairs. The paper contributes to the field by proposing a computational framework for analyzing stigma and destigmatizing online content, and delving into the linguistic features that propagate stigma towards PWUS.

### Major Findings:

1. The study identifies 3,207 posts containing stigmatizing language towards PWUS, with stimulants and cannabis being the most frequently mentioned substances.
2. Stigma is found to be more generally associated with interpersonal relationships and moral judgments.
3. Human evaluations show that the Informed + Stylized system using GPT-4 can reduce stigma while preserving the original tone and relevance.
4. Automatic evaluations further confirm that the approach effectively reduces stigma while maintaining the stylistic and psycholinguistic properties of the original posts.

### Analysis and Critique:

1. The study focuses on a specific social media platform (Reddit) and English-speaking populations, which may limit the generalizability of the findings to other linguistic or cultural contexts.
2. The performance and accuracy of the models used are dependent on their training data, which may not capture all nuances of stigmatizing language.
3. The automated analysis of sensitive topics like substance use disorders carries risks of misinterpretation, necessitating ongoing research and continuous evaluation of ethical challenges in using large language models.
4. The study does not address the potential impact of misinformation in perpetuating stigma or explore the use of external knowledge bases (e.g., DrugBank) to develop more informed and effective de-stigmatization strategies.
5. The study's focus on SUD stigma may not be directly applicable to other marginalized groups, and future work should explore the broader implications of the insights and methodologies presented.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.07873v1](https://arxiv.org/abs/2408.07873v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.07873v1](https://browse.arxiv.org/html/2408.07873v1)       |
| Truncated       | False       |
| Word Count       | 8134       |