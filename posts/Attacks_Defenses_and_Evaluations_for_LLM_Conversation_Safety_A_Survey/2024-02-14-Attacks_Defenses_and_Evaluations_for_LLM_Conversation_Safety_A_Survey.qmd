
---
title: "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey"
id: "2402.09283v1"
description: "TL;DR: Survey covers LLM conversation safety studies on attacks, defenses, and evaluations. Encourages further investigation."
author: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
date: "2024-02-14"
image: "https://browse.arxiv.org/html/2402.09283v1/extracted/5408740/figures/defense_overview.png"
categories: ['security', 'hci', 'production', 'robustness', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.09283v1/extracted/5408740/figures/defense_overview.png)

### **Summary:**
- Large Language Models (LLMs) are widely used in conversation applications but pose risks of misuse for generating harmful responses.
- This survey provides an overview of recent studies on LLM conversation safety, covering attacks, defenses, and evaluations.
- The study categorizes attacks into inference-time and training-time approaches, while defenses include safety alignment, inference guidance, and input/output filters.
- Evaluation methods are discussed, including metrics like attack success rate and other fine-grained metrics.

### **Major Findings:**
1. LLMs can be exploited during conversation to facilitate harmful activities such as fraud, cyberattacks, and the propagation of toxic content, discrimination, and misinformation.
2. Attack methods include inference-time approaches that attack LLMs through adversarial prompts and training-time approaches that involve explicit modifications to LLM weights.
3. Defense strategies encompass safety alignment, inference guidance, and input/output filters to detect and filter malicious inputs or outputs.

### **Analysis and Critique:**
- The survey provides a comprehensive overview of LLM conversation safety, but it is limited in scope due to its focus on LLMs.
- The study does not address potential biases in the research or the societal impact of LLM conversation safety.
- Further research is needed to explore the social implications and ethical considerations of LLM conversation safety, as well as the potential impact on marginalized communities.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.09283v1](https://arxiv.org/abs/2402.09283v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09283v1](https://browse.arxiv.org/html/2402.09283v1)       |
| Truncated       | False       |
| Word Count       | 8506       |