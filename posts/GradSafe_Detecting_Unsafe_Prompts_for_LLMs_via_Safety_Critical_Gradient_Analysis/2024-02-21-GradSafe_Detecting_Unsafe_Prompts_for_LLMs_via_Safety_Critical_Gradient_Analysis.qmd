
---
title: "GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis"
id: "2402.13494v1"
description: "GradSafe detects unsafe prompts in LLMs without extensive training, outperforming existing methods."
author: Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13494v1/x2.png"
categories: ['robustness', 'prompt-engineering', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13494v1/x2.png)

### Summary:
- Large Language Models (LLMs) face threats from unsafe prompts.
- GradSafe is proposed to detect unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs.
- GradSafe outperforms Llama Guard in detecting unsafe prompts without further training.

### Major Findings:
1. GradSafe effectively detects unsafe prompts by analyzing the gradients of safety-critical parameters in LLMs.
2. GradSafe outperforms Llama Guard in detecting unsafe prompts without further training.
3. GradSafe showcases enhanced adaptability over both Llama Guard and the original Llama-2 model.

### Analysis and Critique:
- The study relies on a small number of prompts to identify safety-critical parameters, which may not be representative of all possible prompts.
- The method does not offer fine-grained classification for specific classes of unsafe prompts.
- The effectiveness of GradSafe may vary depending on the base LLM utilized, which needs further exploration.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13494v1](https://arxiv.org/abs/2402.13494v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13494v1](https://browse.arxiv.org/html/2402.13494v1)       |
| Truncated       | False       |
| Word Count       | 6171       |