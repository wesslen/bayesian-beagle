
---
title: "KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion"
id: "2402.02389v1"
description: "KICGPT integrates language model and triple-based KGC retriever for efficient knowledge graph completion."
author: Yanbin Wei, Qiushi Huang, James T. Kwok, Yu Zhang
date: "2024-02-04"
image: "https://browse.arxiv.org/html/2402.02389v1/extracted/5387986/arc3.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.02389v1/extracted/5387986/arc3.png)

### Summary:
- KICGPT is a framework that integrates a large language model (LLM) and a triple-based KGC retriever to address the long-tail problem in knowledge graph completion (KGC).
- The proposed KICGPT uses an in-context learning strategy called Knowledge Prompt to guide the LLM with structural knowledge.
- Empirical results on benchmark datasets demonstrate the effectiveness of KICGPT with smaller training overhead and no finetuning.

### Major Findings:
1. KICGPT integrates a large language model (LLM) and a triple-based KGC retriever to address the long-tail problem in knowledge graph completion (KGC).
2. The framework uses an in-context learning strategy called Knowledge Prompt to guide the LLM with structural knowledge.
3. Empirical results on benchmark datasets demonstrate the effectiveness of KICGPT with smaller training overhead and no finetuning.

### Analysis and Critique:
- The proposed KICGPT framework shows state-of-the-art performance in knowledge graph completion, especially for long-tail entities.
- The use of large language models (LLMs) and in-context learning strategies provides a cost-effective solution for KGC tasks.
- The limitations of the approach include the reliance on a vast knowledge base within the LLM and the inability to inject all relevant facts from the knowledge graph as prompts due to the limited token length in the LLM.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.02389v1](https://arxiv.org/abs/2402.02389v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.02389v1](https://browse.arxiv.org/html/2402.02389v1)       |
| Truncated       | False       |
| Word Count       | 7020       |