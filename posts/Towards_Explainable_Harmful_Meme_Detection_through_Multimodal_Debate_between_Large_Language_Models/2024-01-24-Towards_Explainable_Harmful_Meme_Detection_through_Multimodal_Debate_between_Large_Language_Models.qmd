
---
title: "Towards Explainable Harmful Meme Detection through Multimodal Debate between Large Language Models"
id: "2401.13298v1"
description: "Detecting harmful memes is challenging due to implicit meanings. The proposed explainable approach uses reasoning and debate among language models for better detection."
author: ['Hongzhan Lin', 'Ziyang Luo', 'Wei Gao', 'Jing Ma', 'Bo Wang', 'Ruichao Yang']
date: "2024-01-24"
image: "https://browse.arxiv.org/html/2401.13298v1/x1.png"
categories: ['security', 'prompt-engineering', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.13298v1/x1.png)

### **Summary:**

The article proposes a method for detecting harmful memes through explainable reasoning utilizing Large Language Models (LLMs). The authors address the challenge of identifying harmful memes given the implicit meanings often embedded within them, hampering traditional harmful meme detection methods. By leveraging the reasoning capabilities of LLMs, the proposed approach engages in a multimodal debate between LLMs to generate explicit explanations from contradicting arguments. A small language model then judges harmfulness inference, facilitating multimodal fusion between harmfulness rationales and intrinsic multimodal meme information. Empirical studies across three public meme datasets demonstrate that the proposed approach outperforms state-of-the-art methods, highlighting its effectiveness in detecting harmful memes and providing explanatory insights.

### **Major Findings:**
1. The explainable approach for harmful meme detection achieved significantly better performance than existing state-of-the-art methods across three public meme datasets.
2. Engaging in a multimodal debate between LLMs and fine-tuning a small language model as the judge for harmfulness inference facilitated better dialectical reasoning over implicit harm-indicative patterns within memes.
3. The article proposed a novel perspective for harmfulness explainability in natural texts, harnessing advanced LLMs while conducting multimodal debate for better dialectical thinking on meme harmfulness.

### **Analysis and Critique:**
The article presents a novel and promising approach towards explainable harmful meme detection utilizing the reasoning capabilities of Large Language Models (LLMs), which is significant given the prevalence of harmful memes in the age of social media. The proposed approach demonstrates superior performance and the potential to provide informative explanations for harmful memes, addressing the limitations of existing methods. However, the limitations of this study include the lack of in-depth evaluation of the explainability method, requiring further human evaluation and refinement. Additionally, the reliance on LLMs raises concerns about potential biases and limitations inherent in these models. Moreover, the article mainly focuses on the technical aspects, necessitating further discussion about the broader societal and ethical implications of this research. Further research is needed to address these limitations and ensure the ethical and robust implementation of harmful meme detection methods in real-world applications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [http://arxiv.org/abs/2401.13298v1](http://arxiv.org/abs/2401.13298v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.13298v1](https://browse.arxiv.org/html/2401.13298v1)       |
| Truncated       | True       |
| Word Count       | 16228       |