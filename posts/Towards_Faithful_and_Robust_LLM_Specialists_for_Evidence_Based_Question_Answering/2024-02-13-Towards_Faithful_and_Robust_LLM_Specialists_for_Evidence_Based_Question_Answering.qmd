
---
title: "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering"
id: "2402.08277v1"
description: "Improving Large Language Models' accuracy and reliability through fine-tuning and data quality filters."
author: Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold
date: "2024-02-13"
image: "../../../bayesian-beagle.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The article focuses on improving the performance of Large Language Models (LLMs) in Evidence-Based Question Answering (QA) through the use of synthetic data and data quality filters.
- It introduces a data generation pipeline to synthesize high-quality training and testing data at scale and addresses the challenges of fine-tuning LLMs into faithful evidence-based question answerers.
- The creation process of the SYNSCIQA dataset and the use of RAG tools to analyze companies' sustainability reports and climate-related questions are discussed, along with experiments on Llama-2-chat-13b and Zephyr-7b-Î² models.
- The conclusion presents a data synthesis pipeline for fine-tuning and evaluating LLMs for Evidence-Based QA, emphasizing the critical role of data quality and the potential of synthetic data fine-tuning to improve real-world applications.

### Major Findings:
1. Data quality is more important than quantity in improving Evidence-Based QA.
2. Fine-tuning on synthetic data can enhance performance on both in- and out-of-distribution test sets.
3. Synthetic data fine-tuning can improve real-world applications of LLMs for Evidence-Based QA.

### Analysis and Critique:
- The article provides valuable insights into the significance of data quality and synthetic data in improving LLMs for Evidence-Based QA.
- The limitations and ethics statement ensure transparency and ethical conduct in the research process.
- The experimental setup demonstrates a meticulous approach to evaluating the performance of different models and the significance of format quality in improving attributability scores.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-14       |
| Abstract | [https://arxiv.org/abs/2402.08277v1](https://arxiv.org/abs/2402.08277v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08277v1](https://browse.arxiv.org/html/2402.08277v1)       |
| Truncated       | True       |
| Word Count       | 21072       |