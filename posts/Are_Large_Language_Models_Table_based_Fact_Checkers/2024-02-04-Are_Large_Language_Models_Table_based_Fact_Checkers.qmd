
---
title: "Are Large Language Models Table-based Fact-Checkers?"
id: "2402.02549v1"
description: "LLMs show potential for table-based fact verification with prompt engineering and instruction tuning."
author: Hangwen Zhang, Qingyi Si, Peng Fu, Zheng Lin, Weiping Wang
date: "2024-02-04"
image: "img/2402.02549v1/image_1.png"
categories: ['prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](img/2402.02549v1/image_1.png)

### **Summary:**
- Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables.
- Large Language Models (LLMs) have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown.
- In this work, a preliminary study was conducted to explore whether LLMs are table-based fact-checkers. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly.

### **Major Findings:**
1. LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering.
2. Instruction-tuning can stimulate the TFV capability significantly.
3. Valuable findings about the format of zero-shot prompts and the number of in-context examples were made.

### **Analysis and Critique:**
- LLMs, especially ChatGPT, demonstrated acceptable performance on zero-shot and few-shot TFV with prompt engineering, but LLaMA models with small parameters did not perform as well.
- Instruction-tuning significantly improved the performance of LLaMA models, but they still lagged behind the most advanced task-specific small-scaled models on TFV.
- LLMs may suffer from hallucination for complex questions, and further research is needed to improve their performance on TFV, such as handling long input, specifying inference procedures, and developing table-based LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-06       |
| Abstract | [https://arxiv.org/abs/2402.02549v1](https://arxiv.org/abs/2402.02549v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.02549v1](https://browse.arxiv.org/html/2402.02549v1)       |
| Truncated       | False       |
| Word Count       | 8589       |