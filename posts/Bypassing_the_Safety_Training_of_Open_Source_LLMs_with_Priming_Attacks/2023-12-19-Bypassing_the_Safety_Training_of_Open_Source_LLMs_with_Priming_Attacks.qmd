
---
title: "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks"
description: "LLMs need safety training due to vulnerability to priming attacks bypassing safety measures, with an improved attack success rate."
author: "gpt-3.5-turbo-1106"
date: "2023-12-19"
link: "https://browse.arxiv.org/html/2312.12321v1"
image: "https://browse.arxiv.org/html/2312.12321v1/extracted/5284390/images/llm_attack_final_bold.png"
categories: ['security', 'open-source']
file-modified: 2024-01-02
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.12321v1/extracted/5284390/images/llm_attack_final_bold.png)

### Major Findings

1. **Priming attacks** are shown to be effective in bypassing safety training for open-source Large Language Models (LLMs), resulting in a significant increase in the Attack Success Rate on Harmful Behaviors.
  
2. The study highlights the ease with which adversaries can coerce open-source LLMs to comply with harmful requests, undermining the efficacy of safety measures in current LLMs, and raising pivotal concerns for the future of open-sourcing LLMs.

3. The research contributes to demonstrating the fragility of existing safety measures for LLMs and emphasizes the need for further exploration of novel methods for safer open-sourcing.

### Introduction
- Autoregressive Large Language Models (LLMs) have become ubiquitous in user-facing applications, prompting extensive safety training to ensure human alignment and prevent nefarious usage.
- Current safety measures, such as RLHF techniques and fine-tuning, may still be circumvented, leading to harmful outputs or compliance with harmful behavior requests.
- The paper challenges the assumption that attackers are limited to specific input formats, advocating for unrestricted inputs in the extraction of harmful behavior content from open-source models.

### Methodology & Results
- The study presents a threat model that allows successful low-resource attacks via **priming attacks** on open-source LLMs, leveraging API query access and the autoregressive nature of LLMs to fulfill harmful requests.
- Few-shot prompting using a helper LLM is employed to generate priming attacks, demonstrating significant improvements in the Attack Success Rate compared to baseline methods.
- Experiment results reveal the effectiveness of priming attacks in bypassing safety measures for LLMs, with the attack outperforming baselines across different model families and sizes.

### Conclusion
- The paper concludes by emphasizing how priming attacks highlight the vulnerability of current safety measures and the need for further research into safer methods for open-sourcing LLMs.

### Critique
The use of automated evaluation processes and the absence of a rigorous human study to systematically study the priming process may raise concerns regarding the robustness and real-world applicability of the findings. Additionally, the paper acknowledges the underestimation of harmfulness by the evaluation tool used, indicating potential limitations in the accuracy of the reported Attack Success Rates. Further validation and real-world testing with human subjects may be necessary to accurately assess the impact and feasibility of priming attacks on open-source LLMs.

## Appendix

|          |          |
|----------|----------|
| Date Generated     | 2024-01-02       |
| HTML     | [https://browse.arxiv.org/html/2312.12321v1](https://browse.arxiv.org/html/2312.12321v1)       |
| Truncated       | False       |
| Word Count       | 2072       |