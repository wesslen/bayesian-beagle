
---
title: "Leveraging Print Debugging to Improve Code Generation in Large Language Models"
id: "2401.05319v1"
description: "In-context learning improves large language models' debugging in coding, outperforming rubber duck debugging in Leetcode problems."
author: ['Xueyu Hu', 'Kun Kuang', 'Jiankai Sun', 'Hongxia Yang', 'Fei Wu']
date: "2024-01-10"
image: "https://browse.arxiv.org/html/2401.05319v1/x1.png"
categories: ['architectures', 'programming', 'robustness', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.05319v1/x1.png)

### Key Findings

1. **Large language models (LLMs) have shown significant progress in code generation tasks but struggle with complex programming problems involving intricate data structures and algorithms.**
2. **The proposed in-context learning approach leverages "print debugging" to guide LLMs in debugging by inserting print statements and analyzing logs, leading to a substantial improvement in performance, outperforming rubber duck debugging in easy and medium-level Leetcode problems.**
3. **While the print debugging approach was effective in addressing bugs in easy and medium-level problems, it did not yield improvements in hard-level problems, indicating the need for further research to address challenges requiring sophisticated algorithms.**

### 1. Introduction

- Large language models (LLMs) have shown promise in code generation tasks but struggle with complex programming problems involving intricate data structures and algorithms.
- Existing methods such as Reflexion and Self-debug have limitations in providing real-time variable values and effectively leveraging test cases for debugging.
  
### 2. Related Work

- Researchers have explored chain-of-thought prompting and prompting with feedback to enhance model capabilities in reasoning and iterative refinement tasks.
- Prompting techniques, including auto-cot, least-to-more, and tree-of-thought, have been proposed to enhance model capabilities.
  
### 3. Our Methods

- The proposed approach guides LLMs to employ "print debugging" by adding print statements, executing the code, and analyzing mentioned logs and test cases using one-shot prompting.
- The method involves three main steps: adding print statements, execution, and analyzing & fixing, and results in continuous improvement in performance with iterative debugging rounds.
  
### 4. Experiments

- Experimentation with GPT-4 on Leetcode problems demonstrated the effectiveness of the print debugging approach, outperforming other debugging methods in easy and medium-level problems but showing limitations in hard-level challenges.
- Ablation studies emphasized the significant impact of both test case explanations and logs in effectively debugging the code.
  
### 5. Analysis

- Analysis of the performance of different debugging methods as the procedure progresses highlighted the continuous increase in performance of print debugging over multiple rounds, compared to other methods.
- The distribution of added print statements in the code and the number of lines in the generated logs were examined, showcasing the effectiveness of the print debugging method.
  
### Critique

While the proposed print debugging approach showed effectiveness in improving LLMs' code generation performance, there are some potential concerns:
- Limited application to hard-level problems: The method showed limited effectiveness in addressing hard-level problems, emphasizing the need for further research to address challenges requiring advanced algorithms.
- Overwhelming log length: In some cases, the logs generated from print statements exceeded a predefined limit, indicating a potential challenge in effectively handling excessive log lengths.
- Dependency on iterative rounds: The continuous improvement in performance with iterative debugging rounds may indicate a potential dependency on multiple rounds for effective bug identification and resolution, raising questions about the efficiency of the method in single-round scenarios.

Overall, the proposed print debugging approach provides a valuable contribution to improving LLMs' code generation performance, but further research is warranted to address its limitations and potential challenges.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-31       |
| Abstract | [http://arxiv.org/abs/2401.05319v1](http://arxiv.org/abs/2401.05319v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.05319v1](https://browse.arxiv.org/html/2401.05319v1)       |
| Truncated       | False       |
| Word Count       | 6337       |