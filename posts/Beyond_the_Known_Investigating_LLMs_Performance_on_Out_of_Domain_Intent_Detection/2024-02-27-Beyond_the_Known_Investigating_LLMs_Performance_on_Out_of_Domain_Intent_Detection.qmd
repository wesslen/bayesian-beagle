
---
title: "Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection"
id: "2402.17256v1"
description: "TL;DR: Study evaluates LLMs for OOD intent detection, finding strengths and weaknesses compared to fine-tuned models."
author: Pei Wang, Keqing He, Yejie Wang, Xiaoshuai Song, Yutao Mou, Jingang Wang, Yunsen Xian, Xunliang Cai, Weiran Xu
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17256v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17256v1/x1.png)

### **Summary:**
- Out-of-domain (OOD) intent detection is crucial for task-oriented dialogue (TOD) systems.
- Large language models (LLMs) like ChatGPT have been explored for OOD detection, but their performance is still unclear.
- LLMs exhibit strong zero-shot and few-shot capabilities but struggle with fine-grained semantic distinctions and knowledge transfer from in-domain (IND) to OOD tasks.

### Major Findings:
1. **ChatGPT's Strengths:**
   - Achieves good zero-shot performance without providing any IND intent priors.
   - Better accuracy in few-shot settings than discriminative models with a small number of IND intents.
   - Can output the intent of OOD samples, which discriminative models cannot achieve.

2. **ChatGPT's Weaknesses:**
   - Performs significantly worse than baselines with a large number of IND intents.
   - Struggles with fine-grained semantic distinctions and knowledge transfer from IND to OOD tasks.

3. **Future Improvement Directions for LLMs:**
   - Injecting domain knowledge
   - Strengthening knowledge transfer from IND to OOD
   - Understanding long instructions

### Analysis and Critique:
- **Challenges and Limitations:**
  - Conflict between domain-specific knowledge and general knowledge
  - Difficulty of knowledge transfer from IND to OOD
  - Sensitivity to input length
  - Need for future insights and improvements in LLMs
- **Conclusion:**
  - ChatGPT excels in tasks with a small number of intents but struggles with larger-scale tasks.
  - Future research should focus on improving LLMs for OOD tasks by incorporating domain-specific knowledge and enhancing knowledge transfer from IND to OOD.
- **Limitations:**
  - Quality and diversity of demonstration examples for few-shot OOD detection
  - Use of closed-source LLMs may lead to different results in the future.

Overall, the study provides valuable insights into the performance of LLMs for OOD intent detection and highlights the need for further research and improvements in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.17256v1](https://arxiv.org/abs/2402.17256v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17256v1](https://browse.arxiv.org/html/2402.17256v1)       |
| Truncated       | False       |
| Word Count       | 6407       |