
---
title: "Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images"
id: "2408.08105v1"
description: "VLLMs struggle with causal reasoning from visual cues; new benchmark, MuCR, introduced for evaluation."
author: Zhiyuan Li, Heng Wang, Dongnan Liu, Chaoyi Zhang, Ao Ma, Jieting Long, Weidong Cai
date: "2024-08-15"
image: "https://browse.arxiv.org/html/2408.08105v1/extracted/5793486/images/picture1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.08105v1/extracted/5793486/images/picture1.png)

### Summary:

The paper introduces a novel Multimodal Causal Reasoning (MuCR) benchmark to evaluate Vision Large Language Models (VLLMs) in inferring semantic cause-and-effect relationships based solely on visual cues. The benchmark employs a prompt-driven image synthesis approach to create siamese images with embedded semantic causality and visual cues. Tailored metrics are developed to assess VLLMs' comprehension abilities at multiple levels, including image-level match, phrase-level understanding, and sentence-level explanation.

### Major Findings:

1. Current state-of-the-art VLLMs struggle with multimodal causal reasoning, as revealed by extensive experiments conducted on the MuCR benchmark.
2. Open-source models, such as LLaVa1.6, face significant challenges in visual information comprehension, leading to performance that falls behind in-house models like GPT-4o.
3. Even the best in-house model, GPT-4V, struggles to match human-level performance due to the strong causal knowledge within the language model that can cause them to disregard crucial visual evidence.

### Analysis and Critique:

* The paper identifies the limitations of current causal reasoning benchmarks, which fail to assess the advanced visual capabilities of the latest VLLMs.
* The proposed MuCR benchmark addresses these limitations by comprehensively evaluating VLLMs' multimodal causal reasoning abilities from image, phrase, and sentence levels.
* The paper reveals that general LLM-enhancing strategies like in-context learning and chain-of-thought reasoning can only provide limited improvement or even negative improvement on the benchmark.
* The multi-image input form is suggested as a promising avenue for advancing VLLM research.

### Potential Limitations and Future Research:

* The paper does not discuss the potential biases in the generated siamese images or the impact of these biases on the evaluation of VLLMs.
* The reliance on GPT-4 as a scoring function for semantic similarity may introduce additional biases or limitations.
* The paper does not explore the potential of using other large language models or techniques to improve VLLMs' performance on the Mu

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.08105v1](https://arxiv.org/abs/2408.08105v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.08105v1](https://browse.arxiv.org/html/2408.08105v1)       |
| Truncated       | False       |
| Word Count       | 4315       |