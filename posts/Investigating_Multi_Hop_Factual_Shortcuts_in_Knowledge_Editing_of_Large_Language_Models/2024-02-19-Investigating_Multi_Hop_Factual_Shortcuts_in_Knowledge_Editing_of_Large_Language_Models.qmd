
---
title: "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models"
id: "2402.11900v1"
description: "LLMs can use shortcuts for multi-hop reasoning, but erasing them reduces failures."
author: Tianjie Ju, Yijin Chen, Xinwei Yuan, Zhuosheng Zhang, Wei Du, Yubin Zheng, Gongshen Liu
date: "2024-02-19"
image: "../../img/2402.11900v1/image_1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11900v1/image_1.png)

### **Summary:**
- Investigated the existence of factual shortcuts in large language models (LLMs) when answering multi-hop knowledge questions.
- Analyzed the potential risks of factual shortcuts in multi-hop knowledge editing.
- Proposed a method to reduce the risks associated with factual shortcuts in multi-hop knowledge editing.

### Major Findings:
1. **Factual Shortcuts Existence:**
   - Factual shortcuts are highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora.
   - Few-shot prompting leverages more shortcuts in answering multi-hop questions compared to chain-of-thought prompting.

2. **Risks of Factual Shortcuts:**
   - Approximately 20% of failures in multi-hop knowledge editing are attributed to shortcuts.
   - Instances with higher co-occurrence frequency between initial subjects and terminal objects tend to have more shortcuts.

3. **Reducing Factual Shortcuts:**
   - Erasing shortcut neurons significantly reduces the risks associated with shortcut failures in multi-hop knowledge editing.

### Analysis and Critique:
- The study provides valuable insights into the potential risks associated with factual shortcuts in LLMs when answering multi-hop knowledge questions.
- The proposed method to reduce shortcut failures in multi-hop knowledge editing is effective, but it may not be a comprehensive solution.
- The study is limited by the use of specific LLMs and the need for further research to explore improved pre-training methodologies.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-20       |
| Abstract | [https://arxiv.org/abs/2402.11900v1](https://arxiv.org/abs/2402.11900v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11900v1](https://browse.arxiv.org/html/2402.11900v1)       |
| Truncated       | False       |
| Word Count       | 14061       |