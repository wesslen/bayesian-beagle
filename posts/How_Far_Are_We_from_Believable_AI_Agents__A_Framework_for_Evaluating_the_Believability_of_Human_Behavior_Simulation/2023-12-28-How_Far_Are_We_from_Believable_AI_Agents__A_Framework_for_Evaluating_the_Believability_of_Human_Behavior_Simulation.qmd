
---
title: "How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation"
description: "AI agent believability relies on user trust. Large Language Model agents face challenges, so new metrics are introduced."
author: Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, Pengfei Liu
date: "2023-12-28"
image: "https://browse.arxiv.org/html/2312.17115v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.17115v1/x1.png)

### How Far Are We from Believable AI Agents? A Framework for Evaluating the Believability of Human Behavior Simulation

#### Key Findings

- **Believability Importance**: Believability of AI agents is crucial for establishing trust and fulfilling their goals in various applications.
- **LLM-based Agent Challenges**: Large Language Model (LLM) deficiencies in processing long profile inputs and lack of robustness can undermine believability.
- **Novel Metrics and Benchmark**: The study proposes two new metrics for assessing LLM-based agent believability—consistency and robustness, and introduces SimulateBench, a benchmark to evaluate agent consistency and robustness.

#### Introduction
- AI agents have the potential to simulate human behavior, necessitating believability to facilitate trust and goal fulfillment.
  
#### Evaluating LLM-based Agent Believability
- LLM-based agents have advanced human behavior simulation but face challenges in processing long inputs and lack of robustness.
- Prior research fails to address these issues, prompting the introduction of two metrics for evaluating believability—consistency and robustness, with the SimulateBench benchmark.

#### Related Work
- LLMs are increasingly used in simulating human behaviors and social interactions across various applications.
- Prior evaluations of LLM-based agent believability lack a systematic and fine-grained benchmark, prompting the need for novel metrics and benchmarks.

### Critique
The paper provides valuable insights into evaluating the believability of AI agents. However, the research primarily focuses on evaluating AI agent believability in the context of LLMs and lacks broader analysis of alternative approaches. Additionally, the study's findings are based on a specific set of LLMs, and the generalizability of the results to other AI agents is uncertain. Including a wider range of AI models and expanding the scope of the study to encompass a more diverse array of AI agents would enhance the paper's contributions to the field.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-03       |
| Abstract | [http://arxiv.org/abs/2312.17115v1](http://arxiv.org/abs/2312.17115v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.17115v1](https://browse.arxiv.org/html/2312.17115v1)       |
| Truncated       | False       |
| Word Count       | 8635       |