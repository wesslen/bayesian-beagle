
---
title: "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models"
id: "2402.13064v1"
description: "GLAN is a method for instruction tuning of Large Language Models using a pre-curated taxonomy."
author: Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, Furu Wei
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13064v1/extracted/5420465/images/glan_cmp_v4.png"
categories: ['education', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13064v1/extracted/5420465/images/glan_cmp_v4.png)

### **Summary:**
- Generalized Instruction Tuning (GLAN) is introduced as a method for instruction tuning of Large Language Models (LLMs).
- GLAN utilizes a pre-curated taxonomy of human knowledge and capabilities to generate large-scale synthetic instruction data across all disciplines.
- The process involves decomposing human knowledge and capabilities into various fields, sub-fields, and distinct disciplines, and then designing a syllabus tailored to each subject to generate diverse instructions.

### **Major Findings:**
1. GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks.
2. The synthetic instruction data generated by GLAN covers a broad range of domains and is diverse in difficulty levels.
3. GLAN demonstrates strong performance on most skills, especially on math, coding, and reasoning, but slightly falls short in common-sense related tasks.

### **Analysis and Critique:**
- GLAN's synthetic data is diverse and avoids convergence to any specific domain or style present in existing benchmarks, indicating its general applicability.
- The instruction-following capabilities of GLAN are superior, but there is still a considerable gap compared to GPT-3.5-turbo and GPT-4.
- GLAN showcases superior performance compared to other models in comparison, demonstrating improved ability to process diverse instructions, regardless of their difficulty or complexity.
- GLAN demonstrates its effectiveness on multiple domains, indicating that smaller models may yield general improvements on various domains through strategic fine-tuning.
- GLAN demonstrates less-than-ideal performance across distinct disciplines such as American history, Divinity, or Radiology, indicating the potential for further refinement and development of the methodology within these domains.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-21       |
| Abstract | [https://arxiv.org/abs/2402.13064v1](https://arxiv.org/abs/2402.13064v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13064v1](https://browse.arxiv.org/html/2402.13064v1)       |
| Truncated       | False       |
| Word Count       | 6837       |