
---
title: "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration"
id: "2402.04978v1"
description: "LLMs face challenges; proposed KG-LLM collaboration improves reasoning and transparency, outperforming baselines."
author: Yihao Li, Ru Zhang, Jianyi Liu, Gongshen Liu
date: "2024-02-07"
image: "../../img/2402.04978v1/image_1.png"
categories: ['production', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.04978v1/image_1.png)

### **Summary:**
- Large Language Models (LLMs) have shown exceptional performance in Natural Language Processing (NLP) tasks but face challenges in practical applications.
- This study proposes a collaborative training-free reasoning scheme involving cooperation between Knowledge Graph (KG) and LLMs to overcome these limitations.
- The scheme involves using LLMs to iteratively explore KG, selectively retrieving task-relevant knowledge subgraphs to support reasoning, and explicitly elucidating the reasoning process.

### Major Findings:
1. **Challenges with LLMs:** LLMs face issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process.
2. **Collaborative Training-Free Reasoning Scheme:** The proposed scheme involves tight cooperation between KG and LLMs, enhancing the reasoning abilities of LLMs and effectively utilizing latent knowledge within LLMs.
3. **Experimental Results:** The scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work.

### Analysis and Critique:
- The proposed scheme demonstrates superior performance compared to baseline methods and previous SOTA works, showcasing its ability to handle specialized and complex tasks more efficiently.
- The study acknowledges limitations in answering counting questions and the impact of low-quality knowledge retrieval on LLM reasoning.
- Further experiments demonstrate the adaptability and effectiveness of the scheme with different KGs and LLMs, highlighting its potential to enhance the reasoning capabilities of smaller LLMs and compete effectively with larger LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.04978v1](https://arxiv.org/abs/2402.04978v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04978v1](https://browse.arxiv.org/html/2402.04978v1)       |
| Truncated       | False       |
| Word Count       | 13166       |