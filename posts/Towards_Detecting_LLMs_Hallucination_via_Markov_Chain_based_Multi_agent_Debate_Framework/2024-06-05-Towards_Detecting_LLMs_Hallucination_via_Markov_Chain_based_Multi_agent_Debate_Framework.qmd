
---
title: "Towards Detecting LLMs Hallucination via Markov Chain-based Multi-agent Debate Framework"
id: "2406.03075v1"
description: "Markov Chain-based multi-agent debate improves hallucination detection in LLMs, outperforming baselines."
author: Xiaoxi Sun, Jinpeng Li, Yan Zhong, Dongyan Zhao, Rui Yan
date: "2024-06-05"
image: "https://browse.arxiv.org/html/2406.03075v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.03075v1/x1.png)

### Summary:

The paper introduces a Markov Chain-based multi-agent debate framework to enhance the accuracy of hallucination detection in large language models (LLMs). The proposed method integrates the fact-checking process, including claim detection, evidence retrieval, and multi-agent verification. In the verification stage, multiple agents are deployed through flexible Markov Chain-based debates to validate individual claims, ensuring meticulous verification outcomes. The experimental results across three generative tasks demonstrate that the proposed approach achieves significant improvements over baselines.

### Major Findings:

1. The paper proposes a versatile hallucination detection process applicable to multiple generation tasks for improving verification accuracy.
2. A Markov Chain-based multi-agent debate verification framework is introduced, which simulates human discussion to enhance the precision of validation.
3. Experiments conducted on three generative tasks show that the proposed framework outperforms baselines.

### Analysis and Critique:

The paper presents a novel approach to address the challenge of hallucination detection in LLMs. The proposed method effectively integrates the fact-checking process and leverages the robust capabilities of multi-agent systems to simulate human behavior. However, there are some limitations and potential risks associated with the approach:

1. The method requires frequent interactions with the API of LLMs, resulting in significant overhead, increased cost, and reduced response speed. This may limit its practicality in real-world scenarios.
2. The distinctiveness among prompts for different agents primarily focuses on role definition, which occasionally leads to the partial repetition of opinions from the preceding agent. Enhancing the performance of the base model could substantially alleviate this issue.

Overall, the paper provides a promising solution to improve the accuracy of hallucination detection in LLMs. However, further research is needed to address the limitations and potential risks associated with the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.03075v1](https://arxiv.org/abs/2406.03075v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.03075v1](https://browse.arxiv.org/html/2406.03075v1)       |
| Truncated       | False       |
| Word Count       | 5918       |