
---
title: "Improving Cross-Domain Low-Resource Text Generation through LLM Post-Editing: A Programmer-Interpreter Approach"
id: "2402.04609v1"
description: "Post-editing improves large language model text quality; neural programmer-interpreter enhances performance across domains."
author: Zhuang Li, Levon Haroutunian, Raj Tumuluri, Philip Cohen, Gholamreza Haffari
date: "2024-02-07"
image: "../../img/2402.04609v1/image_1.png"
categories: ['programming', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.04609v1/image_1.png)

### Summary:
The article discusses a neural programmer-interpreter approach to improve the quality of text generated by large language models (LLMs) such as GPT-3.5 or GPT-4. The proposed approach aims to address the limitations of relying solely on smaller language models for post-editing, which can limit the LLMs' ability to generalize across domains. The experiments demonstrate that the programmer-interpreter significantly enhances GPT-3.5â€™s performance in logical form-to-text conversion and low-resource machine translation, surpassing other state-of-the-art LLM post-editing methods in cross-domain settings.

### Major Findings:
1. The proposed neural programmer-interpreter approach preserves the domain generalization ability of LLMs when editing their output.
2. The editing actions in this framework are specifically devised for text generation, leading to substantial text quality improvements.
3. The method outperforms all existing LLM post-editing baselines in low-resource machine translation and logical form-to-text tasks in cross-domain settings.

### Analysis and Critique:
- The article presents a novel approach to improving text generation by large language models, addressing the limitations of existing post-editing methods.
- The experiments demonstrate the effectiveness of the proposed method in enhancing text quality, especially in cross-domain settings.
- However, the study has limitations in in-domain tests, where the proposed approach does not outperform smaller models, and there are potential privacy concerns related to internet transmission of prompt instructions.

Overall, the article provides valuable insights into improving cross-domain low-resource text generation through a programmer-interpreter approach, but further research is needed to address the identified limitations and potential privacy concerns.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.04609v1](https://arxiv.org/abs/2402.04609v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04609v1](https://browse.arxiv.org/html/2402.04609v1)       |
| Truncated       | False       |
| Word Count       | 9497       |