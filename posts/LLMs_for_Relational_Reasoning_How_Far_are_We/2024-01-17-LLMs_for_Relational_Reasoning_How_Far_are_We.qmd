
---
title: "LLMs for Relational Reasoning: How Far are We?"
id: "2401.09042v1"
description: "LLMs struggle with reasoning in complex decision-making and logic tasks."
author: Zhiming Li, Yushi Cao, Xiufeng Xu, Junzhe Jiang, Xu Liu, Yon Shin Teo, Shang-wei Lin, Yang Liu
date: "2024-01-17"
image: "../../../bayesian-beagle.png"
categories: ['production', 'architectures', 'prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](None)

### **Summary:**
The article evaluates the relational reasoning ability of Large Language Models (LLMs) using inductive logic programming benchmarks. It compares the performance of LLMs with neural program induction models and investigates the effectiveness of different prompting techniques.

### **Major Findings:**
1. LLMs' performance with standard natural language prompting is generally far from satisfactory, especially on tasks requiring complex task-solving logic.
2. LLMs with large context windows show consistent or improved performance on general graph reasoning tasks compared to standard natural language prompting.
3. The state-of-the-art prompting technique, chain-of-thought (CoT), is not generally effective for improving LLMs' relational reasoning ability.

### **Analysis and Critique:**
The article provides a comprehensive evaluation of LLMs' relational reasoning ability, highlighting their limitations and the effectiveness of different prompting techniques. However, the study could benefit from a more detailed analysis of the underlying reasons for the observed performance differences and further exploration of potential solutions to improve LLMs' relational reasoning ability. Additionally, the article could discuss the implications of the findings for the development and application of LLMs in real-world scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2401.09042v1](https://arxiv.org/abs/2401.09042v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.09042v1](https://browse.arxiv.org/html/2401.09042v1)       |
| Truncated       | False       |
| Word Count       | 14609       |