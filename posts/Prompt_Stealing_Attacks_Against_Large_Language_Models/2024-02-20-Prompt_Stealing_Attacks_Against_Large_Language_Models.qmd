
---
title: "Prompt Stealing Attacks Against Large Language Models"
id: "2402.12959v1"
description: "TL;DR: Proposed prompt stealing attack aims to steal well-designed prompts from large language models."
author: Zeyang Sha, Yang Zhang
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.12959v1/x1.png"
categories: ['robustness', 'security', 'education', 'hci', 'prompt-engineering', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12959v1/x1.png)

### **Summary:**
- The article introduces prompt stealing attacks against large language models (LLMs) and proposes a novel attack to steal well-designed prompts based on the generated answers.
- The proposed prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstructor, which work together to predict the properties of the original prompts and generate reversed prompts similar to the original prompts.
- Experimental results show the remarkable performance of the proposed attacks, with the parameter extractor successfully predicting the type of the original prompts and the prompt reconstructor generating more similar reversed prompts.

### **Major Findings:**
1. The proposed prompt stealing attacks aim to steal well-designed prompts based on the generated answers, containing two primary modules: the parameter extractor and the prompt reconstructor.
2. The parameter extractor successfully predicts the type of the original prompts, and the prompt reconstructor generates more similar reversed prompts.
3. Experimental results show the remarkable performance of the proposed attacks, with the parameter extractor successfully predicting the type of the original prompts and the prompt reconstructor generating more similar reversed prompts.

### **Analysis and Critique:**
- The article provides a comprehensive exploration of prompt stealing attacks against LLMs, highlighting the vulnerability of prompts to being stolen based on the generated answers.
- The proposed attacks demonstrate strong performance in predicting the properties of the original prompts and generating reversed prompts, raising concerns about the security of LLMs.
- The article lacks a discussion of potential defenses against prompt stealing attacks, and further research is needed to explore more effective defense strategies to mitigate these attacks. Additionally, the trade-off between defense performance and utility needs to be further addressed.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.12959v1](https://arxiv.org/abs/2402.12959v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12959v1](https://browse.arxiv.org/html/2402.12959v1)       |
| Truncated       | False       |
| Word Count       | 10539       |