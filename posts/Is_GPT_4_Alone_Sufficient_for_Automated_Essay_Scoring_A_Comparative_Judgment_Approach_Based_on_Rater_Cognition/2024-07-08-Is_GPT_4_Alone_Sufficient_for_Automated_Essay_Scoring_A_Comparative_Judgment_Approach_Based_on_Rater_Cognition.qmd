
---
title: "Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition"
id: "2407.05733v1"
description: "LLMs with Comparative Judgment outperform traditional rubric-based scoring in AES."
author: Seungju Kim, Meounggun Jo
date: "2024-07-08"
image: "https://browse.arxiv.org/html/2407.05733v1/x2.png"
categories: ['social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.05733v1/x2.png)

### Summary:

This study explores a novel approach to utilizing Large Language Models (LLMs) for Automated Essay Scoring (AES) by employing Comparative Judgment (CJ). The proposed method prompts LLMs to choose the better essay between two given essays without any additional training, using only zero-shot prompting. The research aims to address four main questions:

1. When using a rubric-based scoring strategy, will the GPT-4 model be able to better imitate human-rater scores compared to the GPT-3.5 model?
2. When using a rubric-based scoring strategy, will GPT models be able to better imitate human rater’s scores if an elaborated scoring rubric with descriptors is used?
3. When using a CJ-based scoring strategy, will the GPT model be able to better imitate human rater scores compared to the rubric-based scoring strategy?
4. When using a CJ-based scoring strategy and utilizing fine-grained scores, will GPT models be able to better imitate human rater scores?

The study uses essay sets 7 and 8 from the ASAP dataset, which include multiple raters’ scores and analytical scoring based on 4 and 6 traits, respectively. The LLM models used for inference are the GPT-3.5 model and the GPT-4 model, both developed by OpenAI.

### Major Findings:

1. The GPT-4 model demonstrated substantially better performance compared to GPT-3.5, except for traits 5 and 6 of Essay Set 8, where performance decreased.
2. When using elaborated rubrics with descriptors, the GPT-3.5 model showed an increase in the average QWK values across traits compared to the Basic-type rubric. However, under the GPT-4 model condition, some traits exhibited either no difference or even a decrease in QWK values.
3. Under the CJ-based scoring condition, the average QWK values were 0.573 for GPT-3.5 and 0.674 for GPT-4, representing performance improvements of approximately 30.8% and 18.9%, respectively, compared to

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.05733v1](https://arxiv.org/abs/2407.05733v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.05733v1](https://browse.arxiv.org/html/2407.05733v1)       |
| Truncated       | False       |
| Word Count       | 6518       |