
---
title: "Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model"
id: "2402.07827v1"
description: "Aya is a multilingual language model outperforming others in 101 languages."
author: Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, Sara Hooker
date: "2024-02-12"
image: "../../img/2402.07827v1/image_1.png"
categories: ['production', 'architectures', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.07827v1/image_1.png)

### Summary:
The academic article introduces the Aya model, a multilingual generative language model that follows instructions in 101 languages, with a focus on lower-resourced languages. The Aya model outperforms mT0 and BLOOMZ on most tasks while covering double the number of languages. The authors introduce new evaluation suites that broaden the state-of-the-art for multilingual evaluation across 99 languages. They also conduct investigations on the optimal finetuning mixture composition, data pruning, toxicity, bias, and safety of the models. The authors aim to reduce linguistic inequality and expand language coverage, releasing the Aya model with diverse linguistic representation. Additionally, the article discusses the translation of RealToxicityPrompts (RTP) into multiple languages and the evaluation of toxicity in prompts in different languages. It evaluates the models' ability to detect toxicity in text across languages on the Jigsaw and CivilComments datasets. The Aya model is a massively multilingual LLM that is open-source and instruction-finetuned on 101 languages.

### Major Findings:
1. The Aya model outperforms mT0 and BLOOMZ on most tasks while covering double the number of languages.
2. New evaluation suites broaden the state-of-the-art for multilingual evaluation across 99 languages.
3. Certain languages consistently index as higher toxicity when the same set of English prompts is translated into their language. Instruction-tuned models outperform solely pretrained base models in detecting toxicity across languages. The Aya model vastly improves over other open-source models based on various evaluations and is instruction-finetuned on 101 languages.

### Analysis and Critique:
- The article provides valuable insights into the challenges and strategies for creating diverse and high-quality datasets for instruction-following tasks.
- The expansion of language coverage and the comparison of model performance against baselines provide valuable insights into the effectiveness of different finetuning mixtures and their impact on model performance.
- The findings highlight the importance of evaluating toxicity in multilingual prompts and the potential biases in model outputs against certain identity groups.
- The effectiveness of instruction-tuned models in detecting toxicity across languages and the impact of safety mitigation on the model's performance are significant.
- The details about the Aya model, including its training data, evaluation, potential biases, and limitations, are essential for understanding its capabilities and potential implications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.07827v1](https://arxiv.org/abs/2402.07827v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07827v1](https://browse.arxiv.org/html/2402.07827v1)       |
| Truncated       | True       |
| Word Count       | 107685       |