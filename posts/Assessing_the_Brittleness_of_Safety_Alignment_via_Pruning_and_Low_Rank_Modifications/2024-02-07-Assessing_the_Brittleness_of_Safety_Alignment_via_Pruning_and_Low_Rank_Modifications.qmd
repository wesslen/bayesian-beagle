
---
title: "Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications"
id: "2402.05162v1"
description: "LLMs have safety vulnerabilities, critical regions are sparse, and more robust safety strategies are needed."
author: Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, Peter Henderson
date: "2024-02-07"
image: "https://browse.arxiv.org/html/2402.05162v1/x1.png"
categories: ['security', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.05162v1/x1.png)

### **Summary:**
- The study explores the brittleness of safety alignment in large language models (LLMs) by leveraging pruning and low-rank modifications.
- Critical regions vital for safety guardrails are identified and disentangled from utility-relevant regions at both the neuron and rank levels.
- The isolated regions found are sparse, comprising about  at the parameter level and  at the rank level.
- Removing these regions compromises safety without significantly impacting utility, highlighting the urgent need for more robust safety strategies in LLMs.

### Major Findings:
1. Safety-critical regions are sparse and can be effectively isolated via set difference or orthogonal projection.
2. Removing safety-critical neurons or ranks severely compromises utility.
3. Pruning least safety-relevant neurons or ranks improves safety.

### Analysis and Critique:
- The study provides valuable insights into the brittleness of safety alignment in LLMs, but it is limited by the availability of strong safety-aligned models for experimentation.
- The findings suggest potential future directions for improving safety robustness and call for the development of inherently safer models with less sparse and easily isolated safety-critical regions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.05162v1](https://arxiv.org/abs/2402.05162v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.05162v1](https://browse.arxiv.org/html/2402.05162v1)       |
| Truncated       | False       |
| Word Count       | 12487       |