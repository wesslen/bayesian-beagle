
---
title: "No Such Thing as a General Learner: Language models and their dual optimization"
id: "2408.09544v1"
description: "LLMs, optimized during training and selection, don't settle debates on human cognitive biases in language acquisition."
author: Emmanuel Chemla, Ryan M. Nefdt
date: "2024-08-18"
image: "../../../bayesian-beagle.png"
categories: ['hci', 'social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

# Summary:

The article "No Such Thing as a General Learner: Language models and their dual optimization" discusses the role of Large Language Models (LLMs) in understanding human cognition and language acquisition. The authors argue that neither humans nor LLMs are general learners, as they follow a dual-optimization process. LLMs are optimized during their training and have also been selected through a process akin to natural selection. This perspective challenges the idea that LLMs' performance can easily weigh on debates about the importance of human cognitive biases for language.

## Major Findings:

1. LLMs are not unbiased or unconstrained learners, as there is no such thing as a general learner. Their power comes from their specialization, which allows them to make inductive leaps and go beyond their data via opinionated learning.
2. Modern LLMs are engineering devices optimized for specific tasks, and their evolution-like optimization has been accelerated by the human hand for specific goals. They are not vanilla, off-the-shelf learners and cannot be used to dismiss a nativist approach to language learning.
3. LLMs cannot be considered general learners under any reasonable interpretation of the term. This reduces the impact that a comparison between humans and LLMs can have and how this can inform debates from linguistics and cognitive sciences.

## Analysis and Critique:

The article provides a novel perspective on the role of LLMs in understanding human cognition and language acquisition. However, it raises several questions and potential limitations:

1. The authors argue that LLMs are not general learners, but they do not provide a clear definition of what a general learner is. This lack of clarity makes it difficult to evaluate their argument.
2. The authors claim that LLMs are optimized for specific tasks, but they do not provide empirical evidence to support this claim. It would be helpful to see some examples of how LLMs have been optimized for specific tasks and how this optimization has affected their performance.
3. The authors argue that LLMs cannot be used to dismiss a nativist approach to language learning, but they do not provide a clear explanation of why this is the case. It would be helpful to see a more detailed discussion of the relationship between LLMs and nativist theories of language acquisition

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.09544v1](https://arxiv.org/abs/2408.09544v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.09544v1](https://browse.arxiv.org/html/2408.09544v1)       |
| Truncated       | False       |
| Word Count       | 9197       |