
---
title: "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale"
id: "2406.19280v1"
description: "PubMedVision dataset improves medical multimodal capabilities of MLLMs, outperforming other data construction methods."
author: Junying Chen, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang
date: "2024-06-27"
image: "https://browse.arxiv.org/html/2406.19280v1/x1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.19280v1/x1.png)

# Summary:

The paper introduces a new dataset, PubMedVision, which aims to improve the medical multimodal capabilities of multimodal large language models (MLLMs) like GPT-4V. The dataset consists of 1.3 million medical VQA samples, created by refining medical image-text pairs from PubMed and employing MLLMs to denoise and reformat the data. The authors demonstrate that PubMedVision significantly enhances the medical multimodal capabilities of current MLLMs and outperforms other data construction methods in terms of data quality. The paper also presents a 34B medical MLLM, HuatuoGPT-Vision, trained on PubMedVision, which shows superior performance in medical multimodal scenarios among open-source MLLMs.

# Major Findings:

1. PubMedVision, a dataset containing 1.3 million medical VQA samples, was created by refining medical image-text pairs from PubMed and employing MLLMs to denoise and reformat the data.
2. The dataset significantly enhances the medical multimodal capabilities of current MLLMs, showing significant improvement in benchmarks, including the MMMU Health & Medicine track.
3. Manual checks by medical experts and empirical results validate the superior data quality of PubMedVision compared to other data construction methods.
4. HuatuoGPT-Vision, a 34B medical MLLM trained on PubMedVision, demonstrates superior performance in medical multimodal scenarios among open-source MLLMs.

# Analysis and Critique:

* The paper presents a novel approach to improving the medical multimodal capabilities of MLLMs by creating a high-quality dataset, PubMedVision. The authors demonstrate the effectiveness of their method through various experiments and comparisons with existing datasets and models.
* The use of MLLMs to denoise and reformat the data is a significant contribution, as it addresses the limitations of existing methods that rely on text-only LLMs or manual reformatting.
* The creation of HuatuoGPT-Vision, a 34B medical MLLM trained on PubMedVision, further highlights the potential of the proposed dataset in advancing the field of medical

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.19280v1](https://arxiv.org/abs/2406.19280v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.19280v1](https://browse.arxiv.org/html/2406.19280v1)       |
| Truncated       | False       |
| Word Count       | 6836       |