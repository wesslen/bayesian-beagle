
---
title: "State of What Art? A Call for Multi-Prompt LLM Evaluation"
id: "2401.00595v1"
description: "Advances in large language models are analyzed for their evaluation, suggesting diverse prompts for more reliable assessments."
author: ['Moran Mizrahi', 'Guy Kaplan', 'Dan Malkin', 'Rotem Dror', 'Dafna Shahaf', 'Gabriel Stanovsky']
date: "2023-12-31"
image: "https://browse.arxiv.org/html/2401.00595v1/extracted/5324720/figures/swfigure12.png"
categories: ['robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.00595v1/extracted/5324720/figures/swfigure12.png)

### Major Takeaways

- **Single-prompt evaluations of large language models (LLMs) are prone to brittleness**: The paper highlights the significant impact of prompt variations on LLM performance, challenging the adequacy of single-prompt evaluations. It presents findings that demonstrate the considerable performance discrepancies resulting from minor changes in prompt formulations.

- **Proposal for multi-prompt evaluation metrics**: To address the limitations of single-prompt evaluations, the paper introduces a set of diverse **evaluation metrics** tailored to specific use cases. These metrics aim to provide a more robust and meaningful assessment of LLM capabilities by leveraging a diverse set of instruction templates for each task.

- **Significant divergence in model rankings and performance**: The paper showcases the substantial differences in both absolute performance and relative model rankings resulting from the evaluation using multiple prompt variations. This indicates the inadequacy of traditional single-prompt evaluations for capturing the true capabilities of LLMs.

### Summary of Sections

#### Introduction
- Recent advancements in large language models and the prevalent use of single instruction templates in LLM evaluations are introduced.

#### Background and Definitions
- Discussion on task instruction templates and existing evaluation benchmarks for LLMs, along with an overview of related work on prompt robustness.

#### Experimental Setup
- Description of the tasks and models evaluated in the study, including 39 diverse tasks from three evaluation benchmarks and 16 instruction-tuned LLMs from diverse model families.

#### Single-Prompt Evaluation Leads to Inconsistent Results
- Exploration of the limitations of single-prompt evaluations through statistical analysis and quantification of performance variance due to instruction template paraphrasing.

#### Different Use Cases Merit Different Metrics
- Proposal of four tailored evaluation metrics for LLMs, each suitable for specific scenarios and user needs, emphasizing the need to choose the evaluation metric based on the extrinsic needs of the evaluators.

#### Multi-Prompt Evaluation
- Presentation of an evaluation of various models according to the proposed metrics, revealing differences in model rankings depending on the metric used.

#### Small-Scale Evaluation of OpenAI Models on Prompt Paraphrasing
- Findings from a small-scale evaluation demonstrating the sensitivity of OpenAI models to prompt paraphrasing and the resultant divergence in model performance between original prompts and paraphrases.

#### Conclusions
- Emphasis on the need for nuanced evaluation methods and the proposal of alternative evaluation metrics to ensure more consistent and comparable LLM evaluations.

### Critique

- **Generalizability of Findings**: The studyâ€™s findings are based on a specific set of tasks, LLMs, and evaluation benchmarks, potentially limiting the generalizability of the results.

- **Lack of External Validation**: The study does not provide validation using external datasets or real-world scenarios to demonstrate the practical applicability of the proposed multi-prompt evaluation metrics. This raises questions about the real-world effectiveness of the proposed metrics.

- **Potential Bias in Manual Verification**: The manual verification and filtering of automatic instruction paraphrases may introduce subjective bias, impacting the robustness of the findings.

Overall, while the paper makes a compelling case for the limitations of single-prompt evaluations and proposes alternative evaluation metrics, there is a need for further validation and applicability testing to support the practical adoption of these metrics in real-world LLM evaluation scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-31       |
| Abstract | [http://arxiv.org/abs/2401.00595v1](http://arxiv.org/abs/2401.00595v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.00595v1](https://browse.arxiv.org/html/2401.00595v1)       |
| Truncated       | False       |
| Word Count       | 10053       |