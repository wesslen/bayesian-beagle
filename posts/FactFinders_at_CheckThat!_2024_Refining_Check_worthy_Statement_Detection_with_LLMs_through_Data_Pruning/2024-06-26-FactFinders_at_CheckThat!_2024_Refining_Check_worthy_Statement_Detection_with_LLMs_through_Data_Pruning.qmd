
---
title: "FactFinders at CheckThat! 2024: Refining Check-worthy Statement Detection with LLMs through Data Pruning"
id: "2406.18297v1"
description: "This study explores using open-source LLMs to identify check-worthy political statements, proposing a data pruning approach for efficient learning."
author: Yufeng Li, Rrubaa Panchendrarajan, Arkaitz Zubiaga
date: "2024-06-26"
image: "https://browse.arxiv.org/html/2406.18297v1/extracted/5693105/Distribution_of_Text_Length.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.18297v1/extracted/5693105/Distribution_of_Text_Length.png)

### Summary:

The paper presents the experiments conducted by the FactFinders team for CheckThat! 2024 task 1, check-worthiness estimation in English. The team explored eight open-source LLMs with fine-tuning and prompt engineering to identify check-worthy statements from political transcriptions. The Llama2-7b model fine-tuned on the training data secured the 1st position in the leaderboard, demonstrating the power of open-source models in check-worthy statement detection in the English language. The study also highlights the role of data pruning in identifying high-quality training data for effective learning, achieving competitive or better performance by utilizing only about 44% of training data and saving fine-tuning time in a similar proportion.

### Major Findings:

1. The Llama2-7b model fine-tuned on the training data secured the 1st position in the CheckThat! 2024 task 1 leaderboard, demonstrating the power of open-source models in check-worthy statement detection in the English language.
2. Data pruning techniques, such as a two-step data pruning approach, can help identify high-quality training data for effective learning, achieving competitive or better performance by utilizing only about 44% of training data and saving fine-tuning time in a similar proportion.
3. LLMs can be used for refining prompts and identifying informative verbs in a zero-shot setting, further enhancing their utility in check-worthy statement detection tasks.

### Analysis and Critique:

The paper presents an interesting exploration of open-source LLMs for check-worthy statement detection in the English language. The results demonstrate the potential of these models in this task, with the Llama2-7b model securing the 1st position in the leaderboard. However, the study could have benefited from a more comprehensive analysis of the performance of the other LLMs, as only the Llama models, Mistral, and Mixtral were compared during the testing phase of the competition.

The paper also highlights the importance of data pruning techniques in identifying high-quality training data for effective learning. The proposed two-step data pruning approach is a promising method for achieving competitive or better performance with a reduced training dataset. However, the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.18297v1](https://arxiv.org/abs/2406.18297v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.18297v1](https://browse.arxiv.org/html/2406.18297v1)       |
| Truncated       | False       |
| Word Count       | 7119       |