
---
title: "Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge"
id: "2407.19594v1"
description: "LLMs can self-improve by judging their own judgments, enhancing their instruction-following abilities without human supervision."
author: Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, Sainbayar Sukhbaatar
date: "2024-07-28"
image: "https://browse.arxiv.org/html/2407.19594v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.19594v1/x1.png)

### Summary:

- The paper introduces a novel Meta-Rewarding step to the self-improvement process of LLMs, where the model judges its own judgments and uses that feedback to refine its judgment skills.
- This unsupervised approach improves the model’s ability to judge and follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard.
- The proposed method, called Meta-Rewarding, assigns rewards to its own judgments to train the model’s ability to judge. It introduces a third role of meta-judge, whose task is to evaluate the model’s own judgments.
- The method also addresses the length-bias issue in the judging process by combining the judge score with length information to determine the winning response.
- The experiments show that the proposed method outperforms standard Self-Rewarding training even if it is enhanced with the length-bias improvements.

### Major Findings:

1. The Meta-Rewarding method improves the model’s ability to judge and follow instructions, as demonstrated by a significant win rate improvement on AlpacaEval 2 and Arena-Hard benchmarks.
2. The method introduces a novel meta-judge role, which enables the model to build training data containing preference pairs of judgments, in addition to the standard preferences between actor responses.
3. The proposed method addresses the length-bias issue in the judging process by combining the judge score with length information to determine the winning response.

### Analysis and Critique:

- The paper presents a promising approach to improving the self-improvement process of LLMs by introducing a meta-judge role and addressing the length-bias issue.
- The experimental results demonstrate the effectiveness of the proposed method in improving the model’s ability to judge and follow instructions.
- However, the paper does not provide a detailed analysis of the impact of the meta-judge role on the model’s performance, which could be an interesting direction for future work.
- Additionally, the paper does not discuss the potential limitations or challenges of the proposed method, such as

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.19594v1](https://arxiv.org/abs/2407.19594v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.19594v1](https://browse.arxiv.org/html/2407.19594v1)       |
| Truncated       | False       |
| Word Count       | 7667       |