
---
title: "Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data"
id: "2402.17644v1"
description: "Introducing QRData benchmark to evaluate Large Language Models' quantitative reasoning on real-world data."
author: Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17644v1/x1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17644v1/x1.png)

### Summary:
- The article introduces the Quantitative Reasoning with Data (QRData) benchmark to evaluate Large Language Models' (LLMs) capability in statistical and causal reasoning with real-world data.
- The benchmark consists of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers, as well as an auxiliary set of 290 text-only questions (QRText).
- The strongest model GPT-4 achieves an accuracy of 58%, with open-source models achieving lower accuracy.
- Models encounter difficulties in data analysis and causal reasoning, struggling to integrate causal knowledge with provided data.

### Major Findings:
1. The QRData benchmark evaluates LLMs' quantitative reasoning abilities on data and text, revealing that models struggle with data analysis and causal reasoning.
2. The best-performing model, GPT-4, achieves an accuracy of 58%, indicating a large room for improvement in LLMs' quantitative reasoning abilities.
3. Models face challenges in integrating causal knowledge with provided data, resulting in difficulties in causal reasoning.

### Analysis and Critique:
- Models encounter difficulties in data analysis and causal reasoning, with the best-performing model achieving an accuracy of 58%.
- The article highlights the limitations of current LLMs in conducting quantitative reasoning with data, emphasizing the need for more specialized methods to enhance their abilities.
- The benchmark is limited to English questions and data, and the article calls for future benchmarks in a broader domain and other languages.
- An error analysis of the best-performing model, GPT-4, reveals that model failures often originate from data analysis, knowledge recall, and calculation.

Overall, the article provides valuable insights into the challenges and limitations of LLMs in conducting quantitative reasoning with data, emphasizing the need for further research and specialized methods to enhance their abilities.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.17644v1](https://arxiv.org/abs/2402.17644v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17644v1](https://browse.arxiv.org/html/2402.17644v1)       |
| Truncated       | False       |
| Word Count       | 7187       |