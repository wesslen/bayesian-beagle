
---
title: "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data"
id: "2408.06273v1"
description: "FuxiTranyu: Open-source multilingual LLM with competitive performance on multilingual benchmarks, available for further research."
author: Haoran Sun, Renren Jin, Shaoyang Xu, Leiyu Pan, Supryadi, Menglong Cui, Jiangcun Dui, Yikun Lei, Lei Yang, Ling Shi, Juesi Xiao, Shaolin Zhu, Deyi Xiong
date: "2024-08-12"
image: "https://browse.arxiv.org/html/2408.06273v1/extracted/5787567/content/fig/distribution.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.06273v1/extracted/5787567/content/fig/distribution.png)

### Summary:

FuxiTranyu is a multilingual large language model with 8 billion parameters, trained from scratch on a balanced multilingual dataset covering 43 natural languages and 16 programming languages. The model aims to address the need for balanced and high-performing multilingual capabilities in the research community. In addition to the base model, two instruction-tuned models are developed: FuxiTranyu-8B-SFT and FuxiTranyu-8B-DPO. Extensive experiments demonstrate the competitive performance of FuxiTranyu against existing multilingual LLMs. Interpretability analyses suggest that FuxiTranyu is able to learn consistent multilingual representations across different languages.

### Major Findings:

1. FuxiTranyu is a multilingual large language model with 8 billion parameters, trained from scratch on a balanced multilingual dataset covering 43 natural languages and 16 programming languages.
2. Two instruction-tuned models are developed: FuxiTranyu-8B-SFT and FuxiTranyu-8B-DPO.
3. Extensive experiments demonstrate the competitive performance of FuxiTranyu against existing multilingual LLMs.
4. Interpretability analyses suggest that FuxiTranyu is able to learn consistent multilingual representations across different languages.

### Analysis and Critique:

* The paper does not provide a detailed comparison of FuxiTranyu with other multilingual LLMs in terms of performance on specific tasks or benchmarks.
* The paper does not discuss the potential limitations or biases of the model, such as the quality and diversity of the pre-training data or the impact of the model's size on its performance.
* The paper does not provide a detailed analysis of the model's performance on low-resource languages or its ability to handle code-switching or other linguistic phenomena.
* The paper does not discuss the potential applications or use cases of the model, such as its potential impact on natural language processing research or its potential for commercial or industrial use.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.06273v1](https://arxiv.org/abs/2408.06273v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.06273v1](https://browse.arxiv.org/html/2408.06273v1)       |
| Truncated       | False       |
| Word Count       | 9897       |