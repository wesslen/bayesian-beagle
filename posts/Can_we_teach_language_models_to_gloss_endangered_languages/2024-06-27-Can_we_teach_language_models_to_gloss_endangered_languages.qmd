
---
title: "Can we teach language models to gloss endangered languages?"
id: "2406.18895v1"
description: "LLMs can generate interlinear glossed text with in-context learning, outperforming transformer baselines without training, but still lag behind supervised systems."
author: Michael Ginn, Mans Hulden, Alexis Palmer
date: "2024-06-27"
image: "https://browse.arxiv.org/html/2406.18895v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.18895v1/x1.png)

### Summary:

This paper explores the use of large language models (LLMs) for generating interlinear glossed text (IGT) in endangered languages. The authors investigate the effectiveness of LLMs in producing IGT without any traditional training, focusing on in-context learning. They propose new approaches for selecting examples to provide in-context and observe that targeted selection can significantly improve performance. The study finds that LLM-based methods outperform standard transformer baselines, despite requiring no training. However, they still underperform state-of-the-art supervised systems for the task. The proposed approaches are highly practical for researchers outside the NLP community, requiring minimal effort to use.

### Major Findings:

1. LLMs can be effective at generating IGT in endangered languages using in-context learning, without any traditional training.
2. Targeted selection of examples to provide in-context can significantly improve performance.
3. LLM-based methods outperform standard transformer baselines, despite requiring no training.
4. LLM-based methods still underperform state-of-the-art supervised systems for the task.
5. The proposed approaches are highly practical for researchers outside the NLP community, requiring minimal effort to use.

### Analysis and Critique:

The paper presents an interesting approach to generating IGT in endangered languages using LLMs. The authors' findings suggest that LLMs can be effective at this task, even without traditional training. However, the study has some limitations.

First, the authors do not provide a detailed comparison of the performance of LLM-based methods with state-of-the-art supervised systems. While they mention that LLM-based methods underperform these systems, they do not provide a quantitative comparison.

Second, the authors do not discuss the potential biases or limitations of LLMs in generating IGT. For example, LLMs may struggle with languages that have limited data or are not well-represented in their training data.

Third, the authors do not discuss the potential ethical implications of using LLMs for generating IGT. For example, there may be concerns about the accuracy and reliability of the generated IGT, particularly if it is used for research or language documentation purposes.

Overall, the paper presents an interesting approach to generating IGT in

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.18895v1](https://arxiv.org/abs/2406.18895v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.18895v1](https://browse.arxiv.org/html/2406.18895v1)       |
| Truncated       | False       |
| Word Count       | 6433       |