
---
title: "LocMoE: A Low-overhead MoE for Large Language Model Training"
id: "2401.13920v1"
description: "MoE model for language models is improved with a new routing strategy, reducing training time without sacrificing accuracy."
author: ['Jing Li', 'Zhijie Sun', 'Xuan He', 'Li Zeng', 'Yi Lin', 'Entong Li', 'Binfan Zheng', 'Rongqian Zhao', 'Xin Chen']
date: "2024-01-25"
image: "https://browse.arxiv.org/html/2401.13920v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.13920v1/x1.png)

### **Summary:**

The article introduces **LocMoE**, a low-overhead routing strategy for large language model (LLM) training, aiming to alleviate the performance issues of the widespread Mixtures-of-Experts (MoE) model. The MoE model is favored for its ability to efficiently expand model capacity while controling computational overhead. However, it faces challenges related to load imbalance, communication latency, and redundant computation due to large expert capacity. The authors propose a novel routing strategy that combines load balance and locality, effectively reducing training time without compromising model accuracy. The proposed strategy is applied to the PanGu- model within the MindSpore framework and experiment results demonstrate significant reductions in training time per epoch.

### **Major Findings:**
1. The proposed **LocMoE** reduces training time per epoch by 12.68% to 22.24% compared to classical routers, such as hash router and switch router, without impacting the model accuracy.
  
2. Through the introduction of **orthogonal gating weight with Global Average Pooling (GAP) layer**, the authors were able to not only reduce computational costs but also facilitate explicit routing decisions.

3. The research identified and solved the **critical value of MoEâ€™s expert capacity**, showcasing that the reduction of expert capacity within the critical limit does not compromise model accuracy.

### **Analysis and Critique:**
The article presents an innovative approach to address the limitations of MoE models in large language model training. The proposed LocMoE strategy shows significant promise in reducing training time without sacrificing model accuracy. However, the article heavily focuses on technical and methodological aspects, potentially making it challenging for individuals without a deep understanding of language model training to grasp the significance of the findings. Additionally, the article lacks a comprehensive discussion on the broader implications of the proposed approach and its potential impact on the field of natural language processing. Despite the promising experimental results, comprehensive real-world applicability and scalability tests are necessary to validate the effectiveness of the proposed LocMoE strategy. Moreover, the article would benefit from a deeper discussion of potential limitations, biases, and challenges faced during the experimental setup and model training process. Overall, while the article presents promising findings, further exploration and validation are necessary to establish the broad impact and effectiveness of the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [http://arxiv.org/abs/2401.13920v1](http://arxiv.org/abs/2401.13920v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.13920v1](https://browse.arxiv.org/html/2401.13920v1)       |
| Truncated       | False       |
| Word Count       | 8893       |