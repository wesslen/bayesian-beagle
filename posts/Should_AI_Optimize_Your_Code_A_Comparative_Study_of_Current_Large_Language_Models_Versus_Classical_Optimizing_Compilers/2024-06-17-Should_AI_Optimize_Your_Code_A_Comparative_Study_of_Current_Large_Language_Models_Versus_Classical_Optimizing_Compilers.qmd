
---
title: "Should AI Optimize Your Code? A Comparative Study of Current Large Language Models Versus Classical Optimizing Compilers"
id: "2406.12146v1"
description: "LLMs, like CodeLlama-70B, show potential in code optimization, but may generate incorrect code on large sizes, requiring automated verification. CETUS is the top optimizing compiler, achieving 1.9x speedup. No significant difference found between CoT and IP prompting methods."
author: Miguel Romero Rosas, Miguel Torres Sanchez, Rudolf Eigenmann
date: "2024-06-17"
image: "https://browse.arxiv.org/html/2406.12146v1/extracted/5674038/images/diagram.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.12146v1/extracted/5674038/images/diagram.png)

### Summary:

This paper presents a comparative analysis between two state-of-the-art Large Language Models (LLMs), GPT-4.0 and CodeLlama-70B, and traditional optimizing compilers, assessing their respective abilities and limitations in optimizing code for maximum efficiency. The study introduces a benchmark suite of challenging optimization patterns and an automatic mechanism for evaluating performance and correctness of the code generated by such tools. The results show that while LLMs have the potential to outperform current optimizing compilers, they often generate incorrect code on large code sizes, calling for automated verification methods. CodeLlama-70B is the superior optimizer among the two LLMs, capable of achieving speedups of up to 2.1x, while CETUS is the best among the optimizing compilers, achieving a maximum speedup of 1.9x.

### Major Findings:

1. LLMs have the potential to outperform current optimizing compilers in code optimization, but they often generate incorrect code on large code sizes, requiring automated verification methods.
2. CodeLlama-70B is the superior optimizer among the two LLMs, capable of achieving speedups of up to 2.1x.
3. CETUS is the best among the optimizing compilers, achieving a maximum speedup of 1.9x.
4. No significant difference was found between the two prompting methods: Chain of Thought (Cot) and Instructing prompting (IP).

### Analysis and Critique:

The paper provides a comprehensive comparison between LLMs and traditional optimizing compilers, highlighting the strengths and limitations of each approach. However, the study could have benefited from a more detailed analysis of the specific optimization techniques used by each LLM and optimizing compiler. Additionally, the paper could have explored the potential for combining LLMs and traditional optimizing compilers to achieve even better results. Finally, the study could have included a more diverse set of benchmarks to better evaluate the generalizability of the findings.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.12146v1](https://arxiv.org/abs/2406.12146v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.12146v1](https://browse.arxiv.org/html/2406.12146v1)       |
| Truncated       | False       |
| Word Count       | 7663       |