
---
title: "LLM-based Federated Recommendation"
id: "2402.09959v1"
description: "LLMs enhance recommendation systems, but pose privacy risks. PPLR framework balances performance and preserves privacy."
author: Jujia Zhao, Wenjie Wang, Chen Xu, Zhaochun Ren, See-Kiong Ng, Tat-Seng Chua
date: "2024-02-15"
image: "../../../bayesian-beagle.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
The article introduces a Privacy-Preserving LLM-based Recommendation (PPLR) framework to address the challenges of exacerbated client performance imbalance and substantial client resource costs in LLM-based recommendation systems. The framework employs two primary strategies: dynamic balance strategy and flexible storage strategy to ensure relatively balanced performance across all clients and to save computational and storage resources.

### Major Findings:
1. **Dynamic Balance Strategy:**
   - Involves designing dynamic parameter aggregation and learning speeds for different clients during the training phase to ensure relatively balanced performance across all clients.
   
2. **Flexible Storage Strategy:**
   - Selectively retains certain sensitive layers of the language model on the client side while offloading non-sensitive layers to the server to save computational and storage resources.

3. **Experimental Results:**
   - PPLR not only achieves a balanced performance among clients but also enhances overall system performance in a manner

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-16       |
| Abstract | [https://arxiv.org/abs/2402.09959v1](https://arxiv.org/abs/2402.09959v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09959v1](https://browse.arxiv.org/html/2402.09959v1)       |
| Truncated       | False       |
| Word Count       | 15880       |