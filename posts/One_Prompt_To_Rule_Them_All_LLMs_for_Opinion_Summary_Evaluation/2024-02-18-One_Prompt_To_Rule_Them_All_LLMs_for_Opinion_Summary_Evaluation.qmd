
---
title: "One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation"
id: "2402.11683v1"
description: "New dataset and prompts improve opinion summary evaluation, outperforming previous methods."
author: Tejpalsingh Siledar, Swaroop Nath, Sankara Sri Raghava Ravindra Muddu, Rupasai Rangaraju, Swaprava Nath, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Sudhanshu Shekhar Singh, Muthusamy Chelliah, Nikesh Garera
date: "2024-02-18"
image: "https://browse.arxiv.org/html/2402.11683v1/extracted/5416190/images/comparison.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.11683v1/extracted/5416190/images/comparison.png)

### **Summary:**
- Evaluation of opinion summaries using conventional reference-based metrics has been shown to have a relatively low correlation with human judgments.
- Large Language Models (LLMs) have been suggested as reference-free metrics for NLG evaluation, but remain unexplored for opinion summary evaluation.
- The SummEval-Op dataset has been released to address the limited opinion summary evaluation datasets and covers dimensions related to the evaluation of opinion summaries.

### Major Findings:
1. Op-I-Prompt emerges as a good alternative for evaluating opinion summaries, achieving an average Spearman correlation with humans, outperforming all previous approaches.
2. The study is the first to investigate LLMs as evaluators on both closed-source and open-source models in the opinion summarization domain.
3. Op-I-Prompt generally outperforms existing approaches on both closed-source and open-source models by a significant margin in correlation with human judgments.

### Analysis and Critique:
- The study provides valuable insights into the limitations of conventional reference-based metrics for evaluating opinion summaries.
- The release of the SummEval-Op dataset is a significant contribution to the field, addressing the lack of comprehensive opinion summary evaluation datasets.
- The study's focus on exploring LLMs as evaluators for opinion summaries on both closed-source and open-source models is a novel and important area of research.
- The limitations of the study include the lack of evaluation using GPT- due to cost constraints and the need for further investigation into the applicability of Op-I-Prompt and Op-Prompts to other tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.11683v1](https://arxiv.org/abs/2402.11683v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11683v1](https://browse.arxiv.org/html/2402.11683v1)       |
| Truncated       | False       |
| Word Count       | 7941       |