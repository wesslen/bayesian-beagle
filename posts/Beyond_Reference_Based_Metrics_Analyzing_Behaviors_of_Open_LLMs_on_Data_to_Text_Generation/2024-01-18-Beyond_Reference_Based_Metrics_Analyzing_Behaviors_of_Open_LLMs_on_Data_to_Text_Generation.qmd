
---
title: "Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation"
id: "2401.10186v1"
description: "Open large language models (LLMs) can generate coherent text from structured data, but semantic accuracy remains a major issue."
author: ['Zdeněk Kasner', 'Ondřej Dušek']
date: "2024-01-18"
image: "https://browse.arxiv.org/html/2401.10186v1/x1.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.10186v1/x1.png)

### Summary:
The article explores the use of open large language models (LLMs) in generating coherent and relevant text from structured data in data-to-text (D2T) generation tasks. The authors introduce Quintd-1, a benchmark for five D2T generation tasks using structured data records from public APIs and leverage reference-free evaluation metrics and LLMs' in-context learning capabilities. The findings suggest that open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, semantic accuracy of the outputs remains a major issue, with 80% of outputs containing a semantic error according to human annotators. The authors also provide insights into experimental processes, model selection, observations from preliminary experiments, final experiments, and evaluation strategies.

### Major Findings:
1. Open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings.
2. Semantic accuracy is a major obstacle, with 80% of LLM outputs containing a semantic error according to human annotators and 91% according to the GPT-4-based metric.
3. Long data inputs cause practical issues, including the need for long-context models, increased GPU memory requirements, and unavailability of few-shot approaches.

### Analysis and Critique:
The article provides valuable insights into the use of open LLMs for D2T generation but has some limitations. The study focuses on open LLMs with 7B parameters, potentially overlooking the performance of models with different capacities. Additionally, the evaluation metrics, although innovative, may not fully capture the complexities of D2T generation tasks. The use of human annotators from crowdsourcing platforms may introduce biases, and the reliance on GPT-4 for automatic evaluation may not be universally applicable. Further research is needed to understand the generalizability of the findings and the reproducibility of the experimental setup.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [http://arxiv.org/abs/2401.10186v1](http://arxiv.org/abs/2401.10186v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.10186v1](https://browse.arxiv.org/html/2401.10186v1)       |
| Truncated       | False       |
| Word Count       | 4624       |