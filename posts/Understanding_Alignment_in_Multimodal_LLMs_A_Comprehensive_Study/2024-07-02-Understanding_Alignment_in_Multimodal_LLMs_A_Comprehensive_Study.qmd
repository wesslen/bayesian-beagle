
---
title: "Understanding Alignment in Multimodal LLMs: A Comprehensive Study"
id: "2407.02477v1"
description: "TL;DR: Combining offline and online methods improves MLLMs, BDHS aids multimodal preference data creation."
author: Elmira Amirloo, Jean-Philippe Fauconnier, Christoph Roesmann, Christian Kerl, Rinu Boney, Yusu Qian, Zirui Wang, Afshin Dehghan, Yinfei Yang, Zhe Gan, Peter Grasch
date: "2024-07-02"
image: "../../img/2407.02477v1/image_1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../img/2407.02477v1/image_1.png)

**Summary:**
This academic article focuses on the challenges of hallucination in Multimodal Large Language Models (MLLMs) and the importance of alignment in MLLMs to produce responses more closely aligned with image information. The authors introduce a novel technique called Bias-Driven Hallucination Sampling (BDHS) to address the shortcomings of previous methods. BDHS limits access in the latent space via attention masking, which more directly achieves the underlying motivation of triggering the inherent bias of the underlying language model. The study also introduces a new derivative called MMHALBench-V, which incorporates GPT-4o to provide input images as additional context for evaluating model capabilities. The results of ablation experiments for BDHS show that all BDHS ablations significantly improve performance on LLaVABench-in-the-Wild compared to the DPO baseline and POVID-style

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02477v1](https://arxiv.org/abs/2407.02477v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02477v1](https://browse.arxiv.org/html/2407.02477v1)       |
| Truncated       | True       |
| Word Count       | 29846       |