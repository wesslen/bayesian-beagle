
---
title: "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering"
id: "2402.09911v1"
description: "Framework combines Pseudo-Graph Generation and Atomic Knowledge Verification to enhance Large Language Models."
author: Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao
date: "2024-02-15"
image: "https://browse.arxiv.org/html/2402.09911v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.09911v1/x1.png)

### **Summary:**
- Large Language Models (LLMs) face issues of hallucination and lack of specific domain knowledge when dealing with complex problems.
- Existing methods to mitigate hallucinations in LLMs fall short of effectively addressing unknown factual hallucinations.
- A framework is proposed that combines Pseudo-Graph Generation and Atomic Knowledge Verification to enhance LLMs for open-ended question answering.
- The proposed framework yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions and a minimum accuracy improvement of 7.5 for precise questions.
- The results demonstrate the generalizability of the framework across different Knowledge Graph (KG) sources.

### Major Findings:
1. The proposed framework combines Pseudo-Graph Generation and Atomic Knowledge Verification to enhance LLMs for open-ended question answering.
2. The framework yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions and a minimum accuracy improvement of 7.5 for precise questions.
3. The results demonstrate the generalizability of the framework across different KG sources.

### Analysis and Critique:
- The proposed framework addresses the limitations of existing methods in mitigating hallucinations in LLMs and demonstrates significant improvements in open-ended question answering.
- The study provides valuable insights into the generalizability of the framework across different KG sources, indicating its potential for practical applications.
- However, the study acknowledges certain limitations, such as the possibility of errors in semantic querying and bias towards LLM's pseudo-graph during verification. Further research is needed to address these limitations and enhance the practical applicability of the framework.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.09911v1](https://arxiv.org/abs/2402.09911v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09911v1](https://browse.arxiv.org/html/2402.09911v1)       |
| Truncated       | False       |
| Word Count       | 6203       |