
---
title: "Coordination-free Decentralised Federated Learning on Complex Networks: Overcoming Heterogeneity"
id: "2312.04504v1"
description: "Decentralised Federated Learning (DFL) copes with edge computing challenges, enabling devices to train accurate models using a communication-efficient algorithm."
author: Lorenzo Valerio, Chiara Boldrini, Andrea Passarella, János Kertész, Márton Karsai, Gerardo Iñiguez
date: "2023-12-07"
image: "https://browse.arxiv.org/html/2312.04504v1/x1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.04504v1/x1.png)

### Major Takeaways

1. **Decentralized Federated Learning**: The paper addresses the challenges of coordinating decentralized, federated learning tasks in a highly pervasive and uncoordinated environment.
  
2. **Proposed Solution**: The authors propose a decentralized learning algorithm that tackles both data heterogeneity and the lack of initial coordination between devices, proving to avoid overfitting in a communication-efficient way.

3. **Experimental Results**: The proposed solution, DecDiff+VT, outperforms decentralized competitors and achieves comparable or better performance than Federated Learning with FedAvg.

### Methodology

#### Introduction

The article discusses the shift in AI from centralized to decentralized systems due to data generator attitudes and the increasing computational capabilities of edge devices.

#### Problem Description

- **Partially-Decentralized Federated Learning**: The standard Federated Learning (FL) framework involves a parameter server overseeing the entire process of multiple client edge devices.
- **Fully-Decentralized Federated Learning**: In the absence of a central controller, the article targets a highly pervasive environment where numerous devices generate data and require an efficient mechanism to collaboratively train a local model without central coordination.

#### Proposed Algorithm

- **Aggregation with DecDiff**: The authors propose an aggregation function that updates models constructively, considering the differences between models induced by heterogeneity.
- **Virtual Teacher**: The article leverages a virtual teacher mechanism for improving local training to obtain local models with better generalization capability.

### Results and Discussion

#### Experimental Settings

- **Social Network Topology**: The study considers an Erdős–Rényi graph with 50 nodes and distributed datasets across nodes using a Truncated Zipf distribution.
- **Benchmarks**: The proposed algorithm, DecDiff+VT, is compared with various benchmarks, including Centralized, Isolation, and partially and fully decentralized federated learning methods.

#### Findings

- **Performance Comparison with Non-IID Data**: The DecDiff+VT solution consistently outperforms competitors like CFA and CFA-GE, achieving faster convergence and better accuracy.
- **Ablation Analysis**: The aggregation policy DecDiff and the loss function, including the virtual teacher, significantly improve performance, especially on challenging tasks like EMNIST.
- **Test Loss Analysis and Characteristic Time**: The proposed solution, DecDiff+VT, consistently outperforms the competitors in terms of accuracy and converges faster.
- **Node-Wise Analysis**: DecDiff+VT and CFA-GE result in a more concentrated distribution of accuracy among nodes.

### Critique

The paper provides a robust analysis of the proposed solution, but it could benefit from a more detailed discussion on the limitations and potential challenges in real-world implementations of the algorithm, such as scalability and robustness to fluctuating network conditions. Additionally, the verbosity and technical jargon may pose challenges for non-expert readers to grasp the findings. Further simplification and contextualization of the results could enhance the accessibility of the paper.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-27       |
| Abstract | [http://arxiv.org/abs/2312.04504v1](http://arxiv.org/abs/2312.04504v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.04504v1](https://browse.arxiv.org/html/2312.04504v1)       |
| Truncated       | True       |
| Word Count       | 13550       |