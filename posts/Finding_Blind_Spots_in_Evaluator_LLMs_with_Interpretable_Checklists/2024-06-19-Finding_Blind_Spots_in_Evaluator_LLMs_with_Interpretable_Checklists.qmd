
---
title: "Finding Blind Spots in Evaluator LLMs with Interpretable Checklists"
id: "2406.13439v1"
description: "LLMs often struggle to accurately evaluate text generation in other LLMs, with shortcomings in detecting factual accuracy, coherence, and reasoning proficiency."
author: Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M. Khapra
date: "2024-06-19"
image: "https://browse.arxiv.org/html/2406.13439v1/x1.png"
categories: ['robustness', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.13439v1/x1.png)

### Summary:

- The study investigates the effectiveness of Large Language Models (LLMs) as evaluators for text generation tasks, focusing on four critical abilities: factual accuracy, instruction following, coherence in long-form writing, and reasoning proficiency.
- The proposed FBI framework introduces targeted perturbations in answers generated by LLMs to test the ability of Evaluator LLMs to detect quality drops.
- The study reveals significant shortcomings in current Evaluator LLMs, which failed to identify quality drops in over 50% of cases on average.
- Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance.
- The results underscore the unreliable nature of current Evaluator LLMs and advocate for cautious implementation in practical applications.

### Major Findings:

1. Current Evaluator LLMs have significant shortcomings, failing to identify quality drops in over 50% of cases on average.
2. Single-answer and pairwise evaluations demonstrated notable limitations, whereas reference-based evaluations showed comparatively better performance.
3. The study highlights the need for caution in implementing LLMs as evaluators in practical applications.

### Analysis and Critique:

- The study provides a comprehensive evaluation of LLMs as evaluators for text generation tasks, focusing on four critical abilities.
- The proposed FBI framework offers a novel approach to testing the effectiveness of Evaluator LLMs by introducing targeted perturbations in answers generated by LLMs.
- The findings reveal significant shortcomings in current Evaluator LLMs, which may have implications for the development and deployment of LLMs in various applications.
- However, the study is limited to three primary evaluation paradigms and does not consider multi-agent meta-evaluation or more advanced capabilities such as multilingual generation, tool usage, and planning.
- The study also acknowledges the need for further expansion of the list of perturbation categories and the exploration of more advanced capabilities in future work.
- The study adheres to ethical guidelines and licensing requirements, and the code used for evaluations and perturbation generation will be made publicly available.
- The study was supported by a generous grant from EkStep Foundation and Nilekani Philanthropies, and the authors acknowledge the contributions of various individuals

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.13439v1](https://arxiv.org/abs/2406.13439v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.13439v1](https://browse.arxiv.org/html/2406.13439v1)       |
| Truncated       | False       |
| Word Count       | 7140       |