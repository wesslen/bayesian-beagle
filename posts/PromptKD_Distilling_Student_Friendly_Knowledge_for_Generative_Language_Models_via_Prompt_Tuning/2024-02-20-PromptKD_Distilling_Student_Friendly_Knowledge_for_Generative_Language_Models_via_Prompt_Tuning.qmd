
---
title: "PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning"
id: "2402.12842v1"
description: "Advancements in large language models raise inference costs, prompting research into model compression. PromptKD achieves state-of-the-art performance."
author: Gyeongman Kim, Doohyuk Jang, Eunho Yang
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.12842v1/x1.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.12842v1/x1.png)

### Summary:
- Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression.
- Knowledge distillation (KD) is a prominent method for model compression, but research on KD for generative language models like LLMs is relatively sparse.
- To address this gap, the article proposes PromptKD, a method that utilizes prompt tuning to enable generative language models to transfer student-friendly knowledge.
- Extensive experiments on instruction-following datasets using the GPT-2 model family show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacherâ€™s parameters as prompts.

### Major Findings:
1. PromptKD achieves state-of-the-art performance in instruction-following tasks using the GPT-2 model family.
2. Distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.
3. PromptKD outperforms other knowledge distillation methods, demonstrating its efficiency and effectiveness in model compression for generative language models.

### Analysis and Critique:
- The article provides a comprehensive exploration of extracting and distilling student-friendly knowledge for generative language models, but it still has limitations in terms of its naive extraction approach.
- The method's efficiency and effectiveness are demonstrated through extensive experiments, but there is a need for expansion towards task-agnostic KD to make it applicable during the pre-training process.
- The article addresses potential ethical and social risks associated with pre-trained language models and model compression, emphasizing the need for advanced pre-training objectives and dataset collection methods to mitigate these risks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.12842v1](https://arxiv.org/abs/2402.12842v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12842v1](https://browse.arxiv.org/html/2402.12842v1)       |
| Truncated       | False       |
| Word Count       | 6361       |