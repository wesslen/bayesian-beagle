
---
title: "When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation"
id: "2402.11457v1"
description: "LLMs struggle with knowledge boundaries, but enhancing perception reduces overconfidence and improves performance."
author: Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng
date: "2024-02-18"
image: "../../img/2402.11457v1/image_1.png"
categories: ['robustness', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11457v1/image_1.png)

### Summary:
- Large Language Models (LLMs) have difficulty perceiving their knowledge boundaries and tend to be overconfident in their answers.
- Retrieval Augmentation (RA) has been studied to mitigate LLMs' overconfidence, but it may not be optimal to conduct RA all the time.
- The study proposes methods to enhance LLMs' perception of knowledge boundaries and shows that they are effective in reducing overconfidence.

### Major Findings:
1. LLMs exhibit overconfidence in their answers, leading to unsatisfactory alignment between their claims of knowledge and actual performance.
2. LLMs tend to rely on external documents when expressing uncertainty about a question, and the more uncertain they are, the more they leverage the supporting retrieved documents.
3. Methods aimed at urging LLMs to be prudent and enhancing QA performance have been proposed, showing effectiveness in reducing overconfidence and enhancing alignment.

### Analysis and Critique:
- The study provides valuable insights into LLMs' perception of knowledge boundaries and the effectiveness of adaptive retrieval augmentation.
- However, the study does not address the potential biases in the training data of the models, which may have influenced their overconfidence.
- Further research is needed to explore LLMs' perception of different types of knowledge boundaries and to address the limitations of the proposed methods.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.11457v1](https://arxiv.org/abs/2402.11457v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11457v1](https://browse.arxiv.org/html/2402.11457v1)       |
| Truncated       | False       |
| Word Count       | 13234       |