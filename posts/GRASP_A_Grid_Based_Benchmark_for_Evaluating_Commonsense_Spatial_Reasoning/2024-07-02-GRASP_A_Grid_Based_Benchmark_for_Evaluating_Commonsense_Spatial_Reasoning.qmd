
---
title: "GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial Reasoning"
id: "2407.01892v1"
description: "LLMs like GPT-3.5-Turbo and GPT-4o struggle with satisfactory solutions in spatial reasoning tasks, as shown by the GRASP benchmark."
author: Zhisheng Tang, Mayank Kejriwal
date: "2024-07-02"
image: "https://browse.arxiv.org/html/2407.01892v1/extracted/5704210/benchmark_construction.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.01892v1/extracted/5704210/benchmark_construction.png)

### Summary:

The paper introduces a novel benchmark called GRASP, which evaluates the commonsense spatial reasoning (CSR) abilities of large language models (LLMs) within a structured grid environment. Unlike previous spatial commonsense datasets and benchmarks, GRASP emphasizes practical applications of spatial reasoning, investigating the ability to use and reason through spatial information commonsensically. The benchmark consists of 16,000 grid-based environments, each with an agent tasked with an energy collection problem. The environments include various patterns of energy arrangement, obstacle configurations, and agent constraints. The experimental results indicate that even advanced LLMs struggle to consistently achieve satisfactory solutions.

### Major Findings:

1. GRASP is a large-scale benchmark consisting of 16,000 grid-based environments, each with an agent tasked with an energy collection problem. The environments include various patterns of energy arrangement, obstacle configurations, and agent constraints.
2. GRASP focuses on evaluating the CSR abilities of LLMs, emphasizing practical applications of spatial reasoning rather than interpreting text-based spatial descriptions.
3. The experimental results indicate that even advanced LLMs, such as GPT-3.5-Turbo and GPT-4o, struggle to consistently achieve satisfactory solutions in the GRASP benchmark.

### Analysis and Critique:

The GRASP benchmark provides a valuable contribution to the evaluation of CSR abilities in LLMs. By focusing on practical applications of spatial reasoning, GRASP addresses a gap in existing benchmarks that primarily evaluate the interpretation of text-based spatial descriptions. However, the benchmark has some limitations. For instance, the synthetic nature of the grid environments may not fully capture the complexity of real-world CSR tasks. Additionally, the benchmark does not consider the multi-modal nature of many real-world tasks where abundant visual information is available. Furthermore, the evaluation was limited to zero-shot prompting methods for LLMs, potentially underestimating their true capabilities. Future work could address these limitations by expanding the benchmark to include more dynamic environments, integrating datasets that combine visual and textual spatial information, and investigating more sophisticated prompting methods.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.01892v1](https://arxiv.org/abs/2407.01892v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.01892v1](https://browse.arxiv.org/html/2407.01892v1)       |
| Truncated       | False       |
| Word Count       | 11185       |