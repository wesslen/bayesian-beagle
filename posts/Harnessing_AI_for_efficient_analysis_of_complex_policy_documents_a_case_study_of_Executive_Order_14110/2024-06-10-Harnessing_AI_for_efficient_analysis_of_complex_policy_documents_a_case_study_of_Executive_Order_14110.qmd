
---
title: "Harnessing AI for efficient analysis of complex policy documents: a case study of Executive Order 14110"
id: "2406.06657v1"
description: "AI systems Gemini 1.5 Pro and Claude 3 Opus excel in policy document analysis, rivaling human experts in accuracy but with greater efficiency."
author: Mark A. Kramer, Allen Leavens, Alexander Scarlat
date: "2024-06-10"
image: "../../img/2406.06657v1/image_1.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](../../img/2406.06657v1/image_1.png)

**Summary:**

This study investigates the accuracy and reliability of large language model (LLM)-based AI systems in extracting information from complex policy documents, such as Executive Order 14110. The research focuses on question answering and tasks involving content extraction, comparing the performance of four commercial AI systems (Claude 3 Opus, ChatGPT-4, Gemini Pro 1.5, and Command R+) to manual analysis conducted by human experts. The results show that Gemini and Claude demonstrated the most comprehensive understanding of the EO, consistently providing concise, accurate, and detailed responses. However, achieving acceptable levels of reproducibility and trustworthiness remains a critical challenge that necessitates further research and development.

**Major Findings:**

1. Gemini and Claude demonstrated the most comprehensive understanding of the EO, consistently providing concise, accurate, and detailed responses.
2. Gemini demonstrated retrieval and precision commensurate with human levels of performance, but much faster, accomplishing tasks that took human reviewers 4 hours in a few minutes.
3. Cohere showed potential but was not able to achieve the same level of accuracy as Gemini and Claude.
4. GPT4, in its current state, appears less suitable for policy analysis tasks demanding precision and faithfulness to source material.

**Analysis and Critique:**

The study provides valuable insights into the potential of AI in policy analysis, but there are several limitations to consider:

1. The research was limited to a single case study, which may not generalize to all types of policy documents.
2. Larger, multiple-document corpora, particularly those that exceed current context window sizes, would provide a different test of AI systems' capabilities and limitations.
3. The study focused only on question answering and tasks involving content extraction from policy documents, not summarization, interpretation, impact, or other analyses.
4. The study did not investigate the potential of teaming between human analysts and AI systems, which could potentially lead to better results than either could achieve alone.
5. Only four commercial AI systems were evaluated, and the study is a snapshot of one point in time in a rapidly-evolving field.

Further research could involve testing other AI models, including open-source alternatives, mixture-of-

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.06657v1](https://arxiv.org/abs/2406.06657v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06657v1](https://browse.arxiv.org/html/2406.06657v1)       |
| Truncated       | False       |
| Word Count       | 25409       |