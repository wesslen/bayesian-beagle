
---
title: "Large Language Models as Evaluators for Scientific Synthesis"
id: "2407.02977v1"
description: "LLMs can logically rate scientific summaries but weakly correlate with human ratings, indicating potential and limitations in evaluation."
author: Julia Evans, Jennifer D'Souza, SÃ¶ren Auer
date: "2024-07-03"
image: "../../../bayesian-beagle.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The study explores the use of Large Language Models (LLMs) like GPT-4 and Mistral in evaluating the quality of scientific summaries or syntheses, comparing their evaluations to those of human annotators. The study uses a dataset of 100 research questions and their syntheses made by GPT-4 from abstracts of five related papers, checked against human quality ratings. The study evaluates both the closed-source GPT-4 and the open-source Mistral model's ability to rate these summaries and provide reasons for their judgments. Preliminary results show that LLMs can offer logical explanations that somewhat match the quality ratings, yet a deeper statistical analysis shows a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.

### Major Findings:

1. LLMs can offer logical explanations for their evaluations of scientific syntheses, but these explanations only somewhat match the quality ratings provided by human annotators.
2. A deeper statistical analysis reveals a weak correlation between LLM and human ratings, suggesting the potential and current limitations of LLMs in scientific synthesis evaluation.
3. The study highlights the potential of LLMs in streamlining the evaluation process and reducing the dependency on human-generated ground truth data and human evaluators.

### Analysis and Critique:

The study provides an interesting exploration of the use of LLMs in evaluating scientific syntheses. However, the weak correlation between LLM and human ratings suggests that LLMs may not be ready to replace human evaluators in this context. The study also does not provide a detailed analysis of the reasons for this weak correlation, which could be a valuable area for future research. Additionally, the study only evaluates two LLMs, and it would be interesting to see how other LLMs perform in this task. Finally, the study does not discuss the potential biases or limitations of the LLMs used, which could impact their evaluations.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02977v1](https://arxiv.org/abs/2407.02977v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02977v1](https://browse.arxiv.org/html/2407.02977v1)       |
| Truncated       | False       |
| Word Count       | 6836       |