
---
title: "Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding"
id: "2402.16844v1"
description: "Hybrid approach combines large and small language models for efficient autoregressive decoding. Speeds up tasks with minor performance penalties."
author: Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, Babak Ehteshami Bejnordi
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16844v1/x1.png"
categories: ['prompt-engineering', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16844v1/x1.png)

### Summary:
The article proposes a hybrid approach, LLM-to-SLM, to increase the efficiency of autoregressive decoding while maintaining high performance. The method utilizes a pretrained frozen LLM to encode prompt tokens once in parallel and uses the resulting representations to condition and guide a small language model (SLM) for more efficient response generation. Experiments show substantial speedups of up to 4× with minor performance penalties for translation and summarization tasks compared to the LLM.

### Major Findings:
1. Large language models (LLMs) have become challenging to deploy due to compute limitations and latency requirements on edge devices, leading to higher costs for providers and end users.
2. The proposed LLM-to-SLM method achieves substantial speedups of up to 4× with minor performance penalties for translation and summarization tasks compared to the LLM.
3. LLM-to-SLM outperforms other parameter-efficient fine-tuning methods and matches the performance of speculative decoding while achieving a larger speedup.

### Analysis and Critique:
- The proposed method shows marginal performance penalties compared to the LLM, but there is still a larger performance gap compared to the LLM in challenging tasks such as instruction tuning.
- The method is only used once to encode the prompt, and further study is needed to explore the potential of using the LLM more frequently in more challenging tasks.
- The focus on encoder-decoder LLMs limits the scope of the study, and further investigation into incorporating decoder-only models as LLMs is needed.
- The study is limited to models with up to several billion parameters as LLMs, and further research is needed to explore the impact of using larger models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.16844v1](https://arxiv.org/abs/2402.16844v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16844v1](https://browse.arxiv.org/html/2402.16844v1)       |
| Truncated       | False       |
| Word Count       | 8973       |