
---
title: "Unsupervised Distractor Generation via Large Language Model Distilling and Counterfactual Contrastive Decoding"
id: "2406.01306v1"
description: "Unsupervised Distractor Generation method outperforms GPT-3.5-turbo with 200Ã— fewer parameters, offering a cost-effective solution for reading comprehension."
author: Fanyi Qu, Hao Sun, Yunfang Wu
date: "2024-06-03"
image: "https://browse.arxiv.org/html/2406.01306v1/extracted/5637797/figure/intro.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.01306v1/extracted/5637797/figure/intro.png)

### Summary:

The paper proposes an unsupervised Distractor Generation (DG) framework that leverages Large Language Models (LLMs) as cost-effective annotators to enhance the DG capability of smaller student models. The proposed method uses a dual task training strategy that integrates pseudo distractors from LLMs and original answer information as objective targets with a two-stage training process. Additionally, a counterfactual contrastive decoding mechanism is introduced to increase the distracting capability of the DG model. Experiments show that the proposed unsupervised generation method with Bart-base significantly outperforms GPT-3.5-turbo with only 200 fewer model parameters.

### Major Findings:

1. The proposed unsupervised DG framework with dual task training integrates the original answer information and pseudo distractors generated by LLMs.
2. A new counterfactual contrastive decoding method is devised to improve the distracting level of generated outputs, which can be transferred to other counterfactual generation tasks.
3. The proposed method greatly outperforms teacher LLMs across various evaluation metrics and provides a valuable approach for constructing reading comprehension data in diverse real-world applications, eliminating the need for costly human-annotated data and large-size models.

### Analysis and Critique:

1. The paper does not provide a detailed comparison with other unsupervised DG methods, which could help to better understand the advantages and limitations of the proposed method.
2. The proposed method relies on the availability of high-quality LLMs, which may not always be available or accessible.
3. The paper does not discuss the potential impact of the proposed method on the quality and diversity of the generated distractors, which are important factors in DG.
4. The proposed method assumes that the LLMs used for generating pseudo distractors are reliable and accurate, which may not always be the case.
5. The paper does not provide a detailed analysis of the computational cost and efficiency of the proposed method, which is an important factor in practical applications.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.01306v1](https://arxiv.org/abs/2406.01306v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.01306v1](https://browse.arxiv.org/html/2406.01306v1)       |
| Truncated       | False       |
| Word Count       | 6066       |