
---
title: "The Neglected Tails of Vision-Language Models"
id: "2401.12425v1"
description: "Vision-language models display imbalanced performance, especially with rare concepts. The proposed method measures concept frequency and improves zero-shot recognition accuracy."
author: ['Shubham Parashar', 'Zhiqiu Lin', 'Tian Liu', 'Xiangjue Dong', 'Yanan Li', 'Deva Ramanan', 'James Caverlee', 'Shu Kong']
date: "2024-01-23"
image: "https://browse.arxiv.org/html/2401.12425v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.12425v1/x1.png)

###
**Summary:**
The article focuses on the imbalanced performance of vision-language models (VLMs) in zero-shot recognition, specifically highlighting the challenges posed by long-tailed concept distributions within the VLMs' pretraining data. The authors introduce a novel method for estimating the concept frequency within VLMs' pretraining data and demonstrate a strong correlation between long-tailed concept distributions and VLMs' imbalanced performance in downstream tasks. Furthermore, the article proposes a retrieval-augmented learning (REAL) approach to mitigate VLMs' imbalanced performance in zero-shot recognition, presenting two variants: REAL-Prompt and REAL-Linear. The REAL approach significantly outperforms existing methods in zero-shot recognition while requiring significantly less storage and training time.


### Major Findings:
1. The concept frequency estimation method unveils long-tailed concept distributions in popular VLM datasets, establishing a strong correlation between long-tailed distributions and VLMsâ€™ imbalanced performance in zero-shot recognition.
2. The REAL approach, specifically REAL-Prompt, surpasses human-engineered and LLM-generated prompts over nine benchmark datasets, likely due to the usage of most frequent synonyms found in the pretraining texts. 
3. REAL-Linear outperforms the recent retrieval-augmented solution REACT using significantly less storage and fewer training resources, demonstrating exceptional efficiency and improved zero-shot recognition performance across multiple benchmarks.


### Analysis and Critique:
The article provides valuable insights into the challenges posed by long-tailed concept distributions in VLMs' pretraining data and offers innovative solutions to enhance VLMs' performance in zero-shot recognition. However, the article's estimates of concept frequencies may be limited by the absence of ground-truth annotations in the pretraining data. Additionally, the proposed approach relies heavily on textual captions, potentially overlooking the broader concept coverage offered by images. Furthermore, while the REAL approach demonstrates promising results, it may face challenges in retrieving relevant data for specific fine-grained classes. Finally, the study acknowledges the limitations and future directions, indicating the potential for further research to address these issues.

Overall, the article offers valuable contributions to addressing the imbalanced performance of VLMs in zero-shot recognition, shedding light on the prevalence of long-tail issues in VLMs and proposing effective strategies to mitigate these challenges. However, it also raises the need for future work to overcome limitations and explore applications in diverse domains, such as reducing biases and leveraging retrieval-augmented strategies with modest computing resources.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [http://arxiv.org/abs/2401.12425v1](http://arxiv.org/abs/2401.12425v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.12425v1](https://browse.arxiv.org/html/2401.12425v1)       |
| Truncated       | False       |
| Word Count       | 11477       |