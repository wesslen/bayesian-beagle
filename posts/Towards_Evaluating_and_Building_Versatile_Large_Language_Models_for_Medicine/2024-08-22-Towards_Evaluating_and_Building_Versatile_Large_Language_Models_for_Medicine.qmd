
---
title: "Towards Evaluating and Building Versatile Large Language Models for Medicine"
id: "2408.12547v1"
description: "MedS-Bench evaluates LLMs in clinical tasks; MedS-Ins dataset improves model performance. New model, MMedIns-Llama 3, outperforms existing models."
author: Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie
date: "2024-08-22"
image: "https://browse.arxiv.org/html/2408.12547v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.12547v1/x1.png)

**Summary:**

The study introduces MedS-Bench, a comprehensive benchmark for evaluating the performance of large language models (LLMs) in clinical contexts. Unlike existing benchmarks, MedS-Bench covers 11 high-level clinical tasks, including clinical report summarization, treatment recommendations, diagnosis, named entity recognition, and medical concept explanation. The authors evaluated six leading LLMs and found that even the most sophisticated models struggle with these complex tasks. To address these limitations, they developed MedS-Ins, a large-scale instruction tuning dataset for medicine, comprising 58 medically oriented language corpora, totaling 13.5 million samples across 122 tasks. A proof-of-concept experiment demonstrated the dataset's utility by performing instruction tuning on a lightweight, open-source medical language model, which significantly outperformed existing models across nearly all clinical tasks. The authors have made the MedS-Ins dataset fully accessible and launched a dynamic leaderboard for MedS-Bench to track progress and enhance the adaptation of general LLMs to the medical domain.

**Major Findings:**

1. Existing LLMs, including MEDITRON, Mistral, InternLM 2, Llama 3, GPT-4, and Claude-3.5, struggle with complex clinical tasks, even when utilizing few-shot prompting.
2. MedS-Ins, a large-scale instruction tuning dataset for medicine, was developed to address the limitations of existing models. It comprises 58 medically oriented language corpora, totaling 13.5 million samples across 122 tasks.
3. A proof-of-concept experiment demonstrated that a lightweight, open-source medical language model, fine-tuned on MedS-Ins, significantly outperformed existing models across nearly all clinical tasks.

**Analysis and Critique:**

The study provides a valuable contribution to the field by introducing a comprehensive benchmark for evaluating LLMs in clinical contexts. The authors' findings highlight the limitations of existing models in handling complex clinical tasks and underscore the need for further research in this area. The development of MedS-Ins as a large-scale instruction tuning dataset for medicine is a promising step towards addressing these limitations. However, the study's scope

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.12547v1](https://arxiv.org/abs/2408.12547v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.12547v1](https://browse.arxiv.org/html/2408.12547v1)       |
| Truncated       | False       |
| Word Count       | 14320       |