
---
title: "Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks"
id: "2406.16346v1"
description: "Fine-tuning Video-LLaVA with LORA on cooking tasks improves performance using smaller, task-specific datasets."
author: Daniel Wen, Nafisa Hussain
date: "2024-06-24"
image: "../../img/2406.16346v1/image_1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2406.16346v1/image_1.png)

### Summary:

The paper "Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks" by Daniel Wen and Nafisa Hussain proposes a new approach to fine-tune Large Vision Language Models (LVLMs) for the task of step-by-step instruction generation. The authors focus on the domain of Recipe Generation, where they fine-tune Video-LLaVA-7B to generate thorough step-by-step recipes and a list of ingredients with specific measurements for cooking videos that contain no transcripts or auditory information. The authors fine-tune each modality of Video-LLaVA on a different task related to recipe generation and cooking activities. The experiments show optimistic results in fine-tuning modalities models on distinct tasks for developing a comprehensive understanding of detailed multi-step procedures.

### Major Findings:

1. The authors propose a new approach to fine-tune LVLMs for the task of step-by-step instruction generation in the domain of Recipe Generation.
2. The authors fine-tune each modality of Video-LLaVA on a different task related to recipe generation and cooking activities.
3. The experiments show optimistic results in fine-tuning modalities models on distinct tasks for developing a comprehensive understanding of detailed multi-step procedures.

### Analysis and Critique:

The paper presents an interesting approach to fine-tuning LVLMs for the task of step-by-step instruction generation in the domain of Recipe Generation. The authors' approach of fine-tuning each modality of Video-LLaVA on a different task related to recipe generation and cooking activities is a novel idea. However, the paper does not provide a detailed analysis of the results obtained from the experiments. The authors mention that the experiments show optimistic results, but they do not provide any quantitative or qualitative analysis of the results.

Moreover, the paper does not discuss any potential limitations or shortcomings of the proposed approach. For instance, the authors do not discuss the generalizability of their approach to other domains or tasks. Additionally, the paper does not provide any comparison with other existing approaches for fine-tuning LVLMs for the task of step-by-step instruction generation.

Overall, the paper presents an

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.16346v1](https://arxiv.org/abs/2406.16346v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.16346v1](https://browse.arxiv.org/html/2406.16346v1)       |
| Truncated       | False       |
| Word Count       | 6463       |