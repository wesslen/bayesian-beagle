
---
title: "Recent Advances in Hate Speech Moderation: Multimodality and the Role of Large Models"
id: "2401.16727v1"
description: "Survey explores hate speech moderation, emphasizes role of large language and multimodal models. Identifies research gaps."
author: Ming Shan Hee, Shivam Sharma, Rui Cao, Palash Nandi, Preslav Nakov, Tanmoy Chakraborty, Roy Ka-Wei Lee
date: "2024-01-30"
image: "https://browse.arxiv.org/html/2401.16727v1/x2.png"
categories: ['social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.16727v1/x2.png)

### **Summary:**
The article provides a comprehensive survey of recent advances in hate speech (HS) moderation, focusing on the role of large language models (LLMs) and large multimodal models (LMMs). It delves into the challenges of moderating HS in the evolving landscape of online communication, particularly in the context of the multimodal nature of digital content. The survey highlights the integration of textual, visual, and auditory elements in propagating HS and emphasizes the advances facilitated by LLMs and LMMs. The article also identifies existing gaps in research, particularly in the context of underrepresented languages and cultures, and outlines potential avenues for future research, including the exploration of novel AI methodologies and the ethical governance of AI in moderation.

### Major Findings:
1. The survey reveals the nuanced interplay between textual, visual, and auditory elements in propagating HS, leading to a notable trend towards integrating these modalities.
2. The article emphasizes the pivotal role of large language models (LLMs) and large multimodal models (LMMs) in moderating HS, redefining the boundaries of detection and moderation capabilities.
3. Existing gaps in research, particularly in the context of underrepresented languages and cultures, are identified, highlighting the need for solutions to handle low-resource settings.

### Analysis and Critique:
The article provides a comprehensive overview of recent advances in HS moderation, shedding light on the challenges and opportunities in the field. However, it is important to note that the article contains offensive examples, which may be distressing for some readers. Additionally, the survey acknowledges the existing gaps in research, particularly in the context of underrepresented languages and cultures, but does not provide a detailed plan for addressing these gaps. Furthermore, the article emphasizes the role of large models in HS moderation but does not critically evaluate the potential biases or limitations associated with the use of these models. Future research should focus on developing more context-aware and ethically governed AI methodologies for HS moderation, addressing the challenges of inclusivity and nuanced detection. Overall, the article provides valuable insights into the evolving landscape of HS moderation but could benefit from a more critical analysis of potential biases and limitations in the use of large models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2401.16727v1](https://arxiv.org/abs/2401.16727v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.16727v1](https://browse.arxiv.org/html/2401.16727v1)       |
| Truncated       | False       |
| Word Count       | 6553       |