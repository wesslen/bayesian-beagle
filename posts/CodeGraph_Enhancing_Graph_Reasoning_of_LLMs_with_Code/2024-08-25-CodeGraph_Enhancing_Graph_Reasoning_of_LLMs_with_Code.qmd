
---
title: "CodeGraph: Enhancing Graph Reasoning of LLMs with Code"
id: "2408.13863v1"
description: "CodeGraph, a code-based method, enhances LLMs' graph reasoning abilities, improving performance by up to 58.6% and offering better control and interpretation."
author: Qiaolong Cai, Zhaowei Wang, Shizhe Diao, James Kwok, Yangqiu Song
date: "2024-08-25"
image: "https://browse.arxiv.org/html/2408.13863v1/x2.png"
categories: ['robustness', 'prompt-engineering', 'education', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.13863v1/x2.png)

### Summary:

The paper introduces CodeGraph, a method that enhances the graph reasoning abilities of large language models (LLMs) by encoding graph problem solutions as code. The method solves new graph problems by learning from exemplars, generating programs, and executing them via a program interpreter. The study evaluates CodeGraph using the few-shot setting with the base LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and Mixtral-8x7B Instruct. Experimental results on six tasks with six graph encoding methods in the GraphQA dataset demonstrate that CodeGraph can boost performance on graph reasoning tasks inside LLMs by 1.3% to 58.6%, depending on the task. Compared to existing methods, CodeGraph demonstrates strong performance on arithmetic problems in graph tasks and offers a more controllable and interpretable approach to the reasoning process.

### Major Findings:

1. CodeGraph, a method that encodes graph problem solutions as code, significantly improves the performance of LLMs on graph reasoning tasks, with an increase in performance ranging from 1.3% to 58.6%, depending on the task.
2. CodeGraph demonstrates strong performance on arithmetic problems in graph tasks and offers a more controllable and interpretable approach to the reasoning process compared to existing methods.
3. The study evaluates CodeGraph using the few-shot setting with the base LLM being GPT-3.5 Turbo, Llama3-70B Instruct, Mixtral-8x22B Instruct, and Mixtral-8x7B Instruct.

### Analysis and Critique:

The paper presents a novel approach to enhancing the graph reasoning abilities of LLMs by encoding graph problem solutions as code. The proposed method, CodeGraph, demonstrates significant improvements in performance on graph reasoning tasks, with an increase in performance ranging from 1.3% to 58.6%, depending on the task. The study also highlights the strong performance of CodeGraph on arithmetic problems in graph tasks and its more controllable and interpretable approach to the reasoning process compared to existing methods.

However, the study has some limitations. The evaluation of

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.13863v1](https://arxiv.org/abs/2408.13863v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.13863v1](https://browse.arxiv.org/html/2408.13863v1)       |
| Truncated       | False       |
| Word Count       | 8240       |