
---
title: "Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models"
id: "2405.17374v2"
description: "Finetuning LLMs risks safety; safety basin maintains safety level. New Visage metric measures safety in finetuning. System prompt protects model and its variants within safety basin."
author: ShengYun Peng, Pin-Yu Chen, Matthew Hull, Duen Horng Chau
date: "2024-05-28"
image: "https://browse.arxiv.org/html/2405.17374v2/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.17374v2/x1.png)

# Summary:

The paper introduces the concept of the LLM safety landscape, a tool for understanding the safety of large language models (LLMs) during finetuning. The authors discover a new phenomenon called the "safety basin," where randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood. This discovery leads to the proposal of a new safety metric, Visage, which measures the safety in LLM finetuning by probing its safety landscape.

## Major Findings:

1. **Safety Basin**: The safety basin is a new phenomenon observed in the model parameter space of popular open-source LLMs, where randomly perturbing model weights maintains the safety level of the original aligned model in its local neighborhood.
2. **Visage Safety Metric**: The Visage safety metric is a new measure that quantifies the safety in LLM finetuning by probing its safety landscape. It is inspired by the discovery of the safety basin and is designed to be task-agnostic, measuring the risks in finetuning without assumptions on the finetuning dataset.
3. **System Prompt's Role**: The LLM safety landscape highlights the critical role of the system prompt in protecting a model, and this protection transfers to its perturbed variants within the safety basin. The paper evaluates the impact of system design on LLaMA2, LLaMA3, Vicuna, and Mistral, using each LLM's default system prompt as the baseline.

## Analysis and Critique:

1. The paper provides a novel approach to understanding the safety of LLMs during finetuning, which is a significant contribution to the field. The concept of the safety landscape and the safety basin offers new insights into the behavior of LLMs during finetuning.
2. The Visage safety metric is a promising tool for measuring the safety of LLMs during finetuning. However, its effectiveness and reliability need to be further validated with more extensive experiments and comparisons with other safety metrics.
3. The paper focuses on the role of the system prompt in protecting a model, but it does not discuss other factors that may affect the safety of LLMs during finetuning. Future work could explore these factors and their interactions with the system prompt.
4. The paper does not provide a clear

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.17374v2](https://arxiv.org/abs/2405.17374v2)        |
| HTML     | [https://browse.arxiv.org/html/2405.17374v2](https://browse.arxiv.org/html/2405.17374v2)       |
| Truncated       | False       |
| Word Count       | 7510       |