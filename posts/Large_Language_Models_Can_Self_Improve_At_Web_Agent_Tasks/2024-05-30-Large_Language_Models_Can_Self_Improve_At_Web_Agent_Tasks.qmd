
---
title: "Large Language Models Can Self-Improve At Web Agent Tasks"
id: "2405.20309v1"
description: "LLMs can improve performance in complex tasks, like web navigation, through self-improvement, achieving a 31% increase in task completion rate."
author: Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, Sepp Hochreiter
date: "2024-05-30"
image: "../../img/2405.20309v1/image_1.png"
categories: ['prompt-engineering', 'architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2405.20309v1/image_1.png)

**Summary:**

The paper "Large Language Models Can Self-Improve" explores the potential of large language models (LLMs) to improve their performance as agents in long-horizon tasks within a complex environment, such as a web browser. The authors use the WebArena benchmark to evaluate the performance of LLMs in this context. The study focuses on fine-tuning LLMs on three distinct synthetic training data mixtures and achieving a 31% improvement in task completion rate over the base model on the WebArena benchmark. The authors also introduce new evaluation metrics to better assess the performance, robustness, capabilities, and quality of trajectories of fine-tuned agent models.

**Major Findings:**

1. LLMs can self-improve their performance as agents in long-horizon tasks within a complex environment, as demonstrated by a 31% improvement in task completion rate over the base model on the WebArena benchmark.
2. Fine-tuning LLMs on three distinct synthetic training data mixtures can lead to improved performance, with the best performing mixture yielding a 31% improvement over the base LLM agent on the WebArena benchmark.
3. The authors introduce new evaluation metrics to better assess the performance, robustness, capabilities, and quality of trajectories of fine-tuned agent models, providing a more nuanced understanding of the improvements and degradations than simple, aggregate-level benchmark scores.

**Analysis and Critique:**

The paper presents an interesting approach to improving the performance of LLMs as agents in complex environments. The use of synthetic training data mixtures and fine-tuning techniques shows promise in improving the task completion rate of LLMs. However, the study is limited to the WebArena benchmark, and it is unclear how well these findings would generalize to other complex environments or tasks. Additionally, the authors do not discuss the potential limitations or biases of the synthetic training data, which could impact the performance of the fine-tuned models. Further research is needed to evaluate the generalizability of these findings and to explore the potential limitations and biases of the synthetic training data.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.20309v1](https://arxiv.org/abs/2405.20309v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.20309v1](https://browse.arxiv.org/html/2405.20309v1)       |
| Truncated       | False       |
| Word Count       | 23053       |