
---
title: "On the attribution of confidence to large language models"
id: "2407.08388v1"
description: "LLM credence attributions may be literal, plausible, but subject to skeptical concerns due to potentially non-truth-tracking experimental techniques."
author: Geoff Keeling, Winnie Street
date: "2024-07-11"
image: "../../../bayesian-beagle.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The paper "On the attribution of confidence to large language models" explores the practice of attributing credences, or degrees of belief, to large language models (LLMs) in the empirical literature on LLM evaluation. The authors argue that LLM credence attributions are generally intended as literal ascriptions of beliefs, but the existence of LLM credences is not conclusively proven. Furthermore, even if LLMs have credences, the experimental techniques used to assess them, such as reported confidence, consistency-based estimation, and output probabilities, may not be reliable.

### Major Findings:

1. LLM credence attributions are generally intended as literal ascriptions of beliefs, with scientists providing empirical justifications for their claims.
2. The existence of LLM credences is at least plausible, but current evidence is inconclusive.
3. Even if LLMs have credences, the experimental techniques used to assess them may not be reliable, as they are subject to distorting factors and lack a clear mechanism for reliably indicating LLM credences.

### Analysis and Critique:

The paper raises important questions about the attribution of credences to LLMs and the reliability of the experimental techniques used to assess them. While the authors provide a strong case for the literal interpretation of LLM credences, the evidence for their existence is not conclusive. Additionally, the paper highlights the potential unreliability of the experimental techniques used to assess LLM credences, such as reported confidence, consistency-based estimation, and output probabilities.

One limitation of the paper is that it does not provide a clear mechanism for reliably indicating LLM credences, which is necessary for the development of more reliable experimental techniques. Furthermore, the paper does not discuss the potential implications of the unreliability of LLM credence attributions for the evaluation of LLM capabilities and the development of LLM-based systems.

In conclusion, the paper provides a valuable contribution to the discussion on the attribution of credences to LLMs and the reliability of the experimental techniques used to assess them. However, further research is needed to address the limitations of the paper and to develop more reliable experimental techniques for assessing LLM credences.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.08388v1](https://arxiv.org/abs/2407.08388v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.08388v1](https://browse.arxiv.org/html/2407.08388v1)       |
| Truncated       | False       |
| Word Count       | 11712       |