
---
title: "Task Contamination: Language Models May Not Be Few-Shot Anymore"
id: "2312.16337v1"
description: "Large language models (LLMs) perform better on older datasets, suggesting task contamination affects zero-shot and few-shot tasks."
author: ['Changmao Li', 'Jeffrey Flanigan']
date: "2023-12-26"
image: "https://browse.arxiv.org/html/2312.16337v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.16337v1/x1.png)

### Task Contamination: Language Models May Not Be Few-Shot Anymore

#### Summary
In this paper, the authors investigate the impact of task contamination on the zero-shot and few-shot performance of large language models (LLMs). Task contamination refers to the inclusion of task training examples in the pre-training data, affecting the model's zero or few-shot evaluation. The authors systematically analyze this problem by measuring the scope of task contamination across various models and tasks, conducting training data inspection, task example extraction, and a membership inference attack. They find strong evidence of task contamination for some combinations of models and datasets, particularly in GPT-3 series models.

#### Major Takeaways
1. **Closed models may demonstrate inflated performance** in zero-shot or few-shot evaluation due to task contamination, raising concerns about the trustworthiness of their baselines in these settings.
2. For classification tasks **without demonstrated possibility of task contamination**, LLMs rarely show statistically significant improvements over majority baselines, indicating limited performance improvements in both zero and few-shot settings.
3. The observed increase in **zero-shot or few-shot performance over time** for GPT-3 series models is likely due to task contamination, posing a challenge for fair evaluation in these settings.

#### Critique
While the paper provides valuable insights into the impact of task contamination on LLM performance, there are limitations to consider:
- The study relies on empirical evaluations without a comprehensive exploration of the extent and impact of task contamination.
- The methodology suffers from low recall in detecting task contamination, underscoring the challenges of accurately identifying contamination issues.
- The paper emphasizes the need for publicly releasing training datasets but does not delve into potential solutions or interventions to mitigate task contamination.

#### Related Work
The paper aligns with previous research on data contamination in LLMs, adding to existing knowledge by providing a comprehensive evaluation of task contamination for zero and few-shot learning scenarios.

#### Potential Future Work
The authors recommend additional research be conducted on task contamination for zero and few-shot settings to reveal the extent and impact of task contamination for large language models in these settings. This future work holds promise for addressing the limitations and advancing the understanding of task contamination in LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-31       |
| Abstract | [http://arxiv.org/abs/2312.16337v1](http://arxiv.org/abs/2312.16337v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.16337v1](https://browse.arxiv.org/html/2312.16337v1)       |
| Truncated       | False       |
| Word Count       | 8991       |