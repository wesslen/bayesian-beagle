
---
title: "Do LLMs Exhibit Human-Like Reasoning? Evaluating Theory of Mind in LLMs for Open-Ended Responses"
id: "2406.05659v1"
description: "LLMs struggle with Theory of Mind reasoning in open-ended questions, but incorporating human intentions and emotions can improve their performance, though not fully achieving human-like reasoning."
author: Maryam Amirizaniani, Elias Martin, Maryna Sivachenko, Afra Mashhadi, Chirag Shah
date: "2024-06-09"
image: "https://browse.arxiv.org/html/2406.05659v1/x1.png"
categories: ['hci', 'prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.05659v1/x1.png)

### Summary:

This study evaluates the ability of large language models (LLMs) to understand and apply Theory of Mind (ToM) reasoning in open-ended questions. ToM reasoning involves recognizing that individuals possess their own intentions, emotions, and thoughts, which is crucial for guiding thought processes. The study uses posts from Reddit's ChangeMyView platform, which requires nuanced social reasoning to craft persuasive responses. The analysis compares semantic similarity and lexical overlap metrics between human and LLM responses, revealing disparities in ToM reasoning capabilities in open-ended questions. Even advanced models, such as Zephyr-7B, Llama2-Chat-13B, and GPT-4, show limitations. The research also implements a prompt tuning method that incorporates human intentions and emotions, improving ToM reasoning performance. However, this enhancement still falls short of achieving human-like reasoning.

### Major Findings:

1. LLMs, despite their prowess in tasks like summarization, question answering, and translation, face challenges with ToM reasoning, especially in open-ended questions.
2. Comparative analyses of semantic similarity and lexical overlap scores between human and LLM responses reveal significant disparities in reasoning capabilities within open-ended scenarios.
3. The study underscores the effectiveness of incorporating mental states such as human intentions and emotions into LLM reasoning via prompt tuning.

### Analysis and Critique:

1. The study's reliance on Reddit posts as a data source, while providing a rich dataset, may limit the generalizability of the findings to other contexts.
2. The study does not address potential biases in the data or the LLMs, which could impact the validity of the findings.
3. The study does not explore the potential for LLMs to improve their ToM reasoning capabilities through additional training or fine-tuning.
4. The study does not discuss the ethical implications of LLMs' ability to understand and apply ToM reasoning, such as the potential for misuse or the need for regulation.
5. The study does not consider the potential for LLMs to develop their own form of ToM reasoning, distinct from human reasoning, which could have implications for their ability to understand and interact with humans.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.05659v1](https://arxiv.org/abs/2406.05659v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.05659v1](https://browse.arxiv.org/html/2406.05659v1)       |
| Truncated       | False       |
| Word Count       | 10269       |