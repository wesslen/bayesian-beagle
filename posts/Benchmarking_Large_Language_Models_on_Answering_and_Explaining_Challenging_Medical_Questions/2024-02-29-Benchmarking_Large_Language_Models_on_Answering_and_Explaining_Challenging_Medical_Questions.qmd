
---
title: "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions"
id: "2402.18060v2"
description: "New medical datasets for evaluating LLM performance on complex, explainable clinical QA. Experiments show challenge, highlighting need for new metrics."
author: Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.18060v2/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18060v2/x1.png)

### **Summary:**

- Large language models (LLMs) have shown impressive performance in answering medical questions, but they struggle with complex, real-world clinical cases.
- To address this issue, two new datasets, JAMA Clinical Challenge and Medbullets, are constructed for evaluating LLMs on challenging medical questions.
- Four LLMs are evaluated on these datasets, and the results show that the new tasks are more difficult for the models, with GPT-4 performing the best overall.
- In-context learning and chain-of-thought (CoT) prompting are applied to improve model performance, but their impact varies across datasets and models.
- The study highlights the need for better learning or adaptation strategies to equip LLMs for challenging medical QA tasks and the importance of developing evaluation metrics that align with human judgments.

### Major Findings:

1. **New datasets for evaluating LLMs on challenging medical questions:** JAMA Clinical Challenge and Medbullets consist of real-world clinical questions and recent USMLE Step 2/3 type questions, respectively.
2. **Lower scores on new datasets:** The four evaluated LLMs (GPT-3.5, GPT-4, PaLM 2, and Llama 2) perform worse on the new datasets, indicating that they are more reflective of complex clinical cases.
3. **Inconsistent performance across models and datasets:** GPT-4 performs the best overall, while GPT-3.5 and Llama 2 are tied, and PaLM 2 ranks last in human evaluation.
4. **In-context learning and CoT prompting:** In-context learning does not enhance the adaptation of most models to new tasks, and CoT prompting improves accuracy for almost all models but not for JAMA Clinical Challenge.
5. **Need for better evaluation metrics:** Existing automatic evaluation metrics do not align with human judgments, suggesting the need for future work to define an evaluation metric suitable for this task.

### Analysis and Critique:

- **Limited scope of existing medical QA benchmarks:** Most benchmarks consist of board exam questions or general medical questions, which may not capture the complexity of realistic clinical situations.
- **Potential data leakage:** The better performance of models on Step 2/3 questions

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.18060v2](https://arxiv.org/abs/2402.18060v2)        |
| HTML     | [https://browse.arxiv.org/html/2402.18060v2](https://browse.arxiv.org/html/2402.18060v2)       |
| Truncated       | False       |
| Word Count       | 7933       |