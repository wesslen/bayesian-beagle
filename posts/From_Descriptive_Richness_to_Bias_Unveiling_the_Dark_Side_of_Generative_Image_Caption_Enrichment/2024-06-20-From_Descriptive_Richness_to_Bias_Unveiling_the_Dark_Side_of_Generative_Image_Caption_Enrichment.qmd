
---
title: "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment"
id: "2406.13912v1"
description: "Enriched image captions increase gender bias and hallucination, cautioning against over-descriptiveness."
author: Yusuke Hirota, Ryo Hachiuma, Chao-Han Huck Yang, Yuta Nakashima
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.13912v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.13912v1/x1.png)

### Summary:

This study examines the negative side effects of Generative Caption Enrichment (GCE) methods, which utilize large language models (LLMs) to create more descriptive and semantically enhanced captions for images. While these methods have improved the performance of vision-language models (VLMs) in image captioning, they have also been found to exacerbate societal bias and hallucination.

The study focuses on gender bias and hallucination, using comprehensive metrics to evaluate both datasets and models trained on these datasets for standard captions (COCO captions) and enriched captions (ShareGPT4V, FuseCap, CapsFusion). The analysis reveals that LLM-enriched captions indeed have negative side effects, worsening issues of gender bias and hallucination by making captions more descriptive. Furthermore, models trained on these enriched captions tend to amplify these problems.

### Major Findings:

1. **More Descriptive, More Gender Bias**: The study shows a clear tendency for gender bias to increase as captions become more descriptive. For instance, COCO captions have the lowest object coverage but exhibit the least bias, while ShareGPT4V and FuseCap have higher object coverage but higher gender bias than COCO captions.
2. **Enriched Captions Exhibit Greater Recall Disparity**: Enriched captions, such as those generated by ShareGPT4V, exhibit a more significant recall disparity for all objects compared to COCO captions. This further validates the risk of gender bias in enriched captions.
3. **More Descriptive, More Hallucination**: A similar trend between descriptiveness and hallucination is evident in the study. COCO captions, which have the lowest object coverage, exhibit the lowest hallucination rates, while ShareGPT4V, with the highest object coverage, shows significantly increased hallucination rates compared to COCO captions.
4. **Models Trained on the Datasets Inherit/Amplify Bias and Hallucination**: The study shows that models inherit the datasetâ€™s bias tendencies. Specifically, the model trained on the least descriptive captions (i.e., COCO captions)

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.13912v1](https://arxiv.org/abs/2406.13912v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.13912v1](https://browse.arxiv.org/html/2406.13912v1)       |
| Truncated       | False       |
| Word Count       | 3715       |