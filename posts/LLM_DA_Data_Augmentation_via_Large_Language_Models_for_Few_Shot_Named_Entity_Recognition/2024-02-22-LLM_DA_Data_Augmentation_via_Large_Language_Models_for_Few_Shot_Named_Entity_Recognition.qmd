
---
title: "LLM-DA: Data Augmentation via Large Language Models for Few-Shot Named Entity Recognition"
id: "2402.14568v1"
description: "LLM-DA proposes data augmentation for NER tasks, improving model performance with limited data."
author: Junjie Ye, Nuo Xu, Yikun Wang, Jie Zhou, Qi Zhang, Tao Gui, Xuanjing Huang
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14568v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14568v1/x1.png)

### **Summary:**
- Large language models (LLMs) have impressive rewriting capabilities and extensive world knowledge, but their performance on information extraction tasks is not entirely satisfactory.
- LLM-DA is a novel data augmentation technique based on LLMs for the few-shot Named Entity Recognition (NER) task.
- LLM-DA leverages 14 contextual rewriting strategies, entity replacements, and noise injection to enhance robustness.
- Extensive experiments demonstrate the effectiveness of LLM-DA in enhancing NER model performance with limited data.

### **Major Findings:**
1. LLM-DA significantly improves model performance in the few-shot NER task, particularly in scenarios with limited training samples.
2. Context-level augmentation strategies are more effective in smaller datasets, while entity-level augmentation becomes more beneficial as dataset size increases.
3. LLM-DA consistently outperforms ChatGPT in the NER task and generates data with a more uniform distribution of entities compared to existing methods.

### **Analysis and Critique:**
- LLM-DA demonstrates a balance between diversity and controllability in data generation, outperforming existing methods in terms of syntactic and semantic quality.
- The method is highly effective in low-resource and domain-specific scenarios, but may have limitations in adapting to specific sentence domains and token length restrictions.
- The approach may inherit societal biases present in the pretraining corpus of LLMs, requiring human inspection of the generated data to mitigate the risk of propagating biases to downstream models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.14568v1](https://arxiv.org/abs/2402.14568v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14568v1](https://browse.arxiv.org/html/2402.14568v1)       |
| Truncated       | False       |
| Word Count       | 6427       |