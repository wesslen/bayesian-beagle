
---
title: "Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for Competitive Debate"
id: "2408.04472v1"
description: "Agent4Debate, a multi-agent framework, enhances LLMs' debate capabilities, showing human-like performance in competitive debates."
author: Yiqun Zhang, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song
date: "2024-08-08"
image: "https://browse.arxiv.org/html/2408.04472v1/x1.png"
categories: ['social-sciences', 'robustness', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.04472v1/x1.png)

### Summary:

- The paper introduces Agent for Debate (Agent4Debate), a dynamic, multi-agent framework based on Large Language Models (LLMs) designed to enhance their capabilities in competitive debate.
- The framework consists of four specialized agents (Searcher, Analyzer, Writer, and Reviewer) that dynamically interact and cooperate, covering multiple stages from initial research and argument formulation to rebuttal and summary.
- The Chinese Debate Arena, comprising 66 carefully selected Chinese debate motions, was constructed to comprehensively evaluate the framework's performance.
- Ten experienced human debaters were recruited, and 200 debates involving Agent4Debate, baseline models, and humans were recorded.
- The evaluation employed the Debatrix automatic scoring system and professional human reviewers based on the established Debatrix-Elo and Human-Elo ranking.
- Experimental results indicate that the state-of-the-art Agent4Debate exhibits capabilities comparable to those of humans, and ablation studies demonstrate the effectiveness of each component in the agent structure.

### Major Findings:

1. The Agent4Debate framework effectively enhances the performance of language models of varying scales and types in competitive debate tasks.
2. Each agent in the Agent4Debate framework contributes to the overall performance, with the Analyzer playing a crucial role in the formulation of material analysis, argument refinement, and rebuttal strategy, and the Searcher being essential for appropriately searching and organizing external knowledge.
3. Agent4Debate, especially those using advanced foundation models such as Gemini-1.5-Pro and Claude-3.5-sonnet, demonstrate performance comparable to or surpassing human debaters in both Debatrix-Elo and Human-Elo rankings.

### Analysis and Critique:

- The paper provides a comprehensive evaluation of the Agent4Debate framework, demonstrating its effectiveness in enhancing the performance of LLMs in competitive debate tasks.
- The use of the Chinese Debate Arena and the recruitment of experienced human debaters for evaluation provide a robust and realistic assessment of the framework's capabilities.
- The ablation studies further validate the importance of each agent component in the framework.
- However, the paper

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.04472v1](https://arxiv.org/abs/2408.04472v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.04472v1](https://browse.arxiv.org/html/2408.04472v1)       |
| Truncated       | False       |
| Word Count       | 6382       |