
---
title: "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling"
id: "2401.16380v1"
description: "Large language models need massive data, but web data is noisy. WRAP pre-training improves performance."
author: Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly
date: "2024-01-29"
image: "https://browse.arxiv.org/html/2401.16380v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.16380v1/x1.png)

### Summary:
Large language models are typically trained on massive scrapes of the web, which can be unstructured, noisy, and poorly phrased. This poses challenges in terms of the compute and data required for training, as well as the quality of the data. In this work, the authors propose a method called Web Rephrase Augmented Pre-training (WRAP) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as "like Wikipedia" or in "question-answer format" to jointly pre-train large language models (LLMs) on real and synthetic rephrases. The authors show that using WRAP on the C4 dataset speeds up pre-training and improves perplexity and zero-shot question answer accuracy across different subsets of the Pile. They also investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in out-of-distribution settings.

### Major Findings:
1. Using WRAP on the C4 dataset speeds up pre-training by approximately 3 times and improves perplexity by more than 10% on average across different subsets of the Pile.
2. The impact of the re-phrasing style on the performance of the model offers insights into how the composition of the training data can impact the performance of LLMs in out-of-distribution settings.
3. Pre-training LLMs with synthetic data allows equivalent models to be trained with 5 times less data or 3 times less compute.

### Analysis and Critique:
The article provides valuable insights into the use of synthetic data for pre-training large language models. However, it is important to critically evaluate the potential limitations and biases that may arise from using synthetic data. Additionally, further research is needed to fully understand the impact of synthetic data on the performance and generalization of language models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2401.16380v1](https://arxiv.org/abs/2401.16380v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.16380v1](https://browse.arxiv.org/html/2401.16380v1)       |
| Truncated       | False       |
| Word Count       | 12696       |