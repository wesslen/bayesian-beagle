
---
title: "Efficiently Exploring Large Language Models for Document-Level Machine Translation with In-context Learning"
id: "2406.07081v1"
description: "LLMs struggle with document-level translation. Our Context-Aware Prompting method (CAP) improves LLM translation accuracy, cohesion, and coherence."
author: Menglong Cui, Jiangcun Du, Shaolin Zhu, Deyi Xiong
date: "2024-06-11"
image: "https://browse.arxiv.org/html/2406.07081v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.07081v1/x1.png)

### Summary:

The paper proposes a Context-Aware Prompting (CAP) method to enable large language models (LLMs) to generate more accurate, cohesive, and coherent translations via in-context learning. CAP addresses the challenges of document-level translation (DOCMT) by LLMs, such as incoherent translations and limited length of demonstrations for in-context learning. The method involves selecting the most relevant sentences to the current one as context, generating a summary from these sentences, and retrieving sentences most similar to the summary as demonstrations. The proposed method is evaluated on various DOCMT tasks, demonstrating its effectiveness, particularly in zero pronoun translation (ZPT) and literary translation tasks.

### Major Findings:

1. The proposed Context-Aware Prompting (CAP) method enables LLMs to generate more accurate, cohesive, and coherent translations via in-context learning.
2. CAP addresses the challenges of document-level translation (DOCMT) by LLMs, such as incoherent translations and limited length of demonstrations for in-context learning.
3. The method involves selecting the most relevant sentences to the current one as context, generating a summary from these sentences, and retrieving sentences most similar to the summary as demonstrations.
4. The proposed method is evaluated on various DOCMT tasks, demonstrating its effectiveness, particularly in zero pronoun translation (ZPT) and literary translation tasks.

### Analysis and Critique:

1. The paper does not provide a detailed comparison with other existing methods for addressing the challenges of DOCMT by LLMs.
2. The evaluation of the proposed method is limited to a few specific tasks, and its generalizability to other tasks and domains remains to be explored.
3. The paper does not discuss the potential limitations and biases of the proposed method, such as the reliance on the quality of the selected context and the potential for overfitting to specific tasks or domains.
4. The paper does not provide a detailed analysis of the computational cost and efficiency of the proposed method, which is an important consideration for practical applications.
5. The paper does not discuss the potential ethical implications of using LLMs for document-level translation, such as the risk of perpetuating biases or inaccuracies in the generated transl

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.07081v1](https://arxiv.org/abs/2406.07081v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07081v1](https://browse.arxiv.org/html/2406.07081v1)       |
| Truncated       | False       |
| Word Count       | 6243       |