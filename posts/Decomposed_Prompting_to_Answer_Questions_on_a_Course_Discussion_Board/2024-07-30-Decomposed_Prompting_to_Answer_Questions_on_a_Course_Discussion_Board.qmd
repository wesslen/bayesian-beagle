
---
title: "Decomposed Prompting to Answer Questions on a Course Discussion Board"
id: "2407.21170v1"
description: "System uses LLM to classify student questions, achieving 81% accuracy, and employs different answering strategies for each type."
author: Brandon Jaipersaud, Paul Zhang, Jimmy Ba, Andrew Petersen, Lisa Zhang, Michael R. Zhang
date: "2024-07-30"
image: "../../../bayesian-beagle.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The authors propose a question-answering system that uses decomposed prompting to classify and answer student questions on a course discussion board.
- The system uses a large language model (LLM) to classify questions into one of four types: conceptual, homework, logistics, and not answerable.
- The system achieves 81% classification accuracy using a variant of GPT-3.
- The authors discuss the system's performance on answering conceptual questions from a machine learning course and various failure modes.

### Major Findings:
1. The proposed system uses decomposed prompting to classify and answer student questions on a course discussion board.
2. The system achieves 81% classification accuracy using a variant of GPT-3.
3. The system can effectively differentiate between types of questions, but different answering approaches and contextual cues may be effective for each question type.

### Analysis and Critique:
- The authors do not provide a detailed comparison of their proposed system with existing automated methods for answering student questions.
- The authors do not discuss the potential impact of the system on the workload of instructors and teaching assistants.
- The authors do not provide a detailed analysis of the failure modes of the system on conceptual questions.
- The authors do not discuss the potential limitations of using LLMs for course Q&A, such as the risk of providing incorrect answers and the cost of answering a question incorrectly.
- The authors do not discuss the potential biases in the system, such as the potential for the system to favor certain types of questions or students.
- The authors do not discuss the potential for the system to be used for other types of questions, such as questions about course content or administrative questions.
- The authors do not discuss the potential for the system to be used in other educational contexts, such as online courses or MOOCs.
- The authors do not discuss the potential for the system to be used for other types of tasks, such as grading or feedback.
- The authors do not discuss the potential for the system to be used for other types of applications, such as customer service or technical support.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.21170v1](https://arxiv.org/abs/2407.21170v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.21170v1](https://browse.arxiv.org/html/2407.21170v1)       |
| Truncated       | False       |
| Word Count       | 2329       |