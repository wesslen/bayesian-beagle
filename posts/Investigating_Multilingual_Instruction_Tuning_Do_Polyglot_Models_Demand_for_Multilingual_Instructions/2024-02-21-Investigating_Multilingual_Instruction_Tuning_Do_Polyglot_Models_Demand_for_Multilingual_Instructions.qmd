
---
title: "Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?"
id: "2402.13703v1"
description: "TL;DR: Multilingual LLMs benefit from instruction-tuning on parallel datasets, improving cross-lingual capabilities."
author: Alexander Arno Weber, Klaudia Thellmann, Jan Ebert, Nicolas Flores-Herr, Jens Lehmann, Michael Fromm, Mehdi Ali
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13703v1/x1.png"
categories: ['hci', 'social-sciences', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13703v1/x1.png)

### **Summary:**
- The study investigates the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across major Indo-European languages.
- Results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%.
- The Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets.

### **Major Findings:**
1. Instruction-tuning on parallel datasets benefits cross-lingual instruction following capabilities by up to 4.6%.
2. The Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets.
3. Multilingual instruction-tuning study with a focus on multilingual multi-turn user request performance.

### **Analysis and Critique:**
- The study does not aim to push the boundaries of state-of-the-art performance.
- Single-score evaluations were conducted only once, limiting the ability to calculate comprehensive statistical measures.
- The research scope is confined to languages within the Germanic and Italo-Western language families due to resource constraints, limiting the generalizability of the findings to languages from more distant language families.
- The study lays the groundwork for exploring whether multilingual instruction-tuning benefits languages beyond those examined in this research, opening avenues for further investigation and refinement of multilingual LLM fine-tuning methodologies.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13703v1](https://arxiv.org/abs/2402.13703v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13703v1](https://browse.arxiv.org/html/2402.13703v1)       |
| Truncated       | False       |
| Word Count       | 9518       |