
---
title: "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models"
id: "2401.17043v1"
description: "Benchmarking and Evaluating RAG Systems for Diverse Applications in Create, Read, Update, Delete."
author: Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang, Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen
date: "2024-01-30"
image: "../../../bayesian-beagle.png"
categories: ['production', 'robustness', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

**Summary:**

* CRUD-RAG is a comprehensive Chinese benchmark for Retrieval-Augmented Generation (RAG) of large language models.
* The benchmark consists of four categories: Create, Read, Update, and Delete, each with corresponding evaluation tasks and datasets.
* The evaluation tasks include text continuation, question answering (single-document, multi-document), hallucination modification, and open-domain multi-document summarization.
* The datasets are constructed from high-quality news data crawled from major Chinese news websites.
* The benchmark evaluates all components of the RAG system, including the retriever, context length, knowledge base construction, and the large language model (LLM).

**Major Findings:**

1. The CRUD-RAG benchmark provides a comprehensive evaluation of RAG systems, covering various application scenarios and components.
2. The evaluation tasks and datasets are designed to assess the performance of RAG systems in different contexts, such as generating creative content, answering questions, correcting errors, and summarizing information.
3. The benchmark highlights the importance of considering all components of the RAG system, including the retriever, context length, knowledge base construction, and LLM, in the evaluation process.

**Analysis and Critique:**

* The benchmark focuses on Chinese language models, which may limit its generalizability to other languages.
* The evaluation tasks and datasets may not cover all possible application scenarios and contexts of RAG systems.
* The benchmark does not provide a detailed analysis of the impact of each component on the overall performance of the RAG system.
* The benchmark could benefit from incorporating more recent advancements in RAG research, such as pre-retrieval processing, retrieval model optimization, and post-retrieval processing techniques.
* The benchmark could also include a comparison with other RAG evaluation frameworks and metrics, such as RAGAS, ARES, and TruLens-Eval, to provide a more comprehensive evaluation of RAG systems.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.17043v1](https://arxiv.org/abs/2401.17043v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.17043v1](https://browse.arxiv.org/html/2401.17043v1)       |
| Truncated       | False       |
| Word Count       | 24170       |