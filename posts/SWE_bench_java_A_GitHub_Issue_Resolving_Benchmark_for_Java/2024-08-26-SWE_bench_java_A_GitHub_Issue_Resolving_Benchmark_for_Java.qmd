
---
title: "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java"
id: "2408.14354v1"
description: "SWE-bench-java-verified released for multilingual issue resolving in software engineering, inviting contributions for continuous improvement."
author: Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, Dezhi Ran, Muhan Zeng, Bo Shen, Pan Bian, Guangtai Liang, Bei Guan, Pengjie Huang, Tao Xie, Yongji Wang, Qianxiang Wang
date: "2024-08-26"
image: "https://browse.arxiv.org/html/2408.14354v1/x1.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.14354v1/x1.png)

### Summary:

The paper introduces SWE-bench-java-verified, a Java version of the SWE-bench dataset, which is a benchmark for evaluating issue resolving capabilities of large language models (LLMs). The authors chose to develop a Java version of SWE-bench due to the popularity and platform independence of Java, as well as the need to support more programming languages in the industry. The paper describes the details of dataset construction, the main challenges, and potential problems. The authors also evaluate the performance of SWE-agent with state-of-the-art models on SWE-bench-java-verified.

### Major Findings:

1. The authors have developed a Java version of SWE-bench, named SWE-bench-java-verified, which marks the first step in establishing a multilingual GitHub issue-resolving benchmark with a focus on Java.
2. The dataset, along with a comprehensive evaluation Docker environment and a leaderboard, has been open-sourced to advance further research in this field.
3. The authors implemented SWE-Agent on SWE-bench-java-verified and derived several insightful findings that enhance our understanding of issue resolving in Java projects.

### Analysis and Critique:

The paper provides a detailed description of the construction process of SWE-bench-java-verified and presents a comprehensive statistical analysis of the dataset. The authors have also open-sourced the dataset, evaluation Docker environment, and leaderboard, which is a significant contribution to the field. However, the paper does not provide a detailed analysis of the performance of the evaluated models on SWE-bench-java-verified. Additionally, the paper does not discuss any potential limitations or biases in the dataset. It would be beneficial to have a more in-depth analysis of the performance of the evaluated models and a discussion of any potential limitations or biases in the dataset.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.14354v1](https://arxiv.org/abs/2408.14354v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.14354v1](https://browse.arxiv.org/html/2408.14354v1)       |
| Truncated       | False       |
| Word Count       | 4964       |