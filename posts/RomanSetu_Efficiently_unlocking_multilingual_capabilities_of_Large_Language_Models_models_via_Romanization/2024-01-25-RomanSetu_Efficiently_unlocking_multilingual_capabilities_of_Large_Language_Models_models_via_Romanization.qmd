
---
title: "RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization"
id: "2401.14280v1"
description: "Romanized text enhances performance and efficiency of Large Language Models for non-Latin languages like Hindi."
author: Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Ratish Puduppully, Anoop Kunchukuttan
date: "2024-01-25"
image: "https://browse.arxiv.org/html/2401.14280v1/x1.png"
categories: ['production', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.14280v1/x1.png)

**Summary of the Article:**
The article introduces an innovative approach to extending the capabilities of Large Language Models (LLMs) to non-English languages that use non-Latin scripts. The method involves using the romanized form of text as an interface for LLMs, with the hypothesis that romanized text's frequent informal use and shared tokens with English enhance cross-lingual alignment. The study focuses on Hindi and demonstrates that romanized text significantly improves inference efficiency and achieves competitive performance with limited pre-training. Additionally, a multi-script prompting approach combining romanized and native texts shows promise in further enhancing task performance.

### Major Findings:
1. **Efficiency of Romanized Text:**
    - The fertility of the romanized text is 2x times lower than the native text, making the romanized form far more efficient than the native script.
    - Continual pre-training on romanized data is key to improving performance, with a model continually pre-training with limited romanized data being competitive with the base model using native text.
2. **Inference Efficiency and Task Performance:**
    - Romanized representation can complement the native representation, and a multi-script prompting approach jointly prompting with romanized and native text improves task performance.
3. **Enhancement of LLMs for Non-English Languages:**
    - Leveraging romanization significantly improves inference efficiency and task performance, suggesting the potential of romanization in bridging the language gap for LLM applications.

### Analysis and Critique:
The study presents an innovative and promising approach to extending LLM capabilities to non-English languages using non-Latin scripts. However, some limitations and areas for future research are apparent:
- The generalizability of the findings to other multilingual language models remains uncertain, and the approach's effectiveness with larger models and a wider set of tasks requires further exploration.
- The study is limited to using a 7B LLaMA model due to resource constraints, and further research with larger models could provide a more comprehensive understanding of the approach's generalization and impact.
- While the article provides an ethics statement and highlights the intention to not supplant native scripts with romanized scripts, potential biases within the datasets and the impact on native script performance need to be further addressed and evaluated.
- Future research should aim to expand experiments to more languages and explore a broader range of NLP tasks, with a focus on cross-lingual and cross-task transfer to better understand the approach's scope and impact.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-27       |
| Abstract | [http://arxiv.org/abs/2401.14280v1](http://arxiv.org/abs/2401.14280v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.14280v1](https://browse.arxiv.org/html/2401.14280v1)       |
| Truncated       | False       |
| Word Count       | 6284       |