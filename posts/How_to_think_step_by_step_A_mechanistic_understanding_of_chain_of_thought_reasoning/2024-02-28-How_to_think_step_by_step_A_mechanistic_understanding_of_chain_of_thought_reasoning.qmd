
---
title: "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning"
id: "2402.18312v1"
description: "LLMs use multiple pathways for CoT reasoning, with a functional rift in the middle layers."
author: Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti, Tanmoy Chakraborty
date: "2024-02-28"
image: "../../../bayesian-beagle.png"
categories: ['production', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The study investigates the neural sub-structures within Large Language Models (LLMs) that facilitate Chain-of-Thought (CoT) reasoning from a mechanistic point of view.
- It identifies the components of the language model that are most important for a given task and explores the depth at which the model starts following the context provided as input.
- The section discusses the attention heads in the language model (LLM) and their role in generating answers for different subtasks, as well as the use of fictional and false ontologies in the PrOntoQA dataset to prompt reasoning in large language models.
- It presents Figure 14, illustrating the information flow through attention heads towards answer-writing heads in LLaMA-27B for subtask 5.

### Major Findings:
1. Large Language Models (LLMs) deploy multiple parallel pathways of answer generation for step-by-step reasoning.
2. The tasks are not structurally well-differentiated in the language model, and a good majority of heads share the importance of all three subtasks.
3. The model employs multiple pathways to compute answers, collecting information from different segments of the input.

### Analysis and Critique:
- The findings have implications for understanding the neural functional components involved in CoT reasoning and provide insights into the complex reasoning capabilities of LLMs.
- The identification of parallel pathways for answer generation and the functional rift within the model layers contribute to a deeper understanding of LLMs' reasoning processes.
- The insights contribute to a deeper understanding of the circuitry of step-by-step generation in language models and can inform future research on language modeling and contribute to the development of more interpretable and reliable models.
- The visual representation of the information flow highlights the complexity and multi-layered nature of the model's processing, emphasizing the intricate mechanisms involved in generating responses and predicting specific tokens.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-29       |
| Abstract | [https://arxiv.org/abs/2402.18312v1](https://arxiv.org/abs/2402.18312v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18312v1](https://browse.arxiv.org/html/2402.18312v1)       |
| Truncated       | True       |
| Word Count       | 24207       |