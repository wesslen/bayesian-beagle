
---
title: "Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space"
id: "2402.09063v1"
description: "Adversarial robustness research focuses on open-source LLMs, proposing embedding space attacks as a threat model."
author: Leo Schwinn, David Dobre, Sophie Xhonneux, Gauthier Gidel, Stephan Gunnemann
date: "2024-02-14"
image: "../../../bayesian-beagle.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The article discusses the impact of embedding space attacks on open-source language models (LLMs), comparing their success rate to fine-tuning in bypassing safety alignment and unlearning methods. It evaluates the impact of embedding space attacks on perplexity and toxicity of generated responses from different models and provides details on the hyperparameters used for embedding attacks, models used in the experiments, fine-tuning hyperparameters, and examples of unsuccessful attacks. The section also demonstrates how embedding space attacks can be used to generate harmful and toxic content in response to user prompts.

### Major Findings:
1. Embedding space attacks are more efficient than fine-tuning at bypassing safety alignment and can uncover allegedly deleted information in unlearned models.
2. The rigorous methodology and thorough evaluation of embedding attacks on various language models provide valuable insights into the effectiveness of the attacks and the robustness of the models.
3. The success rates of universal attacks on specific datasets indicate the effectiveness of these attacks in generating harmful content, highlighting the potential dangers of embedding space attacks in LLMs.

### Analysis and Critique:
- The findings suggest that embedding space attacks present a viable threat in open-source models and are a cost-effective yet potent method for probing undesirable behaviors in LLMs.
- The section emphasizes the need for awareness and responsible deployment of these models in critical sectors, given the potential misuse and inherent limitations of LLMs.
- The examples of unsuccessful attacks shed light on the limitations and challenges of embedding space attacks in large language models, contributing to a comprehensive understanding of their impact and implications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.09063v1](https://arxiv.org/abs/2402.09063v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09063v1](https://browse.arxiv.org/html/2402.09063v1)       |
| Truncated       | True       |
| Word Count       | 16441       |