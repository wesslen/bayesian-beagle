
---
title: "Learning to Ask Informative Questions: Enhancing LLMs with Preference Optimization and Expected Information Gain"
id: "2406.17453v1"
description: "LLM-generated questions improved via Direct Preference Optimization (DPO) for better information gain in 20-question games."
author: Davide Mazzaccara, Alberto Testoni, Raffaella Bernardi
date: "2024-06-25"
image: "https://browse.arxiv.org/html/2406.17453v1/extracted/5690260/images/FIG_1.png"
categories: ['hci', 'prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17453v1/extracted/5690260/images/FIG_1.png)

### Summary:

The paper proposes a method to enhance the informativeness of Large Language Models (LLMs) generated questions in 20-question game dialogues. The authors use the Llama 2-chat 7B model to generate multiple questions for each game and create pairs of low-EIG and high-EIG questions. They then apply a Direct Preference Optimization (DPO) algorithm to improve the effectiveness of the questions, even in domains different from those used to train the DPO model.

### Major Findings:

1. The proposed method, which involves sampling multiple questions, evaluating them based on Expected Information Gain (EIG), and training with preference optimization, leads to more informative and effective questions generated by LLMs.
2. The results show that EIG is a strong training signal for improving the question-asking capabilities of current LLMs and overcoming their shortcomings in asking effective questions.
3. The method generalizes well to different domains, demonstrating its potential for improving the reasoning capabilities of LLMs in information-seeking dialogues.

### Analysis and Critique:

1. The study focuses on one model (Llama 2-chat 7B) and one preference optimization strategy (DPO), which may limit the generalizability of the findings. Further work is required to determine if this training strategy holds with other models and other preference optimization strategies.
2. The EIG computation depends on the yes/no annotation, which could introduce inaccuracies, especially for questions that are difficult to answer with only yes/no.
3. The study assumes that LLMs have priors conditioning their question generation, which may not always be the case.
4. When computing the EIG of follow-up questions, the model is assumed to be able to sequentially rule out candidates excluded in the dialogue history, which could be a strong assumption for a generative language model.
5. The study does not perform extensive hyperparameter tuning, which could potentially lead to better results for the proposed approach.

In conclusion, the paper presents a promising method for improving the informativeness of LLM-generated questions in 20-question game dialogues. However, further research is needed to address the identified limitations and validate the findings with other models and preference optimization strategies.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17453v1](https://arxiv.org/abs/2406.17453v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17453v1](https://browse.arxiv.org/html/2406.17453v1)       |
| Truncated       | False       |
| Word Count       | 5253       |