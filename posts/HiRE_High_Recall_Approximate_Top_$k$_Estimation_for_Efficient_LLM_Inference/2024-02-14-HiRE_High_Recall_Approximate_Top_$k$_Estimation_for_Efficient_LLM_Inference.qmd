
---
title: "HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference"
id: "2402.09360v1"
description: "Autoregressive decoding with LLMs on accelerators can improve latency using HiRE compression scheme."
author: Yashas Samaga B L, Varun Yerram, Chong You, Srinadh Bhojanapalli, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli
date: "2024-02-14"
image: "https://browse.arxiv.org/html/2402.09360v1/extracted/5409158/figures/herd.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.09360v1/extracted/5409158/figures/herd.png)

### Summary:
- Autoregressive decoding with generative Large Language Models (LLMs) on accelerators is often memory-bound, leading to significant latency.
- Recent works have shown that LLMs have inherent sparsity or redundancy in the feedforward (FFN) layers, which can be exploited to reduce the transfer of model parameters and latency.
- The authors introduce HiRE (High Recall Approximate Top-k Estimation) to address these issues, which comprises a compression scheme to predict top- rows/columns with high recall and an efficient multi-device approximate top-k operator.

### Major Findings:
1. Autoregressive decoding for small batch sizes is memory-bound, and the time taken for transferring model parameters across different hierarchies of memory is the largest component of the total inference latency.
2. HiRE applied to both the softmax and feedforward layers achieves almost matching pretraining and downstream accuracy and speeds up inference latency by 1.47× on a single TPUv5e device.
3. Implementing HiRE with DA-TOP-k improves the latency by 2.27× compared to the vanilla implementation of HiRE, with comparable quality on average across downstream tasks.

### Analysis and Critique:
- The article provides a comprehensive overview of the challenges associated with autoregressive decoding with LLMs and introduces HiRE as a solution to improve inference latency.
- The experimental results demonstrate the effectiveness of HiRE in reducing latency without compromising the quality of the model's predictions.
- The authors also discuss the importance of high recall estimation and the potential impact of HiRE on energy-efficient large model inference.
- However, the article could benefit from a more detailed discussion of potential limitations or trade-offs associated with the implementation of HiRE, as well as the broader societal implications of making large models more accessible.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.09360v1](https://arxiv.org/abs/2402.09360v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09360v1](https://browse.arxiv.org/html/2402.09360v1)       |
| Truncated       | False       |
| Word Count       | 7507       |