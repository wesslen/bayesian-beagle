
---
title: "A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation"
id: "2401.09760v1"
description: "Comparison of Language Models and Crowdsourcing for label aggregation reveals potential enhancement with a hybrid approach."
author: Jiyi Li
date: "2024-01-18"
image: "../../../bayesian-beagle.png"
categories: ['production', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](None)

**Summary of the Article:**
The article explores the comparative annotation quality of crowdsourcing and Large Language Models (LLMs) by aggregating labels. It investigates the use of existing crowdsourcing datasets, compares the quality of individual crowd and LLM labels, and evaluates the aggregated labels. Additionally, it proposes a Crowd-LLM hybrid label aggregation method and finds that adding LLM labels to existing crowdsourcing datasets enhances the quality of the aggregated labels, surpassing the quality of LLM labels themselves.


### Major Findings:
1. Existing Crowdsourcing Datasets for Comparative Study:
   - The article addresses the underutilization of existing crowdsourcing datasets in evaluating the annotation quality, aiming to provide reliable evaluations from a different viewpoint.
   - It investigates which datasets can be used for comparative studies, creating a benchmark for reliable evaluations.

2. Quality Comparison between Crowd and LLM Labels:
   - The study compares the quality of individual crowd labels and LLM labels, finding that good LLM labels enhance the quality of aggregated labels, surpassing the quality of LLM labels themselves.
   - It also examines the performance of LLM workers, proposing a hybrid label aggregation method utilizing both crowd and LLM labels.

3. Label Aggregation Evaluation:
   - The article evaluates the quality of label aggregation using traditional crowd label aggregation models and proposes a Crowd-LLM hybrid label aggregation method.
   - It demonstrates that adding good LLM labels to existing crowdsourcing datasets enhances the quality of the aggregated labels, outperforming the quality of LLM labels alone. It also suggests that collecting more crowd labels can further improve the quality of aggregated labels.


### Analysis and Critique:
The article provides valuable insights into the comparison of annotation quality between crowdsourcing and LLMs. However, it is limited to categorical labels, excluding other types of labels like numerical and textual labels. Additionally, while the study highlights the enhanced quality of aggregated labels with LLM inputs, it does not address potential biases in the LLM-generated labels or the impact of dataset characteristics on the LLM's performance. Further research is needed to explore these aspects and expand the comparative studies to include other types of labels for a comprehensive understanding of annotation quality.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-27       |
| Abstract | [http://arxiv.org/abs/2401.09760v1](http://arxiv.org/abs/2401.09760v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.09760v1](https://browse.arxiv.org/html/2401.09760v1)       |
| Truncated       | False       |
| Word Count       | 4581       |