[
  {
    "objectID": "posts/2022-09-28-prodigy-chinese/index.html",
    "href": "posts/2022-09-28-prodigy-chinese/index.html",
    "title": "Using Prodigy with Chinese",
    "section": "",
    "text": "To get started you need to make sure you have one of the pipelines installed.\npython -m spacy download zh_core_web_sm\nNext, we need an input file. Prodigy can use .txt, .csv, and .jsonl files if they follow these input data formats. Let’s say we have a file named zh_headlines.txt of 10 article news headlines.\n# zh_headlines.txt\n新闻人物：有望成为意大利首位女总理的右翼党魁梅洛尼是谁？\n安倍晋三国葬为何在日本充满争议\n中共二十大“懒人包”：你可能想了解的几个基本问题\nNASA进行防御实验，以飞行器直接撞击小行星\n查尔斯国王如何帮助拯救英国农家奶酪\n斯诺登在美国面临间谍指控，可能导致数十年监禁。\n普京授予斯诺登俄罗斯公民身份\n疫情管控放松\n各国新冠疫情渐次“收尾” 大流行怎么定义？\n世界奇观：拥有两千年历史的土耳其地下城\nSince we have an initial ner component, let’s consider refining three entity types: ORG, PERSON, and DATE so we’ll use ner.correct. We can also modify Prodigy configuration and change the user interface instructions to Chinese. It is one of seven languages that are available (English, German, Dutch, Spanish, Portuguese, French, and Chinese). You can implement it through setting PRODIGY_CONFIG_OVERRIDES.\nPRODIGY_CONFIG_OVERRIDES='{\"ui_lang\": \"zh\"}' python -m prodigy ner.correct example_dataset zh_core_web_sm ./chinese_headlines.txt --label ORG,PERSON,DATE\n\nYou can also play with a demo of this interface."
  },
  {
    "objectID": "posts/2022-10-10-spacy-shiny/index.html",
    "href": "posts/2022-10-10-spacy-shiny/index.html",
    "title": "spaCy in Shiny for Python",
    "section": "",
    "text": "Recently, RStudio (soon-to-be-renamed, Posit) announced they’re alpha testing an extension of Shiny to Python!\nSo to learn, I created a prototype for spaCy using Shiny for Python, see my GitHub repo for spacy-shiny.\nIn this post, I want to briefly describe the basics of the app. I’ll reference some of the documentation used on the Shiny for Python documentation."
  },
  {
    "objectID": "posts/2022-10-10-spacy-shiny/index.html#what-makes-up-a-shiny-app",
    "href": "posts/2022-10-10-spacy-shiny/index.html#what-makes-up-a-shiny-app",
    "title": "spaCy in Shiny for Python",
    "section": "What makes up a Shiny app?",
    "text": "What makes up a Shiny app?\nShiny apps have two major parts:\n\nthe UI (user-interface)\nthe Server function\n\nHere’s a generic Shiny app for Python:\nfrom shiny import App, ui\n\n# Part 1: ui ----\napp_ui = ui.page_fluid(\n    \"Hello, world!\",\n)\n\n# Part 2: server ----\ndef server(input, output, session):\n    ...\n\n# Combine into a shiny app.\n# Note that the variable must be \"app\".\napp = App(app_ui, server)\nTo run this file, we’ll need to call from shiny import App, ui. And then to create the app itself, we’ll combine the ui app_ui along with the server function into App(). Easy so far, right?"
  },
  {
    "objectID": "posts/2022-10-10-spacy-shiny/index.html#basics-of-our-spacy-app-server-side",
    "href": "posts/2022-10-10-spacy-shiny/index.html#basics-of-our-spacy-app-server-side",
    "title": "spaCy in Shiny for Python",
    "section": "Basics of our spaCy app: Server Side",
    "text": "Basics of our spaCy app: Server Side\nTo describe my spaCy app (see code here), let’s work backwards, starting with the server function first.\ndef server(input, output, session):\n    @output\n    @render.text\n    @reactive.event(input.run) # Take a dependency on the button\n    def result():\n        doc, nlp = process_text(input.spacy_model(), input.text())\n        ...\nThe first line is defining the name and arguments for the server() function. The next line (@output) is a decorator that indicates that the result will be displayed in the UI. Last, the @reactive.event(input.run) means that this function will be a dependent event on some input we’re calling run input.run. We’ll explain this in just a second.\nWe’ll next define a function result(), which will result our main server action. The function will call process_text(), which is a combination of two helper functions:\ndef load_model(name):\n    return spacy.load(name)\n\ndef process_text(model_name, text):\n    nlp = load_model(model_name)\n    return nlp(text), nlp\nThis should be familiar if you’ve used spacy before to load models before. Notice that for process_text(), we also included a second argument for the text. This is what we’ll want to revisit in our user interface as this will become reactive based on the user’s input.\nAlso, we’ll fill in the ... with five additional lines of code, providing how to handle different spaCy components in our server. Notice the logic will look whether our spaCy pipeline has either \"parser\" and/or \"ner\" components and then render the html for each respective component using our input text (input.text()). We then concatenate the html for each part and that is what our server will return.\ndef server(input, output, session):\n    @output\n    @render.text\n    @reactive.event(input.run) # Take a dependency on the button\n    def result():\n        doc, nlp = process_text(input.spacy_model(), input.text())\n        if \"parser\" in nlp.pipe_names:\n            html_parser = get_parser(doc, nlp)\n        if \"ner\" in nlp.pipe_names:\n            html_ner = get_ner(doc, nlp)\n        return html_parser + html_ner\nTo define how we get our html outputs, we’ll provide two additional helper functions in our app.py where we specify what we want to extract from the doc object for either ner or our parser tagging. The key is we want to use displacy, spaCy’s nifty visualizer to render either part-of-speech or named entities into the user interface. So to use either, we want to use display.render().\nHTML_WRAPPER = \"\"\"<div style=\"overflow-x: auto; border: 1px solid #e6e9ef; border-radius: 0.25rem; padding: 1rem; margin-bottom: 2.5rem\">{}</div>\"\"\"\n\ndef get_parser(doc, nlp):\n    options = {\n        \"collapse_punct\": True,\n        \"collapse_phrases\": True,\n        \"compact\": True,\n    }\n    docs = [span.as_doc() for span in doc.sents] if True else [doc]\n    for sent in docs:\n        html = displacy.render(sent, style=\"dep\", options=options)\n        html = html.replace(\"\\n\\n\", \"\\n\")\n        html = HTML_WRAPPER.format(html)\n    return html\nIf our spaCy pipeline has a part-of-speech (pos) component, we first can separate the document as sentences by using a list comprehension. We do this because displaCy will display pos tags from left to right, and if we have mulitiple sentences than we’ll need to scroll. By doing this, we’ll stack each pos visualizer vertically, one for each sentence.\nWe’ll then loop through each sentence, inputting it into displacy.render(), adding an additional new line (\\n) to improve the cosmetic look with extra vertical space, then format our html output. Notice that for displacy.render() we included the argument style=\"dep\" to indicate that we were using the dependency parsing.\nAlternatively for ner, we only need to provide the possible labels as an additional argument for display.render, which we can get from nlp.get_pipe(\"ner\").labels. We do need to specify that our style is \"ent\" as we’re interested in visualizing the entities. Then we do a similar trick to add white space by replacing blank lines with a new line character (\\n) and then format the html with HTML_WRAPPER.format().\nHTML_WRAPPER = \"\"\"<div style=\"overflow-x: auto; border: 1px solid #e6e9ef; border-radius: 0.25rem; padding: 1rem; margin-bottom: 2.5rem\">{}</div>\"\"\"\n\ndef get_ner(doc, nlp):\n    labels = nlp.get_pipe(\"ner\").labels\n    html = displacy.render(doc, style=\"ent\", options={\"ents\": labels})\n    html = html.replace(\"\\n\", \" \")\n    html = HTML_WRAPPER.format(html)\n    return html"
  },
  {
    "objectID": "posts/2022-10-10-spacy-shiny/index.html#basics-of-our-spacy-app-ui",
    "href": "posts/2022-10-10-spacy-shiny/index.html#basics-of-our-spacy-app-ui",
    "title": "spaCy in Shiny for Python",
    "section": "Basics of our spaCy app: UI",
    "text": "Basics of our spaCy app: UI\nWe also have to provide details for the user interface on what to display for our app.\nShiny uses a nested layout design for its user interface that typically starts with a ui.page_fluid() function that represents the fluid page. YOu can include other components like the panel_title() for the panel’s title or parts within a sidebar layout.\napp_ui = ui.page_fluid(\n    ui.panel_title(),\n    ui.layout_sidebar(\n        ui.panel_sidebar(\n            ...\n        ),\n        ui.panel_main(\n            ...\n        ),\n    ),\n)\n\n\n\n\n\nShiny’s nested layout design\n\n\nWhat’s great about ui objects is that they are just functions that output html that will be rendered in the user interface.\nFor example, let’s say we want to see what the ui.page_fluid() function does. We can then run the function to see that it simply outputs html code to render the text.\nfrom shiny import ui\n\nui.page_fluid(\"This is my page\")\n\n# <html>\n#   <head></head>\n#   <body>\n#     <div class=\"container-fluid\">This is my page.</div>\n#   </body>\n# </html>\nThere are many other ui objects that you can create and can provide widgets for your user interface.\nFor our interface, we want to provide the user with three options.\nFirst, the user can select their model they want to use. In the current app, I include the core small models for English, Spanish and German.\nSecond, the user can input as a text box what is the text they want their respective model to analyze.\nLast, since Shiny implements [reactive programming], we will want to add a button that executes the model select to run on the text provided only when the user clicks a button.\nTo do this, we’ll use the documentation for the UI inputs and identify that we’ll want to use the three functions:\n\nui.input_select(): this will input which spaCy model we’ll use\nui.input_text_area(): this will input what text we want to process in spaCy\nui.input_action_button(): this will be an action button that we’ll execute our server function only when this button is selected.\n\nSPACY_MODEL_NAMES = [\"en_core_web_sm\", \"de_core_news_sm\", \"es_core_news_sm\"]\nDEFAULT_TEXT = \"Tim Cook is the CEO of Apple.\"\n\napp_ui = ui.page_fluid(\n    ui.layout_sidebar(\n        ui.panel_sidebar(    \n            ui.input_select(id = \"spacy_model\", label = \"Model name\", choices = SPACY_MODEL_NAMES),\n            ui.input_text_area(id = \"text\", label = \"Text to analyze\", value = DEFAULT_TEXT),\n            ui.input_action_button(\"run\", \"Run doc!\"),\n        ),\n        ui.panel_main(\n            ui.output_ui(\"result\", placeholder=True),\n        ),\n    )\n)\nAs a last specification, we want to use the layout_sidebar() to include these three inputs in the sidebar of our user interface. We can list each of these three functions inside of the layout_sidebar() function. Last, it’s important to provide a unique id for each of our input functions. We need this as these unique id (names) will help us identify the values within the server side.\nFor example, recall that we named the reactive event in our server function @reactive.event(input.run). Notice that its input is input.run, the name of our input_action_button. Hence, this is how we can provide conditional logic for our reactive flow. We only run the result() function when the user will click the input.run button.\nLast, our only missing element is that we need to specify where we are displaying our display output that returned from our server function. For this, we can use the ui.output_ui() within the ui.panel_main(), which is the main display view of the ui.layout_sidebar(). For the user interface to know what is going to be run, we use result as the first argument as this is the name of our function within the server-side of the app.\nAs a final step, we then combine our UI and server with one final line of code:\napp = App(app_ui, server)\nWe can run this app locally but what if we want others to view our app? We can deploy it."
  },
  {
    "objectID": "posts/2022-10-10-spacy-shiny/index.html#deployment",
    "href": "posts/2022-10-10-spacy-shiny/index.html#deployment",
    "title": "spaCy in Shiny for Python",
    "section": "Deployment",
    "text": "Deployment\nThe app below is a deployed version of app (see the GitHub repo) on shinyapps.io. This is one of the fastest ways to deploy an app as this is a cloud service provided by Posit. It provides free and paid tiers depending on your use. For more details about preparing your app for deployment, check out the deployment docs.\nWith all of these details, we can now run our app!"
  },
  {
    "objectID": "posts/2022-10-12-prodigy-peewee-db/index.html",
    "href": "posts/2022-10-12-prodigy-peewee-db/index.html",
    "title": "Prodigy DB: ORM’s, peewee and Python",
    "section": "",
    "text": "In a previous post, I described some Prodigy database recipes that enable you to manipulate annotations saved in the default SQLite database. However, you may find you want more database customization. In this post, I will dig into Prodigy’s database core concepts.\nI’ll also show how you can interact with Prodigy’s default database too from Python to enable a more programmatic approach. This enables you to pull from the database using Python, not SQL.\nLast, I’ll outline experimental database features to highlight possible changes to Prodigy’s database system.\nIf you’re new to Prodigy’s database, I recommend starting with Prodigy’s Database documentation before this post."
  },
  {
    "objectID": "posts/2022-10-12-prodigy-peewee-db/index.html#under-the-hood-orm-and-peewee",
    "href": "posts/2022-10-12-prodigy-peewee-db/index.html#under-the-hood-orm-and-peewee",
    "title": "Prodigy DB: ORM’s, peewee and Python",
    "section": "Under the hood: ORM and peewee",
    "text": "Under the hood: ORM and peewee\nAs of v1.11.8, Prodigy’s database uses an ORM, or “object-relational mapper”. The key idea of an ORM is to implement an object-oriented paradigm to managing databases.\nBy contrast, if you’re from data analytics, you’ve likely used databases as SQL database management systems (DBMS) such as SQLite or MySQL. These systems are not object-oriented. They can only store and manipulate scalar values like strings and integers within tables.\nNow wait - you may know that Prodigy implements by default SQLite. How can Prodigy use an ORM approach but still implement a DBMS like SQLite?\nThat’s because Prodigy also includes peewee, which is a common open-source ORM and operates as Prodigy’s ORM. There are many alternative ORMs and they vary by programming language.\nTODO: https://www.fullstackpython.com/object-relational-mappers-orms.html\nAn ORM provides a library to convert (“map”) between objects in code and database tables (“relations”). With an ORM, you normally create a class that represents a table in a SQL database, each attribute of the class represents a column, with a name and a type. For example, a class named Animals can represent a SQL table animals. Each instance object of that class represents a row in the database.\nTODO: explain model and connect to second part\nSo why would we even use an ORM?\nORM’s enable us to interact with our database using our language of choice instead of SQL. Because of Prodigy’s ORM, this enables us to interface with our database using Python and not SQL.\nBut another advantage to ORMs is that they make it easier to switch between different relational databases. For example, a developer could use SQLite for local development and MySQL in production. Since Prodigy has configurations for either SQLite, MySQL, or PostgreSQL, this makes it even easier.\n\n\nTo globally modify your database, you can edit the db and db_settings in Prodigy’s configuration file, prodigy.json."
  },
  {
    "objectID": "posts/2022-10-12-prodigy-peewee-db/index.html#accessing-the-database-programmatically-with-python",
    "href": "posts/2022-10-12-prodigy-peewee-db/index.html#accessing-the-database-programmatically-with-python",
    "title": "Prodigy DB: ORM’s, peewee and Python",
    "section": "Accessing the database programmatically with Python",
    "text": "Accessing the database programmatically with Python\nProdigy’s database model is available from prodigy.components.db in Python.\nfrom prodigy.components.db import connect\n\ndb = connect(\"sqlite\", {\"name\": \"prodigy.db\"}) # default\nexamples = db.get_dataset(\"my_dataset\")\nTODO: How to use\nTODO: Show data format for data.\nfrom prodigy.components.db import connect\n\nexamples = [{\"text\": \"hello world\", \"_task_hash\": 123, \"_input_hash\": 456}]\n\ndb = connect()                               # uses settings from prodigy.json\ndb.add_dataset(\"test_dataset\")               # add dataset\nassert \"test_dataset\" in db                  # check that dataset was added\ndb.add_examples(examples, [\"test_dataset\"])  # add examples to dataset\ndataset = db.get_dataset(\"test_dataset\")     # retrieve a dataset\nassert len(dataset) == 1                     # check that examples were added\nAlternatively, you can view the datasets like:\nprint(db.datasets)\nTODO: give an interesting example"
  },
  {
    "objectID": "posts/2022-10-12-prodigy-peewee-db/index.html#down-the-road-changes",
    "href": "posts/2022-10-12-prodigy-peewee-db/index.html#down-the-road-changes",
    "title": "Prodigy DB: ORM’s, peewee and Python",
    "section": "Down the road changes",
    "text": "Down the road changes\nEarlier in 2022, the Prodigy team released a new experimental branch in Prodigy Support that outlines experimental features in Prodigy’s database system.\nThe new branch includes a new Feed system stores all examples that are to be shown to annotators in the Database. Currently, Prodigy creates and maps three tables:\n\n\n\n\n\n\n\nTable\nDescription\n\n\n\n\nDataset\nThe dataset / session IDs and meta information.\n\n\nExample\nThe individual annotation examples. Each example is only added once, so if you add the same annotation to multiple datasets, it’ll only have one record here.\n\n\nLink\nExample IDs linked to datasets. This is how Prodigy knows which examples belong to which datasets and sessions.\n\n\n\nWhat is new is a fourth table, Feed, which serves as a buffer to handle the batch of data for multiple annotators. Ultimately, this new Feed will replace Prodigy’s current generator-based streams and will aim to reduce issues with multi-user annotators.\nThis change requires schema changes so it also aligns with a switch to SQLAlchemy from peewee for Prodigy’s ORM. One advantage of this switch is that it can enable more database systems beyond SQLite, PostgreSQL, and MySQL.\nIf you’re interested in learning more, see this Prodigy Support issue, or you can install it:\npip install prodigy==1.11.8a4 --extra-index-url https://{YOUR_LICENSE_KEY}@download.prodi.gy\nand modify prodigy.json such that:\n\n\nprodigy.json\n\n{\n    \"experimental_feed\": true\n}"
  },
  {
    "objectID": "posts/2022-09-27-prodigy-iaa-textcat/index.html",
    "href": "posts/2022-09-27-prodigy-iaa-textcat/index.html",
    "title": "Inter-annotator Agreement (IAA) in Prodigy",
    "section": "",
    "text": "TODO: add preview image"
  },
  {
    "objectID": "posts/2022-09-27-prodigy-iaa-textcat/index.html#multi-user-sessions-in-prodigy",
    "href": "posts/2022-09-27-prodigy-iaa-textcat/index.html#multi-user-sessions-in-prodigy",
    "title": "Inter-annotator Agreement (IAA) in Prodigy",
    "section": "Multi-user sessions in Prodigy",
    "text": "Multi-user sessions in Prodigy\nSince v1.7.0 was released, Prodigy has offered multi-user sessions within the same Prodigy instance. This functionality enables dividing up Prodigy annotations across different annotators when saving annotations to the same dataset and performing an identical task.\nTODO: 2-3 sentences on example, link to past TECH issues\n# news_headlines.jsonl\n{'text': 'Uber’s Lesson: Silicon Valley’s Start-Up Machine Needs Fixing',\n 'meta': {'source': 'The New York Times'}\n}\nTODO: add section on data input/formatting\n\n\nTerminal\n\npython -m prodigy textcat.manual news_textcat news_headlines.jsonl --label TECHNOLOGY\n\nUsing 1 label(s): TECHNOLOGY\n\n✨  Starting the web server at http://localhost:8080 ...\nOpen the app in your browser and start annotating!\n\n\n\n\nTo create a custom named session, add ?session=xxx to the annotation app URL. For example, annotator Jordan may access a running Prodigy project via http://localhost:8080/?session=jordan. The example shows running the textcat.manual Prodigy recipe but this works for any Prodigy recipe. This will enable use to track each annotator so we can calculate inter-annotator agreement.\n\nInternally, this will request and send back annotations with a session identifier consisting of the current dataset name and the session ID – for example, textcat-jordan. Every time annotator Jordan labels examples for this dataset, their annotations will be associated with this session identifier.\nLet’s say that in addition to Jordan, we also asked a second annotator, Alex, do both label 20 records in the dataset to determine whether the news headlines are technology-related or not. We provide both their respective URL and have them complete their annotations.\nTo pull their annotations, we’ll use Prodigy’s get_dataset_examples() function:\n\nfrom prodigy.components.db import connect\nimport pprint\n\ndb = connect()\nexamples = db.get_dataset_examples(\"news_textcat\")\n\npprint.pprint(examples[0])\n\n{'_annotator_id': 'news_textcat-jordan',\n '_input_hash': 1886699658,\n '_session_id': 'news_textcat-jordan',\n '_task_hash': -257308161,\n '_timestamp': 1659908691,\n '_view_id': 'classification',\n 'answer': 'accept',\n 'label': 'TECHNOLOGY',\n 'meta': {'source': 'The New York Times'},\n 'text': 'Uber’s Lesson: Silicon Valley’s Start-Up Machine Needs Fixing'}\n\n\n\n\n\n\n\n\nHint\n\n\n\nTODO: deprecation of get_dataset\n\n\nSince we’re interested in text classification annotations, we’ll focus on the \"answer\" values comparing those that are \"reject\" versus \"accept\".\n\nimport pandas as pd\n\n# keep only the \"accept\" and \"reject\" answers\nanno = [eg for eg in examples if eg[\"answer\"] in [\"accept\", \"reject\"]]\n\n# convert to a dataframe\ndf = pd.DataFrame(anno)\n\ndf_annotations = df.pivot(\n    index=['_input_hash'], \n    columns='_session_id', \n    values='answer'\n)\n\ndf_annotations.head(n=1)\n\n\n\n\n\n  \n    \n      _session_id\n      news_textcat-alex\n      news_textcat-jordan\n    \n    \n      _input_hash\n      \n      \n    \n  \n  \n    \n      -584314991\n      reject\n      reject"
  },
  {
    "objectID": "posts/2022-09-27-prodigy-iaa-textcat/index.html#cohens-kappa",
    "href": "posts/2022-09-27-prodigy-iaa-textcat/index.html#cohens-kappa",
    "title": "Inter-annotator Agreement (IAA) in Prodigy",
    "section": "Cohen’s Kappa",
    "text": "Cohen’s Kappa\nTODO: 2-3 sentences on Cohen’s Kappa.\n\nfrom sklearn.metrics import cohen_kappa_score\n\nkappa = cohen_kappa_score(\n    df_annotations['news_textcat-alex'], \n    df_annotations['news_textcat-jordan'], \n    labels=None, \n    weights=None\n)\n\nprint(kappa)\n\n0.736842105263158\n\n\nSo we’ve found a Cohen’s Kappa of 0.737, which is fairly high.\nTODO: Discussion on interpreting Cohen’s Kappa\n\n\n\n\n\n\nImportant\n\n\n\nIn practice, you may have many more complexities like saving different annotations to different datasets, multi-class classification, span-based tasks like named entity recognition (NER) or spancat, or handling for more than two annotators. This use case is the simplest case; however, I hope to create more advanced use cases in the future."
  },
  {
    "objectID": "posts/2022-10-11-prodigy-db-intro/index.html",
    "href": "posts/2022-10-11-prodigy-db-intro/index.html",
    "title": "Retrieving annotations in Prodigy",
    "section": "",
    "text": "By default, Prodigy includes a SQLite database to save annotations. This enables using SQLite to be used out of the box without additional configuration. While there are many ways to customize the database, many users may be able to interact with their annotations only requiring three helpful Prodigy recipes: db-out, db-in, and db-merge.\nIn this post, I want to briefly review these recipes which will cover many of your database needs for Prodigy. However, by no means is this all there is to Prodigy’s Database and I’ll have future posts on them as well. Also, I encourage the interested reader to see the Prodigy database documentation or Prodigy Support issues tagged as database."
  },
  {
    "objectID": "posts/2022-10-11-prodigy-db-intro/index.html#db-out",
    "href": "posts/2022-10-11-prodigy-db-intro/index.html#db-out",
    "title": "Retrieving annotations in Prodigy",
    "section": "db-out",
    "text": "db-out\nLet’s start first with the problem of exporting out annotations to files. For this recipe, we technically need only one argument: the name of a dataset that we want to export (like my_dataset). However, if we run db-out with only the name of the dataset, it will print out the dataset to terminal. Therefore, usually db-out will include a second argument > /path/to/data.jsonl which is the file location of the exported dataset.\npython -m prodigy db-out my_dataset > /path/to/data.jsonl\n\n\nThe > directs the output of a command into a file. Here’s a helpful Stack Exchange post on common control and redirection operators in shells.\nTODO: Explain jq\npython -m prodigy db-out my_dataset | jq \nTODO: show example"
  },
  {
    "objectID": "posts/2022-10-11-prodigy-db-intro/index.html#db-in",
    "href": "posts/2022-10-11-prodigy-db-intro/index.html#db-in",
    "title": "Retrieving annotations in Prodigy",
    "section": "db-in",
    "text": "db-in\nThe db-in is a recipe to load examples into your Prodigy database.\n\n\n\n\n\n\nNote\n\n\n\nThe db-in command is typically used to import existing annotations into your Prodigy datasets – for example, if you’ve already labelled data with some other process and want to combine it with new annotations or if you want to re-import annotations to a new dataset.\nIf you just want to annotate data, you do not have to import anything upfront – you can just start the server with your input data and Prodigy will stream it in, let you annotate and save the collected annotations to the database.\n\n\nProdigy prefers newline-delimited JSON (or JSONL), as it can contain detailed information and metadata.\nUnlike regular JSON, JSONL doesn’t require parsing the entire file, which results in overall better performance when working with large volumes of text.\nTODO: change to NER example\nAs an example, we’ll use an annotation that is in the format of the ner_manual user interface with one annotation:\n\n\nner_input.jsonl\n\n{\n  \"text\": \"First look at the new MacBook Pro\",\n  \"spans\": [\n    {\"start\": 22, \"end\": 33, \"label\": \"PRODUCT\", \"token_start\": 5, \"token_end\": 6}\n  ],\n  \"tokens\": [\n    {\"text\": \"First\", \"start\": 0, \"end\": 5, \"id\": 0},\n    {\"text\": \"look\", \"start\": 6, \"end\": 10, \"id\": 1},\n    {\"text\": \"at\", \"start\": 11, \"end\": 13, \"id\": 2},\n    {\"text\": \"the\", \"start\": 14, \"end\": 17, \"id\": 3},\n    {\"text\": \"new\", \"start\": 18, \"end\": 21, \"id\": 4},\n    {\"text\": \"MacBook\", \"start\": 22, \"end\": 29, \"id\": 5},\n    {\"text\": \"Pro\", \"start\": 30, \"end\": 33, \"id\": 6}\n  ]\n}\n\nFor input .jsonl files, the key \"text\" required and contains the text contents of the document typically a sentence.\nThe db-in recipe requires two arguments:\n\nthe name of the new dataset (new_dataset)\na path to the input file (/path/to/data.jsonl)\n\npython -m prodigy db-in new_dataset /path/to/data.jsonl\nFor example, let’s say we have in our current working folder a file named ner-sample.jsonl that includes 1 documents, each with a sentence from a CEO’s Letter to Shareholder. We can then read that file into the database as:\npython -m prodigy db-in ner_sample ner-input.jsonl\n✔ Created dataset 'ner_sample' in database SQLite\n✔ Imported 1 annotations to 'ner_sample' (session\n2022-10-11_13-22-26) in database SQLite\nFound and keeping existing \"answer\" in 0 examples\nA few points to note. First, by running this command, Prodigy will create a new dataset named ner_sample into our default SQLite database.\nTODO: explain other aspects. Example of db-in options\n\n\n\n\n\n\nNote\n\n\n\nWhat if your documents are not on a sentence level to start with? That’s okay, you can use spaCy and its sentence segmenter in the en_core_web_sm model.\n\n\nsentences.jsonl\n\n{\"text\": \"This is a sentence. This is another sentence. And then this is a sentence.\"}\n\n\nimport spacy\nimport srsly\n\nnlp = spacy.load(\"en_core_web_sm\")\nexamples = srsly.read_jsonl(\"sentences.jsonl\")\ntexts = (eg[\"text\"] for eg in examples)\n\nsentences = []\nfor doc in nlp.pipe(texts):\n  for sent in doc.sents:\n    sentences.append({\"text\": sent.text}) \n\nprint(sentences)\n\n[{'text': 'This is a sentence.'}, {'text': 'This is another sentence.'}, {'text': 'And then this is a sentence.'}]\n\n\nYou can use srsly.write_jsonl to export the data to a .jsonl:\nsrsly.write_jsonl(output_loc, new_examples)\nAlternatively, with Prodigy you can train your own sentence segmenter or use one of spaCy universe’s projects like pySBD.\nSeveral off-the-shelf Prodigy recipes like ner.correct or ner.teach use a sentence segmenter as it’s default behavior. You may turn off this behavior to not segment sentences by adding --unsegmented to the recipe."
  },
  {
    "objectID": "posts/2022-10-11-prodigy-db-intro/index.html#db-merge",
    "href": "posts/2022-10-11-prodigy-db-intro/index.html#db-merge",
    "title": "Retrieving annotations in Prodigy",
    "section": "db-merge",
    "text": "db-merge\nTODO: explain db-merge.\npython -m prodigy db-merge new_dataset  \nTODO: Give an illustrative example\nTODO: Problem with db-merge memory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Wesslen",
    "section": "",
    "text": "prodigy\n\n\ndatabase\n\n\norm\n\n\n\nORM’s, peewee, and how to access Prodigy’s database with Python.\n\n\n\nRyan Wesslen\n\n\nOct 12, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\nprodigy\n\n\ndatabase\n\n\nsqlite\n\n\n\nHow to use Prodigy database recipes for handling annotated data.\n\n\n\nRyan Wesslen\n\n\nOct 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvis\n\n\nspacy\n\n\n\nCreating an app using spaCy with Shiny for Python\n\n\n\nRyan Wesslen\n\n\nOct 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprodigy\n\n\nchinese\n\n\nner\n\n\nconfig\n\n\n\nOverview of how to annotate with Chinese in Prodigy\n\n\n\nRyan Wesslen\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprodigy\n\n\ntextcat\n\n\ninter-rater relilability\n\n\nmulti-user sessions\n\n\n\n\n\n\n\nRyan Wesslen\n\n\nSep 27, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ryan Wesslen",
    "section": "",
    "text": "I’m a machine learning engineer at Explosion"
  }
]