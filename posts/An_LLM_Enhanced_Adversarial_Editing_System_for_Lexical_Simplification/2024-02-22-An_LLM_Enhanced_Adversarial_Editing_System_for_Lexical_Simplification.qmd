
---
title: "An LLM-Enhanced Adversarial Editing System for Lexical Simplification"
id: "2402.14704v1"
description: "Proposed LS method uses Adversarial Editing System and LLM-enhanced loss for lexical simplification."
author: Keren Tan, Kangyang Luo, Yunshi Lan, Zheng Yuan, Jinlong Shu
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14704v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14704v1/x1.png)

### **Summary:**
- Lexical Simplification (LS) aims to simplify text at the lexical level.
- The proposed method, LAE-LS, employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences.
- LAE-LS introduces an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system.

### Major Findings:
1. LAE-LS achieves the best results on LexMTurk and NNSeval datasets for the CWI task, outperforming LSBert without any annotated parallel corpora.
2. LAE-LS outperforms all baselines on the three datasets for the SG task, demonstrating the effectiveness of the Difficulty-aware Filling module.
3. LAE-LS consistently outperforms the baselines when integrating CWI and SG together for the LS task, showcasing its effectiveness in identifying complex words and predicting substitutes.

### Analysis and Critique:
- The proposed LAE-LS method demonstrates effectiveness in addressing the lexical simplification task without parallel corpora, showcasing potential for leveraging large language models to enhance the simplification process.
- The method's performance is competitive with significantly smaller parameter size compared to powerful large language models.
- The ablation study shows that the loss functions and the Difficulty-aware Filling module are vital for the training of the Edit Predictor, contributing to the method's effectiveness.
- Case studies demonstrate that LAE-LS accurately identifies complex words and generates simplified versions with consideration of semantic preservation and simplified ratio, outperforming other methods.

This article provides a comprehensive and effective method for lexical simplification without parallel corpora, showcasing the potential of leveraging large language models to enhance the simplification process. The method's performance is competitive with significantly smaller parameter size compared to powerful large language models. The ablation study and case studies further support the effectiveness of the proposed method. However, further research could explore the potential biases and ethical considerations in the application of large language models for text simplification.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.14704v1](https://arxiv.org/abs/2402.14704v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14704v1](https://browse.arxiv.org/html/2402.14704v1)       |
| Truncated       | False       |
| Word Count       | 7222       |