
---
title: "An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach"
id: "2402.13871v1"
description: "Phishing emails are a serious threat. Our DistilBERT model effectively detects them with high accuracy."
author: Mohammad Amaz Uddin, Iqbal H. Sarker
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13871v1/extracted/5401888/figs/Methodology.png"
categories: ['robustness', 'prompt-engineering', 'security', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13871v1/extracted/5401888/figs/Methodology.png)

### **Summary:**
- Phishing emails are a significant cyber threat that poses risks to individuals and organizations.
- Large Language Models (LLMs) like DistilBERT have shown promise in detecting phishing emails.
- The study fine-tuned the DistilBERT model and achieved high accuracy in detecting phishing emails.
- Explainable AI techniques like LIME and Transformer Interpret were used to understand the model's decision-making process.

### **Major Findings:**
1. Large Language Models like DistilBERT are effective in detecting phishing emails.
2. The fine-tuned DistilBERT model achieved high accuracy in both imbalanced and balanced datasets.
3. Explainable AI techniques like LIME and Transformer Interpret provided insights into the model's decision-making process.

### **Analysis and Critique:**
- The study effectively demonstrated the effectiveness of the fine-tuned DistilBERT model in detecting phishing emails.
- The use of explainable AI techniques provided transparency and insights into the model's decision-making process.
- The comparison with previous studies showed the competitive performance of the proposed model.
- Future research could explore the use of different BERT-based models and datasets to further enhance the model's robustness.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.13871v1](https://arxiv.org/abs/2402.13871v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13871v1](https://browse.arxiv.org/html/2402.13871v1)       |
| Truncated       | False       |
| Word Count       | 8097       |