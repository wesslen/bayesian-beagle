
---
title: "How do Large Language Models Handle Multilingualism?"
id: "2402.18815v1"
description: "LLMs Understand, Solve Problems, and Respond in Multilingual Contexts; PLND Detects Language-Specific Neurons."
author: Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, Lidong Bing
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.18815v1/x1.png"
categories: ['social-sciences', 'education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18815v1/x1.png)

### **Summary:**

- Large language models (LLMs) have demonstrated remarkable performance across various languages.
- The study introduces a framework detailing how LLMs process multilingual inputs:
  - Initial layers understand the question, converting multilingual inputs into English.
  - Intermediate layers engage in problem-solving using English and incorporating multilingual knowledge.
  - Last layers generate responses in the original language of the query.
- A novel Parallel Language-specific Neuron Detection (PLND) method is proposed to identify neurons activated by input language without labels.
- The study verifies the framework through comprehensive ablation analysis and demonstrates that it can enhance the multilingual ability with less training effort.

### Major Findings:
1. LLMs process multilingual inputs in distinct layers, initially converting them into English, solving problems using English and multilingual knowledge, and generating responses in

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.18815v1](https://arxiv.org/abs/2402.18815v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18815v1](https://browse.arxiv.org/html/2402.18815v1)       |
| Truncated       | False       |
| Word Count       | 5829       |