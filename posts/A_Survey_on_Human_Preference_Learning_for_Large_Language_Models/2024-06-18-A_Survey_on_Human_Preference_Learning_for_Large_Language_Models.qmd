
---
title: "A Survey on Human Preference Learning for Large Language Models"
id: "2406.11191v2"
description: "This survey explores human preference learning for large language models, covering feedback sources, modeling, usage, and evaluation."
author: Ruili Jiang, Kehai Chen, Xuefeng Bai, Zhixuan He, Juntao Li, Muyun Yang, Tiejun Zhao, Liqiang Nie, Min Zhang
date: "2024-06-18"
image: "https://browse.arxiv.org/html/2406.11191v2/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.11191v2/x1.png)

### Summary:

This survey reviews the progress in exploring human preference learning for large language models (LLMs) from a preference-centered perspective. It covers the sources and formats of preference feedback, the modeling and usage of preference signals, as well as the evaluation of the aligned LLMs. The survey categorizes human feedback according to data sources and formats, summarizes techniques for human preferences modeling, and presents various preference usage methods sorted by the objectives to utilize human preference signals. The survey also summarizes some prevailing approaches to evaluate LLMs in terms of alignment with human intentions and discusses the outlooks on the human intention alignment for LLMs.

### Major Findings:

1. Human preference learning can effectively align LLMs with human intentions by optimizing LLMs according to feedback information on their outputs that reflects the preferences and thus specifies the intentions of humans.
2. The quality and scale of preference feedback are of great importance for human preference learning, while the sources of feedback collection can heavily influence them.
3. The feedback formats adopted in works on human preference learning broadly include relative relations that are natural for preference expression but less informative, and absolute properties that are more informative about human preferences but harder to collect.

### Analysis and Critique:

The survey provides a comprehensive review of the development timeline and recent advances in human preference learning for LLMs. However, it does not discuss the limitations and challenges of human preference learning for LLMs. For instance, the survey does not address the issue of bias in human preference feedback, which can lead to biased LLMs. Additionally, the survey does not discuss the potential risks of using LLMs to simulate human feedback, such as the risk of overfitting to the feedback data. Furthermore, the survey does not provide a critical evaluation of the effectiveness of the different preference usage methods presented. It would be beneficial to compare the performance of these methods and identify the most effective ones.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.11191v2](https://arxiv.org/abs/2406.11191v2)        |
| HTML     | [https://browse.arxiv.org/html/2406.11191v2](https://browse.arxiv.org/html/2406.11191v2)       |
| Truncated       | False       |
| Word Count       | 12234       |