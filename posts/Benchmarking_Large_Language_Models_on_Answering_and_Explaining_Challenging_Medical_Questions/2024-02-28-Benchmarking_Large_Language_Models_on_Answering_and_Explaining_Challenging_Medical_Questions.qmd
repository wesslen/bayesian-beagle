
---
title: "Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions"
id: "2402.18060v1"
description: "LLMs perform well on medical questions, but current benchmarks lack complexity. New datasets address this."
author: Hanjie Chen, Zhouxiang Fang, Yash Singla, Mark Dredze
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18060v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18060v1/x1.png)

### **Summary:**
- Large language models (LLMs) have shown impressive performance in answering medical questions, including passing medical licensing exams.
- However, existing benchmarks do not capture the complexity of realistic clinical cases and lack reference explanations for answers, hindering the evaluation of model explanations.
- To address these challenges, two new datasets, JAMA Clinical Challenge and Medbullets, have been constructed, consisting of challenging clinical cases and USMLE Step 2&3 style clinical questions, respectively.
- Four LLMs were evaluated on the two datasets, and it was found that the datasets are harder than previous benchmarks, highlighting the need for new metrics to support future research on explainable medical question-answering.

### Major Findings:
1. The new datasets, JAMA Clinical Challenge and Medbullets, are harder than previous benchmarks, posing a new challenge for medical LLM research.
2. The inconsistency between automatic and human evaluations of model-generated explanations emphasizes the necessity of developing new metrics for explainable medical question-answering.
3. LLMs have shown promising results in generating explanations for medical questions, but the quality of these explanations is challenging to evaluate.

### Analysis and Critique:
- The article effectively highlights the limitations of existing benchmarks and the need for more challenging datasets to evaluate LLMs in the medical domain.
- The inconsistency between automatic and human evaluations of model-generated explanations raises concerns about the reliability of current evaluation metrics and the need for more robust measures.
- The article provides valuable insights into the challenges of evaluating LLMs in the medical domain and emphasizes the importance of developing new metrics to support future research in this area. However, the article could benefit from a more detailed discussion of potential biases in the datasets and the ethical implications of using LLMs in medical question-answering tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-29       |
| Abstract | [https://arxiv.org/abs/2402.18060v1](https://arxiv.org/abs/2402.18060v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18060v1](https://browse.arxiv.org/html/2402.18060v1)       |
| Truncated       | False       |
| Word Count       | 7906       |