
---
title: "Is ChatGPT a Better Explainer than My Professor?: Evaluating the Explanation Capabilities of LLMs in Conversation Compared to a Human Baseline"
id: "2406.18512v1"
description: "LLMs can enhance expert explainers' conversational skills, improving science communication, especially when using concise responses and thought-provoking questions."
author: Grace Li, Milad Alshomary, Smaranda Muresan
date: "2024-06-26"
image: "https://browse.arxiv.org/html/2406.18512v1/extracted/5693706/images/labeled-dialogue.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.18512v1/extracted/5693706/images/labeled-dialogue.png)

### Summary:

- The study evaluates the explanation capabilities of Large Language Models (LLMs) in conversational settings compared to a human baseline.
- The 5-Levels dataset, annotated with explanatory acts, is used to audit the ability of LLMs in engaging in explanation dialogues.
- Three different strategies are compared: (1) Baseline - human explainer response, (2) GPT4 Standard - GPT explainer response given the previous conversational context, and (3) GPT4 w/ EA - GPT explainer response given the previous conversational context and a sequence of explanatory act(s) to integrate into its response.
- The results show that GPT generated explainer responses were preferred over the human baseline, emphasizing the challenge of effective science communication between experts and everyday people.
- Annotators preferred S2: GPT Standard responses over S2: GPT w/ EA responses mainly due to the concise and succinct responses.
- For the few times that S3 outperformed S2, annotators noted dimensions of explainee engagement and use of thought-provoking questions as the main reasons for better performance.
- The results demonstrate the ability of LLMs to generate responses based on sequences of explanatory acts, allowing for future research to explore the specific contexts and strategies of explanations to improve science communication.

### Major Findings:

1. GPT generated explainer responses were preferred over the human baseline, emphasizing the challenge of effective science communication between experts and everyday people.
2. Annotators preferred S2: GPT Standard responses over S2: GPT w/ EA responses mainly due to the concise and succinct responses.
3. For the few times that S3 outperformed S2, annotators noted dimensions of explainee engagement and use of thought-provoking questions as the main reasons for better performance.

### Analysis and Critique:

- The study provides valuable insights into the capabilities of LLMs in generating explainer responses and engaging in explanation dialogues.
- The preference for S2: GPT Standard responses over S2: GPT w/ EA responses highlights the importance of concise and succinct responses in effective science communication.
- The instances where S3: GPT w/ EA outperformed S2

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.18512v1](https://arxiv.org/abs/2406.18512v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.18512v1](https://browse.arxiv.org/html/2406.18512v1)       |
| Truncated       | False       |
| Word Count       | 4167       |