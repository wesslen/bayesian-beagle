
---
title: "Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera"
id: "2408.07982v1"
description: "LLMs can recognize user emotions from facial expressions, enabling AI agents to interact based on emotional states, especially for high-scoring emotions like Happy and Angry."
author: Hiroki Tanioka, Tetsushi Ueta, Masahiko Sano
date: "2024-08-15"
image: "https://browse.arxiv.org/html/2408.07982v1/x1.png"
categories: ['education', 'social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.07982v1/x1.png)

### Summary:

The study explores the use of Large Language Models (LLMs) like ChatGPT© in multimodal dialogue systems, focusing on the recognition of user emotions from facial expressions. The authors propose a system called FacingBot (FBot) that employs a python library FER for emotion recognition. The FBot system configuration involves generating user faces using ChatGPT-4o, recognizing emotions from these faces, and adding the emotional information to prompts in JSON format. The study confirms that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry.

### Major Findings:

1. The study confirms that AI agents can interact with users according to their emotional states by capturing the user in dialogue with a camera, recognizing emotions from facial expressions, and adding such emotion information to prompts.
2. The results confirm that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry.
3. The study also confirms that gpt-3.5-turbo can recognize emotional states from users’ facial expressions using FER and can respond to multimodal queries with emotion information according to the emotional state.

### Analysis and Critique:

While the study provides a promising approach to multimodal dialogue systems, there are several limitations and areas for improvement. The recognition results by FER are variable and can fluctuate depending on the proximity, brightness, and angle of the face. The study also does not address how to summarize the emotional information added to the text of the query by the user, as the facial expressions of the user are not always constant. Furthermore, the study does not consider the ethical implications of using facial recognition technology, such as privacy concerns and potential biases in emotion recognition. Future research should address these issues to improve the robustness and ethical considerations of the proposed system.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.07982v1](https://arxiv.org/abs/2408.07982v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.07982v1](https://browse.arxiv.org/html/2408.07982v1)       |
| Truncated       | False       |
| Word Count       | 2786       |