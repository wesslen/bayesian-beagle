
---
title: "Using Pretrained Large Language Model with Prompt Engineering to Answer Biomedical Questions"
id: "2407.06779v1"
description: "Team built a biomedical QA system using LLMs, achieving notable scores in BioASQ 2024 Task 12b."
author: Wenxin Zhou, Thuy Hang Ngo
date: "2024-07-09"
image: "https://browse.arxiv.org/html/2407.06779v1/extracted/5720066/images/data_format.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.06779v1/extracted/5720066/images/data_format.png)

### Summary:

The paper presents a two-level information retrieval and question-answering system based on pre-trained large language models (LLM) for the BioASQ 2024 Task12b and Synergy tasks. The system focuses on LLM prompt engineering and response post-processing, using prompts with in-context few-shot examples and post-processing techniques like resampling and malformed response detection. The performance of various pre-trained LLM models, including Mixtral, OpenAI GPT, and Llama2, is compared on this challenge. The best-performing system achieved 0.14 MAP score on document retrieval, 0.05 MAP score on snippet retrieval, 0.96 F1 score for yes/no questions, 0.38 MRR score for factoid questions, and 0.50 F1 score for list questions in Task 12b.

### Major Findings:

1. The proposed two-level information retrieval and question-answering system based on pre-trained LLM models achieved promising results in the BioASQ 2024 Task12b and Synergy tasks.
2. The system uses prompt engineering and response post-processing techniques, such as in-context few-shot examples and malformed response detection, to improve performance.
3. The performance of various pre-trained LLM models, including Mixtral, OpenAI GPT, and Llama2, was compared, with Mixtral 47B being the best-performing model overall.

### Analysis and Critique:

1. The paper does not provide a detailed comparison of the proposed system with other state-of-the-art methods in the BioASQ 2024 Task12b and Synergy tasks.
2. The paper does not discuss the limitations and potential biases of the proposed system, which could be important for future research and development.
3. The paper does not provide a comprehensive analysis of the performance of different pre-trained LLM models, which could be useful for selecting the most suitable model for a given task.
4. The paper does not discuss the potential impact of the proposed system on the biomedical research community and its practical applications.
5. The

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.06779v1](https://arxiv.org/abs/2407.06779v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.06779v1](https://browse.arxiv.org/html/2407.06779v1)       |
| Truncated       | False       |
| Word Count       | 5905       |