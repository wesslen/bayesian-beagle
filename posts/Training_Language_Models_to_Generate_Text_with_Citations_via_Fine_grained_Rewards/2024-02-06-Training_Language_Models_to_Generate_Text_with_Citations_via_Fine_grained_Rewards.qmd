
---
title: "Training Language Models to Generate Text with Citations via Fine-grained Rewards"
id: "2402.04315v1"
description: "LLMs need in-text citations for credibility. Proposed training framework improves citation generation. Outperforms GPT-3.5-turbo."
author: Chengyu Huang, Zeqiu Wu, Yushi Hu, Wenya Wang
date: "2024-02-06"
image: "https://browse.arxiv.org/html/2402.04315v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.04315v1/x1.png)

### Summary:
In this article, the authors propose a training framework using fine-grained rewards to teach Large Language Models (LLMs) to generate highly supportive and relevant citations. They conduct extensive experiments on Question Answering (QA) datasets and validate the model’s generalizability using EXPERTQA. The results show that training with fine-grained rewards significantly improves the performance of LLMs, enabling smaller LLMs to surpass larger ones like ChatGPT. The authors also compare fine-grained rewards with holistic ones and find that fine-grained rewards are more effective in almost all training setups and datasets.

### Major Findings:
1. Training with fine-grained rewards greatly boosts performance, especially when combined with reinforcement learning (RL).
2. Fine-grained rewards help smaller LLMs surpass ChatGPT, closing the performance gap between them.
3. Fine-grained rewards are better than holistic rewards, showing higher performance gains in almost all metrics and datasets.

### Analysis and Critique:
- The authors highlight the need for further improvement in LLMs' correctness recall on QA datasets, especially in capturing the remaining answers in the documents.
- They also discuss the limitations of their method, such as the need for an initial distillation step with ChatGPT, which may hinder accessibility when larger, more capable LLMs are not available.
- The authors propose future directions for exploration, including enhancing LLMs’ reading comprehension and synthesis ability and iteratively using In-Context Learning and beam search sampling to bootstrap responses of high quality.

Overall, the article provides valuable insights into training language models to generate text with citations and highlights the potential for further research and improvement in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.04315v1](https://arxiv.org/abs/2402.04315v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04315v1](https://browse.arxiv.org/html/2402.04315v1)       |
| Truncated       | False       |
| Word Count       | 10252       |