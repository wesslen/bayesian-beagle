
---
title: "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4"
description: "This paper presents 26 principles for querying large language models, validated through experiments on different models."
author: "['Sondos Mahmoud Bsharat', 'Aidar Myrzakhan', 'Zhiqiang Shen']"
date: "2023-12-26"
image: "https://browse.arxiv.org/html/2312.16171v1/x1.png"
categories: ['prompt engineering']
authors: Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.16171v1/x1.png)

### Major Takeaways

1. Large language models (LLMs) like ChatGPT have demonstrated impressive abilities but there is a challenge in designing optimal instructions or prompts for them, especially for common users.
2. The paper introduces 26 guiding principles for formulating queries and prompts to enhance user comprehension and improve the quality of responses from pretrained LLMs.
3. Extensive experiments on LLaMA-1/2, GPT-3.5/4 show that the proposed principles can significantly improve the quality, accuracy, and correctness of LLM responses.

### Principles

- **Motivation**: Crafting prompts that LLMs can comprehend and respond to effectively to program the interaction between a user and the LLM.
- **Conciseness and Clarity**: Prompts should be concise, specific, and clear to guide the model effectively.
- **Contextual Relevance**: Providing context that helps the model understand the background and domain of the task.
- **Task Alignment**: Phrasing prompts to clearly indicate the nature of the task to the model.
- **Avoiding Bias**: Design prompts to minimize biases and use neutral language for sensitive topics.
- **Incremental Prompting**: Structuring prompts to guide the model through a sequence of steps.

### Experiments and Results

- The experiments show that the proposed principles lead to a significant improvement in the quality, accuracy, and correctness of LLM responses across different model scales.
- The boosts in response quality and correctness are particularly pronounced in larger-scale models such as GPT-3.5/4.

### Conclusion

- The paper demonstrates that carefully crafted principled instructions can significantly enhance the relevance, brevity, and objectivity of LLM responses.
- Future exploration could involve refining base models to align with principled instructions further with alternative strategies and integrating successful strategies into standard LLM operations.

### Critique

- The effectiveness of the principles may diminish with complex or highly specialized questions, and different LLM architectures may respond differently to these principles.
- The assessment of the principles was based on a limited selection of questions, and expanding the question set in future research could yield more generalized findings.

In summary, the paper provides valuable insights into the design of prompts for large language models and presents evidence for the effectiveness of principled instructions in improving LLM performance. However, it is important to consider potential limitations and acknowledge the need for further research to validate the principles across different models and a wider range of question types.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| HTML     | []()       |
| Truncated       | False       |
| Word Count       | 5205       |