
---
title: "Pushing The Limit of LLM Capacity for Text Classification"
id: "2402.07470v1"
description: "RGPT boosts text classification LLM performance, outperforming 8 PLMs and 7 LLMs by 1.36% on average."
author: Yazhou Zhang, Mengyao Wang, Chenyu Ren, Qiuchi Li, Prayag Tiwari, Benyou Wang, Jing Qin
date: "2024-02-12"
image: "https://browse.arxiv.org/html/2402.07470v1/extracted/5403443/images/model.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.07470v1/extracted/5403443/images/model.png)

### Summary:
- The article introduces RGPT, an adaptive boosting framework for text classification using large language models (LLMs).
- RGPT aims to explore the potential of LLMs in text classification by adjusting sample distributions and recurrently ensembling strong base learners.
- Through empirical comparison, RGPT outperforms state-of-the-art PLMs and LLMs on four benchmarks by 1.36% on average.
- The proposed framework achieves the state-of-the-art in text classification through a boosting strategy.

### Major Findings:
1. RGPT significantly outperforms 8 SOTA PLMs and 7 SOTA LLMs on four benchmarks by 1.36% on average.
2. The proposed RGPT model achieves state-of-the-art results in text classification through a boosting strategy.
3. LLMs demonstrate greater advancedness over PLMs for text classification.

### Analysis and Critique:
- The article does not thoroughly examine how well the model may work on a wider range of text classification tasks.
- The proposed RGPT model has high computational cost due to its iterative nature, involving multiple rounds of fine-tuning LLMs.
- The study does not address the potential risks of overfitting during the repeated fine-tuning of large language models.

Overall, the article presents a promising approach for text classification using LLMs, but it has limitations in terms of computational cost and potential overfitting risks. Further research is needed to evaluate the model's performance across a wider range of text classification tasks and address the potential risks associated with overfitting.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.07470v1](https://arxiv.org/abs/2402.07470v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07470v1](https://browse.arxiv.org/html/2402.07470v1)       |
| Truncated       | False       |
| Word Count       | 6164       |