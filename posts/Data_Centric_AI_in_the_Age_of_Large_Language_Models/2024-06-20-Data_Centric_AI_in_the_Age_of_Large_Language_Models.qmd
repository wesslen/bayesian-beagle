
---
title: "Data-Centric AI in the Age of Large Language Models"
id: "2406.14473v1"
description: "Data-centric viewpoint for AI research: Prioritizing data in large language models for benchmarks, attribution, knowledge transfer, and inference contextualization."
author: Xinyi Xu, Zhaoxuan Wu, Rui Qiao, Arun Verma, Yao Shu, Jingtan Wang, Xinyuan Niu, Zhenfeng He, Jiangwei Chen, Zijian Zhou, Gregory Kang Ruey Lau, Hieu Dao, Lucas Agussurja, Rachael Hwee Ling Sim, Xiaoqiang Lin, Wenyang Hu, Zhongxiang Dai, Pang Wei Koh, Bryan Kian Hsiang Low
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.14473v1/extracted/5679193/flow.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.14473v1/extracted/5679193/flow.png)

# Summary

This position paper proposes a data-centric viewpoint of AI research, focusing on large language models (LLMs). The authors argue that data plays a crucial role in the developmental and inferential stages of LLMs, yet it receives disproportionately low attention from the research community. They identify four specific scenarios centered around data: data-centric benchmarks and data curation, data attribution, knowledge transfer, and inference contextualization. In each scenario, the authors highlight the importance of data, promising research directions, and potential impacts on the research community and society.

## Major Findings:

1. **Data-Centric Benchmarks and Data Curation**: The authors advocate for a suite of data-centric benchmarks tailored to the scale and complexity of data for LLMs. These benchmarks can be used to develop new data curation methods and document research efforts and results, which can help promote openness and transparency in AI and LLM research.

2. **Data Attribution**: The authors emphasize the importance of data attribution for legal and safety purposes, such as respecting copyright/intellectual property rights and mitigating problematic outputs of LLMs. They describe promising directions for data attribution and removal.

3. **Knowledge Transfer**: The authors discuss the potential of transferring the knowledge of trained LLMs to compact and specialized models. They highlight existing efforts and new opportunities where the outputs of a trained LLM are treated as (synthesized) data.

4. **Inference Contextualization with Data**: The authors describe how LLMs can flexibly use data at inference to augment the outputsâ€™ factuality or quality. They elaborate on this paradigm with respect to two prevalent technical frameworks and highlight how it can improve the personalization of LLMs.

## Analysis and Critique:

1. **Limited Research on Data-Centric Approaches**: While the paper provides a comprehensive overview of the role of data in LLMs, it also highlights the lack of research in this area. The authors argue that the bulk of research to date has focused on modeling improvements, with little attention paid to how to best use data for the developmental and inferential stages of LLMs.

2. **Challenges in Data Attribution and Unlearning**:

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.14473v1](https://arxiv.org/abs/2406.14473v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.14473v1](https://browse.arxiv.org/html/2406.14473v1)       |
| Truncated       | False       |
| Word Count       | 10052       |