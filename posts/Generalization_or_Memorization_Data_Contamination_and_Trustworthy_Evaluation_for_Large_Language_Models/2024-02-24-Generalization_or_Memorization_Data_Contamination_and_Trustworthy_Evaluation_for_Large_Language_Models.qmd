
---
title: "Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models"
id: "2402.15938v1"
description: "Large language models (LLMs) are susceptible to data contamination. CDD and TED mitigate this issue."
author: Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Ge Li
date: "2024-02-24"
image: "https://browse.arxiv.org/html/2402.15938v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.15938v1/x1.png)

### Summary:
- Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks.
- LLMs are more susceptible to data contamination due to the vast size and wide-ranging sources of their training data.
- The proposed CDD and TED approaches aim to detect data contamination and ensure trustworthy evaluation for LLMs by using the LLM's output distribution.

### Major Findings:
1. CDD achieves an average relative improvement of 21.8%-30.2% over other contamination detection approaches in terms of Accuracy, F1 Score, and AUC metrics.
2. TED significantly mitigates performance improvements up to 66.9% attributed to data contamination across 24 settings and 21 contamination degrees.
3. ChatGPT exhibits a high potential to suffer from data contamination on the HumanEval benchmark.

### Analysis and Critique:
- The proposed CDD and TED approaches show promising results in detecting data contamination and mitigating its impact. However, the study is mainly focused on benchmarks for code generation and logical reasoning, and further validation on other benchmarks is needed.
- The approaches require multiple samplings to compute the output distribution, which may lead to time overhead.
- The study assumes that the base LLMs used do not suffer from data leakage on the selected benchmarks, which may not reflect the real-world scenario accurately. Further research is needed to address these limitations.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.15938v1](https://arxiv.org/abs/2402.15938v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.15938v1](https://browse.arxiv.org/html/2402.15938v1)       |
| Truncated       | False       |
| Word Count       | 5896       |