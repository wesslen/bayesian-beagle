
---
title: "How Can LLM Guide RL? A Value-Based Approach"
id: "2402.16181v1"
description: "RL algorithms need extensive trial-and-error; LLM guidance improves sample efficiency in planning tasks."
author: Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang
date: "2024-02-25"
image: "https://browse.arxiv.org/html/2402.16181v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16181v1/x1.png)

### **Summary:**
- Reinforcement learning (RL) algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement.
- Recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks.
- The paper studies how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms.

### **Major Findings:**
1. The proposed algorithm incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning.
2. The algorithm achieves state-of-the-art success rates and surpasses previous RL and LLM approaches in terms of sample efficiency.
3. The algorithm simplifies the construction of the value function and employs sub-goals to reduce the search complexity.

### **Analysis and Critique:**
- The paper provides a novel approach to integrating LLMs into RL algorithms, addressing the limitations of both RL and LLMs.
- The proposed algorithm demonstrates improved sample efficiency and success rates across various interactive environments.
- The theoretical analysis and empirical results support the effectiveness of the algorithm in leveraging LLM guidance for decision-making tasks.
- However, the paper does not address potential limitations or challenges in real-world applications, and further research is needed to validate the algorithm's performance in more complex and diverse environments.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.16181v1](https://arxiv.org/abs/2402.16181v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16181v1](https://browse.arxiv.org/html/2402.16181v1)       |
| Truncated       | False       |
| Word Count       | 8575       |