
---
title: "Training-Free Long-Context Scaling of Large Language Models"
id: "2402.17463v1"
description: "DCA enables LLMs to process long sequences without continual training, achieving comparable performance to finetuned models."
author: Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17463v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17463v1/x1.png)

### Summary:
The article introduces Dual Chunk Attention (DCA) as a method to enable Large Language Models (LLMs) to support context windows of more than 100k tokens without continual training. DCA decomposes the attention computation for long sequences into chunk-based modules, effectively capturing relative positional information within and across chunks. The method achieves performance on practical long-context tasks comparable to or better than finetuned models and is a viable open-source alternative. The article also provides a comprehensive evaluation of the models on various tasks, including language modeling, passkey retrieval, and real-world long-context applications.

### Major Findings:
1. **Extrapolation:** DCA allows LLMs with a 4k context window to be expanded to more than 32k without training, maintaining a negligible increase in Perplexity (PPL).
2. **Orthogonality:** DCA is orthogonal to existing popular scaled positional encodings and supports a context length of 192k while maintaining high passkey retrieval accuracy and low perplexity.
3. **Long-Context Understanding:** DCA achieves performance comparable to or surpassing that of existing state-of-the-art models built through costly continual training.

### Analysis and Critique:
- **Efficiency:** DCA sustains similar GPU memory consumption and inference speed without adding considerable overhead.
- **Ablation Study:** The integration of intra-chunk, inter-chunk, and successive chunk attention mechanisms in DCA results in both low PPL and high retrieval accuracy.
- **Performance on Unseen Data:** ChunkLlama 70B exhibits a remarkably high accuracy rate on elementary questions and promising outcomes on more challenging queries, but still faces difficulties with questions requiring global understanding.

Overall, the article presents a novel and efficient approach to overcoming context length limitations in LLMs, with potential societal consequences and significant impact on the industry. However, the performance of the models on unseen data and challenging queries remains a point of concern.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.17463v1](https://arxiv.org/abs/2402.17463v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17463v1](https://browse.arxiv.org/html/2402.17463v1)       |
| Truncated       | False       |
| Word Count       | 8871       |