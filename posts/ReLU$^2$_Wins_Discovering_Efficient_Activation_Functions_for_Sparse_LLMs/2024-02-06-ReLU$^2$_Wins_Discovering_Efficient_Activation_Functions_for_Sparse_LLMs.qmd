
---
title: "ReLU$^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs"
id: "2402.03804v1"
description: "Sparse computation for Large Language Models in low-resource scenarios, using non-ReLU activation functions. ReLU$^2$ is most efficient."
author: Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong Sun
date: "2024-02-06"
image: "../../../bayesian-beagle.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The article discusses the use of sparse computation for Large Language Models (LLMs) in low-resource scenarios, proposing a systematic framework to examine the sparsity of LLMs and comparing the performance of different activation functions.
- It presents findings from experiments analyzing the impact of different activation functions on LLMs, highlighting ReLU2 as the most efficient activation function for sparse LLMs.
- The section also discusses the computational relationships between tokens and neurons in LLMs, identifying ReLU2 as the most efficient activation function for reducing computational cost and I/O overhead significantly.
- Additionally, the article provides detailed information about the experimental setup and training process of the 1.3B model, including the architecture, pre-training data, and training hyperparameters, as well as the threshold-finding method used in the experiments.

### Major Findings:
1. ReLU2 is identified as the most efficient activation function for sparse LLMs, offering substantial reductions in computational cost and I/O overhead.
2. The performance of LLMs is not sensitive to tail truncation when the sparsity ratio is smaller than 0.7, but drops significantly when the sparsity ratio is larger than 0.7.
3. ReGLU-based LLaMA models achieve comparable performance with the original LLaMA-27B models under certain sizes, but there is a performance gap under larger sizes, potentially due to insufficient pre-training data.

### Analysis and Critique:
- The article provides valuable insights into optimizing LLMs and can guide future research in developing more efficient language models.
- The findings suggest that the ReGLU-based LLaMA models may require additional pre-training data to achieve optimal performance, highlighting a potential area for further research.
- The threshold-finding method and the concept of hot-activated neurons are crucial for understanding the efficiency of activation functions in sparse LLMs, providing avenues for further exploration and research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-07       |
| Abstract | [https://arxiv.org/abs/2402.03804v1](https://arxiv.org/abs/2402.03804v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.03804v1](https://browse.arxiv.org/html/2402.03804v1)       |
| Truncated       | True       |
| Word Count       | 27556       |