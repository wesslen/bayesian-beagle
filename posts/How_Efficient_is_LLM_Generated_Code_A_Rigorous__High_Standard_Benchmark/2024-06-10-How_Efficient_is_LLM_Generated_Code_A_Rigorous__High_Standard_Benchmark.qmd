
---
title: "How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark"
id: "2406.06647v1"
description: "LLMs struggle to generate expert-level efficient code, per new benchmark ENAMEL, which evaluates efficiency and correctness of LLM-generated code."
author: Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, Christopher Lott
date: "2024-06-10"
image: "https://browse.arxiv.org/html/2406.06647v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.06647v1/x1.png)

### Summary:

The paper presents a new benchmark called ENAMEL for evaluating the efficiency of code generated by large language models (LLMs). The authors propose a new metric called eff@, which generalizes the pass@ metric from correctness to efficiency and handles right-censored execution time. They also derive an unbiased and variance-reduced estimator of eff@ and provide a numerically stable implementation. The benchmark includes a set of 142 problems, excluding trivial ones with O(1) time complexity, and employs a human expert to design best algorithms and implementations as reference solutions. The authors also use strong test case generators to filter out wrong code and differentiate suboptimal algorithms. The results of an extensive study across 30 popular LLMs show that LLMs still fall short of generating expert-level efficient code. The authors conclude that LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization.

### Major Findings:

1. The authors propose a new efficiency metric called eff@, which generalizes the pass@ metric from correctness to efficiency and handles right-censored execution time.
2. The authors derive an unbiased and variance-reduced estimator of eff@ and provide a numerically stable implementation.
3. The benchmark includes a set of 142 problems, excluding trivial ones with O(1) time complexity, and employs a human expert to design best algorithms and implementations as reference solutions.
4. The results of an extensive study across 30 popular LLMs show that LLMs still fall short of generating expert-level efficient code.
5. The authors conclude that LLMs struggle in designing advanced algorithms and are barely aware of implementation optimization.

### Analysis and Critique:

* The paper presents a novel and rigorous benchmark for evaluating the efficiency of code generated by LLMs.
* The proposed metric eff@ is a significant improvement over existing metrics, as it generalizes the pass@ metric from correctness to efficiency and handles right-censored execution time.
* The use of a human expert to design best algorithms and implementations as reference solutions is a strength of the benchmark, as it ensures a high standard for efficiency evaluation.
* The study across 30 popular LLMs provides a comprehensive evaluation of the efficiency of code generated

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.06647v1](https://arxiv.org/abs/2406.06647v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06647v1](https://browse.arxiv.org/html/2406.06647v1)       |
| Truncated       | False       |
| Word Count       | 8226       |