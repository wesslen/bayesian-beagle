
---
title: "SLMRec: Empowering Small Language Models for Sequential Recommendation"
id: "2405.17890v1"
description: "SLMRec: Small Language Model for Sequential Recommendation achieves 6.6x training, 8.0x inference speedups with 13% of LLM-based model parameters."
author: Wujiang Xu, Zujie Liang, Jiaojiao Han, Xuying Ning, Wenfang Lin, Linxun Chen, Feng Wei, Yongfeng Zhang
date: "2024-05-28"
image: "https://browse.arxiv.org/html/2405.17890v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.17890v1/x1.png)

### Summary:

The paper "SLMRec: Empowering Small Language Models for Sequential Recommendation" explores the impact of Large Language Models (LLMs) on sequential recommendation systems. The authors find that while LLMs have significantly improved performance, they also escalate model size and pose challenges for real-world applications. The study aims to reassess the need for large language models in sequential recommendation and investigate the effects of reducing the number of parameters during training and inference stages.

### Major Findings:

1. The authors discover that most intermediate layers of LLMs are redundant, which motivates them to empower small language models for sequential recommendation (SLMRec) using a simple yet effective knowledge distillation method.
2. SLMRec attains the best performance using only 13% of the parameters found in LLM-based recommendation models, while simultaneously achieving up to 6.6x and 8.0x speedups in training and inference time costs, respectively.
3. The proposed SLMRec model, with a model size of less than 1 billion parameters, can deliver performance that is remarkably competitive with baselines using LLMs sized over 7 billion parameters.

### Analysis and Critique:

1. The paper presents an innovative approach to addressing the challenges posed by large language models in sequential recommendation systems. By focusing on knowledge distillation and reducing the number of parameters, the authors demonstrate that small language models can achieve competitive performance with significantly fewer resources.
2. The study's findings have important implications for real-world applications, as they suggest that smaller, more efficient models can be used to achieve similar performance to larger models. This could lead to more practical and cost-effective solutions for sequential recommendation tasks.
3. However, the paper does not discuss potential limitations or unanswered questions, such as the generalizability of the findings to other types of recommendation tasks or the impact of different knowledge distillation techniques on performance. Additionally, the study does not address the potential biases or methodological issues that may have influenced the results.
4. Further research is needed to explore the potential of small language models in other recommendation tasks and to compare the performance of different knowledge distillation techniques. Additionally, future studies should consider potential biases and methodological issues that may impact the results.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2405.17890v1](https://arxiv.org/abs/2405.17890v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.17890v1](https://browse.arxiv.org/html/2405.17890v1)       |
| Truncated       | False       |
| Word Count       | 6690       |