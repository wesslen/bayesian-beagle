
---
title: "Zero-shot cross-lingual transfer in instruction tuning of large language model"
id: "2402.14778v1"
description: "TL;DR: Instruction tuning in multilingual settings successful with proper hyperparameter tuning and large data."
author: Nadezhda Chirkova, Vassilina Nikoulina
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14778v1/x1.png"
categories: ['education', 'architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14778v1/x1.png)

### Summary:
- The article investigates zero-shot cross-lingual transfer in instruction tuning (IT) of large language models (LLMs).
- The study finds that cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multilinguality is taken into account in IT hyperparameter tuning and with large enough IT data.
- English-trained LLMs are capable of generating correct-language, comprehensive, and helpful responses in other languages, but suffer from low factuality and may occasionally have fluency errors.

### Major Findings:
1. Cross-lingual transfer does happen successfully in IT even if all stages of model training are English-centric, but only if multilinguality is taken into account in IT hyperparameter tuning and with large enough IT data.
2. Models trained on English are capable of generating correct-language, comprehensive, and helpful responses in other languages, even with complex instructions, e.g. generate the answer in a given style or language.
3. The main challenge is low factuality in non-English instruction following. Occasional fluency and logical errors, as well as infrequent code-switching can also take place.

### Analysis and Critique:
- The study provides valuable insights into the capabilities and limitations of zero-shot cross-lingual transfer in instruction tuning of LLMs.
- The article's methodology and evaluation strategy are comprehensive and provide a detailed analysis of multilingual instruction following.
- However, the study could benefit from further exploration of the factors influencing cross-lingual transfer and the potential biases in the evaluation process.
- Additionally, the article could address the implications of the findings for real-world applications and future research directions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.14778v1](https://arxiv.org/abs/2402.14778v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14778v1](https://browse.arxiv.org/html/2402.14778v1)       |
| Truncated       | False       |
| Word Count       | 7161       |