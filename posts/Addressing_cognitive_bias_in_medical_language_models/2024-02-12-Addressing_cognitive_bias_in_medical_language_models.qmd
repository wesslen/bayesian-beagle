
---
title: "Addressing cognitive bias in medical language models"
id: "2402.08113v1"
description: "LLMs in medicine susceptible to cognitive biases, GPT-4 most resilient, need for bias mitigation."
author: Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, Rama Chellappa
date: "2024-02-12"
image: "https://browse.arxiv.org/html/2402.08113v1/extracted/5405201/main_results_sam.png"
categories: ['prompt-engineering', 'social-sciences', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.08113v1/extracted/5405201/main_results_sam.png)

### Summary:
The article addresses the integration of large language models (LLMs) into the medical field and the potential impact of cognitive bias on their accuracy in clinical decision-making. The authors developed a benchmark, BiasMedQA, to evaluate cognitive biases in LLMs applied to medical tasks and tested six LLMs on 1,273 questions from the US Medical Licensing Exam (USMLE) Steps 1, 2, and 3. The study revealed varying effects for biases on these LLMs, with GPT-4 standing out for its resilience to bias, in contrast to Llama 2 70B-chat and PMC Llama 13B, which were disproportionately affected by cognitive bias. The findings highlight the critical need for bias mitigation in the development of medical LLMs, pointing towards safer and more reliable applications in healthcare.

### Major Findings:
1. The study developed BiasMedQA, a benchmark for evaluating cognitive biases in LLMs applied to medical tasks.
2. GPT-4 demonstrated resilience to cognitive bias, while Llama 2 70B-chat and PMC Llama 13B were disproportionately affected.
3. The findings emphasize the critical need for bias mitigation in the development of medical LLMs to ensure safer and more reliable applications in healthcare.

### Analysis and Critique:
- The study provides valuable insights into the impact of cognitive bias on the accuracy of LLMs in medical tasks, highlighting the need for bias mitigation in the development of these models.
- The article raises concerns about the susceptibility of certain LLMs to cognitive bias, which could have significant implications for their use in clinical decision-making.
- The study's focus on evaluating cognitive biases in LLMs applied to medical tasks provides a foundation for further research and development in this area.
- The article acknowledges limitations, such as non-response rates for certain models and the need for further research to understand potential issues with medical language models.
- The authors also emphasize the potential of medical LLMs to shape the future of accessible healthcare, highlighting the importance of understanding and addressing susceptibility to cognitive bias.

Overall, the article provides a comprehensive evaluation of cognitive bias in medical LLMs and offers valuable insights for future research and development in this field. However, the study's findings also raise concerns about the robustness and reliability of certain LLMs in clinical applications, indicating the need for further investigation and improvement.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-14       |
| Abstract | [https://arxiv.org/abs/2402.08113v1](https://arxiv.org/abs/2402.08113v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08113v1](https://browse.arxiv.org/html/2402.08113v1)       |
| Truncated       | False       |
| Word Count       | 8222       |