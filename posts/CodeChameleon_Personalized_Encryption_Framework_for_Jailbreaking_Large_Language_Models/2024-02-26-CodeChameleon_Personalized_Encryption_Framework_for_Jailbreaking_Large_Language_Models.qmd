
---
title: "CodeChameleon: Personalized Encryption Framework for Jailbreaking Large Language Models"
id: "2402.16717v1"
description: "Adversarial jailbreaking of LLMs addressed with CodeChameleon framework, achieving high Attack Success Rate."
author: Huijie Lv, Xiao Wang, Yuansen Zhang, Caishuang Huang, Shihan Dou, Junjie Ye, Tao Gui, Qi Zhang, Xuanjing Huang
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16717v1/x2.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16717v1/x2.png)

### **Summary:**
- Large Language Models (LLMs) are susceptible to adversarial attacks, particularly through 'jailbreaking' that circumvents safety and ethical protocols.
- This paper introduces CodeChameleon, a novel jailbreak framework based on personalized encryption tactics, to elude intent security recognition and guarantee response generation functionality.
- Extensive experiments on 7 LLMs show that CodeChameleon achieves state-of-the-art average Attack Success Rate (ASR), with an 86.6% ASR on GPT-4-1106.

### Major Findings:
1. CodeChameleon achieves an average ASR of 77.5%, surpassing existing jailbreaking methods, including white-box attacks.
2. The ASR does not significantly decrease with larger model sizes, suggesting that defense capabilities do not increase as the model size grows.
3. CodeChameleon leverages the model's code capabilities, showing higher susceptibility to LLMs with greater code capabilities.

### Analysis and Critique:
- The study is limited by the insufficient variety of LLMs used for evaluation in the experimental phase, requiring more comprehensive experiments to validate the effectiveness and universality of the CodeChameleon framework.
- The paper does not address potential ethical concerns or implications of using CodeChameleon for adversarial purposes, raising questions about the ethical implications of the proposed framework.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-28       |
| Abstract | [https://arxiv.org/abs/2402.16717v1](https://arxiv.org/abs/2402.16717v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16717v1](https://browse.arxiv.org/html/2402.16717v1)       |
| Truncated       | False       |
| Word Count       | 6215       |