
---
title: "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4"
id: "2312.16171v1"
description: "26 principles for efficient queries and prompts for large language models, verified on various models, to aid researchers."
author: Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen
date: "2023-12-26"
image: "https://browse.arxiv.org/html/2312.16171v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.16171v1/x1.png)

### Major Takeaways

1. **Promising Results**: The paper introduces 26 guiding principles for optimizing instructions and prompts for large language models (LLMs), demonstrating considerable improvement in response quality and correctness.

2. **Comprehensive Research**: The study investigates a wide range of behaviors when feeding prompts into LLMs, covering aspects such as prompt structure and clarity, specificity and information, user interaction and engagement, content and language style, and complex tasks and coding prompts.

3. **Applicability and Future Directions**: The principles aim to enhance the ability of LLMs to focus on crucial input context elements, but their effectiveness may vary for complex or specialized questions. The study suggests potential integration of successful strategies into standard LLM operations and further exploration via alternative strategies such as fine-tuning and reinforcement learning.

### Principles

- **Motivation**: Crafting prompts that LLMs can comprehend and respond to effectively.
- **Overview**: Grouping principles into categories such as Prompt Structure and Clarity, Specificity and Information, User Interaction and Engagement, Content and Language Style, and Complex Tasks and Coding Prompts.
- **Design Principles**: Including principles such as Conciseness and Clarity, Contextual Relevance, Task Alignment, Example Demonstrations, Avoiding Bias, Incremental Prompting, and the use of programming-like logic.

### Experiments and Results

- **Setup and Implementation**: Evaluation performed on the ATLAS benchmark, manually crafted for principled prompt evaluation.
- **Boosting and Correctness**: Assessment of response quality improvement and correctness across small, medium, and large-scale LLMs, demonstrating significant enhancements in both aspects.
- **Individual LLMs**: Detailed results demonstrating stable improvement across different LLMs and noticeable trends in correctness enhancements with larger models.

### Critique

The paper presents comprehensive research on principled instructions for querying and prompting large language models, showcasing promising results and practical guidance. However, the effectiveness of the proposed principles may be limited for very complex or highly specialized questions, and the assessment of improvement and correctness percentages was based on a limited question set, raising questions about generalizability.


## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-12       |
| Abstract | [http://arxiv.org/abs/2312.16171v1](http://arxiv.org/abs/2312.16171v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.16171v1](https://browse.arxiv.org/html/2312.16171v1)       |
| Truncated       | False       |
| Word Count       | 5205       |