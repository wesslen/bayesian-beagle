
---
title: "Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals"
id: "2405.20152v1"
description: "LVLMs' text output influenced by race, gender, and physical attributes in input images, potentially perpetuating harmful stereotypes and toxic content."
author: Phillip Howard, Kathleen C. Fraser, Anahita Bhiwandiwalla, Svetlana Kiritchenko
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.20152v1/x1.png"
categories: ['prompt-engineering', 'social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.20152v1/x1.png)

# Summary

The study "Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals" investigates the social biases in Large Vision-Language Models (LVLMs) by presenting them with identical open-ended text prompts while conditioning on images from different counterfactual sets. The counterfactual sets contain images that are largely identical in their depiction of a common subject but vary in terms of intersectional social attributes such as race and gender. The study evaluates the text produced by different models under this counterfactual generation setting at scale, producing over 57 million responses from popular LVLMs. The multi-dimensional analysis reveals that social attributes depicted in input images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of depicted individuals. The study also explores the relationship between social bias in LVLMs and their corresponding LLMs, as well as inference-time strategies to mitigate bias.

# Major Findings

1. Social attributes such as race, gender, and physical characteristics depicted in input images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of depicted individuals.
2. The study reveals that all evaluated open-source LVLMs exhibit bias along these dimensions to varying degrees, with particularly significant biases observed when images depict obese or Black people.
3. The investigation of the relationship between bias in LVLMs and their corresponding LLMs shows that the observed bias in toxicity scores across social groups follows a similar distribution in each LVLM and the LLM from which it was trained.

# Analysis and Critique

The study provides valuable insights into the social biases present in LVLMs and their potential impact on the generated text. However, there are some limitations and potential areas for improvement:

1. The study focuses on a limited set of social attributes and does not explore other factors that may contribute to biases in LVLMs, such as age, religion, or disability.
2. The study does not provide a comprehensive analysis of the potential sources of bias in LVLMs, such as the training data, model architecture, or optimization techniques.
3. The study does not evaluate the impact of biases in LVLMs on downstream tasks or applications, such as image

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.20152v1](https://arxiv.org/abs/2405.20152v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.20152v1](https://browse.arxiv.org/html/2405.20152v1)       |
| Truncated       | False       |
| Word Count       | 12319       |