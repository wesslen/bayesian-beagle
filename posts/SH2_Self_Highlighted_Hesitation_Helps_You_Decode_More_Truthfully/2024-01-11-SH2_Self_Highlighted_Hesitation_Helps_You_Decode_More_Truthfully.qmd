
---
title: "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully"
id: "2401.05930v1"
description: "TL;DR: Self-Highlighted Hesitation (SH2) method improves LLMs' accuracy and reduces hallucinations during text generation."
author: ['Jushi Kai', 'Tianhang Zhang', 'Hai Hu', 'Zhouhan Lin']
date: "2024-01-11"
image: "https://browse.arxiv.org/html/2401.05930v1/x1.png"
categories: ['robustness', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.05930v1/x1.png)

# Summary
## Findings
- The paper introduces an inference-time **method**, SH2, to help large language models (LLMs) decode more truthfully by highlighting and hesitating on key tokens.
- SH2 demonstrates significant and consistent improvements for LLMs on multiple hallucination tasks without requiring additional data or models.
- Experimental results show that SH2 effectively helps LLMs elicit factual knowledge and distinguish hallucinated contexts.

## Sections
### Introduction
- Large language models (LLMs) exhibit text generation performance but suffer from hallucinations resulting in non-factual answers.

### Related Work
- Existing approaches address LLM hallucinations through retrieval augmentation and decoding reformulation methods.

### Self-Highlighted Hesitation
- Illustration of SH2 and its aim to help LLMs decode more truthfully by highlighting and hesitating on key tokens.

### Experiment
- SH2 experimental results on multiple **benchmarks**, including TruthfulQA, FACTOR, and HaluEval-Sum utilizing LLaMA-7b and LLaMA2-7b.
- SH2 outperforms other state-of-the-art methods on various tasks.

### Analysis
- Analysis of different choices of highlighted tokens and the effect of contrastive decoding on hesitations.

## Critique
The paper lacks an explicit comparison with existing literature in the discussion section, and there is a need to address potential limitations and challenges in practical deployment of the proposed SH2 method. Additionally, the authors should provide more thorough details on the hyperparameter selection process and how they affect the performance of the SH2 method.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [http://arxiv.org/abs/2401.05930v1](http://arxiv.org/abs/2401.05930v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.05930v1](https://browse.arxiv.org/html/2401.05930v1)       |
| Truncated       | False       |
| Word Count       | 7878       |