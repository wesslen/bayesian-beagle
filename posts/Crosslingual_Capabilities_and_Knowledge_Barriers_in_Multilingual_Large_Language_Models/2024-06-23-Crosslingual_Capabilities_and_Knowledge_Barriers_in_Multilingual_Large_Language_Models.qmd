
---
title: "Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models"
id: "2406.16135v1"
description: "LLMs struggle with crosslingual knowledge transfer, but fine-tuning on mixed-language data helps improve performance."
author: Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chulin Xie, Chiyuan Zhang
date: "2024-06-23"
image: "https://browse.arxiv.org/html/2406.16135v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.16135v1/x1.png)

### Summary:

This study evaluates six state-of-the-art large language models (LLMs) on inherently crosslingual tasks. The models show promising surface-level crosslingual abilities on machine translation and embedding space analyses. However, they struggle with deeper crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in both general (MMLU benchmark) and domain-specific (Harry Potter quiz) contexts. Simple inference-time mitigation methods offer limited improvement. The study proposes fine-tuning LLMs on mixed-language data, which effectively reduces these gaps, even when using out-of-domain datasets like WikiText. The findings suggest the need for explicit optimization to unlock the full crosslingual potential of LLMs.

### Major Findings:

1. LLMs show promising surface-level crosslingual abilities on machine translation and embedding space analyses.
2. LLMs struggle with deeper crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in both general and domain-specific contexts.
3. Simple inference-time mitigation methods offer limited improvement in addressing the crosslingual knowledge barrier.
4. Fine-tuning LLMs on mixed-language data effectively reduces the crosslingual knowledge barrier, even when using out-of-domain datasets like WikiText.

### Analysis and Critique:

The study provides a comprehensive evaluation of LLMs' crosslingual capabilities, highlighting their strengths and limitations. However, it does not address the potential impact of the size and diversity of the pretraining corpus on the models' crosslingual abilities. Additionally, the study does not explore the potential of using parallel corpora for fine-tuning to improve crosslingual knowledge transfer. Furthermore, the study does not discuss the potential implications of the crosslingual knowledge barrier for real-world applications of LLMs. Future research could address these limitations to provide a more comprehensive understanding of LLMs' crosslingual capabilities.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.16135v1](https://arxiv.org/abs/2406.16135v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.16135v1](https://browse.arxiv.org/html/2406.16135v1)       |
| Truncated       | False       |
| Word Count       | 11266       |