
---
title: "Can LLMs Produce Faithful Explanations For Fact-checking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate"
id: "2402.07401v1"
description: "Fact-checking LLMs need better explanations; MADR framework improves faithfulness, credibility, and trustworthiness."
author: Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, Heng Ji
date: "2024-02-12"
image: "../../img/2402.07401v1/image_1.png"
categories: ['prompt-engineering', 'robustness', 'hci']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.07401v1/image_1.png)

### **Summary:**
- The study investigates the capability of Large Language Models (LLMs) to generate faithful explanations for fact-checking.
- It introduces the Multi-Agent Debate Refinement (MADR) framework, which leverages multiple LLMs as agents in an iterative refining process to enhance faithfulness in generated explanations.
- Experimental results demonstrate that MADR significantly improves the faithfulness of LLM-generated explanations to the evidence.

### Major Findings:
1. LLMs often produce unfaithful explanations for fact-checking in a zero-shot prompting setup.
2. The Multi-Agent Debate Refinement (MADR) framework significantly improves the faithfulness of LLM-generated explanations to the evidence.
3. Correlation analysis reveals that a granular evaluation aligns better with human judgments, and incorporating an error typology into automatic evaluations enhances the quality of LLM assessments.

### Analysis and Critique:
- The study did not thoroughly investigate the sensitivity of various systems to changes in input prompts, which may limit the applicability of the findings.
- The discrepancy between automatic and human evaluations suggests that even advanced LLMs fail to reliably judge the faithfulness of generated explanations for fact-checking.
- The study did not thoroughly investigate the sensitivity of various systems to changes in input prompts, which may limit the applicability of the findings.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-13       |
| Abstract | [https://arxiv.org/abs/2402.07401v1](https://arxiv.org/abs/2402.07401v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07401v1](https://browse.arxiv.org/html/2402.07401v1)       |
| Truncated       | False       |
| Word Count       | 11048       |