
---
title: "FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency Trade-off in Language Model Inference"
id: "2401.04044v1"
description: "Pretrained Language Models need model compression for efficient deployment on commodity hardware."
author: Zirui Liu, Qingquan Song, Qiang Charles Xiao, Sathiya Keerthi Selvaraj, Rahul Mazumder, Aman Gupta, Xia Hu
date: "2024-01-08"
image: "../../../bayesian-beagle.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
The article discusses the challenges of deploying Pretrained Language Models (LLMs) on commodity hardware due to their resource-intensive nature. The authors propose a method called FFSplit to optimize the trade-off between model accuracy and efficiency by splitting the Feed-forward network (FFN) component of LLMs based on the concept of "heavy hitters."

### Major Findings:
1. The authors observe that only a few neurons of the FFN module have large output norm for any input tokens, known as heavy hitters, while the others are sparsely triggered by different tokens.
2. By explicitly splitting the FFN into two parts according to the heavy hitters, the authors improve the efficiency-accuracy trade-off of existing compression methods.
3. The proposed method can reduce model size by 43.1% and bring 1.25 ∼ 1.56× wall clock time speedup on different hardware with negligible accuracy drop.

### Analysis and Critique:
The article provides valuable insights into optimizing the efficiency-accuracy trade-off in LLMs. However, it primarily focuses on the technical aspects of the proposed method and lacks a broader discussion of potential limitations, unanswered questions, or biases. Additionally, the article could benefit from a more detailed comparison with existing compression methods and a discussion of the practical implications of implementing the FFSplit method. Further research and real-world applications of the proposed method are necessary to validate its effectiveness in diverse settings.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.04044v1](https://arxiv.org/abs/2401.04044v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.04044v1](https://browse.arxiv.org/html/2401.04044v1)       |
| Truncated       | False       |
| Word Count       | 7140       |