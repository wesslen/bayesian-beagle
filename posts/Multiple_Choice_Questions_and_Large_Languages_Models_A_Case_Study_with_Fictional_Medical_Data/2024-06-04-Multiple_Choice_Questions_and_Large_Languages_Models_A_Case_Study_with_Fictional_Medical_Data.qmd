
---
title: "Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data"
id: "2406.02394v1"
description: "LLMs' medical knowledge assessment via MCQs may overemphasize pattern recognition, not clinical reasoning. New evaluation methods needed."
author: Maxime Griot, Jean Vanderdonckt, Demet Yuksel, Coralie Hemptinne
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02394v1/extracted/5642899/Images/accuracy_plot.png"
categories: ['architectures', 'robustness', 'hci', 'social-sciences', 'production', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02394v1/extracted/5642899/Images/accuracy_plot.png)

### Summary:

This study evaluates the effectiveness of multiple-choice questions (MCQs) in assessing the performance of Large Language Models (LLMs) in the medical field. The authors developed a fictional medical benchmark focused on a non-existent gland, the Glianorex, to isolate the knowledge of the LLM from its test-taking abilities. They used GPT-4 to generate a comprehensive textbook on the Glianorex in both English and French and developed corresponding multiple-choice questions in both languages. The models achieved average scores around 67%, with minor performance differences between larger and smaller models. Performance was slightly higher in English than in French. Fine-tuned medical models showed some improvement over their base versions in English but not in French. The uniformly high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs’ clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills.

### Major Findings:

1. The average score of LLMs on the fictional medical benchmark was around 67%, with minor performance differences between larger and smaller models.
2. Performance was slightly higher in English than in French, with fine-tuned medical models showing some improvement over their base versions in English but not in French.
3. The uniformly high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs’ clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills.

### Analysis and Critique:

* The study's approach of using a fictional medical benchmark is a novel and effective way to isolate the knowledge of the LLM from its test-taking abilities.
* The results highlight the limitations of traditional MCQ-based benchmarks in accurately measuring LLMs’ clinical knowledge and reasoning abilities.
* The study could have explored alternative evaluation methods, such as open-ended questions or scenario-based assessments, to better assess the deeper understanding and reasoning capabilities of LLMs.
* The study's findings underscore the need for more robust evaluation methods to better assess the true capabilities of LLMs in medical contexts.
* The study's focus on a single fictional medical benchmark may limit the generalizability of its findings to other medical domains or contexts.
* The study's sample size

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02394v1](https://arxiv.org/abs/2406.02394v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02394v1](https://browse.arxiv.org/html/2406.02394v1)       |
| Truncated       | False       |
| Word Count       | 5249       |