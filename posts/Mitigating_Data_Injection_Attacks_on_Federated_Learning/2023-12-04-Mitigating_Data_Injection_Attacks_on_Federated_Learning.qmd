
---
title: "Mitigating Data Injection Attacks on Federated Learning"
description: "Federated learning has privacy benefits, but false data attacks are a risk. A new method detects and mitigates these attacks."
author: "['Or Shalom', 'Amir Leshem', 'Waheed U. Bajwa']"
date: "2023-12-04"
image: "https://browse.arxiv.org/html/2312.02102v2/extracted/5299805/Figures/FederatedSimple3.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.02102v2/extracted/5299805/Figures/FederatedSimple3.png)

### Summary

#### Major Takeaways
- Federated learning, while preserving data privacy, is vulnerable to **data injection attacks** by malicious participants who manipulate the learning model.
- The proposed technique uses a local method to **detect and mitigate data injection attacks** during the training process.
- Simulations demonstrate that the proposed technique can **identify and isolate attackers**, leading to the recovery and convergence of the model.

#### Introduction to Federated Learning
- Federated learning is a method for training machine learning models collaboratively while preserving data privacy.
- Multiple independent agents train local models using their private datasets, and the model parameters are exchanged with a coordinating node to produce a global model.

#### Problem Formulation
- Data injection attacks involve malicious participants injecting false data into the training process to manipulate the global model.
- The paper formulates the federated learning problem and describes data injection attacks, including various attack schemes such as label flipping and constant output attacks.

#### Attacker Detection and Avoidance
- The coordinating node uses a low-complexity metric to compare updates received from edge agents over time to detect potential attackers.
- A detection method based on evaluating gradients of updates is proposed, allowing for continuous operation regardless of the model convergence time.

#### Simulations
- Simulated attacks, such as constant-output and label-flip attacks, demonstrate the performance of the proposed detection and mitigation technique.
- Results show that the proposed technique leads to the **detection and isolation of attackers** and the **recovery and convergence of the model** under attack.

### Critique
The paper provides a novel approach to detecting and mitigating data injection attacks in federated learning. However, the simulations are limited to specific attack scenarios, and the generalizability of the proposed technique to diverse attack types and real-world applications could be further explored. Additionally, the paper mentions that the proofs of the lemmas and the bounds on the attacker detection probability will be presented in an extended work, which may limit the current paper's validation of the proposed technique.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| HTML     |        |
| Truncated       | False       |
| Word Count       | 7092       |