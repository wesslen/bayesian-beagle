
---
title: "Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The Impact of Prompt Engineering and Knowledge Retrieval"
id: "2408.02964v1"
description: "LLMs show promise in health apps, but evaluations in nutrition are limited. This study evaluates GPT-4o, Claude 3.5, and Gemini 1.5 Pro using RD exam questions, revealing varying performance with different prompts and question domains. GPT-4o with CoT-SC prompting outperformed others, while Gemini 1.5 Pro with ZS showed highest consistency."
author: Iman Azimi, Mohan Qi, Li Wang, Amir M. Rahmani, Youlin Li
date: "2024-08-06"
image: "https://browse.arxiv.org/html/2408.02964v1/x1.png"
categories: ['social-sciences', 'education', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.02964v1/x1.png)

### Summary:

This paper evaluates the performance of three large language models (LLMs), GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, in addressing nutrition-related inquiries using the Registered Dietitian (RD) exam. The evaluation includes 1050 multiple-choice questions with different proficiency levels and covers four nutrition domains. The study investigates the impact of four prompting techniques: Zero Shot (ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC), and Retrieval Augmented Prompting (RAP). The results show that all approaches obtained a score of over 88% in selecting the correct option, with GPT-4o achieving the highest score. The study also examines the consistency of the responses by performing repeated measurements and comparing the responses within and across groups.

### Major Findings:

1. GPT-4o with CoT-SC prompting outperformed the other approaches in terms of accuracy, while Gemini 1.5 Pro with ZS prompting showed the highest consistency.
2. The lowest average percentage score was 89.22% for Gemini 1.5 Pro with CoT, which also showed the lowest agreement in repeated measurements, with a coefficient of 0.902.
3. GPT-4o recorded the highest accuracy overall, with a score of 95% using CoT-SC prompting.

### Analysis and Critique:

1. The study is limited to the leading proprietary LLM models, which are user-friendly and highly powerful. However, growing concerns are being raised about their lack of openness and limited access. Future research should evaluate the performance of open-source LLMs in the diet and nutrition field.
2. The evaluation primarily concentrates on the accuracy and consistency of the models. Given the sensitivity of health and nutrition applications, ensuring high accuracy and consistency is essential. However, it is important to assess LLMs from other perspectives, such as safety, bias, privacy, and emotional support.
3. The study examines the impacts of prompt engineering methods on LLM answers to diet and nutrition questions. However, various studies have explored the role of fine-tuning and agentic methods. Future research should

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.02964v1](https://arxiv.org/abs/2408.02964v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.02964v1](https://browse.arxiv.org/html/2408.02964v1)       |
| Truncated       | False       |
| Word Count       | 7951       |