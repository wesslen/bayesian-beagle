
---
title: "CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare"
id: "2407.19705v2"
description: "TL;DR: Diverse, well-distributed datasets can optimize LLM performance, enabling smaller models to achieve high scores."
author: Jingwei Zhu, Minghuan Tan, Min Yang, Ruixue Li, Hamid Alinejad-Rokny
date: "2024-07-30"
image: "../../../bayesian-beagle.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- The study focuses on the Comprehensive Medical Benchmark in Chinese (CMB) and explores how dataset diversity and distribution in supervised fine-tuning (SFT) can enhance LLM performance.
- The authors successfully trained a smaller base model to achieve scores comparable to larger models, indicating that a diverse and well-distributed dataset can optimize performance regardless of model size.
- The study suggests that even smaller models may reach high performance levels with carefully curated and varied datasets.
- The authors integrated a wide range of instructional content to address potential issues such as data quality inconsistencies.
- The results imply that a broader spectrum of training data may enhance a model’s ability to generalize and perform effectively across different medical scenarios, highlighting the importance of dataset quality and diversity in fine-tuning processes.

### Major Findings:
1. A smaller base model was successfully trained to achieve scores comparable to larger models, demonstrating that a diverse and well-distributed dataset can optimize performance regardless of model size.
2. The study suggests that even smaller models may reach high performance levels with carefully curated and varied datasets.
3. Integrating a wide range of instructional content can address potential issues such as data quality inconsistencies.
4. A broader spectrum of training data may enhance a model’s ability to generalize and perform effectively across different medical scenarios.
5. The importance of dataset quality and diversity in fine-tuning processes is highlighted.

### Analysis and Critique:
- The study effectively demonstrates the potential of using diverse datasets to improve model performance using SFT.
- The authors acknowledge the limitations of their method, such as the loss of conversational abilities in fine-tuned smaller models and the common problem of hallucination in smaller models.
- The trade-off between specialized task performance and general conversational ability is an important consideration for real-world applications.
- The study highlights the need for further innovation to fully realize the benefits of using diverse datasets in SFT.
- The authors' findings challenge the conventional belief that larger models are inherently superior and emphasize the importance of dataset diversity and careful dataset selection and distribution.
- The study's focus on the CMB benchmark may limit the generalizability of its findings to other medical benchmarks or domains.
- The authors' use

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.19705v2](https://arxiv.org/abs/2407.19705v2)        |
| HTML     | [https://browse.arxiv.org/html/2407.19705v2](https://browse.arxiv.org/html/2407.19705v2)       |
| Truncated       | False       |
| Word Count       | 3001       |