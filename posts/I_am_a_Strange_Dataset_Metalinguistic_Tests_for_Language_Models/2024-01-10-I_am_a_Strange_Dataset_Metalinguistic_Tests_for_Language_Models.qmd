
---
title: "I am a Strange Dataset: Metalinguistic Tests for Language Models"
id: "2401.05300v1"
description: "New dataset I am a Strange Dataset tests large language models in metalinguistic tasks, with mixed results."
author: Tristan Thrush, Jared Moore, Miguel Monares, Christopher Potts, Douwe Kiela
date: "2024-01-10"
image: "https://browse.arxiv.org/html/2401.05300v1/extracted/5340562/robotexplode.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.05300v1/extracted/5340562/robotexplode.png)

### Summary of "I am a Strange Dataset: Metalinguistic Tests for Language Models"

#### Major Findings:
1. "I am a Strange Dataset" presents a new dataset to evaluate language models' capabilities in handling metalinguistic self-reference. The dataset comprises of two subtasks: generation and verification, with additional metalinguistic non-self-reference examples for control testing.

2. The dataset was hand-crafted by experts and validated by non-expert annotators. Testing several open-source and closed-source language models, the study found that all models performed close to chance across both subtasks and even on the non-self-referential metalinguistic control data. GPT 4 was the only model that consistently performed significantly better than chance, though it still only scored in the 60% range, while untrained human annotators scored in the 89â€“93% range.

3. The findings suggest that current language models struggle with understanding and generating self-referential and metalinguistic language, with limited evidence of improvement with model scale. This poses a serious challenge for even the best present-day models.

#### I. Introduction
- Self-reference, especially metalinguistic self-reference, plays a crucial role in various domains and is considered a key aspect of higher intelligence or consciousness in philosophy. Humans generally have no trouble with metalinguistic language, which involves reasoning about metalinguistic properties and resolving self-reference.

#### II. Related Work
- The paper presents "I am a Strange Dataset" as the first AI challenge dataset targeting metalinguistics. It also discusses previous work on self-reference with language models, focusing on models' ability to improve on themselves or their outputs.

#### III. I am a Strange Dataset
- The dataset is constructed to test whether language models can produce and understand self-referential and metalinguistic statements. It includes self-referential statements and non-self-referential metalinguistic problem categories. The dataset is comprised of 208 examples, and an additional 10 "Impossible Dataset" examples that experts struggled to understand.

##### A. Tags
- The dataset includes 10 tags to categorize examples, capturing different aspects of the mental facilities required to solve the problems.

##### B. Metrics
- The study focuses on testing whether models can generate and understand self-referential and metalinguistic statements, presenting several metrics for generation and validation.

##### C. Non-Self-Referential Control
- The study compares the performance of models on non-self-referent examples to the original self-referential examples.

#### IV. Human Experiment Details
- A human baseline is established from annotations by Mechanical Turk workers, demonstrating that humans perform significantly better than language models on the task.

#### V. Results
- Language models perform close to the level of chance on the "I am a Strange Dataset." GPT 4 is the only model to achieve scores significantly above random, but still below human performance. Models also struggle with non-self-referential metalinguistic aspects, and there is limited evidence that GPT 4 struggles more with self-referential metalinguistic problems than non-self-referential problems. Model performance improves with scale.

#### VI. Conclusion
- The dataset presents a serious challenge for language models, indicating that self-referential language is particularly difficult for them. The findings suggest that scale beyond 70B parameters may be needed for comparable performance from models.

### Critique
- The study does not delve into potential solutions or improvements for language models to better handle metalinguistic self-reference. There may also be limitations in the study's evaluation methodology and dataset design that could be explored further. Additionally, the impact of different tokenizers and limitations imposed by training data were only briefly discussed and could be areas for deeper investigation.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-11       |
| Abstract | [http://arxiv.org/abs/2401.05300v1](http://arxiv.org/abs/2401.05300v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.05300v1](https://browse.arxiv.org/html/2401.05300v1)       |
| Truncated       | False       |
| Word Count       | 9407       |