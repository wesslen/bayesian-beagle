
---
title: "Can Large Language Models Automatically Jailbreak GPT-4V?"
id: "2407.16686v1"
description: "AutoJailbreak, a novel technique, uses LLMs for red-teaming and in-context learning, achieving a 95.3% Attack Success Rate, highlighting GPT-4V security concerns."
author: Yuanwei Wu, Yue Huang, Yixin Liu, Xiang Li, Pan Zhou, Lichao Sun
date: "2024-07-23"
image: "https://browse.arxiv.org/html/2407.16686v1/x1.png"
categories: ['prompt-engineering', 'robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.16686v1/x1.png)

### Summary:

The paper introduces AutoJailbreak, a novel automatic jailbreak technique inspired by prompt optimization. This method leverages Large Language Models (LLMs) for red-teaming to refine the jailbreak prompt and employs weak-to-strong in-context learning prompts to boost efficiency. The study also presents an effective search method that incorporates early stopping to minimize optimization time and token expenditure. The experiments demonstrate that AutoJailbreak significantly surpasses conventional methods, achieving an Attack Success Rate (ASR) exceeding 95.3%. This research highlights the potential for LLMs to be exploited in compromising GPT-4V integrity and underscores the need for strengthening GPT-4V security.

### Major Findings:

1. AutoJailbreak, a groundbreaking jailbreak strategy, harnesses the LLM's native prompt optimization capabilities, automating the jailbreak process and significantly reducing the need for manual input.
2. Within the AutoJailbreak framework, a weak-to-strong in-context learning strategy and an efficient search mechanism inspired by early stopping are innovated to enhance jailbreak effectiveness while curbing time and token expenditure.
3. Through rigorous testing on facial identity recognition tasks featuring prominent celebrities across three languages, the AutoJailbreak method has proven capable of penetrating the defenses of GPT-4V, highlighting the urgent need for developers to reinforce the security measures of Multimodal Large Language Models (MLLMs) against such sophisticated attacks.

### Analysis and Critique:

1. The paper does not discuss the potential ethical implications of using LLMs for red-teaming and refining jailbreak prompts.
2. The study focuses on the vulnerabilities of GPT-4V, but it is unclear if the findings can be generalized to other MLLMs.
3. The paper does not provide a detailed comparison of AutoJailbreak with other existing jailbreak techniques, making it difficult to assess its relative effectiveness.
4. The paper does not discuss the potential countermeasures that could be employed to mitigate the risks posed by AutoJailbreak.
5. The study does not explore the potential impact of AutoJailbreak on the privacy and security of individuals whose facial recognition data is used in the training

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.16686v1](https://arxiv.org/abs/2407.16686v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.16686v1](https://browse.arxiv.org/html/2407.16686v1)       |
| Truncated       | False       |
| Word Count       | 6844       |