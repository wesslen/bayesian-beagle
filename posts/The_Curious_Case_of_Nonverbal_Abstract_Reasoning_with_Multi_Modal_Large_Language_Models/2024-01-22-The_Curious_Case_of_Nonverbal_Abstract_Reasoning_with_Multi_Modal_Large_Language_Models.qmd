
---
title: "The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models"
id: "2401.12117v1"
description: "MLLMs integrate verbal and visual info, but struggle with abstract reasoning. Chain-of-Thought prompting improves performance."
author: Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, Jay Pujara
date: "2024-01-22"
image: "../../../bayesian-beagle.png"
categories: ['architectures', 'prompt-engineering', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
The academic article delves into the nonverbal abstract reasoning abilities of open-source and closed-source Multi-modal Large Language Models (MLLMs) using variations of Raven's Progressive Matrices (RPM). It highlights the performance gap between open-source and closed-source models and the critical shortcomings with individual visual and textual modules. The study also emphasizes the potential for improvement through methods such as Chain-of-Thought prompting. The article provides a comprehensive evaluation of the models' performance on nonverbal abstract reasoning tasks, comparing different models and scoring methods on the IQ50 dataset.

### Major Findings:
1. The one-by-one scoring method shows that instruction-tuned models outperform pre-trained models significantly, indicating that specific instructions can improve reasoning abilities.
2. Even with the one-by-one scoring method, models struggle to consistently outperform random baselines, revealing the challenges in solving nonverbal abstract reasoning tasks.
3. The study exposes the performance gap between open-source and closed-source models, highlighting the limitations of current language models in understanding and solving reasoning tasks.

### Analysis and Critique:
The article provides valuable insights into the reasoning abilities of MLLMs and the potential for improvement through specific instructions. However, it also reveals critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. The study emphasizes the need for further analysis to address the limitations of current language models in understanding and solving reasoning tasks. Additionally, the comparison of various models for vision-language tasks offers valuable insights into the state-of-the-art in vision-language models and their performance on different benchmarks. The examples provided in the Marvel dataset help in understanding the types of reasoning required for solving visual reasoning tasks. However, the article could benefit from further exploration of potential biases and methodological issues in evaluating the models' performance.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2401.12117v1](https://arxiv.org/abs/2401.12117v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.12117v1](https://browse.arxiv.org/html/2401.12117v1)       |
| Truncated       | True       |
| Word Count       | 17714       |