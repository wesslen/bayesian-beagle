
---
title: "EXAONE 3.0 7.8B Instruction Tuned Language Model"
id: "2408.03541v2"
description: "EXAONE 3.0: Open, 7.8B-parameter LLM excels in Korean, complex reasoning, and general tasks. Available at Hugging Face."
author: LG AI Research, :, Soyoung An, Kyunghoon Bae, Eunbi Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Yeonjung Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Euisoon Kim, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Moontae Lee, Seungjun Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Boseong Seo, Sihoon Yang, Heuiyeen Yeen, Kyungjae Yoo, Hyeongu Yun
date: "2024-08-08"
image: "../../../bayesian-beagle.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The EXAONE 3.0 7.8B Instruction-Tuned Language Model is a bilingual model developed by LG AI Research, focusing on English and Korean. The model is based on a decoder-only transformer architecture and uses a BBPE tokenizer with a vocabulary size of 102,400. The training process includes extensive pre-training on a diverse dataset and advanced post-training techniques to enhance instruction-following capabilities. The model is trained on Google Cloud Platform and a cluster powered by NVIDIA H100 GPUs and NVIDIA NeMo Framework, optimized by NVIDIA TensorRT-LLM.

### Major Findings:

1. The EXAONE 3.0 7.8B Instruction-Tuned Language Model demonstrates highly competitive real-world performance with instruction-following capability against other state-of-the-art open models of similar size.
2. The model excels particularly in Korean, while achieving compelling performance across general tasks and complex reasoning.
3. The model's strong real-world effectiveness and bilingual proficiency contribute to advancements in Expert AI.

### Analysis and Critique:

1. The model's performance in Korean is a significant strength, as it addresses a gap in the current landscape of large language models.
2. The model's instruction-following capabilities are a valuable feature, as they allow for more precise and accurate responses to user queries.
3. The model's training process, which includes extensive pre-training and advanced post-training techniques, is a strength that contributes to its strong performance.
4. The model's reliance on a diverse dataset for training is a strength, as it allows for a more comprehensive understanding of language and context.
5. The model's optimization using NVIDIA TensorRT-LLM is a strength, as it allows for more efficient and effective processing.
6. The model's release for non-commercial, research purposes is a strength, as it allows for further innovation and collaboration within the AI community.
7. The model's potential limitations, such as the risk of generating inappropriate or biased responses, should be considered and addressed through ongoing research and development.
8. The model's potential

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.03541v2](https://arxiv.org/abs/2408.03541v2)        |
| HTML     | [https://browse.arxiv.org/html/2408.03541v2](https://browse.arxiv.org/html/2408.03541v2)       |
| Truncated       | False       |
| Word Count       | 8000       |