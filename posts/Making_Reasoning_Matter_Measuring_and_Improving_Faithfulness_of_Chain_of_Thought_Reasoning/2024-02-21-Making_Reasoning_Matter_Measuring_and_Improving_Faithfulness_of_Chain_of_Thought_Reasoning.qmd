
---
title: "Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning"
id: "2402.13950v1"
description: "LLMs need help reasoning; Frodo framework improves reasoning and answer accuracy."
author: Debjit Paul, Robert West, Antoine Bosselut, Boi Faltings
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13950v1/extracted/5423366/figure/figure_1.png"
categories: ['production', 'prompt-engineering', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13950v1/extracted/5423366/figure/figure_1.png)

### **Summary:**
Large language models (LLMs) have shown improved performance when reasoning step-by-step before answering a question. However, it is unclear to what degree the model's final answer is faithful to the stated reasoning steps. In this paper, a causal mediation analysis is performed on twelve LLMs to examine how intermediate reasoning steps influence the final outcome. The study finds that LLMs do not reliably use their intermediate reasoning steps when generating an answer. To address this issue, a framework called Frodo is introduced to tailor small-sized LMs to generate correct reasoning steps and robustly reason over these steps. Frodo significantly outperforms four competitive baselines and improves the robustness and generalization ability of the reasoning LM. The study concludes that Frodo's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.

### Major Findings:
1. LLMs do not reliably use their intermediate reasoning steps when generating an answer.
2. Frodo significantly outperforms four competitive baselines.
3. Frodo's rationales are more faithful to its final answer predictions than standard supervised fine-tuning.

### Analysis and Critique:
- The study relies on counterfactual data generated by LLMs, which can be expensive and time-consuming.
- The training process for Frodo is complex and time-consuming, requiring additional counterfactual data and a two-step training process.
- The study acknowledges the limitations of applying Frodo in production environments, especially when making critical decisions or exposing its generated contents directly to human end users.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13950v1](https://arxiv.org/abs/2402.13950v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13950v1](https://browse.arxiv.org/html/2402.13950v1)       |
| Truncated       | False       |
| Word Count       | 7610       |