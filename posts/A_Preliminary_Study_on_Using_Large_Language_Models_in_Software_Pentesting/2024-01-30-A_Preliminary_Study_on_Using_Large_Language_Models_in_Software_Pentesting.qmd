
---
title: "A Preliminary Study on Using Large Language Models in Software Pentesting"
id: "2401.17459v1"
description: "LLMs can automate security tasks, improve over time with human interaction, and outperform static code analyzers."
author: Kumar Shashwat, Francis Hahn, Xinming Ou, Dmitry Goldgof, Lawrence Hall, Jay Ligatti, S. Raj Rajgopalan, Armin Ziaie Tabari
date: "2024-01-30"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'security', 'robustness', 'education']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:
The article investigates the use of Large Language Models (LLMs) in software pentesting to automatically identify software security vulnerabilities in source code. The study hypothesizes that an LLM-based AI agent can be improved over time for a specific security task as human operators interact with it. The authors utilize the OWASP Benchmark Project 1.2, containing 2,740 hand-crafted source code test cases, to evaluate the performance of LLM-based AI agents. The results show that using LLMs is a viable approach to build an AI agent for software pentesting that can improve through repeated use and prompt engineering.

### Major Findings:
1. Without prompt engineering, the LLMsâ€™ accuracy is either below or on par with that of SonarQube.
2. With prompt engineering, GPT-4-Turbo using the Assistants API demonstrated substantial improvements on the accuracy, outperforming or being on par with SonarQube in most of the vulnerability categories.
3. The study shows that there is a viable path for using LLM to build an AI agent that can be constantly improved through prompt engineering driven by usage.

### Analysis and Critique:
The study provides valuable insights into the potential of using LLMs in software pentesting. However, it is important to note that the research is preliminary and focuses on a specific set of test cases. Further research is needed to evaluate the generalizability of the findings across different types of software and security vulnerabilities. Additionally, the study does not address potential ethical concerns or biases that may arise from using AI agents in security tasks. It is crucial to consider the broader implications of integrating LLMs into security operations and address any potential limitations or biases in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-01       |
| Abstract | [https://arxiv.org/abs/2401.17459v1](https://arxiv.org/abs/2401.17459v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.17459v1](https://browse.arxiv.org/html/2401.17459v1)       |
| Truncated       | False       |
| Word Count       | 2780       |