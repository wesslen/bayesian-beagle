
---
title: "Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought"
id: "2402.11541v1"
description: "Using knowledge graphs to enhance language models' comprehension, messy KG knowledge is effectively handled."
author: Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Guilin Qi
date: "2024-02-18"
image: "https://browse.arxiv.org/html/2402.11541v1/x1.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.11541v1/x1.png)

### Summary:
- The article explores the ability of Large Language Models (LLMs) to comprehend knowledge graphs (KGs) and the impact of different methods of knowledge injection on LLM performance.
- The study reveals that LLMs can effectively handle messy, noisy, and linearized KG knowledge, outperforming methods that employ well-designed natural language textual prompts.
- The findings suggest that LLMs consistently outperform well-crafted and fluent text prompts when presented with disorganized, noisy, and abstract knowledge prompts.

### Major Findings:
1. LLMs consistently outperform well-crafted and fluent text prompts when presented with disorganized, noisy, and abstract knowledge prompts.
2. The presence of redundant or irrelevant knowledge does not necessarily diminish the reasoning capability of LLMs; in fact, it can enhance accuracy by filtering out irrelevant information and leveraging relevant details.
3. LLMs exhibit exceptional proficiency in addressing 1-hop questions, but their performance drops significantly when answering questions with more than 2 hops, suggesting the need for further research to improve their reasoning capabilities.

### Analysis and Critique:
- The study provides valuable insights into the ability of LLMs to comprehend structured knowledge, but it is limited by the simplicity and limited number of questions in the QALD-7 dataset, potentially biasing the evaluation results.
- The study is also limited by the use of datasets based on Wikidata, which may restrict the generalizability of the findings to other knowledge bases.
- The article lacks an analysis of model interpretability in the experimental results, which could provide further insights into the reasoning capabilities of LLMs.
- The study highlights the importance of designing rigorous experiments to validate the effectiveness of knowledge injection methods for LLMs, but it does not delve into the principles of understanding structured knowledge by models at the vector level, which could be a focus of future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.11541v1](https://arxiv.org/abs/2402.11541v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11541v1](https://browse.arxiv.org/html/2402.11541v1)       |
| Truncated       | False       |
| Word Count       | 7638       |