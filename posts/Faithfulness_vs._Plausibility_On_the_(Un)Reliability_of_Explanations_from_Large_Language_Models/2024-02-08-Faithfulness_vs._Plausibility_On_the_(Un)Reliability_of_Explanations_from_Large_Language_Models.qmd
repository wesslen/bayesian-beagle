
---
title: "Faithfulness vs. Plausibility: On the (Un)Reliability of Explanations from Large Language Models"
id: "2402.04614v2"
description: "LLMs generate self-explanations, but their faithfulness is questionable. Plausibility may compromise faithfulness."
author: Chirag Agarwal, Sree Harsha Tanneru, Himabindu Lakkaraju
date: "2024-02-08"
image: "https://browse.arxiv.org/html/2402.04614v2/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.04614v2/x1.png)

### **Summary:**
- Large Language Models (LLMs) generate self-explanations (SEs) that are plausible but may not be faithful.
- The dichotomy between faithfulness and plausibility in SEs is a concern, especially in high-stakes decision-making applications.
- The current trend towards increasing the plausibility of explanations may come at the cost of diminishing their faithfulness.

### **Major Findings:**
1. LLMs are adept at generating plausible explanations but may not accurately reflect the modelâ€™s actual reasoning process.
2. The lack of ground truth explanations makes evaluating the faithfulness of SEs a non-trivial problem.
3. Different applications demand varying levels of faithfulness and plausibility in SEs provided by LLMs.

### **Analysis and Critique:**
- Evaluating the faithfulness of explanations is challenging due to the black-box nature of LLMs, making it difficult to use classical eXplainable Artificial Intelligence (XAI) metrics.
- Plausible explanations that are not necessarily faithful can lead to misplaced trust, over-reliance, and security concerns.
- Faithful explanations that are not necessarily plausible may result in non-intuitive and less user-friendly interactions.
- Different applications demand varying levels of faithfulness and plausibility in SEs provided by LLMs, highlighting the importance of tailoring the explanation style to the application domain.
- The community should focus on developing reliable metrics to characterize the faithfulness of explanations and pioneering novel strategies to generate more faithful SEs.

Overall, the article highlights the need for LLMs to balance plausibility and faithfulness in self-explanations, emphasizing the importance of tailoring the explanation style to the application domain and the need for the community to focus on developing reliable metrics and novel strategies to enhance the faithfulness of generated explanations.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-10       |
| Abstract | [https://arxiv.org/abs/2402.04614v2](https://arxiv.org/abs/2402.04614v2)        |
| HTML     | [https://browse.arxiv.org/html/2402.04614v2](https://browse.arxiv.org/html/2402.04614v2)       |
| Truncated       | False       |
| Word Count       | 7365       |