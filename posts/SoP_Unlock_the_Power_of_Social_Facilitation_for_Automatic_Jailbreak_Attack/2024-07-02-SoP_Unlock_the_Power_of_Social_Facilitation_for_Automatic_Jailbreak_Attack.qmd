
---
title: "SoP: Unlock the Power of Social Facilitation for Automatic Jailbreak Attack"
id: "2407.01902v1"
description: "SoP framework generates jailbreak prompts, bypassing GPT-3.5 and GPT-4 safety with 88% and 60% success, respectively."
author: Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Hailiang Huang, Guanhua Chen, Yun Chen
date: "2024-07-02"
image: "../../img/2407.01902v1/image_1.png"
categories: ['security']
format:
  html:
    code-overflow: wrap
---

![](../../img/2407.01902v1/image_1.png)

# Summary

**Summary:**
The paper introduces a red-teaming strategy called SoP, a simple yet effective framework for designing jailbreak prompts automatically. Inspired by the social facilitation concept, SoP generates and optimizes multiple jailbreak characters to bypass the guardrails of the target LLM. Unlike previous work, SoP can generate and optimize jailbreak prompts in a cold-start scenario using open-sourced LLMs without any seed jailbreak templates. Experimental results show that SoP achieves attack success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106 and GPT-4, respectively.

**Major Findings:**
1. SoP is a simple and effective framework for designing jailbreak prompts automatically, inspired by the social facilitation concept.
2. SoP generates and optimizes multiple jailbreak characters to bypass the guardrails of the target LLM.
3. SoP can generate and optimize jailbreak prompts in a cold-start scenario using open-sourced LLMs without any seed jailbreak templates.
4. Experimental results show that SoP achieves attack success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106 and GPT-4, respectively.

**Analysis and Critique:**
The paper presents a novel approach to designing jailbreak prompts automatically using the social facilitation concept. The use of multiple jailbreak characters to bypass the guardrails of the target LLM is an innovative idea. However, the paper does not provide a detailed analysis of the limitations and potential biases of the proposed approach. Additionally, the paper does not discuss the potential impact of the proposed approach on the safety and security of LLMs. Further research is needed to evaluate the effectiveness and limitations of the proposed approach in real-world scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.01902v1](https://arxiv.org/abs/2407.01902v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.01902v1](https://browse.arxiv.org/html/2407.01902v1)       |
| Truncated       | False       |
| Word Count       | 14863       |