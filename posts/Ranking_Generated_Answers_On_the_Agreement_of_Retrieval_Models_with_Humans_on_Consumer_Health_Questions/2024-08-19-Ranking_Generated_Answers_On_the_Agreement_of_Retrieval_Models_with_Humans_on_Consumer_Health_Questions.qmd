
---
title: "Ranking Generated Answers: On the Agreement of Retrieval Models with Humans on Consumer Health Questions"
id: "2408.09831v1"
description: "Ranking signals can evaluate LLM answers, correlating with human expert preferences, and improving with model size and prompt strategies."
author: Sebastian Heineking, Jonas Probst, Daniel Steinbach, Martin Potthast, Harrisen Scells
date: "2024-08-19"
image: "https://browse.arxiv.org/html/2408.09831v1/x1.png"
categories: ['prompt-engineering', 'social-sciences', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.09831v1/x1.png)

### Summary:

The paper presents a method for evaluating the quality of answers generated by large language models (LLMs) using ranking signals as a substitute for explicit relevance judgments. The proposed method, called normalized rank position (NRP), measures the effectiveness of generated answers by ranking them alongside human-written documents using a retrieval model known to have high effectiveness in ranking documents with respect to their relevance judgments. The method is evaluated using the CLEF eHealth 2021 test data, which comprises 55 health-related queries obtained from medical experts and Reddit discussions. The results show that the choice of prompting strategy and model size have a large influence on the effectiveness of generated answers, and these effects are measurable with NRP. The paper also compares the preferences of the chosen ranking model with that of an expert annotator and shows a high agreement between the two.

### Major Findings:

1. The proposed NRP method can effectively evaluate the quality of answers generated by LLMs using ranking signals as a substitute for explicit relevance judgments.
2. The choice of prompting strategy and model size have a large influence on the effectiveness of generated answers, and these effects are measurable with NRP.
3. The proposed method shows a high agreement with the preferences of an expert annotator, indicating its validity in evaluating the quality of generated answers.

### Analysis and Critique:

The proposed method provides a scalable and automatic way to evaluate the quality of generated answers to consumer health questions. However, the results of the experiments may be limited in their ability to discriminate highly effective LLMs as the dataset used was potentially too easy for these models. The paper also acknowledges the need for further research to extend the NRP method to assess retrieval-augmented generation (RAG) systems that ground their answers in documents by referring to them. Additionally, the paper focuses on evaluating generated answers for consumer health search questions, and future work is needed to develop and adapt test collections for other domains. Overall, the paper contributes a new way to automatically assess LLM capabilities by evaluating them in the context of human-annotated documents.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.09831v1](https://arxiv.org/abs/2408.09831v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.09831v1](https://browse.arxiv.org/html/2408.09831v1)       |
| Truncated       | False       |
| Word Count       | 3362       |