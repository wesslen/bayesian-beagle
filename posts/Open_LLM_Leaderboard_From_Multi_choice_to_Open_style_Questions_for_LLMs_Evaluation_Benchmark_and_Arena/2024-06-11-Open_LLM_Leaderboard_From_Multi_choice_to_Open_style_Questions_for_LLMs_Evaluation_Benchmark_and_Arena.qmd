
---
title: "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena"
id: "2406.07545v1"
description: "LLMs may favor certain answer IDs due to biases. Open-style questions can eliminate this, but pose new challenges. We introduce the Open-LLM-Leaderboard to track LLM performance using open-style questions."
author: Aidar Myrzakhan, Sondos Mahmoud Bsharat, Zhiqiang Shen
date: "2024-06-11"
image: "https://browse.arxiv.org/html/2406.07545v1/x1.png"
categories: ['architectures', 'production', 'prompt-engineering', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.07545v1/x1.png)

### Summary:

- The paper introduces the Open-LLM-Leaderboard, a new benchmark for evaluating large language models (LLMs) using open-style questions to address the limitations of multiple-choice questions (MCQs).
- Open-style questions can eliminate selection bias and random guessing issues, but identifying suitable questions and validating the correctness of LLM responses are significant challenges.
- The authors propose an automatic coarse-to-fine selecting protocol and a task-specific prompt to evaluate the correctness of LLM responses against human-annotated ground-truths.
- The Open-LLM-Leaderboard includes well-recognized LLMs, such as GPT-4o, GPT-4, ChatGPT, Claude-3 Opus, Gemini-Pro, and Mistral-Large, and demonstrates a high correlation between the rankings produced by the open-style benchmark and those derived from user-based evaluations or direct human assessments.

### Major Findings:

1. The Open-LLM-Leaderboard is a new benchmark for evaluating LLMs using open-style questions, which can fundamentally eliminate selection bias and random guessing issues.
2. The authors propose an automatic coarse-to-fine selecting protocol and a task-specific prompt to evaluate the correctness of LLM responses against human-annotated ground-truths.
3. The Open-LLM-Leaderboard includes well-recognized LLMs and demonstrates a high correlation between the rankings produced by the open-style benchmark and those derived from user-based evaluations or direct human assessments.

### Analysis and Critique:

- The paper addresses the limitations of MCQs in evaluating LLMs and proposes a new benchmark using open-style questions.
- The authors provide a detailed methodology for identifying suitable open-style questions and validating the correctness of LLM responses.
- The Open-LLM-Leaderboard includes well-recognized LLMs and demonstrates a high correlation between the rankings produced by the open-style benchmark and those derived from user-based evaluations or direct human assessments.
- However, the paper does not discuss the potential limitations or biases of the proposed benchmark, such as the selection

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.07545v1](https://arxiv.org/abs/2406.07545v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07545v1](https://browse.arxiv.org/html/2406.07545v1)       |
| Truncated       | False       |
| Word Count       | 5687       |