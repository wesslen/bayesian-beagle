
---
title: "The current status of large language models in summarizing radiology report impressions"
id: "2406.02134v1"
description: "LLMs struggle to replace radiologists in summarizing radiology reports, despite few-shot prompts improving conciseness and verisimilitude."
author: Danqing Hu, Shanyuan Zhang, Qing Liu, Xiaofeng Zhu, Bing Liu
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02134v1/x1.png"
categories: ['social-sciences', 'prompt-engineering', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02134v1/x1.png)

### Summary:

This study explores the capability of eight large language models (LLMs) in summarizing radiology report impressions. The authors collected three types of radiology reports (CT, PET-CT, and Ultrasound) from Peking University Cancer Hospital and Institute. They used the report findings to construct zero-shot, one-shot, and three-shot prompts with complete example reports to generate impressions. The evaluation metrics included automatic quantitative evaluation (BLEU, ROUGE-L, and METEOR) and human evaluation (completeness, correctness, conciseness, verisimilitude, and replaceability). Two thoracic surgeons and one radiologist compared the generated impressions with reference impressions and scored each impression under the five human evaluation metrics.

### Major Findings:

1. There is a gap between the generated impressions and reference impressions, with LLMs achieving comparable performance in completeness and correctness but lower scores in conciseness and verisimilitude.
2. Using few-shot prompts can improve the LLMs' performance in conciseness and verisimilitude, but clinicians still think the LLMs cannot replace radiologists in summarizing the radiology impressions.
3. The best LLMs for each task were Tongyi Qianwen for PET-CT impression summarization, ERNIE Bot for CT impression summarization, and ChatGPT for ultrasound impression summarization.

### Analysis and Critique:

1. The study focuses on a limited number of LLMs and does not include other popular models like GPT-4 or PaLM.
2. The evaluation metrics used in the study may not fully capture the nuances of radiology report summarization, as they were originally designed for general text summarization tasks.
3. The study does not provide a detailed analysis of the errors made by the LLMs, which could help identify areas for improvement.
4. The human evaluation was conducted by a small number of clinicians, which may not be representative of the broader clinical community.
5. The study does not discuss the potential impact of LLMs on radiology workflows or the potential benefits and drawbacks of using LLMs for radiology report summarization.
6. The study does not explore the potential for fine-tuning LL

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02134v1](https://arxiv.org/abs/2406.02134v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02134v1](https://browse.arxiv.org/html/2406.02134v1)       |
| Truncated       | False       |
| Word Count       | 7591       |