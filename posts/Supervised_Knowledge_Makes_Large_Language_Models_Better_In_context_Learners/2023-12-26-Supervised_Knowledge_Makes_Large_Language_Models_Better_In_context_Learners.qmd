
---
title: "Supervised Knowledge Makes Large Language Models Better In-context Learners"
description: "TL;DR: A framework enhances Large Language Models' reliability, generalizability, and factuality, using discriminative models during inference."
author: "['Linyi Yang', 'Shuibai Zhang', 'Zhuohao Yu', 'Guangsheng Bao', 'Yidong Wang', 'Jindong Wang', 'Ruochen Xu', 'Wei Ye', 'Xing Xie', 'Weizhu Chen', 'Yue Zhang']"
date: "2023-12-26"
image: "https://browse.arxiv.org/html/2312.15918v1/x1.png"
categories: ['prompt engineering']
authors: Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, Yue Zhang
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.15918v1/x1.png)

# Supervised Knowledge in Large Language Models

## Key Findings
- **Large Language Models (LLMs)** demonstrate emerging in-context learning abilities through prompt engineering and have garnered significant performance across diverse tasks.
- The study introduces **SuperContext**, a framework that uses task-Specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage.
- Using SuperContext, **enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality**.

## Introduction
- **Large language models (LLMs)** have shown robust performance across various tasks, but face challenges such as substantial resources for training and deployment, slow inference times, and susceptibility to hallucinations.

## Method
- **In-context Learning Baseline**: Traditional in-context learning involves using in-domain data for several Natural Language Understanding (NLU) tasks with 16-shot examples.
- **SuperContext**: A simple and general approach that incorporates the auxiliary knowledge from a small, discriminative model with LLMs when making predictions for new tasks.

## Experiments
- **Setup**: Tested on 8 NLU tasks and 1 generation task to validate SuperContext on GLUE-X benchmark and SQuAD 2.0.
- **NLU Results**: SuperContext outperformed both SLMs and LLMs across NLU tasks, surpassing the supervised task-specific model, ELECTRA-large, as well.
- **QA Results**: SuperContext significantly improved accuracy for open questions in the QA task.

## Analysis and Discussion
- **Reversed Predictions**: SuperContext lead to the correction of predictions made by LLMs in both NLU and QA tasks.
- **Interpretation Analysis**: LLMs demonstrated the ability to recall influential in-context examples and output rationales, with SuperContext resulting in higher performance and overlap with human rationale.
- **The Effect of SLM Confidence**: There is a positive correlation between SLM confidence and LLM performance, emphasizing the importance of including both prediction and confidence in the prompt design.

## Critique
- The study lacked a comparison with other large-scale language models, potentially limiting the generalizability of the findings.
- The effectiveness of SuperContext was not evaluated in real-world applications, limiting its practical implications.

Overall, the study sheds light on the potential of incorporating supervised knowledge from SLMs to enhance the performance of LLMs in various NLU and QA tasks. The findings highlight the importance of leveraging discriminative models for improving the reliability and factuality of LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| HTML     | []()       |
| Truncated       | False       |
| Word Count       | 12183       |