
---
title: "Eagle: Ethical Dataset Given from Real Interactions"
id: "2402.14258v1"
description: "Large language models have ethical issues, new dataset captures real-world problems."
author: Masahiro Kaneko, Danushka Bollegala, Timothy Baldwin
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14258v1/extracted/5424235/abst.png"
categories: ['robustness', 'hci', 'social-sciences', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14258v1/extracted/5424235/abst.png)

### Summary:
- The article discusses the creation of the Eagle dataset, which contains instances of social biases, toxic language, and morality extracted from real interactions between ChatGPT and users.
- Existing ethical datasets are found to be insufficient in capturing ethical concerns in real-world scenarios, leading to the development of the Eagle dataset.
- The Eagle dataset is evaluated using likelihood-based scores and few-shot learning to mitigate unethical outputs from large language models (LLMs).

### Major Findings:
1. Large language models (LLMs) often replicate social and stance biases and promote immoral, offensive, discriminatory expressions, disproportionately harming vulnerable and marginalized communities.
2. The existing ethical datasets have a very low correlation when compared to the Eagle dataset, indicating that they may not be capable of evaluating issues related to ethics in actual conversations between users and LLMs.
3. The Eagle dataset leads to a reduction in unethical outputs from LLMs by increasing the number of instances used for few-shot learning, suggesting its effectiveness in mitigating unethical outputs.

### Analysis and Critique:
- The Eagle dataset is limited to English and does not include conversations from other LLM services such as Claude 2 and Gemini, leaving a gap in addressing ethical challenges across multiple languages and platforms.
- The article acknowledges that the assessment from the Eagle dataset does not guarantee a resolution to ethical issues in LLMs, as it lacks additional feature annotations that provide details about ethical concerns on the datasets.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.14258v1](https://arxiv.org/abs/2402.14258v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14258v1](https://browse.arxiv.org/html/2402.14258v1)       |
| Truncated       | False       |
| Word Count       | 6296       |