
---
title: "Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment"
id: "2402.19085v1"
description: "CPO aligns language models with human preferences, improving multi-objective alignment and reducing alignment tax impact."
author: Yiju Guo, Ganqu Cui, Lifan Yuan, Ning Ding, Jiexin Wang, Huimin Chen, Bowen Sun, Ruobing Xie, Jie Zhou, Yankai Lin, Zhiyuan Liu, Maosong Sun
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19085v1/x1.png"
categories: ['social-sciences', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19085v1/x1.png)

### Summary:

- Controllable Preference Optimization (CPO) is a method for aligning large language models (LLMs) with human preferences and values, specifically targeting helpful, honest, and harmless LLMs.
- The "3H" principle can lead to trade-offs between objectives, known as the "alignment tax," where improving one alignment objective might negatively impact others.
- CPO introduces controllable preference supervised fine-tuning (CPSFT) and controllable direct preference optimization (CDPO) to optimize LLMs based on explicit preference conditions, mitigating the alignment tax and achieving Pareto improvements in multi-objective alignment.

### Major Findings:
1. **Controllable Preference Optimization**: CPO, consisting of CPSFT and CDPO, explicitly specifies preference scores for different objectives, guiding the model to generate responses that meet the requirements, sur

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19085v1](https://arxiv.org/abs/2402.19085v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19085v1](https://browse.arxiv.org/html/2402.19085v1)       |
| Truncated       | False       |
| Word Count       | 6069       |