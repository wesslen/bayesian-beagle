
---
title: "Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection"
id: "2402.11621v1"
description: "Study evaluates GPT-3.5 Turbo, GPT-4, and Flan-T5 in detecting framing bias in news headlines."
author: Valeria Pastorino, Jasivan A. Sivakumar, Nafise Sadat Moosavi
date: "2024-02-18"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
This academic article examines the performance of large language models (LLMs) in detecting framing bias in news headlines. The study evaluates the effectiveness of GPT-3.5 Turbo, GPT-4, and FLAN-T5 models in zero-shot, few-shot, and explainable prompting methods. The study finds that explainable prompting enhances the reliability of these models, with GPT-4 demonstrating enhanced performance in few-shot scenarios. However, FLAN-T5's performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. The study also highlights the challenge of distinguishing between genuine emotional expression and intentional framing bias in news headlines.

### Major Findings:
1. **Explainable Prompting Enhances Reliability:** The study finds that explainable prompting consistently yielded more reliable outcomes in both zero-shot and few-shot variations, reducing variance in accuracy and F1 scores.
2. **Optimal Performance in Few-Shot with Diverse Examples:** GPT-4 achieved the highest accuracy and F1 scores in few-shot scenarios with a wide range of in-domain examples, indicating its superior capability in recognizing framing bias.
3. **Challenges with Cross-Domain Examples:** F1 scores dropped in cross-domain settings without explainable prompts, suggesting the effectiveness of zero-shot approaches with clear definitions or explainable prompts for new domain applications.

### Analysis and Critique:
- The study identifies a pattern in GPT-4's errors, where the model frequently interprets emotional language as an indicator of framing bias, leading to biased errors.
- The study also highlights discrepancies in data annotations, with some contested annotations and ambiguous cases, suggesting potential inaccuracies in existing datasets.
- Evaluation of the models in a novel dataset from a website known for featuring framed headlines shows that GPT-4 predominantly identified headlines as framed, indicating a potential bias in classifying inputs as framed.

Overall, the study provides valuable insights into the performance of LLMs in detecting framing bias, but it also raises concerns about potential biases and inaccuracies in data annotations, as well as the need for further research in evaluating LLMs in real-world conditions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.11621v1](https://arxiv.org/abs/2402.11621v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11621v1](https://browse.arxiv.org/html/2402.11621v1)       |
| Truncated       | False       |
| Word Count       | 11304       |