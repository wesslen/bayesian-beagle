
---
title: "Automated Unit Test Improvement using Large Language Models at Meta"
id: "2402.09171v1"
description: "TestGen-LLM: Improving Existing Tests with LLMs; 75% correctness, 57% reliability, 11.5% improvement in Meta test-a-thons."
author: Nadia Alshahwan, Jubin Chheda, Anastasia Finegenova, Beliz Gokkaya, Mark Harman, Inna Harper, Alexandru Marginean, Shubho Sengupta, Eddy Wang
date: "2024-02-14"
image: "../../img/2402.09171v1/image_1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.09171v1/image_1.png)

### Automated Unit Test Improvement using Large Language Models at Meta

**Summary:**
- Meta developed TestGen-LLM, a tool that uses Large Language Models (LLMs) to improve existing human-written tests.
- TestGen-LLM verifies generated test classes using filters that ensure measurable improvement over the original test suite.
- The tool was deployed in test-a-thons for Instagram and Facebook platforms, improving 11.5% of all classes to which it was applied and having 73% of its recommendations accepted for production deployment.

**Major Findings:**
1. TestGen-LLM improved 75% of its test cases for Reels and Stories products on Instagram, with 57% passing reliably and 25% increasing coverage.
2. During Meta's Instagram and Facebook test-a-thons, TestGen-LLM improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment.
3. The study demonstrates the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.

**Analysis and Critique:**
- The study does not mention the specific limitations or potential biases of the LLMs used.
- The evaluation is based on two products (Reels and Stories) for one platform (Instagram), which may not be representative of other products or platforms.
- The acceptance rate of recommendations by software engineers could be influenced by various factors, including the engineers' understanding of the tool and their workload, which might not be directly related to the quality of the generated code.
- The study could benefit from comparing the performance of TestGen-LLM to other test improvement techniques or tools.
- Further investigation is needed to determine the generalizability of the results to other contexts and applications.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.09171v1](https://arxiv.org/abs/2402.09171v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09171v1](https://browse.arxiv.org/html/2402.09171v1)       |
| Truncated       | False       |
| Word Count       | 17410       |