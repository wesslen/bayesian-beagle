
---
title: "NeBuLa: A discourse aware Minecraft Builder"
id: "2406.18164v1"
description: "TL;DR: Model (NeBuLa) improves language to action tasks by considering conversation context, doubling F1 score over baseline."
author: Akshay Chaturvedi, Kate Thompson, Nicholas Asher
date: "2024-06-26"
image: "https://browse.arxiv.org/html/2406.18164v1/extracted/5692610/builder-input-new.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.18164v1/extracted/5692610/builder-input-new.png)

### Summary:

The paper presents a model called NeBuLa (Neural Builder with Llama) that aims to improve the "language to action" component of collaborative tasks. The model is fine-tuned to predict actions based on prior context and has been shown to double the net-action F1 score over the baseline on the task of Jayannavar et al., (2020). The model's ability to construct shapes and understand location descriptions is also investigated using a synthetic dataset.

### Major Findings:

1. NeBuLa, a large language model, has been shown to improve the "language to action" component of collaborative tasks by incorporating prior discourse and nonlinguistic context.
2. The model has been fine-tuned to predict actions based on prior context and has been shown to double the net-action F1 score over the baseline on the task of Jayannavar et al., (2020).
3. The model's ability to construct shapes and understand location descriptions has been investigated using a synthetic dataset.

### Analysis and Critique:

1. The paper does not provide a detailed analysis of the limitations of the model or the potential biases that may have been introduced during the training process.
2. The paper does not discuss the potential impact of the model on real-world applications or the ethical implications of using such a model.
3. The paper does not provide a comparison of the performance of NeBuLa with other state-of-the-art models in the field.
4. The paper does not discuss the potential for the model to be used in other domains or the generalizability of the results.
5. The paper does not provide a detailed discussion of the methodology used to fine-tune the model or the specific techniques used to improve its performance.
6. The paper does not provide a detailed discussion of the synthetic dataset used to evaluate the model's ability to construct shapes and understand location descriptions.
7. The paper does not provide a detailed discussion of the evaluation metrics used to assess the model's performance.
8. The paper does not provide a detailed discussion of the potential applications of the model in real-world scenarios.
9. The paper does not provide a detailed discussion of the potential for the model to be used in conjunction with other models or technologies

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.18164v1](https://arxiv.org/abs/2406.18164v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.18164v1](https://browse.arxiv.org/html/2406.18164v1)       |
| Truncated       | False       |
| Word Count       | 6185       |