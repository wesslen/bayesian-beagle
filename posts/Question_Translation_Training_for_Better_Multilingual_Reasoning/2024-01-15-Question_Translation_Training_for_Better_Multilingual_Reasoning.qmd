
---
title: "Question Translation Training for Better Multilingual Reasoning"
id: "2401.07817v1"
description: "Large language models struggle in non-English languages, but question alignment improves multilingual reasoning."
author: Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, Alexandra Birch
date: "2024-01-15"
image: "../../../bayesian-beagle.png"
categories: ['education', 'prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
The article explores the benefits of question alignment for training large language models (LLMs) to translate reasoning questions into English. The authors propose a method to unlock LLMs' multilingual reasoning abilities by training them to translate reasoning questions into English. Experimental results show consistent improvements over the translate-training approach, with an average improvement of 11.3% and 16.1% accuracy across ten languages on mathematical reasoning benchmarks.

### **Major Findings:**
1. Large language models trained with question alignment show consistent improvements over the translate-training approach.
2. Question alignment leads to an average improvement of 11.3% and 16.1% accuracy across ten languages on mathematical reasoning benchmarks.
3. The proposed question alignment method successfully narrows the performance gap between English and non-English tasks for LLMs.

### **Analysis and Critique:**
The article presents a novel approach to training large language models for multilingual reasoning tasks. The question alignment method shows promising results in improving LLMs' multilingual reasoning abilities. However, the article does not thoroughly discuss potential limitations or biases in the experimental design. Further research is needed to validate the generalizability of the proposed method across different types of reasoning tasks and languages. Additionally, the article could benefit from a more in-depth discussion of the practical implications and applications of the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.07817v1](https://arxiv.org/abs/2401.07817v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.07817v1](https://browse.arxiv.org/html/2401.07817v1)       |
| Truncated       | False       |
| Word Count       | 11312       |