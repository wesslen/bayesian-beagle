
---
title: "CogBench: a large language model walks into a psychology lab"
id: "2402.18225v1"
description: "CogBench benchmarks LLMs using cognitive psychology metrics, highlighting the role of model size and RLHF."
author: Julian Coda-Forno, Marcel Binz, Jane X. Wang, Eric Schulz
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18225v1/x1.png"
categories: ['education', 'hci', 'architectures', 'production', 'social-sciences', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18225v1/x1.png)

### **Summary:**
- CogBench is a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments to evaluate large language models (LLMs).
- The study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior.
- Open-source models are less risk-prone than proprietary models, and fine-tuning on code does not necessarily enhance LLMs' behavior.

### **Major Findings:**
1. CogBench includes ten behavioral metrics derived from seven cognitive psychology experiments to evaluate LLMs.
2. Model size and reinforcement learning from human feedback (RLHF) play a crucial role in improving performance and aligning with human behavior.
3. Open-source models are less risk-prone than proprietary models, and fine-tuning on code does not necessarily enhance LLMs' behavior.

### **Analysis and Critique:**
- The study provides valuable insights into the behavior of LLMs, but the limited transparency of certain proprietary models poses challenges to the regression analysis.
- The study highlights the importance of behavioral metrics and cognitive modeling in evaluating LLMs and presents a novel benchmark for this purpose.
- Future work should focus on extending the set of included tasks to cover a broader set of domains and properly automate the benchmark, especially for prompt engineering techniques.

### **List of LLMs used:**
The table lists 35 LLMs used in the study, including features such as the number of parameters, RLHF usage, open-source status, size of the dataset, and more.

### **Comprehensive list & explanation of the cognitive experiments:**
The appendix provides a detailed explanation of the cognitive experiments used in the study, including summaries, methods, prompts for LLMs, and metrics for each experiment.

### **Full benchmark results for rest of LLMs:**
The appendix includes visualizations of performance metrics and behavioral metrics for the rest of the LLMs used in the study.

### **Prompt Engineering techniques:**
The study discusses the challenges and complexities of implementing prompt engineering techniques, such as chain-of-thought (CoT) and take-a-step-back (SB) prompting, for LLMs. It highlights the need to balance flexibility and control in the design and operation of LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.18225v1](https://arxiv.org/abs/2402.18225v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18225v1](https://browse.arxiv.org/html/2402.18225v1)       |
| Truncated       | False       |
| Word Count       | 10699       |