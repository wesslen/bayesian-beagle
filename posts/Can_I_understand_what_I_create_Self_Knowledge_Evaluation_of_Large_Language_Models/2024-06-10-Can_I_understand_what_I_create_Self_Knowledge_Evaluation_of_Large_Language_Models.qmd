
---
title: "Can I understand what I create? Self-Knowledge Evaluation of Large Language Models"
id: "2406.06140v1"
description: "LLMs struggle with self-generated questions due to human-alignment issues, but fine-tuning improves math performance."
author: Zhiquan Tan, Lai Wei, Jindong Wang, Xing Xie, Weiran Huang
date: "2024-06-10"
image: "https://browse.arxiv.org/html/2406.06140v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.06140v1/x1.png)

### Summary:

- The paper introduces a self-knowledge evaluation framework for large language models (LLMs) and large multi-modal models (LMMs) to assess their ability to understand and respond to self-generated questions.
- The framework is inspired by Feynman's principle of understanding through creation and is easy to implement.
- The evaluation of 7 popular LLMs across 9 tasks, including counting words, math, theorem proving, etc., reveals significant gaps in the model's self-knowledge ability.
- Further analysis indicates that these gaps may be due to misalignment with human attention mechanisms.
- Fine-tuning on self-generated math tasks may enhance the model's math performance, highlighting the potential of the framework for efficient and insightful model evaluation.

### Major Findings:

1. Modern LLMs and LMMs have unsatisfactory behaviors on self-knowledge evaluations, which is far from perfect.
2. By analyzing a designated word counting task, models become much similar to human-inspired attention-based mechanisms when the model gets a higher self-knowledge score.
3. Only GPT-4 and Gemma achieve 100% accuracy when the question-generating process is given in context, and their accuracy is reduced when the context is added with noisy contents.
4. Fine-tuning the data generated by the self-knowledge math task may improve the performance on GSM-8k.
5. Expert-based prompts may usually improve self-knowledge ability, but chain-of-thought prompting may usually not.

### Analysis and Critique:

- The paper provides a novel framework for evaluating the self-knowledge of LLMs and LMMs, which is easy to implement and offers an efficient and insightful method for model evaluation.
- The evaluation of multiple models across diverse tasks reveals significant gaps in the model's self-knowledge ability, highlighting the need for further research in this area.
- The analysis of the results suggests that the misalignment with human attention mechanisms may be a contributing factor to the poor performance of LLMs and LMMs in self-knowledge tasks.
- The potential of fine-tuning on self-generated data to enhance model performance is an interesting finding that warr

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.06140v1](https://arxiv.org/abs/2406.06140v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06140v1](https://browse.arxiv.org/html/2406.06140v1)       |
| Truncated       | False       |
| Word Count       | 7449       |