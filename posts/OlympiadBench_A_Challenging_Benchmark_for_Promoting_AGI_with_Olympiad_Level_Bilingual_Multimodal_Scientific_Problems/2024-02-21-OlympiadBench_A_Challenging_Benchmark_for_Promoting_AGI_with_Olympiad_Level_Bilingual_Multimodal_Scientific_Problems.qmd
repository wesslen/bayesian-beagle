
---
title: "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems"
id: "2402.14008v1"
description: "Large Language and Multimodal Models surpass human capabilities, but struggle with rigorous Olympiad-level challenges. GPT-4V scores 17.23%."
author: Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, Maosong Sun
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.14008v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14008v1/x1.png)

### **Summary:**
- Recent advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs) have surpassed general human capabilities in various tasks.
- OlympiadBench is an Olympiad-level bilingual multimodal scientific benchmark featuring 8,952 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam.
- The best-performing model, GPT-4V, attains an average score of 17.23% on OlympiadBench, highlighting the benchmark's rigor and the intricacy of physical reasoning.

### **Major Findings:**
1. OlympiadBench is more challenging than existing benchmarks, providing a new perspective to compare LMMs.
2. There is a significant difference between the most powerful closed-source models and open-source models, with a large model size needed for high accuracy.
3. The challenge lies more on questions with images, physics, and non-English text.

### **Analysis and Critique:**
- The article provides a comprehensive evaluation of the performance of various models on OlympiadBench, highlighting the challenges and differences between closed-source and open-source models.
- The limitations of the study include the inability to assess specific analysis based on the particulars of each question and the extensive manual effort required in gathering and annotating data.
- The article raises important ethical considerations, such as the release of the dataset and accompanying scripts publicly to reduce carbon footprint and compliance with all licenses for models and data.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.14008v1](https://arxiv.org/abs/2402.14008v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14008v1](https://browse.arxiv.org/html/2402.14008v1)       |
| Truncated       | False       |
| Word Count       | 8343       |