
---
title: "Chain-of-Feedback: Mitigating the Effects of Inconsistency in Responses"
id: "2402.02648v1"
description: "LLMs struggle with knowledge-based questions, leading to inconsistent and unreliable responses. Recursive feedback may improve accuracy."
author: Jinwoo Ahn
date: "2024-02-05"
image: "../../img/2402.02648v1/image_1.png"
categories: ['prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.02648v1/image_1.png)

### **Summary:**
- Large Language Models (LLMs) often provide inconsistent outputs for knowledge-intensive questions, decreasing the reliability and validity of responses.
- The Chain-of-Feedback (CoF) system triggers LLMs to deviate more from the actual answer, while Recursive Chain of Feedback (R-CoF) is a novel prompting method being studied to mitigate inconsistencies.
- Relying heavily on AI agents like ChatGPT can lead to the public perceiving them as reliable sources of information, despite potential inaccuracies and inconsistencies.

### **Major Findings:**
1. LLMs are prone to generating contradicting sentences and being distracted with irrelevant context, leading to unreliable responses.
2. Meaningless feedback requesting another attempt from LLMs decreases the quality of the response, highlighting the need for improved prompting methods.
3. R-CoF aims to break down complex problems into smaller steps, allowing users to verify correctness and adjust incorrect reasoning to reach the correct solution.

### **Analysis and Critique:**
- The article raises awareness of the risks associated with relying on AI agents for information, highlighting the potential for misleading or inaccurate responses.
- The preliminary experiments show promising insights into the impact of prompting methods on LLM responses, but further research is needed to validate the effectiveness of R-CoF.
- The limitations of the ongoing work, such as time constraints and the need for extensive experiments with larger datasets and different public models, indicate the need for more comprehensive research in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.02648v1](https://arxiv.org/abs/2402.02648v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.02648v1](https://browse.arxiv.org/html/2402.02648v1)       |
| Truncated       | False       |
| Word Count       | 3742       |