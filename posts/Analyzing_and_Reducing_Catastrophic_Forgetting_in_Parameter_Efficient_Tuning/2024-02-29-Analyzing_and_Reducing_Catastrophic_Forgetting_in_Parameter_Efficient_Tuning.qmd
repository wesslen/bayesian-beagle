
---
title: "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning"
id: "2402.18865v1"
description: "I-LoRA balances plasticity and stability in continual fine-tuning of LLMs, with up to 11% performance gains."
author: Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, Wei Qin
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.18865v1/x1.png"
categories: ['programming', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18865v1/x1.png)

### Summary

- Large Language Models (LLMs) suffer from catastrophic forgetting when continuously fine-tuned on complex domain-specific tasks.
- The study explores the geometric connections of different minima in the continual learning scenario of LLMs through the lens of mode connectivity.
- The authors propose Interpolation-based LoRA (I-LoRA), a simple yet effective method that strikes a balance between plasticity and stability in LLMs' continual learning.
- I-LoRA consistently outperforms previous state-of-the-art approaches in eight domain-specific CL benchmarks with up to 10% performance gains.

### Major Findings

1. **Mode Connectivity in LLMs:** The study uncovers the mode connectivity phenomenon in the LLMs continual learning scenario, where different minima can be connected by a low-loss valley.
2. **I-LoRA

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.18865v1](https://arxiv.org/abs/2402.18865v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18865v1](https://browse.arxiv.org/html/2402.18865v1)       |
| Truncated       | False       |
| Word Count       | 6698       |