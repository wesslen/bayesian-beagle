
---
title: "Evaluating Very Long-Term Conversational Memory of LLM Agents"
id: "2402.17753v1"
description: "Long-term dialogue models struggle with understanding lengthy conversations and lag behind human performance."
author: Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17753v1/x1.png"
categories: ['production', 'hci', 'architectures', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17753v1/x1.png)

### **Summary:**
The article introduces the LoCoMo dataset, a collection of very long-term conversations, and proposes an evaluation benchmark to measure the proficiency of models in handling long-term dialogues. The dataset is generated using a machine-human pipeline, leveraging large language models (LLMs) and human annotators to ensure high-quality conversations. The evaluation benchmark includes three tasks: question answering, event summarization, and multi-modal dialogue generation. Experimental results indicate that LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues. The study also highlights the limitations and potential biases of using LLMs for long-term dialogue generation.

### Major Findings:
1. LLMs exhibit challenges in understanding lengthy conversations and comprehending long-range temporal and causal dynamics within dialogues.
2. Long-context LLMs and retrieval-augmented generation (RAG) techniques offer improvements in question answering tasks, but still lag behind human performance.
3. RAG offers a balanced compromise, combining the accuracy of short-context LLMs with the extensive comprehension of wide-context LLMs.

### Analysis and Critique:
- The study is limited by the use of hybrid human-machine generated data, closed-source LLMs, and the potential for misinformation and social biases in multi-modal dialogue generation.
- The dataset is released under a CC BY-NC 4.0 DEED license, and the annotator details are not available.
- The study raises ethical concerns about the potential impact of generative agents on human behavior and the need for disclaimers when deploying such frameworks.

The article provides valuable insights into the challenges and limitations of using LLMs for very long-term conversational memory evaluation. However, it is important to address the ethical concerns and potential biases associated with generative agents. Further research is needed to overcome the limitations and biases identified in this study.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.17753v1](https://arxiv.org/abs/2402.17753v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17753v1](https://browse.arxiv.org/html/2402.17753v1)       |
| Truncated       | False       |
| Word Count       | 9641       |