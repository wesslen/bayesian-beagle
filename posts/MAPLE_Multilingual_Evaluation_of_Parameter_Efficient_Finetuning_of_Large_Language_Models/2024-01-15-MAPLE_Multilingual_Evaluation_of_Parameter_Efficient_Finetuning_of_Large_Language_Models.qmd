
---
title: "MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models"
id: "2401.07598v1"
description: "Parameter efficient finetuning improves language model performance, but can impact English and low-resource languages."
author: Divyanshu Aggarwal, Ashutosh Sathe, Sunayana Sitaram
date: "2024-01-15"
image: "../../../bayesian-beagle.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](None)

### Summary:

The article investigates the impact of Parameter Efficient Finetuning (PEFT) on multilingual downstream tasks, analyzing the effects of different configurations and settings on model performance. The study evaluates the performance of finetuned open-source, multilingual Large Language Models (LLMs) on various downstream tasks across multiple languages. The findings highlight the trade-offs between model performance on different languages, the impact of finetuning on low-resource and high-resource languages, and the potential for finetuning to bridge the performance gap between smaller open-source models and larger proprietary models.

### Major Findings:
1. The impact of PEFT on multilingual downstream tasks varies across different languages, with higher rank and quantization values benefiting low-resource languages.
2. Finetuning can bridge the performance gap between smaller open-source models and larger proprietary models for some downstream applications, but it may lead to a degradation in English performance and performance on high-resource languages.
3. The performance of finetuned models across various tasks and languages underscores the importance of considering the impact of quantization, rank, and multilingual finetuning on model performance.

### Analysis and Critique:
The article provides valuable insights into the effects of PEFT on multilingual downstream tasks and the trade-offs between model performance on different languages. However, potential limitations include data contamination and the need for further research to improve multilingual performance. The detailed hyperparameters and experimental setup contribute to the broader understanding of language model optimization, while the performance analysis of models on different datasets and languages highlights the complexities of model performance in a multilingual context. Further investigation into the optimization of models for multilingual tasks is warranted to enhance the generalizability and robustness of these models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [https://arxiv.org/abs/2401.07598v1](https://arxiv.org/abs/2401.07598v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.07598v1](https://browse.arxiv.org/html/2401.07598v1)       |
| Truncated       | True       |
| Word Count       | 25280       |