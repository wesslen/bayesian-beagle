
---
title: "AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents"
id: "2401.12963v1"
description: "AutoRT system leverages vision-language & large language models to guide autonomous robot deployment in new scenarios. Significantly scales up data collection."
author: ['Michael Ahn', 'Debidatta Dwibedi', 'Chelsea Finn', 'Montse Gonzalez Arenas', 'Keerthana Gopalakrishnan', 'Karol Hausman', 'Brian Ichter', 'Alex Irpan', 'Nikhil Joshi', 'Ryan Julian', 'Sean Kirmani', 'Isabel Leal', 'Edward Lee', 'Sergey Levine', 'Yao Lu', 'Isabel Leal', 'Sharath Maddineni', 'Kanishka Rao', 'Dorsa Sadigh', 'Pannag Sanketi', 'Pierre Sermanet', 'Quan Vuong', 'Stefan Welker', 'Fei Xia', 'Ted Xiao', 'Peng Xu', 'Steve Xu', 'Zhuo Xu']
date: "2024-01-23"
image: "https://browse.arxiv.org/html/2401.12963v1/x1.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.12963v1/x1.png)

### **Summary:**

The article presents "AutoRT," a system designed to leverage existing foundation models to scale up the deployment of operational robots in unseen scenarios with minimal human supervision. The system utilizes vision-language models (VLMs) for scene understanding, grounding, and leverages large language models (LLMs) for proposing diverse and novel instructions to be performed by a fleet of robots. This approach addresses the challenge of collecting large-scale, "in-the-wild" data grounded in the physical world. The system aims to guide the data collection of a fleet of robots, considering autonomy tradeoffs and safety while significantly scaling up data collection for robot learning. The system was demonstrated in real-world settings, proposing instructions to over 20 robots across multiple buildings and collecting 77,000 real robot episodes via teleoperation and autonomous robot policies.

### **Major Findings:**
1. AutoRT demonstrates the scalability of robot deployment by allowing 1 human to supervise 3-5 mobile manipulators, resulting in the collection of diverse data for robot learning.
2. The data collected by AutoRT is significantly more diverse, and the use of LLMs allows for instruction following data collection robots that align with human preferences.

### **Analysis and Critique:**
The article presents a sophisticated system, AutoRT, for large-scale orchestration of robotic agents, showcasing its effectiveness in collecting diverse, real-world robot data. However, several limitations and challenges were identified during the article:
1. **Dependence on Scripted and Learned Policies:** The reliance on scripted and learned policies may limit the system's ability to handle complex tasks or perform well in unseen settings, potentially affecting the throughput of successful episodes.

2. **Information Bottleneck and Model Limitations:** Communication bandwidth between scene description and language model may introduce an information bottleneck, and foundation models face challenges in reasoning about embodiment-specific information, such as the physics of objects and robot capabilities.

3. **Sparse Data and Learning Challenges:** The highly diverse data collected by AutoRT may present a challenging learning problem, especially for existing state-of-the-art robot learning methods, requiring a balance between data quality and quantity.

4. **Safety and Human Supervision:** While constitutional prompting improves safety of tasks generated, it does not guarantee the robot's adherence to the instructions, necessitating a significant degree of human supervision.

In conclusion, while AutoRT presents a promising system for large-scale robotic data collection, addressing the identified limitations is crucial for its widespread and effective implementation. Future directions may involve advancing robust and diverse autonomous collect policies and exploring the integration of model improvement and data collection as a unified goal.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [http://arxiv.org/abs/2401.12963v1](http://arxiv.org/abs/2401.12963v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.12963v1](https://browse.arxiv.org/html/2401.12963v1)       |
| Truncated       | False       |
| Word Count       | 12795       |