
---
title: "Using Large Language Models for Commit Message Generation: A Preliminary Study"
id: "2401.05926v1"
description: "Study evaluates using large language models like Llama 2 and ChatGPT to generate Git commit messages. Results show promising potential."
author: Linghao Zhang, Jingshu Zhao, Chong Wang, Peng Liang
date: "2024-01-11"
image: "https://browse.arxiv.org/html/2401.05926v1/x1.png"
categories: ['production', 'architectures', 'robustness', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.05926v1/x1.png)

### Major Findings

1. **Large language models (LLMs)**, such as Llama 2 and ChatGPT, outperformed existing methods in **human evaluations** for commit message generation in 78% of the 366 samples.
2. LLMs demonstrated comparable performance to previous techniques on **BLEU and Rouge-L metrics** but showed a distinct advantage over all existing methods in **human evaluation**.
3. The study highlighted the limitations of existing metrics, **BLEU and Rouge-L**, in evaluating the quality of automatically generated commit messages, raising the need for more robust evaluation metrics.

### I Introduction

- Commit messages are crucial in the Git version control system, but manually writing them is time-consuming, leading to the need for automatic generation methods.
- Large Language Models (LLMs) have shown promise in various domains, but their application in **commit message generation** has been underexplored.

### II Related Work

- Previous studies have proposed **generation-based and retrieval-based methods** for commit message generation, but this work introduces the novel application of LLMs to this task.
- Existing methods provide baselines for **evaluation and comparative analysis** of LLMs.

### III Research Design

- The research question focuses on exploring the feasibility and effectiveness of LLMs in **commit message generation**. The study uses a **two-phase evaluation** to assess the quality of generated commit messages.

### III-A Overview of Our Approach

- The study leverages two LLMs, ChatGPT and Llama 2, to generate commit messages and implements a **two-phase evaluation** process.
- The dataset used for the study is publicly available and contains pairs of code diffs and their corresponding commit messages.

### III-B Selection and Settings of LLMs

- Two representative LLMs, ChatGPT and Llama 2, are selected for the study based on their generality and zero-shot prompting capabilities.

### III-C Metrics and Baselines used in Evaluation: Phase I

- Evaluation metrics including **BLEU and Rouge-L** are employed to compare the quality of commit messages generated by LLMs with existing baseline models.

### III-F Human Evaluation: Phase II

- A **human evaluation** is conducted to assess which method of commit message generation best fits the code differences, with LLMs outperforming other methods in human preference.

### IV Results & Discussion

- LLMs achieve decent scores compared to baseline models on the **metrics evaluation** and are preferred by humans in the **human evaluation**.
- The study uncovers quality issues in **human-written commit messages**, highlighting the need for more robust evaluation metrics aligning with human judgment.

### V Limitations

- The study points out limitations such as the closed-source nature of ChatGPT, the preliminary nature of the evaluation, and potential subjective biases in human evaluation.

### VI Conclusions & Future Work

- The study demonstrates the potential of LLMs for **commit message generation** and calls for the development of **robust evaluation metrics** aligning with human judgment.
- Future work aims to explore more prompt strategies to improve LLM performance and develop **LLM-integrated commit message generation methods**.

### Critique

The study provides valuable insights into the use of LLMs for commit message generation and highlights important limitations of existing evaluation metrics. However, there are potential issues with the closed-source nature of ChatGPT, and the relatively small sample size in the human evaluation could introduce bias. Additionally, further research is needed to address the observed limitations and expand the scope of evaluation metrics.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-27       |
| Abstract | [http://arxiv.org/abs/2401.05926v1](http://arxiv.org/abs/2401.05926v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.05926v1](https://browse.arxiv.org/html/2401.05926v1)       |
| Truncated       | False       |
| Word Count       | 5302       |