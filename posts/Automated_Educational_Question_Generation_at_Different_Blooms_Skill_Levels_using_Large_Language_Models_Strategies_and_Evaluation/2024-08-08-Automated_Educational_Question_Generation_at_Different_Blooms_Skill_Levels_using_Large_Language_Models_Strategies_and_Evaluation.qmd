
---
title: "Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Models: Strategies and Evaluation"
id: "2408.04394v1"
description: "LLMs can generate diverse, high-quality educational questions with proper prompting, but human evaluation is superior to automated evaluation."
author: Nicy Scaria, Suma Dharani Chenna, Deepak Subramani
date: "2024-08-08"
image: "https://browse.arxiv.org/html/2408.04394v1/extracted/5781131/prompts_plot.png"
categories: ['social-sciences', 'education', 'hci', 'programming', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.04394v1/extracted/5781131/prompts_plot.png)

### Summary:

The study examines the ability of five state-of-the-art large language models (LLMs) of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloomâ€™s taxonomy. The researchers used advanced prompting techniques with varying complexity for automated educational question generation (AEQG). Expert and LLM-based evaluations were conducted to assess the linguistic and pedagogical relevance and quality of the questions.

The findings suggest that LLMs can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLMs considered. The study also shows that automated evaluation is not on par with human evaluation.

### Major Findings:

1. LLMs can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information.
2. There is a significant variance in the performance of the five LLMs considered in the study.
3. Automated evaluation is not on par with human evaluation.

### Analysis and Critique:

The study provides valuable insights into the potential of LLMs for AEQG. However, there are some limitations and areas for improvement.

1. The study does not provide a detailed analysis of the performance of each LLM, making it difficult to compare their strengths and weaknesses.
2. The study does not discuss the potential biases and limitations of the LLMs used, which could impact the quality and relevance of the generated questions.
3. The study does not explore the potential of using LLMs for other educational tasks, such as content creation and feedback provision.
4. The study does not discuss the potential ethical implications of using LLMs for AEQG, such as the risk of perpetuating biases and stereotypes.

Overall, the study provides a useful starting point for further research into the use of LLMs for AEQG. However, more work is needed to fully understand the potential and limitations of this approach.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.04394v1](https://arxiv.org/abs/2408.04394v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.04394v1](https://browse.arxiv.org/html/2408.04394v1)       |
| Truncated       | False       |
| Word Count       | 5838       |