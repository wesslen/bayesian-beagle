
---
title: "Defending LLMs against Jailbreaking Attacks via Backtranslation"
id: "2402.16459v1"
description: "New method defends language models from jailbreaking attacks using backtranslation prompts."
author: Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16459v1/x1.png"
categories: ['security', 'production', 'architectures', 'prompt-engineering', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16459v1/x1.png)

### **Summary:**
- The paper proposes a defense method for large language models (LLMs) against jailbreaking attacks using backtranslation.
- The backtranslation method prompts the LLM to infer a possible prompt that can lead to the response, and then checks if the model refuses the backtranslated prompt.
- The defense method significantly outperforms existing baselines and has little impact on the generation quality for benign input prompts.

### **Major Findings:**
1. The proposed backtranslation defense method significantly outperforms existing defense baselines against jailbreaking attacks.
2. The defense method leverages the inherent ability of the target model to refuse harmful requests and does not require additional training for an additional task.
3. The defense is cheap, efficient, and has little impact on the generation quality for benign requests.

### **Analysis and Critique:**
- The proposed defense method is highly effective and efficient, providing a significant improvement over existing baselines.
- The defense method has little impact on the generation quality for benign requests, maintaining the overall quality of the LLM's responses.
- The paper acknowledges some limitations, such as the effectiveness of backtranslation relying on the assumption that the model without defense is able to refuse clean harmful requests, and the potential for over-refusal due to errors in backtranslation.
- The study provides valuable insights into the development of ethical and safe LLMs, but further research is needed to address the identified limitations and potential biases.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.16459v1](https://arxiv.org/abs/2402.16459v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16459v1](https://browse.arxiv.org/html/2402.16459v1)       |
| Truncated       | False       |
| Word Count       | 6994       |