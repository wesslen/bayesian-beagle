
---
title: "Enhancing Large Language Models for Text-to-Testcase Generation"
id: "2402.11910v1"
description: "TL;DR: GPT-3.5 fine-tuned for text-to-testcase generation outperforms other language models."
author: Saranya Alagarsamy, Chakkrit Tantithamthavorn, Chetan Arora, Aldeida Aleti
date: "2024-02-19"
image: "https://browse.arxiv.org/html/2402.11910v1/x1.png"
categories: ['prompt-engineering', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.11910v1/x1.png)

### **Summary:**
- The article introduces a text-to-testcase generation approach based on a fine-tuned large language model with an effective prompting design.
- The approach substantially outperforms other large language models (LLMs) in generating test cases for open-source projects.

### **Major Findings:**
1. **Effectiveness of the Proposed Approach:**
   - The proposed approach outperforms all other LLMs, achieving 78.5% syntactic correctness, 67.09% requirement alignment, and 61.7% code coverage for generated test cases.
2. **Effect of Fine-Tuning:**
   - Fine-tuning the LLMs improves syntax correctness by 223%, requirement alignment by 164%, and code coverage by 153%.
3. **Effect of Prompting:**
   - Improved prompts substantially improve the performance of LLMs, increasing syntax correctness by 223%, requirement alignment by 164%, and code coverage by 153%.

### **Analysis and Critique:**
- **Internal Validity:**
  - The hyper-parameter optimization and potential training data leakage of LLMs could impact the study's internal validity.
- **Construct Validity:**
  - The subjectivity in prompt tuning and robustness may affect the study's construct validity.
- **External Validity:**
  - The generalizability of the findings to other open-source projects could impact the study's external validity.

The study provides valuable insights into the effectiveness of fine-tuning and prompting for LLMs in text-to-testcase generation. However, potential threats to validity and generalizability should be considered in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.11910v1](https://arxiv.org/abs/2402.11910v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11910v1](https://browse.arxiv.org/html/2402.11910v1)       |
| Truncated       | False       |
| Word Count       | 9643       |