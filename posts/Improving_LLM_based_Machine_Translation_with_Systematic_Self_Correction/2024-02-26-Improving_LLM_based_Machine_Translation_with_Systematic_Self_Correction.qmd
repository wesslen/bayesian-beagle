
---
title: "Improving LLM-based Machine Translation with Systematic Self-Correction"
id: "2402.16379v1"
description: "LLMs have translation errors, but self-correction framework TER improves quality across languages."
author: Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, Zuozhu Liu
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16379v1/extracted/5431276/Figures/cases.png"
categories: ['robustness', 'architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16379v1/extracted/5431276/Figures/cases.png)

### Summary:
- Large Language Models (LLMs) have achieved impressive results in Machine Translation (MT), but translations still contain errors.
- A systematic LLM-based self-correcting translation framework, named TER, has been introduced to improve translation performance.
- The framework successfully assists LLMs in improving translation quality across a wide range of languages, exhibits superior systematicity and interpretability, and different estimation strategies yield varied impacts on AI feedback.

### Major Findings:
1. The self-correction framework successfully assists LLMs in improving their translation quality across a wide range of languages.
2. TER exhibits superior systematicity and interpretability compared to previous methods.
3. Different estimation strategies yield varied impacts on AI feedback, directly affecting the effectiveness of the final corrections.

### Analysis and Critique:
- The article provides a comprehensive exploration of the TER framework, showcasing the impact of different components on its performance.
- The study identifies potential limitations in the estimation prompting strategies, which may limit the effectiveness of the self-correction framework.
- The article highlights the need for further research to identify the optimal strategy for applying LLMs to self-correct translations and the potential for future work in exploring the iterative self-correction process.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.16379v1](https://arxiv.org/abs/2402.16379v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16379v1](https://browse.arxiv.org/html/2402.16379v1)       |
| Truncated       | False       |
| Word Count       | 6433       |