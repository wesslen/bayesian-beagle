
---
title: "AI-Assisted Generation of Difficult Math Questions"
id: "2407.21009v1"
description: "This paper introduces MATH$^2$, a dataset of diverse, challenging math questions generated by combining LLMs with human-in-the-loop approach, leveraging LLM metacognition skills and iterative refinement."
author: Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Nan Rosemary Ke, Michael Mozer, Yoshua Bengio, Sanjeev Arora, Anirudh Goyal
date: "2024-07-30"
image: "../../img/2407.21009v1/image_1.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2407.21009v1/image_1.png)

**Summary:**

The paper presents a design framework that combines the strengths of large language models (LLMs) with a human-in-the-loop approach to generate a diverse array of challenging math questions. The framework leverages LLM metacognition skills to extract core "skills" from existing math datasets, which serve as the basis for generating novel and difficult questions. The use of two very different skills within each question makes finding such questions an "out of distribution" task for both LLMs and humans. The pipeline employs LLMs to iteratively generate and refine questions and solutions through multi-turn prompting, with human annotators verifying and further refining the questions. The proposed framework was applied to skills extracted from the MATH dataset, resulting in a dataset of higher quality math questions, as evidenced by lower performance of all models on MATH2 than on MATH and higher performance on MATH when using MATH2 questions as in-context examples.

**Major Findings:**

1. The proposed framework combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions.
2. The framework leverages LLM metacognition skills to extract core "skills" from existing math datasets, which serve as the basis for generating novel and difficult questions.
3. The use of two very different skills within each question makes finding such questions an "out of distribution" task for both LLMs and humans.
4. The pipeline employs LLMs to iteratively generate and refine questions and solutions through multi-turn prompting, with human annotators verifying and further refining the questions.
5. The proposed framework was applied to skills extracted from the MATH dataset, resulting in a dataset of higher quality math questions, as evidenced by lower performance of all models on MATH2 than on MATH and higher performance on MATH when using MATH2 questions as in-context examples.

**Analysis and Critique:**

The proposed framework presents a promising approach to generating challenging math questions by combining the strengths of LLMs with a human-in-the-loop approach. The use of LLM metacognition skills to extract core "skills" from existing math datasets is an innovative approach to generating novel and difficult questions. However, the framework relies on the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.21009v1](https://arxiv.org/abs/2407.21009v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.21009v1](https://browse.arxiv.org/html/2407.21009v1)       |
| Truncated       | False       |
| Word Count       | 22429       |