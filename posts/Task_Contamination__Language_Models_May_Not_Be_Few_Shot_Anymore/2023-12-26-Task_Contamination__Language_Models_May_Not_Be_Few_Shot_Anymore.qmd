
---
title: "Task Contamination: Language Models May Not Be Few-Shot Anymore"
description: "Large language models (LLMs) perform better in zero-shot and few-shot tasks on datasets released before their training data creation date, possibly due to task contamination."
author: Changmao Li, Jeffrey Flanigan
date: "2023-12-26"
image: "https://browse.arxiv.org/html/2312.16337v1/x1.png"
categories: ['prompt engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.16337v1/x1.png)

### Summary

The paper explores the phenomenon of task contamination in large language models (LLMs), which affects their zero-shot and few-shot performance. The study uses a range of LLMs and tasks to demonstrate how LLMs may be exposed to task examples during pre-training, leading to inflated performance in zero-shot and few-shot settings. The authors employ various methods, such as training data inspection, task example extraction, and chronological analysis, to provide evidence of task contamination. The results indicate that closed-sourced models may demonstrate inflated performance in zero-shot or few-shot evaluation, and are therefore not trustworthy baselines in these settings. Additionally, the paper uncovers challenges in assessing task contamination due to different formats of training data and the difficulty in extracting task examples. The findings suggest a need for caution in relying on LLMs for zero-shot and few-shot tasks and call for additional research to understand the extent of task contamination for large language models.

### Major Takeaways

1. Closed-sourced models may demonstrate inflated performance in zero-shot or few-shot evaluation, casting doubt on their trustworthiness as baselines in these settings.
2. LLMs rarely show statistically significant improvements over majority baselines in both zero and few-shot settings for tasks without demonstrated possibility of task contamination.
3. The observed increase over time in the zero-shot or few-shot performance of LLMs, especially in the GPT-3 series, is likely due to task contamination.

### Critique

The paper provides valuable insights into the phenomenon of task contamination in large language models, but there are some potential limitations and areas for improvement:

1. The study focuses primarily on the chronological analysis and task contamination for specific models, potentially limiting the generalizability of the findings to a broader range of LLMs and tasks.
2. The challenges and limitations of the methods used for detecting task contamination, such as training data inspection and task example extraction, raise concerns about the reliability and completeness of the evidence presented.
3. The paper lacks a comprehensive discussion of potential strategies or solutions to mitigate task contamination in large language models, leaving an opportunity for further exploration in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| Abstract | [http://arxiv.org/abs/2312.16337v1](http://arxiv.org/abs/2312.16337v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.16337v1](https://browse.arxiv.org/html/2312.16337v1)       |
| Truncated       | False       |
| Word Count       | 8991       |