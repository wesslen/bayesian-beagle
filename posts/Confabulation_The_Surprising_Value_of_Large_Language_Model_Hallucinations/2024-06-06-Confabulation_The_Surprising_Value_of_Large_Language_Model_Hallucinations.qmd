
---
title: "Confabulation: The Surprising Value of Large Language Model Hallucinations"
id: "2406.04175v1"
description: "LLM confabulations mirror human narrativity, offering potential value in AI communication."
author: Peiqi Sui, Eamon Duede, Sophie Wu, Richard Jean So
date: "2024-06-06"
image: "https://browse.arxiv.org/html/2406.04175v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.04175v1/x1.png)

### Summary:

This paper presents a novel perspective on large language model (LLM) hallucinations, or 'confabulations,' arguing that they can be a valuable resource rather than a categorically negative pitfall. The authors challenge the standard view that confabulations are inherently problematic and should be eliminated from AI research. Instead, they argue that measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication.

The authors analyze popular hallucination benchmarks and reveal that hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs. This finding suggests that the tendency for LLMs to confabulate may be intimately associated with a positive capacity for coherent narrative-text generation.

### Major Findings:

1. LLM confabulations, or hallucinations, can be a valuable resource rather than a negative pitfall.
2. Measurable semantic characteristics of LLM confabulations mirror a human propensity to utilize increased narrativity as a cognitive resource for sense-making and communication.
3. Hallucinated outputs display increased levels of narrativity and semantic coherence relative to veridical outputs.

### Analysis and Critique:

While the paper presents an interesting perspective on LLM confabulations, there are several potential limitations and areas for further research. The authors acknowledge that their findings reveal intriguing associations between increased narrativity and significant increases in coherence, but they do not assert that narrativity drives coherence. More robust methods for modeling narratives and more comprehensive human evaluations are needed to elucidate the intricacies of this association.

Additionally, the extent to which the affordances of confabulations generalize to human-AI interactions needs to be further validated with human-based evaluations. The authors plan to follow up this study with experiments with human participants to verify the benefits of narrative engagement as hypothesized.

Finally, the paper could benefit from a more in-depth discussion of the potential risks and ethical considerations associated with LLM confabulations. While the authors acknowledge that hallucinations can present an imminent risk to model trustworthiness, they do not fully explore the potential consequences of these risks in different contexts.

In

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.04175v1](https://arxiv.org/abs/2406.04175v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.04175v1](https://browse.arxiv.org/html/2406.04175v1)       |
| Truncated       | False       |
| Word Count       | 5509       |