
---
title: "Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance"
id: "2402.14531v1"
description: "Politeness in prompts affects LLM performance across languages, cultural context matters."
author: Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14531v1/extracted/5424386/ilst.png"
categories: ['prompt-engineering', 'hci', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14531v1/extracted/5424386/ilst.png)

### Summary:
- The article investigates the impact of politeness levels in prompts on the performance of large language models (LLMs).
- The study assesses the impact of politeness in prompts on LLMs across English, Chinese, and Japanese tasks.
- The findings suggest that impolite prompts often result in poor performance, but overly polite language does not guarantee better outcomes. The best politeness level is different according to the language.

### Major Findings:
1. **LLMs reflect human desire**: Impolite prompts may lead to a deterioration in model performance, including generations containing mistakes, stronger biases, and omission of information. The best level of politeness for performance is different across languages, which is strongly related to their cultural background.
2. **Politeness and Respect**: Humans are highly sensitive to politeness and respect in communications. Politeness and respect are expressed differently in various languages, such as English, Chinese, and Japanese.
3. **LLMs and Prompt Engineering**: LLMs are sensitive and vulnerable to prompts. Minor changes can lead to significant differences in the output. Adjusting prompts is primarily conducted manually at present and requires numerous experiments.

### Analysis and Critique:
- The study provides valuable insights into the impact of politeness levels in prompts on LLM performance. However, the research faces limitations in prompt quantity and diversity, task configuration, and language selection. The findings highlight the need to consider cultural background during the development and corpus collection of LLMs. The study also raises ethical considerations regarding the potential misuse of LLMs to manipulate or mislead users.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.14531v1](https://arxiv.org/abs/2402.14531v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14531v1](https://browse.arxiv.org/html/2402.14531v1)       |
| Truncated       | False       |
| Word Count       | 9592       |