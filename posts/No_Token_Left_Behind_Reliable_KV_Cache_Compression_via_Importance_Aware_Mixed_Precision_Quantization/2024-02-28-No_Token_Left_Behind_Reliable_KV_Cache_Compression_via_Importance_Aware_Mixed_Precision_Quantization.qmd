
---
title: "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization"
id: "2402.18096v1"
description: "KV caching accelerates Large Language Models, but eviction can harm generation quality. MiKV compresses effectively."
author: June Yong Yang, Byeongwook Kim, Jeongin Bae, Beomseok Kwon, Gunho Park, Eunho Yang, Se Jung Kwon, Dongsoo Lee
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18096v1/x1.png"
categories: ['architectures', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18096v1/x1.png)

### Summary:
- The article discusses the challenges posed by the memory footprint of Key-Value (KV) caching in Large Language Models (LLMs) and proposes a reliable cache compression method, Mixed-precision KV cache (MiKV), to address these challenges.
- Recent methods for KV cache eviction to reduce memory consumption have been proposed, but the potential risks and impact on the generative process have not been thoroughly examined.
- The proposed MiKV method retains evicted KV pairs in low precision and important KV pairs in high precision, effectively balancing compression ratio and performance.

### Major Findings:
1. The detrimental impact of cache eviction on the generative process, leading to safety breaches, hallucinations, and context loss.
2. Preserving even a small amount of information contained in the evicted KV pairs via reduced precision quantization substantially recovers the incurred degradation.
3. MiKV offers a state-of-the-art trade-off between compression ratio and performance compared to other baselines.

### Analysis and Critique:
- The article provides a comprehensive analysis of the risks associated with KV cache eviction and proposes a novel method, MiKV, to address these risks.
- The experiments conducted demonstrate the effectiveness of MiKV in preserving generation quality while achieving a high compression rate.
- The proposed method addresses the limitations of existing cache compression strategies and provides a promising solution for the challenges posed by the memory footprint of KV caching in LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.18096v1](https://arxiv.org/abs/2402.18096v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18096v1](https://browse.arxiv.org/html/2402.18096v1)       |
| Truncated       | False       |
| Word Count       | 7020       |