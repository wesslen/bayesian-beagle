
---
title: "InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration"
id: "2402.11441v1"
description: "LLMs struggle with knowledge tasks. InfuserKI framework efficiently integrates new knowledge without forgetting."
author: Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, Haifeng Chen
date: "2024-02-18"
image: "../../img/2402.11441v1/image_1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11441v1/image_1.png)

### **Summary:**
The academic article discusses the development of a novel framework called InfuserKI, which aims to integrate unknown knowledge from knowledge graphs into large language models (LLMs) without affecting known knowledge. The framework utilizes an infuser-guided approach to selectively add new information to LLMs, minimizing the impact on prior knowledge and preventing catastrophic forgetting. The article presents evaluations on two domain knowledge graphs, UMLS and MetaQA, demonstrating the effectiveness of InfuserKI in integrating knowledge with less forgetting, sustained performance with large-scale data, and exceptional generality on unseen templates and downstream tasks.

### Major Findings:
1. InfuserKI outperforms existing model editing (ME) baselines, such as CALINET and T-Patcher, by effectively integrating new knowledge without affecting known knowledge.
2. Compared to parameter-efficient fine-tuning (PEFT) methods, such as LoRA and QLoRA, InfuserKI achieves superior locality and generality, demonstrating its effectiveness in minimizing knowledge forgetting and maintaining robust performance on unseen templates and downstream tasks.

### Analysis and Critique:
- The article provides a comprehensive and well-structured framework for integrating knowledge into large language models, addressing the critical issue of catastrophic forgetting.
- The evaluations on UMLS and MetaQA demonstrate the effectiveness of InfuserKI in minimizing knowledge forgetting and maintaining robust performance on large-scale data and unseen templates.
- However, the article could benefit from a more detailed discussion of potential limitations and future research directions for the InfuserKI framework. Additionally, a critical analysis of the methodological approach and potential biases would enhance the overall quality of the article.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.11441v1](https://arxiv.org/abs/2402.11441v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11441v1](https://browse.arxiv.org/html/2402.11441v1)       |
| Truncated       | False       |
| Word Count       | 14173       |