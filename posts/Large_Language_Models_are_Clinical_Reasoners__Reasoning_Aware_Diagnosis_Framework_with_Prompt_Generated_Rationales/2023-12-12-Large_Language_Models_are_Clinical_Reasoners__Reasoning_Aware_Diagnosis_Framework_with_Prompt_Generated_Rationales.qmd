
---
title: "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales"
description: "Proposing a time-efficient framework for clinical reasoning in disease diagnosis using prompt-based learning and machine-generated rationales."
author: "['Taeyoon Kwon', 'Kai Tzu-iunn Ong', 'Dongjin Kang', 'Seungjun Moon', 'Jeong Ryong Lee', 'Dosik Hwang', 'Yongsik Sim', 'Beomseok Sohn', 'Dongha Lee', 'Jinyoung Yeo']"
date: "2023-12-12"
image: "https://browse.arxiv.org/html/2312.07399v1/x1.png"
categories: ['prompt engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.07399v1/x1.png)

### Findings 

1. Large Language Models (LLMs) demonstrate the capability for clinical reasoning in disease diagnosis via prompt-based learning, resulting in better performance in disease diagnosis through extensive experiments and analyses.

2. Reasoning-aware diagnosis framework has shown potential in data-scarce scenarios, with multimodal student models consistently outperforming vision-only and vision-language baseline models even with limited training data.

3. Ethical considerations were highlighted, and potential societal impacts such as data bias, accountability, and legal challenges were acknowledged before applying the method to real clinical settings.

### Methodology
- Reasoning in clinical reasoning, also known as clinical reasoning or diagnostic reasoning, is a dynamic thinking process between the observed clinical evidence and the identification of disease.
- Machine reasoning has been exploited in the framework for reasoning-aware diagnosis. The diagnosis, based on the patient description and the rationale, is formulated as chain-of-thought reasoning, specifically Clinical Chain-of-Thought (Clinical CoT).

### Framework Overview
- **Module I: Clinical Rationalization**
  - Generating clinical CoT rationales by prompting a LLM to rationalize the presented clinical data.
- **Module II-1: Few-shot CoT Reasoning**
  - Investigating the success of LLMs in clinical reasoning with few-shot disease diagnosis.
- **Module II-2: Unimodal-Student Distillation**
  - Distilling the knowledge of diagnostic reasoning from the LLM into smaller language models for real clinical settings.
- **Module II-3: Multimodal-Student Distillation**
  - Extending knowledge distillation in clinical diagnosis to vision-language models.

### Experiments
- Extensive evaluation and analysis of generated rationales demonstrate the potential of LLMs and distilled models to replicate the reasoning of clinical professionals in a human-like manner.
- Human evaluations and analysis of generated rationales establish the potential of utilizing LLMs to model clinical reasoning in disease diagnosis.

### Critique
- Length restriction in prompt-based learning might affect models' performance in rationale generation and diagnosis.
- Lack of exploration of paradigms, such as jointly predicting the rationale generation and diagnosis via multi-task learning or dividing them into separate stages.
- No incorporation of the framework into real-world clinical settings.

The paper introduces an innovative approach emerging from the findings and leverages the capabilities to demonstrate the significance of using prompt-based learning. The emphasis on real-world applications and ethical considerations denotes a strong foundation for future research. However, the study's potential limitations and lack of integration into clinical settings must be addressed for practical use.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-02       |
| HTML     |        |
| Truncated       | False       |
| Word Count       | 10273       |