
---
title: "Arithmetic Reasoning with LLM: Prolog Generation & Permutation"
id: "2405.17893v1"
description: "LLMs generate Prolog programs for math problems, outperforming CoT and improving robustness via data augmentation."
author: Xiaocheng Yang, Bingsen Chen, Yik-Cheung Tam
date: "2024-05-28"
image: "https://browse.arxiv.org/html/2405.17893v1/extracted/5625446/latex/graphs/overview.drawio.pdf.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.17893v1/extracted/5625446/latex/graphs/overview.drawio.pdf.png)

### Summary:

The research investigates the use of large language models (LLMs) to generate Prolog programs for solving mathematical questions. The authors hypothesize that LLMs should focus on extracting predicates and generating symbolic formulas from the math problem description, allowing the underlying calculation to be done via an external code interpreter. The study introduces the GSM8K-Prolog dataset, which contains arithmetic reasoning problems and their corresponding Prolog code solutions. Experimental results show that Prolog code generation outperforms CoT generation in the GSM8K benchmark across three distinct LLMs. The authors also propose predicate permutation as a data augmentation method to improve LLM training.

### Major Findings:

1. Prolog code generation is consistently better than CoT on the arithmetic reasoning task, indicating that LLMs can focus on predicate extractions and rely on an external tool to calculate and perform logical induction to address mathematical problems.
2. The non-sequential nature of predicates in Prolog code allows for predicate permutation as a data augmentation method, which has been demonstrated to be effective in robust LLM training.
3. The GSM8K-Prolog dataset, curated and open-sourced by the authors, contains high-quality arithmetic reasoning problems and their corresponding Prolog code solutions.

### Analysis and Critique:

The study presents an innovative approach to solving arithmetic reasoning problems using LLMs and Prolog code generation. The authors' hypothesis that LLMs should focus on extracting predicates and generating symbolic formulas is well-supported by their experimental results. However, the research could be further strengthened by addressing the following points:

1. The study does not provide a detailed comparison of the performance of different LLMs in generating Prolog code. A more comprehensive analysis of the strengths and weaknesses of each model would be beneficial.
2. The authors mention that the non-sequential nature of predicates in Prolog code allows for predicate permutation as a data augmentation method. However, they do not discuss the potential limitations or drawbacks of this approach, such as the increased complexity of the generated code or the risk of introducing errors.
3. The study could benefit from a more in-depth analysis of the GSM8K-Prolog dataset

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.17893v1](https://arxiv.org/abs/2405.17893v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.17893v1](https://browse.arxiv.org/html/2405.17893v1)       |
| Truncated       | False       |
| Word Count       | 4556       |