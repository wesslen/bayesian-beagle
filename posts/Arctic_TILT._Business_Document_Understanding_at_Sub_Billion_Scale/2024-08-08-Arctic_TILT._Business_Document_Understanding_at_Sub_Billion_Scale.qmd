
---
title: "Arctic-TILT. Business Document Understanding at Sub-Billion Scale"
id: "2408.04632v1"
description: "Arctic-TILT: Small, accurate LLM for document QA, reducing costs and improving efficiency."
author: Łukasz Borchmann, Michał Pietruszka, Wojciech Jaśkowski, Dawid Jurkiewicz, Piotr Halama, Paweł Józiak, Łukasz Garncarek, Paweł Liskowski, Karolina Szyndler, Andrzej Gretkowski, Julita Ołtusek, Gabriela Nowakowska, Artur Zawłocki, Łukasz Duhr, Paweł Dyda, Michał Turski
date: "2024-08-08"
image: "https://browse.arxiv.org/html/2408.04632v1/x2.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.04632v1/x2.png)

### Summary:

The Arctic-TILT model is introduced to address the limitations of the TILT model in handling multi-modal input, suboptimal training procedure, and maximum context length. The model establishes state-of-the-art performance on seven benchmarks demanding text, vision, and layout comprehension. It demonstrates that within the industrial applications setting and while keeping the parameter count below 1B, one could achieve performance better or comparable to vastly larger LLMs and LVLMs. The model presents a novel modality fusion mechanism inspired by tensor product representations and shows how effectively apply it across the transformer encoder. The model also demonstrates how, with well-designed attention sparsity patterns and numerous other optimizations, consume extensive input sequences during training and inference, given a single cost-efficient GPU, while maintaining competitive accuracy of the model.

### Major Findings:

1. The Arctic-TILT model establishes state-of-the-art performance on seven benchmarks demanding text, vision, and layout comprehension.
2. The model demonstrates that within the industrial applications setting and while keeping the parameter count below 1B, one could achieve performance better or comparable to vastly larger LLMs and LVLMs.
3. The model presents a novel modality fusion mechanism inspired by tensor product representations and shows how effectively apply it across the transformer encoder.
4. The model demonstrates how, with well-designed attention sparsity patterns and numerous other optimizations, consume extensive input sequences during training and inference, given a single cost-efficient GPU, while maintaining competitive accuracy of the model.

### Analysis and Critique:

The Arctic-TILT model is a significant improvement over the TILT model in handling multi-modal input, suboptimal training procedure, and maximum context length. The model's performance on seven benchmarks demanding text, vision, and layout comprehension is impressive, and it demonstrates that smaller models can achieve performance better or comparable to vastly larger LLMs and LVLMs. The novel modality fusion mechanism inspired by tensor product representations is a significant contribution to the field. However, the model's performance on other benchmarks and real-world applications needs to be evaluated to establish its generalizability. Additionally, the model's computational efficiency and memory usage

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-13       |
| Abstract | [https://arxiv.org/abs/2408.04632v1](https://arxiv.org/abs/2408.04632v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.04632v1](https://browse.arxiv.org/html/2408.04632v1)       |
| Truncated       | False       |
| Word Count       | 7255       |