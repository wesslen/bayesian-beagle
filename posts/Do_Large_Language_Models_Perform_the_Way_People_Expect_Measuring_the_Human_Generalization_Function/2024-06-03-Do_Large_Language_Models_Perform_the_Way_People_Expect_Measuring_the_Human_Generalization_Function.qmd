
---
title: "Do Large Language Models Perform the Way People Expect? Measuring the Human Generalization Function"
id: "2406.01382v1"
description: "LLMs' diverse uses make evaluation challenging. Misalignment with human generalization can lead to poor performance, especially in high-stakes scenarios."
author: Keyon Vafa, Ashesh Rambachan, Sendhil Mullainathan
date: "2024-06-03"
image: "https://browse.arxiv.org/html/2406.01382v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.01382v1/x1.png)

### Summary:

The paper introduces a framework for evaluating large language models (LLMs) based on human generalization. The authors propose that humans form beliefs about LLM capabilities through interaction and evolve via a human generalization function. They collect a dataset of 19K examples of human generalizations across 79 tasks from the MMLU and BIG-Bench benchmarks. The results show that the human generalization function can be predicted using NLP methods, and people have consistent structured ways to generalize. However, more capable models (e.g., GPT-4) can perform worse on the instances people choose to use them for, especially when the cost of mistakes is high, due to misalignment with the human generalization function.

### Major Findings:

1. The human generalization function can be predicted using NLP methods, and people have consistent structured ways to generalize.
2. More capable models (e.g., GPT-4) can perform worse on the instances people choose to use them for, especially when the cost of mistakes is high, due to misalignment with the human generalization function.
3. The human generalization function is sparse, with most pairs of questions not resulting in people updating their beliefs.

### Analysis and Critique:

The paper presents an interesting and novel approach to evaluating LLMs based on human generalization. The authors' findings highlight the importance of considering human beliefs and generalization when deploying LLMs in real-world applications. However, the paper has some limitations. For instance, the authors do not discuss the potential impact of cultural or individual differences on the human generalization function. Additionally, the paper does not address the potential biases in the human generalization function, which could be influenced by factors such as prior experiences or preconceptions about LLMs.

Another potential issue is the reliance on a single dataset for collecting human generalizations. While the authors use a large and diverse dataset, it is possible that the results may not generalize to other datasets or contexts. Furthermore, the paper does not discuss the potential implications of the findings for the design and development of LLMs. For example, it would be interesting to explore how LLMs could be designed to better align with the human generalization function.

In conclusion, the paper presents a valuable contribution to the field of LLM

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.01382v1](https://arxiv.org/abs/2406.01382v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.01382v1](https://browse.arxiv.org/html/2406.01382v1)       |
| Truncated       | False       |
| Word Count       | 10525       |