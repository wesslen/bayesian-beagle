
---
title: "Generative Debunking of Climate Misinformation"
id: "2407.05599v1"
description: "LLMs can automatically debunk climate myths using the truth sandwich structure, with GPT-4 and Mixtral showing promising results."
author: Francisco Zanartu, Yulia Otmakhova, John Cook, Lea Frermann
date: "2024-07-08"
image: "https://browse.arxiv.org/html/2407.05599v1/x1.png"
categories: ['social-sciences', 'education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.05599v1/x1.png)

# Summary:

The study presents an approach called generative debunking, which combines generative AI with past research on climate contrarian claim classification and fallacy detection. The goal is to automatically detect and correct climate misinformation at scale. The authors build upon the CARDS classifier and the FLICC model to develop a system that produces structured and psychologically grounded "truth sandwich" debunkings. The system is tested with three unique combinations of prompting strategies and large language models (LLMs) of different sizes. The results reveal promising performance of GPT-4 and Mixtral when combined with structured prompts. However, the study also identifies specific challenges, such as a lack of factuality and relevancy, even with the latest LLMs.

# Major Findings:

1. The generative debunking approach adopts elements of the 4D framework, which involves detecting, deconstructing, debunking, and deploying corrective interventions.
2. The study combines open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with prompting strategies of varying complexity to produce structured and psychologically grounded "truth sandwich" debunkings.
3. Experiments reveal promising performance of GPT-4 and Mixtral when combined with structured prompts.

# Analysis and Critique:

1. The study identifies specific challenges of debunking generation and human evaluation, such as a lack of factuality and relevancy, even with the latest LLMs.
2. The authors acknowledge that their system is not currently fit for broader deployment and that a more thorough evaluation is needed in future work.
3. The study does not systematically study the impact of individual prompt design decisions, nor does it exhaustively combine all prompts with all LLMs.
4. The authors did not evaluate their current models' abilities to distinguish input myths from fact, which is outside the scope of this study.
5. The study was supported by the Melbourne Center of AI and Digital Ethics and the Australian Research Council Discovery Early Career Research Award.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.05599v1](https://arxiv.org/abs/2407.05599v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.05599v1](https://browse.arxiv.org/html/2407.05599v1)       |
| Truncated       | False       |
| Word Count       | 6823       |