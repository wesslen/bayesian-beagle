
---
title: "GeoEval: Benchmark for Evaluating LLMs and Multi-Modal Models on Geometry Problem-Solving"
id: "2402.10104v1"
description: "Advancements in LLMs and MMs for geometry problems, WizardMath model excels, GPT-series rephrasing effective."
author: Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, Yashar Moshfeghi
date: "2024-02-15"
image: "../../img/2402.10104v1/image_1.png"
categories: ['hci', 'education', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.10104v1/image_1.png)

### Summary:
- The GeoEval benchmark evaluates the performance of Large Language Models (LLMs) and Multi-Modal Models (MMs) in solving geometry math problems.
- The benchmark includes four subsets: GeoEval-2000, GeoEval-backward, GeoEval-aug, and GeoEval-hard, each designed to facilitate a thorough evaluation.
- The evaluation of ten cutting-edge L

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.10104v1](https://arxiv.org/abs/2402.10104v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.10104v1](https://browse.arxiv.org/html/2402.10104v1)       |
| Truncated       | False       |
| Word Count       | 15987       |