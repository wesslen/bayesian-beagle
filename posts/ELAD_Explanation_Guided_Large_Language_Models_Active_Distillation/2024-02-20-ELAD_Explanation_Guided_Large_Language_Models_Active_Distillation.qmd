
---
title: "ELAD: Explanation-Guided Large Language Models Active Distillation"
id: "2402.13098v1"
description: "TL;DR: ELAD framework improves LLM distillation efficiency with active learning and sample selection."
author: Yifei Zhang, Bo Pan, Chen Ling, Yuntong Hu, Liang Zhao
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13098v1/extracted/5420568/framework.png"
categories: ['education', 'production', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13098v1/extracted/5420568/framework.png)

### **Summary:**
- The ELAD framework proposes an active learning strategy to optimize the balance between annotation costs and model performance.
- It introduces an explanation-guided sample selection method to identify challenging samples for reasoning and a customized LLM-annotated explanation revision technique to correct flaws in the student model's reasoning.
- Experiments across various reasoning datasets demonstrate that the framework significantly enhances the efficiency of LLM knowledge distillation.

### Major Findings:
1. The ELAD framework significantly enhances active learning through the use of LLM explanations.
2. An explanation-guided sample selection method identifies challenging samples for reasoning by exploiting explanation stepwise uncertainties.
3. A customized LLM-annotated explanation revision technique allows LLM to guide the pinpointing and correction of inaccuracies in the reasoning steps of small models.

### Analysis and Critique:
- The ELAD framework demonstrates significant improvements in annotating efficiency and reasoning performance across various datasets.
- The proposed explanation-guided sample selection method and customized LLM-annotated explanation revision technique outperform traditional sample selection and completion generation methods.
- The study acknowledges potential limitations related to prompt design and data privacy concerns when using third-party services via APIs. Additionally, the study did not utilize the most recently released GPT-4.0 as the teacher model in the experiments.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-21       |
| Abstract | [https://arxiv.org/abs/2402.13098v1](https://arxiv.org/abs/2402.13098v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13098v1](https://browse.arxiv.org/html/2402.13098v1)       |
| Truncated       | False       |
| Word Count       | 6889       |