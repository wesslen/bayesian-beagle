
---
title: "Large Language Models Assume People are More Rational than We Really are"
id: "2406.17055v1"
description: "LLMs incorrectly assume humans are more rational, aligning with expected value theory, but match human expectations of rational behavior."
author: Ryan Liu, Jiayi Geng, Joshua C. Peterson, Ilia Sucholutsky, Thomas L. Griffiths
date: "2024-06-24"
image: "https://browse.arxiv.org/html/2406.17055v1/extracted/5688645/figures/main_fig.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17055v1/extracted/5688645/figures/main_fig.png)

### Summary:

- The study examines the implicit decision-making models of Large Language Models (LLMs) by comparing their behavior and predictions to a large dataset of human decisions.
- The findings reveal that LLMs assume people are more rational than they actually are, aligning more closely with a classic model of rational choice—expected value theory.
- Interestingly, people also tend to assume that others are rational when interpreting their behavior, leading to a high correlation between the inferences that LLMs and people draw from the decisions of others.
- The study uses two experimental paradigms from psychology: a risky choice task and an inference task, to assess the implicit assumptions that LLMs make about human decision-making.
- The results show that LLMs model people as highly rational decision-makers, with their predictions and simulations of human choices being more rational than actual human behavior.
- The inverse modeling paradigm also reveals that the inferences that LLMs make from people's choices are consistent with the assumption that humans are rational actors.

### Major Findings:

1. LLMs assume people are more rational than they actually are, aligning more closely with a classic model of rational choice—expected value theory.
2. People also tend to assume that others are rational when interpreting their behavior, leading to a high correlation between the inferences that LLMs and people draw from the decisions of others.
3. LLMs model people as highly rational decision-makers, with their predictions and simulations of human choices being more rational than actual human behavior.

### Analysis and Critique:

- The study provides valuable insights into the implicit decision-making models of LLMs and their alignment with human behavior.
- However, the reliance on human judgments to study these implicit decision-making models may be problematic, as existing psychology literature has shown that people's own perceptions of others may be more rational than they actually are.
- The study also highlights the potential for LLMs to develop mistaken impressions of how humans actually behave, as training data such as blog posts, news articles, and books often go through rounds of editing that remove logical fallacies and other mistakes.
- The findings suggest that LLMs may not be accurate at simulating or predicting human behavior, but their assumption that people are more rational than we really are aligns with the assumption that

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17055v1](https://arxiv.org/abs/2406.17055v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17055v1](https://browse.arxiv.org/html/2406.17055v1)       |
| Truncated       | False       |
| Word Count       | 9964       |