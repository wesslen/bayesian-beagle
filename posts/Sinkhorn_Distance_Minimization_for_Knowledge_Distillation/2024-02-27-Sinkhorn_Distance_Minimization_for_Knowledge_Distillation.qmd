
---
title: "Sinkhorn Distance Minimization for Knowledge Distillation"
id: "2402.17110v1"
description: "KD compresses LLMs. Existing methods have limitations. SinKD uses Sinkhorn distance for effective supervision. Superior to state-of-the-art."
author: Xiao Cui, Yulei Qin, Yuting Gao, Enwei Zhang, Zihan Xu, Tong Wu, Ke Li, Xing Sun, Wengang Zhou, Houqiang Li
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17110v1/extracted/5431483/distributions.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17110v1/extracted/5431483/distributions.png)

### **Summary:**
- Knowledge distillation (KD) is used to compress large language models (LLMs) by transferring knowledge from a teacher model to a smaller student model.
- Existing KD methods, such as KL, RKL, and JS divergences, have limitations in effectively supervising when there is little distribution overlap between the teacher and student.
- The proposed Sinkhorn Knowledge Distillation (SinKD) addresses the limitations of existing divergences and introduces a batch-wise reformulation to capture geometric intricacies of distributions across samples in the high-dimensional space.
- SinKD outperforms state-of-the-art methods on various LLM architectures in terms of comparability, validity, and generalizability.

### Major Findings:
1. Existing KD methods, such as KL, RKL, and JS divergences, suffer from issues of mode-averaging, mode-collapsing, and mode-underestimation, respectively, which deteriorates logits-based KD for diverse NLP tasks.
2. The proposed SinKD exploits the Sinkhorn distance to ensure a nuanced and precise assessment of the disparity between teacher and student distributions, and the batch-wise reformulation captures geometric intricacies of distributions across samples in the high-dimensional space.
3. SinKD showcases robustness across model choices and outperforms state-of-the-art methods on all kinds of LLMs with encoder-only, encoder-decoder, and decoder-only architectures.

### Analysis and Critique:
- The article provides a comprehensive evaluation of the proposed SinKD and compares it with existing KD methods, demonstrating its superiority.
- The study includes an extensive analysis of hyperparameters, demonstrating the impact of different parameters on the performance of SinKD.
- The proposed SinKD has the potential to be applied beyond the field of NLP and could facilitate the compression of larger vision and language models for smaller, cost-efficient ones.

Overall, the article provides valuable insights into the limitations of existing KD methods and introduces a novel approach, SinKD, which shows promising results in knowledge distillation for large language models. The study's thorough analysis and potential for broader impact make it a significant contribution to the field of knowledge distillation.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.17110v1](https://arxiv.org/abs/2402.17110v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17110v1](https://browse.arxiv.org/html/2402.17110v1)       |
| Truncated       | False       |
| Word Count       | 7505       |