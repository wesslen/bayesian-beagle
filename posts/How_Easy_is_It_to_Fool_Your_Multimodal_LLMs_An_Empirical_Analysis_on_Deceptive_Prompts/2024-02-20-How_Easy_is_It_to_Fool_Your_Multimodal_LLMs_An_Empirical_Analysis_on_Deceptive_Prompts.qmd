
---
title: "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts"
id: "2402.13220v1"
description: "MAD-Bench tests MLLMs' vulnerability to deceptive prompts, showing GPT-4V outperforms other models. Proposed remedy improves accuracy."
author: Yusu Qian, Haotian Zhang, Yinfei Yang, Zhe Gan
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13220v1/x1.png"
categories: ['robustness', 'architectures', 'security', 'production', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13220v1/x1.png)

### Summary:
- The article presents MAD-Bench, a benchmark to evaluate the resilience of Multimodal Large Language Models (MLLMs) against deceptive prompts.
- The study finds that MLLMs, including GPT-4V, are vulnerable to deceptive prompts, with significant performance gaps observed between different models.
- A simple remedy is proposed to boost model performance by adding an additional paragraph to the deceptive prompts, resulting in improved accuracy.

### Major Findings:
1. MLLMs, including GPT-4V, exhibit vulnerability to deceptive prompts, with significant performance gaps observed between different models.
2. The proposed remedy of adding an additional paragraph to the deceptive prompts results in improved model accuracy, particularly for LLaVA-1.5 and GPT-4V.
3. GPT-4V demonstrates superior performance across all metrics compared to other models, but still exhibits substantial room for improvement.

### Analysis and Critique:
- The study provides valuable insights into the vulnerability of MLLMs to deceptive prompts, highlighting the need for further research to enhance model resilience.
- The proposed remedy to boost model performance is effective, but the absolute accuracy of the models is still too low to be satisfactory.
- The study acknowledges the limitations of the benchmark in capturing all potential scenarios where MLLMs can be deceived, emphasizing the need for ongoing research in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.13220v1](https://arxiv.org/abs/2402.13220v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13220v1](https://browse.arxiv.org/html/2402.13220v1)       |
| Truncated       | False       |
| Word Count       | 6498       |