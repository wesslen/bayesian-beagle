
---
title: "Language Models Do Hard Arithmetic Tasks Easily and Hardly Do Easy Arithmetic Tasks"
id: "2406.02356v1"
description: "LLMs can predict the first digit of multiplication tasks but struggle with the last. Providing higher-order digits improves performance."
author: Andrew Gambardella, Yusuke Iwasawa, Yutaka Matsuo
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02356v1/extracted/5643500/1_592_392_first_digit_unconditional/Llama-2-7b-hf_592392_token_dist_position_1.png"
categories: ['architectures', 'robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02356v1/extracted/5643500/1_592_392_first_digit_unconditional/Llama-2-7b-hf_592392_token_dist_position_1.png)

### Summary:

This paper explores the ability of large language models (LLMs) to perform arithmetic tasks, specifically focusing on their performance in -digit by -digit multiplication. The authors find that LLMs can confidently and correctly predict the first digit of the multiplication result, despite this task requiring compounding operations to solve. However, LLMs often fail to correctly or confidently predict the last digit of the multiplication, a task equivalent to -digit by -digit multiplication which can be easily learned or memorized. The authors show that the latter task can be solved more robustly when the LLM is conditioned on all of the correct higher-order digits.

### Major Findings:

1. LLMs can confidently and correctly predict the first digit of -digit by -digit multiplication tasks, despite these tasks requiring compounding operations to solve.
2. LLMs often fail to correctly or confidently predict the last digit of -digit by -digit multiplication, a task equivalent to -digit by -digit multiplication which can be easily learned or memorized.
3. The confidence of LLMs in predicting the last digit can be increased by conditioning the generation of the last digit on the correct intervening digits, despite the computation of the last digit not depending on the correct computations of the higher-order digits at all.

### Analysis and Critique:

The paper provides an interesting exploration of the capabilities and limitations of LLMs in performing arithmetic tasks. However, the study is limited to a few specific models (Llama 2 and Mistral) and does not provide a broader analysis of other LLMs. Additionally, the paper does not discuss the potential implications of these findings for the development and application of LLMs in various fields. Further research is needed to understand the generalizability of these findings and their potential impact on the use of LLMs in arithmetic tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02356v1](https://arxiv.org/abs/2406.02356v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02356v1](https://browse.arxiv.org/html/2406.02356v1)       |
| Truncated       | False       |
| Word Count       | 3722       |