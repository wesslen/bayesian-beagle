
---
title: "Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts"
id: "2402.19150v1"
description: "LMMs' vulnerability to typographic attacks: a study on typographic distractibility and mitigation methods."
author: Hao Cheng, Erjia Xiao, Renjing Xu
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19150v1/x1.png"
categories: ['security', 'architectures', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19150v1/x1.png)

### **Summary:**

- Large Multimodal Models (LMMs) incorporate pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) to perform various multimodal tasks in the joint space of vision and language.
- A security vulnerability to LMMs is the Typographic Attack, which disrupts VLMs.
- This study investigates the distractibility of LMMs by typography and proposes a prompt information enhancement method to mitigate the effects of typography.

### **Major Findings:**

1. LMMs can partially distinguish visual contents and typos when confronted with typographic attacks, indicating that embeddings from vision encoders contain enough information to distinguish visual contents and typos in images.
2. By providing more informative texts to match images, CLIP's performance of zero-shot classification on typo-ridden images can be

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19150v1](https://arxiv.org/abs/2402.19150v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19150v1](https://browse.arxiv.org/html/2402.19150v1)       |
| Truncated       | False       |
| Word Count       | 6409       |