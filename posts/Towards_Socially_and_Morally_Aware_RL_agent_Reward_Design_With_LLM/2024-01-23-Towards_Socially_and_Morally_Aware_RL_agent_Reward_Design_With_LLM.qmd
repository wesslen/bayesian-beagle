
---
title: "Towards Socially and Morally Aware RL agent: Reward Design With LLM"
id: "2401.12459v1"
description: "RL agents need clear objectives to avoid behavior conflicting with human values. Language models may help assess and guide agent behavior."
author: ['Zhaoyue Wang']
date: "2024-01-23"
image: "https://browse.arxiv.org/html/2401.12459v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.12459v1/x1.png)

**Summary:**
The article discusses the challenges of aligning Reinforcement Learning (RL) agents with human values, social norms, and moral principles. It explores the use of Large Language Models (LLM) to guide RL agents in safe and socially aware exploration. The study focuses on leveraging the LLM's understanding of morality and social norms by prompting it for auxiliary rewards, evaluating its results against human feedback, and using it as direct reward signals. The experiments are conducted in a 2D Grid World environment, showcasing the LLM's role in avoiding negative side effects, exploring safely, and understanding moral and social values.

### Major Findings:
1. **Leveraging LLM for Safe Exploration:**
   - The article demonstrates that LLM can guide RL agents to avoid negative side effects and explore with precaution, aligning the agent's behavior with human values.

2. **Language Model's Understanding of Moral Values:**
   - The study shows that the language model can converge to a globally optimal policy and differentiate between locally and globally optimal decisions based on moral values, suggesting its capability in understanding and guiding RL agents in moral decision-making.

3. **Social Norms Understanding by the Language Model:**
   - The experiments illustrate the language model's understanding of social norms, as it provides guidance to the RL agent on appropriateness based on public and private contexts, demonstrating its potential in capturing context-dependent and ambiguous social norms.

### Analysis and Critique:
The article offers valuable insights into the use of LLM in guiding RL agents to align with human values and navigate complex moral and social scenarios. However, it has several limitations:

1. **Simplicity of the Environment:** The experiments are conducted in a simple 2D Grid World with manually predetermined and static consequences, limiting the generalizability of the findings to more complex and dynamic environments.

2. **Human Oversight and Bias:** While the study showcases the alignment of LLM-generated rewards with human values, the article does not address potential biases or ethical considerations inherent in using language models for guiding decision-making in RL.

3. **Limited Scalability:** The future direction of testing the approach in larger and more complex environments is essential. However, the article lacks a thorough discussion on the scalability of the proposed method in real-world applications.

4. **Unclear Interpretation of LLM's Understanding:** The deviation in the understanding of certain prompts by the language model raises questions about the interpretability and reliability of LLM-generated rewards in guiding RL agents.

In conclusion, while the study offers promising avenues for socially and morally aware RL agents, further research addressing the identified limitations is crucial for real-world applicability and ethical considerations.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [http://arxiv.org/abs/2401.12459v1](http://arxiv.org/abs/2401.12459v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.12459v1](https://browse.arxiv.org/html/2401.12459v1)       |
| Truncated       | False       |
| Word Count       | 5148       |