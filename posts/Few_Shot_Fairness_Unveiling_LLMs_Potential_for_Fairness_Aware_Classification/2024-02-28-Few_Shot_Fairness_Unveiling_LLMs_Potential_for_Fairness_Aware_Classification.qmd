
---
title: "Few-Shot Fairness: Unveiling LLM's Potential for Fairness-Aware Classification"
id: "2402.18502v1"
description: "TL;DR: Using LLMs for fairness in AI, GPT-4 shows superior accuracy and fairness."
author: Garima Chhikara, Anurag Sharma, Kripabandhu Ghosh, Abhijnan Chakraborty
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18502v1/x1.png"
categories: ['social-sciences', 'production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18502v1/x1.png)

### **Summary:**
- The study explores the potential of Large Language Models (LLMs) for achieving fairness in classification tasks through in-context learning.
- Experiments conducted with different LLMs indicate that GPT-4 delivers superior results in terms of both accuracy and fairness compared to other models.
- The study introduces a framework outlining fairness regulations aligned with various fairness definitions, with each definition being modulated by varying degrees of abstraction.

### **Major Findings:**
1. GPT-4 demonstrates improvements in both accuracy and F1-score for fairness rules  and .
2. LLaMA-2 experiences a decline in accuracy when subjected to fairness rules  and , but demonstrates positive outcomes in certain fairness metrics.
3. Gemini exhibits subpar performance in the zero-shot setup, displaying decrease in both performance and fairness metrics.

### **Analysis and Critique:**
- The study is limited by selection bias, as it utilizes a dataset specific to the United States, and existing evidence indicates that LLMs exhibit bias towards English-speaking countries.
- The study focuses solely on one demographic, namely gender, and could benefit from a more comprehensive study incorporating additional demographics and a larger dataset.
- Conducting experiments with paid Large Language Models such as GPT-4 and LLaMA-2 through the Replicate API has incurred a significant financial cost, contributing to an increase in carbon emissions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.18502v1](https://arxiv.org/abs/2402.18502v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18502v1](https://browse.arxiv.org/html/2402.18502v1)       |
| Truncated       | False       |
| Word Count       | 8973       |