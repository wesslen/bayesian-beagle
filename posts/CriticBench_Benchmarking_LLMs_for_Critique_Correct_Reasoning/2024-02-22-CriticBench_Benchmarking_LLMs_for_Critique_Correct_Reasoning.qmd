
---
title: "CriticBench: Benchmarking LLMs for Critique-Correct Reasoning"
id: "2402.14809v1"
description: "CriticBench evaluates LLMs' critique and correction reasoning across tasks, revealing key performance factors."
author: Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, Yujiu Yang
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14809v1/x3.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14809v1/x3.png)

### Summary:
- The paper introduces CriticBench, a benchmark designed to evaluate the critique and correction skills of Large Language Models (LLMs) across various reasoning tasks.
- CriticBench encompasses five reasoning domains: mathematical, commonsense, symbolic, coding, and algorithmic, compiling 15 datasets and incorporating responses from three LLM families.
- The findings reveal a linear relationship in critique-focused training, task-dependent variation in critique and correction effectiveness, and knowledge inconsistencies that decrease as model size increases.
- The paper also discusses the impact of base models, training strategies, prompt strategies, and oracle feedback on the critique-correct reasoning performance of LLMs.

### Major Findings:
1. Linear relationship in critique-focused training enhances performance.
2. Task-dependent variation in critique and correction effectiveness.
3. Knowledge inconsistencies decrease as model size increases.

### Analysis and Critique:
- The paper provides valuable insights into the critique-correct reasoning of LLMs, but it has limitations and potential biases that need to be addressed.
- The evaluation of critique ability remains a challenge, and there is a need for alternative evaluation methodologies to mitigate reliance on costly human annotations.
- The paper acknowledges the potential risks involved in using the critique ability of LLMs, such as potential biases, and emphasizes the need for careful discernment of the discriminate results.

The detailed results and analysis in the paper provide a comprehensive understanding of the critique-correct reasoning abilities of LLMs, but future work should address the identified challenges and potential biases.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.14809v1](https://arxiv.org/abs/2402.14809v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14809v1](https://browse.arxiv.org/html/2402.14809v1)       |
| Truncated       | False       |
| Word Count       | 6198       |