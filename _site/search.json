[
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "This paper presents novel prompting techniques to improve the performance of automatic summarization systems for scientific articles. The authors feed summarizers with lists of key terms extracted from articles, such as author keywords or automatically generated keywords, to guide summarization systems. The techniques are tested with various summarization models and input texts, showing performance gains, especially for smaller models summarizing sections separately.\n\n\n\n\nAutomatic text summarization aims to produce shortened versions of documents while retaining the most relevant information.\nScientific articles are challenging to summarize due to their length, complexity, and irregular organizational structures.\n\n\n\n\n\nEarlier approaches to automatic summarization relied on extractive methods, while current state-of-the-art systems are based on abstractive summarization models such as transformer architectures.\nPrior work includes techniques like planning with learned entity prompts and faceted summarization to guide summarization systems.\n\n\n\n\n\nPrompting Technique Dimension: The paper evaluates five distinct prompting techniques for generating prompts to provide scientific summarizers with useful contextual information.\nModel Dimension: The authors study the effects of prompting techniques integrated with a range of current state-of-the-art transformer models for scientific summarization.\nInput Text Dimension: The study explores three main conditions for input texts to understand when contextual information from prompts provides significant gains.\n\n\n\n\n\nThe authors use a dataset of open-access biomedical papers from PubMed Central for training and evaluation.\nThe experiments demonstrate consistent performance improvements from prompting techniques on smaller models, especially when summarizing sections independently.\nTargeted confusion testing is conducted to isolate the benefits of prompting.\n\n\n\n\nThe findings suggest that prompting is an effective approach to overcoming the limitations of smaller, less capable summarization systems. The study also underscores the potential of prompting as a promising technique to assist smaller models, particularly when computational resources are constrained.\n\n\n\nThe paper proposes novel prompting techniques to provide key term context and enhance scientific literature summarizers, presenting promising results for aiding smaller summarization models, particularly in the context of section-level summarization.\nThis summary is authored by Aldan Creo, Manuel Lama, and Juan C. Vidal from the University of Santiago de Compostela.\n\n\nLink: https://browse.arxiv.org/html/2312.08282v2  Truncated: True  Word Count: 46040"
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#abstract",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#abstract",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "This paper presents novel prompting techniques to improve the performance of automatic summarization systems for scientific articles. The authors feed summarizers with lists of key terms extracted from articles, such as author keywords or automatically generated keywords, to guide summarization systems. The techniques are tested with various summarization models and input texts, showing performance gains, especially for smaller models summarizing sections separately."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#introduction",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#introduction",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "Automatic text summarization aims to produce shortened versions of documents while retaining the most relevant information.\nScientific articles are challenging to summarize due to their length, complexity, and irregular organizational structures."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#related-work",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#related-work",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "Earlier approaches to automatic summarization relied on extractive methods, while current state-of-the-art systems are based on abstractive summarization models such as transformer architectures.\nPrior work includes techniques like planning with learned entity prompts and faceted summarization to guide summarization systems."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#methods",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#methods",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "Prompting Technique Dimension: The paper evaluates five distinct prompting techniques for generating prompts to provide scientific summarizers with useful contextual information.\nModel Dimension: The authors study the effects of prompting techniques integrated with a range of current state-of-the-art transformer models for scientific summarization.\nInput Text Dimension: The study explores three main conditions for input texts to understand when contextual information from prompts provides significant gains."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#results",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#results",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The authors use a dataset of open-access biomedical papers from PubMed Central for training and evaluation.\nThe experiments demonstrate consistent performance improvements from prompting techniques on smaller models, especially when summarizing sections independently.\nTargeted confusion testing is conducted to isolate the benefits of prompting."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#discussion",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#discussion",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The findings suggest that prompting is an effective approach to overcoming the limitations of smaller, less capable summarization systems. The study also underscores the potential of prompting as a promising technique to assist smaller models, particularly when computational resources are constrained."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#conclusion",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#conclusion",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The paper proposes novel prompting techniques to provide key term context and enhance scientific literature summarizers, presenting promising results for aiding smaller summarization models, particularly in the context of section-level summarization.\nThis summary is authored by Aldan Creo, Manuel Lama, and Juan C. Vidal from the University of Santiago de Compostela.\n\n\nLink: https://browse.arxiv.org/html/2312.08282v2  Truncated: True  Word Count: 46040"
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "Data has become a crucial asset in various industries, emphasizing the need for privacy and security. Federated learning is a popular approach for training machine learning models collaboratively while preserving data privacy. However, federated learning is susceptible to various security threats, including data injection attacks, where malicious participants inject false data into the training process to manipulate the global model.\n\n\n\n\n\n\nFederated learning involves multiple agents refining model parameters using their private data to minimize an objective function using a gradient descent approach.\nThe agents exchange local model parameters with a coordinating node, and the goal is to minimize the objective function using a gradient descent approach.\n\n\n\n\n\nMalicious agents inject false data into the training process to manipulate the global model, leading to suboptimal models.\nDifferent attack schemes include label-flipping attacks, constant output attacks, and randomized attacks.\n\n\n\n\n\nThe coordinating agent uses a metric to compare updates received from edge agents and identifies malicious agents. A detection metric is proposed, and a low-complexity metric is computed over time to localize the attacker. If an agent is suspected to be malicious, its updates are ignored for a certain period. The proposed detection method allows for continuous operation, even during the convergence time of the joint model.\n\n\n\n\n\n\nThe simulation shows the impact of a constant-output attack by a single attacker on various network sizes with and without detection.\nThe proposed detection scheme successfully detects the attacker before it affects the network, allowing convergence of the model to a good model.\n\n\n\n\n\nThe simulation illustrates the impact of a label-flip attack by a single attacker with and without detection.\nWith the proposed detection, the attacker is identified, and the average classification error is mitigated.\n\n\n\n\n\nThe paper presents a robust federated learning algorithm that can operate in the presence of data injection attacks. It provides conditions for identifying malicious agents and demonstrates the performance of the proposed technique on various data injection attacks.\n\nThe paper addresses the significant challenge of mitigating data injection attacks on federated learning systems. It proposes a novel technique for detecting and mitigating such attacks, showcasing its effectiveness through simulations. The proposed detection and mitigation methods offer a promising approach to safeguard federated learning systems from malicious activities. The paper concludes by emphasizing the robustness and performance of the proposed technique in addressing data injection attacks.\n\n\nLink: https://browse.arxiv.org/html/2312.02102v2  Truncated: False  Word Count: 13622"
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#introduction",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#introduction",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "Data has become a crucial asset in various industries, emphasizing the need for privacy and security. Federated learning is a popular approach for training machine learning models collaboratively while preserving data privacy. However, federated learning is susceptible to various security threats, including data injection attacks, where malicious participants inject false data into the training process to manipulate the global model."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#problem-formulation",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#problem-formulation",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "Federated learning involves multiple agents refining model parameters using their private data to minimize an objective function using a gradient descent approach.\nThe agents exchange local model parameters with a coordinating node, and the goal is to minimize the objective function using a gradient descent approach.\n\n\n\n\n\nMalicious agents inject false data into the training process to manipulate the global model, leading to suboptimal models.\nDifferent attack schemes include label-flipping attacks, constant output attacks, and randomized attacks."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#attacker-detection-and-avoidance",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#attacker-detection-and-avoidance",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The coordinating agent uses a metric to compare updates received from edge agents and identifies malicious agents. A detection metric is proposed, and a low-complexity metric is computed over time to localize the attacker. If an agent is suspected to be malicious, its updates are ignored for a certain period. The proposed detection method allows for continuous operation, even during the convergence time of the joint model."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#simulations",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#simulations",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The simulation shows the impact of a constant-output attack by a single attacker on various network sizes with and without detection.\nThe proposed detection scheme successfully detects the attacker before it affects the network, allowing convergence of the model to a good model.\n\n\n\n\n\nThe simulation illustrates the impact of a label-flip attack by a single attacker with and without detection.\nWith the proposed detection, the attacker is identified, and the average classification error is mitigated."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#conclusions",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#conclusions",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The paper presents a robust federated learning algorithm that can operate in the presence of data injection attacks. It provides conditions for identifying malicious agents and demonstrates the performance of the proposed technique on various data injection attacks.\n\nThe paper addresses the significant challenge of mitigating data injection attacks on federated learning systems. It proposes a novel technique for detecting and mitigating such attacks, showcasing its effectiveness through simulations. The proposed detection and mitigation methods offer a promising approach to safeguard federated learning systems from malicious activities. The paper concludes by emphasizing the robustness and performance of the proposed technique in addressing data injection attacks.\n\n\nLink: https://browse.arxiv.org/html/2312.02102v2  Truncated: False  Word Count: 13622"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ryan Wesslen",
    "section": "",
    "text": "I’m a machine learning engineer at Explosion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Wesslen",
    "section": "",
    "text": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n\n\nsecurity\n\n\nopen-source\n\n\n\n\n\n\n\ngpt-3.5-turbo-1106\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrompting LLMs with content plans to enhance the summarization of scientific articles\n\n\n\nprompt engineering\n\n\n\n\n\n\n\ngpt-3.5-turbo-1106\n\n\nDec 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMitigating Data Injection Attacks on Federated Learning\n\n\n\nsecurity\n\n\n\n\n\n\n\ngpt-3.5-turbo-1106\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Autoregressive Large Language Models (LLMs) are widely used but require safety training for human alignment to prevent nefarious uses.\nIt’s possible to bypass safety alignment and obtain harmful outputs from open-source LLMs through optimization-free attacks called priming attacks.\n\n\n\n\n\n\n\nUsed a non-safety-trained LLM to generate priming attacks for harmful behaviors based on a new prompt format.\nShowed that priming with slightly more prompt-dependent content can improve the attack success rate by up to 3.3× compared to baseline attacks.\n\n\n\n\n\nUsed the pre-trained Llama-2 model for few-shot prompting with specific prompts and affirmative initial responses.\nEvaluated the attack success rate using the SOTA response classification tool Llama Guard.\n\n\n\n\n\nPriming attacks outperformed baselines for all models, indicating the fragility of safety measures.\nManual evaluation indicated that Llama Guard might underestimate harmfulness.\n\n\n\n\n\n\nHighlighted the fragility of current LLM safety measures under increasingly practical assumptions.\nSuggested the need for further research into novel methods for safer open-sourcing.\n\n\n\n\n\nThe paper provides a list of references for further exploration into the topic.\n\nFor more detailed information, refer to the original text.\n\n\nLink: https://browse.arxiv.org/html/2312.12321v1  Truncated: False  Word Count: 10319"
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#introduction",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#introduction",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Autoregressive Large Language Models (LLMs) are widely used but require safety training for human alignment to prevent nefarious uses.\nIt’s possible to bypass safety alignment and obtain harmful outputs from open-source LLMs through optimization-free attacks called priming attacks."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#methodology-results",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#methodology-results",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Used a non-safety-trained LLM to generate priming attacks for harmful behaviors based on a new prompt format.\nShowed that priming with slightly more prompt-dependent content can improve the attack success rate by up to 3.3× compared to baseline attacks.\n\n\n\n\n\nUsed the pre-trained Llama-2 model for few-shot prompting with specific prompts and affirmative initial responses.\nEvaluated the attack success rate using the SOTA response classification tool Llama Guard.\n\n\n\n\n\nPriming attacks outperformed baselines for all models, indicating the fragility of safety measures.\nManual evaluation indicated that Llama Guard might underestimate harmfulness."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#conclusion",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#conclusion",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "Highlighted the fragility of current LLM safety measures under increasingly practical assumptions.\nSuggested the need for further research into novel methods for safer open-sourcing."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#references",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#references",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "The paper provides a list of references for further exploration into the topic.\n\nFor more detailed information, refer to the original text.\n\n\nLink: https://browse.arxiv.org/html/2312.12321v1  Truncated: False  Word Count: 10319"
  }
]