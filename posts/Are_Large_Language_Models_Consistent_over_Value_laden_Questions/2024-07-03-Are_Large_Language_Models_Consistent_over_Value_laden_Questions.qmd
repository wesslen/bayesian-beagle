
---
title: "Are Large Language Models Consistent over Value-laden Questions?"
id: "2407.02996v1"
description: "LLMs show consistency across paraphrases, use-cases, and translations, but inconsistencies remain, especially on controversial topics."
author: Jared Moore, Tanvi Deshpande, Diyi Yang
date: "2024-07-03"
image: "https://browse.arxiv.org/html/2407.02996v1/x1.png"
categories: ['robustness', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.02996v1/x1.png)

### Summary:

This study investigates the consistency of large language models (LLMs) in expressing values across various settings. The authors define value consistency as the similarity of answers across paraphrases, related questions, multiple-choice and open-ended use-cases, and multilingual translations. They apply these measures to several large LLMs, including llama-3 and gpt-4o, using 8,000 questions spanning more than 300 topics.

The study finds that models are relatively consistent across paraphrases, use-cases, translations, and within a topic. However, some inconsistencies remain, with models being more consistent on uncontroversial topics than on controversial ones. Base models are more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics than others.

### Major Findings:

1. LLMs are relatively consistent across paraphrases, use-cases, translations, and within a topic.
2. Models are more consistent on uncontroversial topics than on controversial ones.
3. Base models are more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics than others.

### Analysis and Critique:

The study provides a comprehensive analysis of the consistency of LLMs in expressing values. However, it is important to note that the study only focuses on a few large LLMs and a specific set of questions. The results may not generalize to other models or different types of questions.

Moreover, the study does not address the potential impact of the training data on the consistency of the models. The training data could significantly influence the expressed values and the degree of consistency of the models. Future research could explore this aspect in more detail.

The lack of Schwartz steerability found in the study does not necessarily mean that models do not encode values. It could be that the models encode values in a different way than what was measured in the study.

The inconsistencies found in the study could drive biases in LLMs, such as the failure of safety fine-tuning to generalize across different situations. These inconsistencies could also serve

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02996v1](https://arxiv.org/abs/2407.02996v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02996v1](https://browse.arxiv.org/html/2407.02996v1)       |
| Truncated       | False       |
| Word Count       | 11041       |