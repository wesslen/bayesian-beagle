
---
title: "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark"
id: "2402.04788v1"
description: "MLLMs show potential in human-like discernment but face challenges and biases."
author: Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, Lichao Sun
date: "2024-02-07"
image: "../../img/2402.04788v1/image_1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.04788v1/image_1.png)

### Summary:
- The article introduces a novel benchmark, MLLM-AS-A-JUDGE, to assess the ability of Multimodal Large Language Models (MLLMs) in assisting judges. It discusses the challenges MLLMs face in judgment, including diverse biases, hallucinatory responses, and inconsistencies, even for advanced models such as GPT-4V. The study reveals that MLLMs demonstrate remarkable human-like discernment in Pair Comparisons, but there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks. The section also discusses the experiments conducted on human agreement to address situations that traditional metrics may not capture adequately. It highlights the performance of MLLMs in scoring evaluation, pair comparison, and batch ranking tasks, as well as their consistency in decision-making. The section also presents the analysis of the "Analyze-then-Judge" setting, focusing on image-instruction pairs and their impact on mitigating hallucinations. The authors present two high-quality subsets of data, one with human preference scores of 5 and the other with instances of hallucinations. The section also provides a comprehensive overview of the capabilities and limitations of Large Language Models (LLMs) as evaluators in Natural Language Processing (NLP) tasks. It discusses the rapid development of LLMs, their use as evaluators, and advancements in their abilities, such as Chain-of-Thought reasoning and training-free instruction following. The section also discusses the use of various AI models as judges in assessing the quality of responses to user instructions, along with the results of their performance and additional experimental tests. Finally, it presents a scenario where different AI assistants provide responses to user instructions, which are then evaluated based on their adherence to the user's original instruction and how well they address the user's inquiry.

### Major Findings:
1. MLLMs demonstrate remarkable human-like discernment in Pair Comparisons.
2. There is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking tasks.
3. The performance of different AI models as judges in assessing the quality of responses to user instructions varies significantly.

### Analysis and Critique:
- The findings emphasize the pressing need for enhancements and further research efforts regarding MLLMs as fully reliable evaluators, particularly in tasks that require human-like judgment and decision-making.
- The study provides valuable insights into the performance and limitations of MLLMs in judging tasks, highlighting the need for improvements in consistency and mitigation of biases and hallucinations.
- The content is significant as it provides a comprehensive understanding of the capabilities and limitations of LLMs and MLLMs as evaluators, emphasizing the need for a more holistic approach to evaluation.
- The results of the experiments shed light on the effectiveness and limitations of AI models in evaluating responses to user instructions, highlighting the importance of human agreement bias checking and the impact of response length distribution on the judgments made by the AI models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.04788v1](https://arxiv.org/abs/2402.04788v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.04788v1](https://browse.arxiv.org/html/2402.04788v1)       |
| Truncated       | True       |
| Word Count       | 24799       |