
---
title: "Effectiveness of ChatGPT in explaining complex medical reports to patients"
id: "2406.15963v1"
description: "ChatGPT struggles to accurately explain complex cancer reports to patients, facing issues like inaccuracies, language, personalization, and distrust."
author: Mengxuan Sun, Ehud Reiter, Anne E Kiltie, George Ramsay, Lisa Duncan, Peter Murchie, Rosalind Adam
date: "2024-06-23"
image: "https://browse.arxiv.org/html/2406.15963v1/extracted/5680786/MDT.png"
categories: ['hci', 'social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.15963v1/extracted/5680786/MDT.png)

# Summary:

The study explores the potential of ChatGPT (GPT 4) in explaining complex medical reports, specifically colorectal and prostate cancer MDT reports, to patients. The research aims to address two main questions: the challenges of using ChatGPT for this purpose and how to enhance its effectiveness. The study involved creating six mock MDT reports, prompting ChatGPT to respond to questions about the MDT, and evaluating the responses through pilot studies, annotations, and focus groups.

## Major Findings:

1. **Inaccurate Information**: ChatGPT's explanations contained errors, including incorrect interpretation of abbreviations, URLs, and test results.
2. **Inappropriate Language**: The language used by ChatGPT was sometimes too complex, grammatically incorrect, or used American English, which is inappropriate in the UK.
3. **Limited Personalization**: The responses were not always tailored to the patient, and the content was often too vague or technical.
4. **AI Distrust**: Patients and doctors expressed reluctance to trust ChatGPT responses unless they were checked, preferably by clinicians. Some patients did not want to use them at all.
5. **Integration Challenges**: Integrating ChatGPT into existing clinical workflows, including getting approval from the NHS, poses significant challenges.

## Analysis and Critique:

The study highlights the potential of ChatGPT in assisting with complex medical reports but also underscores the need for improvements. The issues identified, such as inaccurate information, inappropriate language, limited personalization, and AI distrust, need to be addressed before LLMs can be effectively used to explain complex personal medical information to patients. The study also points out the challenges of integrating LLMs into clinical workflow and the need for more research on what patients and doctors need from such tools.

The study's limitations include the small sample size for annotations and the lack of comprehensive data on focus group participants, which may have introduced bias. The use of only the webpage version of ChatGPT4 also limits the applicability of the findings to other LLMs.

Ethical considerations were addressed, with two ethical approvals obtained and all experiments conducted with the informed consent of the participants.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.15963v1](https://arxiv.org/abs/2406.15963v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.15963v1](https://browse.arxiv.org/html/2406.15963v1)       |
| Truncated       | False       |
| Word Count       | 7576       |