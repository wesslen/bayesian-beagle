
---
title: "HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation"
id: "2406.07070v1"
description: "HalluDial: A Comprehensive Benchmark for Automatic Dialogue-Level Hallucination Evaluation in LLMs."
author: Wen Luo, Tianshu Shen, Wei Li, Guangyue Peng, Richeng Xuan, Houfeng Wang, Xi Yang
date: "2024-06-11"
image: "../../https://browse.arxiv.org/html/2406.07070v1/extracted/5658624/img/hallu_intro.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../https://browse.arxiv.org/html/2406.07070v1/extracted/5658624/img/hallu_intro.png)

# Summary:

The paper introduces HalluDial, a large-scale benchmark for automatic dialogue-level hallucination evaluation in Large Language Models (LLMs). The benchmark includes 4,094 dialogues with a total of 146,856 samples, covering both spontaneous and induced hallucination scenarios, and addressing factuality and faithfulness hallucinations. The authors conduct a comprehensive meta-evaluation of LLMs' hallucination evaluation capabilities in information-seeking dialogues and introduce a specialized judge language model, HalluJudge. The high data quality of HalluDial enables HalluJudge to achieve superior or competitive performance in hallucination evaluation, facilitating the automatic assessment of dialogue-level hallucinations in LLMs.

# Major Findings:

1. The paper proposes HalluDial, the first large-scale dialogue-level hallucination benchmark, addressing the limitations of existing benchmarks.
2. The authors conduct a comprehensive meta-evaluation of LLMs' capabilities in hallucination evaluations and develop a hallucination judge language model named HalluJudge, which demonstrates superior or competitive capacity in HalluDial and other generalization settings.
3. The authors utilize HalluDial and HalluJudge to conduct an automatic evaluation of dialogue-level hallucination present in current LLMs.

# Analysis and Critique:

1. The paper successfully addresses the limitations of existing hallucination benchmarks by providing a large-scale, diverse dataset that covers both spontaneous and induced hallucination scenarios, as well as factuality and faithfulness hallucinations.
2. The introduction of HalluJudge, a specialized judge language model, is a significant contribution to the field, as it enables the automatic assessment of dialogue-level hallucinations in LLMs.
3. However, the paper does not discuss potential biases or limitations in the data generation process, which could impact the generalizability of the results. Additionally, the evaluation of HalluJudge's performance in other generalization settings is not extensively discussed.
4. The paper could benefit from a more detailed analysis of the implications of the findings for the development and deployment of LLMs in real-world applications.
5. The paper does not discuss the potential impact of the proposed

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.07070v1](https://arxiv.org/abs/2406.07070v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07070v1](https://browse.arxiv.org/html/2406.07070v1)       |
| Truncated       | False       |
| Word Count       | 10462       |