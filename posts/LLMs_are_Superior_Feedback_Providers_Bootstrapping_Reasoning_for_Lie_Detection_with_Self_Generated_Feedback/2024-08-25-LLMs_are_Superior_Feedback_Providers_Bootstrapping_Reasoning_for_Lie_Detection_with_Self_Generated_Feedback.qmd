
---
title: "LLMs are Superior Feedback Providers: Bootstrapping Reasoning for Lie Detection with Self-Generated Feedback"
id: "2408.13915v1"
description: "LLMs improve lie detection in games with self-generated feedback, rivaling supervised learning results."
author: Tanushree Banerjee, Richard Zhu, Runzhe Yang, Karthik Narasimhan
date: "2024-08-25"
image: "https://browse.arxiv.org/html/2408.13915v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.13915v1/x1.png)

### Summary:

The paper proposes a bootstrapping framework that leverages self-generated feedback to enhance the reasoning capabilities of large language models (LLMs) for lie detection. The framework consists of three stages: suggestion, feedback collection, and modification. In the suggestion stage, a cost-effective language model generates initial predictions based on game state and dialogue. The feedback-collection stage involves a language model providing feedback on these predictions. In the modification stage, a more advanced language model refines the initial predictions using the auto-generated feedback.

The proposed framework is applied to detect betrayal and deception in Diplomacy games and compared with feedback from professional human players. The LLM-generated feedback exhibits superior quality and significantly enhances the performance of the model. The approach achieves a 39% improvement over the zero-shot baseline in lying-F1 without the need for any training data, rivaling state-of-the-art supervised learning results.

### Major Findings:

1. The proposed bootstrapping framework effectively enhances the reasoning capabilities of LLMs for lie detection in Diplomacy games.
2. LLM-generated feedback exhibits superior quality and significantly improves the performance of the model, outperforming the zero-shot baseline and rivaling state-of-the-art supervised learning results.
3. The framework achieves a 39% improvement over the zero-shot baseline in lying-F1 without the need for any training data.

### Analysis and Critique:

The paper presents an innovative approach to improving the reasoning capabilities of LLMs for lie detection in Diplomacy games. The proposed bootstrapping framework leverages self-generated feedback to refine initial predictions, resulting in superior performance compared to the zero-shot baseline and rivaling state-of-the-art supervised learning results.

However, the paper does not discuss the potential limitations or biases of the proposed framework. For instance, the reliance on self-generated feedback may introduce biases or errors if the initial predictions are inaccurate or incomplete. Additionally, the paper does not address the potential impact of the quality and diversity of the feedback on the performance of the model.

Furthermore, the paper does not provide a comprehensive comparison with other approaches for lie detection in Diplomacy games. While the proposed framework outperforms the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.13915v1](https://arxiv.org/abs/2408.13915v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.13915v1](https://browse.arxiv.org/html/2408.13915v1)       |
| Truncated       | False       |
| Word Count       | 12835       |