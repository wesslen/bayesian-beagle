{"id": "2312.12321v1", "text": "# Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n## 1 Introduction\n\n- Autoregressive Large Language Models (LLMs) are widely used but require safety training for human alignment to prevent nefarious uses.\n- It's possible to bypass safety alignment and obtain harmful outputs from open-source LLMs through optimization-free attacks called **priming attacks**.\n\n## 2 Methodology & Results\n\n### A. Few-Shot Priming Attacks\n- Used a non-safety-trained LLM to generate priming attacks for harmful behaviors based on a new prompt format.\n- Showed that priming with slightly more prompt-dependent content can improve the attack success rate by up to 3.3\u00d7 compared to baseline attacks.\n\n### B. Experimental Setup\n- Used the pre-trained Llama-2 model for few-shot prompting with specific prompts and affirmative initial responses.\n- Evaluated the attack success rate using the SOTA response classification tool Llama Guard.\n\n### Results\n- Priming attacks outperformed baselines for all models, indicating the fragility of safety measures.\n- Manual evaluation indicated that Llama Guard might underestimate harmfulness.\n\n## 3 Conclusion\n\n- Highlighted the fragility of current LLM safety measures under increasingly practical assumptions.\n- Suggested the need for further research into novel methods for safer open-sourcing.\n\n## References\n- The paper provides a list of references for further exploration into the topic.\n\nFor more detailed information, refer to the original text.", "meta": {"url": "https://browse.arxiv.org/html/2312.12321v1", "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks", "subtitle": "LLMs vulnerable to simple attacks bypassing safety training, improving harmful behavior detection by up to 3.3x.", "categories": ["security", "open-source"], "publish_date": "2023-12-19", "model": "gpt-3.5-turbo-1106", "word_count": 10319, "is_truncated": false}}
{"id": "2312.02102v2", "text": "# Summary of \"Mitigating Data Injection Attacks on Federated Learning\"\n\n## Introduction\n\nData has become a crucial asset in various industries, emphasizing the need for privacy and security. Federated learning is a popular approach for training machine learning models collaboratively while preserving data privacy. However, federated learning is susceptible to various security threats, including data injection attacks, where malicious participants inject false data into the training process to manipulate the global model.\n\n## Problem Formulation\n\n### Federated Learning\n- Federated learning involves multiple agents refining model parameters using their private data to minimize an objective function using a gradient descent approach.\n- The agents exchange local model parameters with a coordinating node, and the goal is to minimize the objective function using a gradient descent approach.\n\n### Data Injection Attacks\n- Malicious agents inject false data into the training process to manipulate the global model, leading to suboptimal models.\n- Different attack schemes include label-flipping attacks, constant output attacks, and randomized attacks.\n\n## Attacker Detection and Avoidance\n\nThe coordinating agent uses a metric to compare updates received from edge agents and identifies malicious agents. A detection metric is proposed, and a low-complexity metric is computed over time to localize the attacker. If an agent is suspected to be malicious, its updates are ignored for a certain period. The proposed detection method allows for continuous operation, even during the convergence time of the joint model.\n\n## Simulations\n\n### Example 1: Constant-Output Attack\n- The simulation shows the impact of a constant-output attack by a single attacker on various network sizes with and without detection.\n- The proposed detection scheme successfully detects the attacker before it affects the network, allowing convergence of the model to a good model.\n\n### Example 2: Label-Flip Attack\n- The simulation illustrates the impact of a label-flip attack by a single attacker with and without detection.\n- With the proposed detection, the attacker is identified, and the average classification error is mitigated.\n\n## Conclusions\n\nThe paper presents a robust federated learning algorithm that can operate in the presence of data injection attacks. It provides conditions for identifying malicious agents and demonstrates the performance of the proposed technique on various data injection attacks.\n\n---\n\nThe paper addresses the significant challenge of mitigating data injection attacks on federated learning systems. It proposes a novel technique for detecting and mitigating such attacks, showcasing its effectiveness through simulations. The proposed detection and mitigation methods offer a promising approach to safeguard federated learning systems from malicious activities. The paper concludes by emphasizing the robustness and performance of the proposed technique in addressing data injection attacks.", "meta": {"url": "https://browse.arxiv.org/html/2312.02102v2", "title": "Mitigating Data Injection Attacks on Federated Learning", "subtitle": "TL;DR: Proposed method detects and mitigates false data injection attacks in federated learning systems.", "categories": ["security"], "publish_date": "2023-12-04", "model": "gpt-3.5-turbo-1106", "word_count": 13622, "is_truncated": false}}
{"id": "2312.08282v2", "text": "# Prompting LLMs with Content Plans to Enhance the Summarization of Scientific Articles\n\n## Abstract\nThis paper presents **novel prompting techniques** to improve the performance of automatic summarization systems for scientific articles. The authors feed summarizers with lists of key terms extracted from articles, such as author keywords or automatically generated keywords, to guide summarization systems. The techniques are tested with various summarization models and input texts, showing performance gains, especially for smaller models summarizing sections separately.\n\n## Introduction\n- **Automatic text summarization** aims to produce shortened versions of documents while retaining the most relevant information.\n- **Scientific articles are challenging** to summarize due to their length, complexity, and irregular organizational structures.\n\n## Related Work\n- Earlier approaches to automatic summarization relied on **extractive methods**, while current state-of-the-art systems are based on **abstractive summarization models** such as transformer architectures.\n- Prior work includes techniques like **planning with learned entity prompts** and **faceted summarization** to guide summarization systems.\n\n## Methods\n1. **Prompting Technique Dimension**: The paper evaluates five distinct prompting techniques for generating prompts to provide scientific summarizers with useful contextual information.\n2. **Model Dimension**: The authors study the effects of prompting techniques integrated with a range of current state-of-the-art transformer models for scientific summarization.\n3. **Input Text Dimension**: The study explores three main conditions for input texts to understand when contextual information from prompts provides significant gains.\n\n## Results\n- The authors use a dataset of open-access biomedical papers from PubMed Central for training and evaluation.\n- The experiments demonstrate consistent performance improvements from prompting techniques on smaller models, especially when summarizing sections independently.\n- Targeted confusion testing is conducted to isolate the benefits of prompting.\n\n## Discussion\nThe findings suggest that **prompting is an effective approach** to overcoming the limitations of smaller, less capable summarization systems. The study also underscores the potential of prompting as a promising technique to assist smaller models, particularly when computational resources are constrained.\n\n## Conclusion\nThe paper proposes **novel prompting techniques** to provide key term context and enhance scientific literature summarizers, presenting promising results for aiding smaller summarization models, particularly in the context of section-level summarization.\n\nThis summary is authored by Aldan Creo, Manuel Lama, and Juan C. Vidal from the University of Santiago de Compostela.", "meta": {"url": "https://browse.arxiv.org/html/2312.08282v2", "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles", "subtitle": "New prompting techniques improve scientific article summarization, especially for smaller models summarizing sections separately.", "categories": ["prompt engineering"], "publish_date": "2023-12-13", "model": "gpt-3.5-turbo-1106", "word_count": 46040, "is_truncated": true}}
{"id": "2312.15523v1", "text": "# The Persuasive Power of Large Language Models\n\n## Introduction\n\nThe study focusses on **Large Language Models (LLMs)** and their ability to act as persuasive agents in social dynamics, specifically in the context of opinion change. It raises critical questions regarding the potential of these LLMs to influence public opinion and interact with each other to simulate human-like persuasion dynamics.\n\n## Methods\n\nThe experimental design involved setting up a synthetic persuasion dialogue scenario on climate change, where a \"convincer\" LLM generated persuasive arguments for a \"skeptic\" LLM. The study explored different persuasion strategies and levels of skeptics' stubbornness, while human judges evaluated the persuasiveness of machine-generated arguments.\n\n## Results\n\n- The study found that arguments containing **factual knowledge, trust, status, and support** were most effective in changing the skeptic's opinion, as perceived by both LLMs and human judges.\n- Trust and support were particularly effective in altering the skeptic's viewpoint, with a decrease in persuasive power correlating with increased skepticism in the skeptic's responses.\n\n## Discussion\n\nThe findings indicate that LLMs are capable of emulating some dynamics of persuasion and opinion change similar to human social systems. However, discrepancies were noted in how humans and LLMs perceive the effectiveness of different persuasive dimensions.\n\n## Key Findings\n\n1. LLMs can effectively mimic dynamics of persuasion and opinion change observed in human discourse.\n2. Arguments rich in factual knowledge, trust, status, and support were perceived as persuasive by both LLMs and human judges.\n\n## Limitations and Future Work\n\n- The study's limitations include the simplified setup and the need for further research on multi-turn conversations, diverse agent profiles, and the development of effective system prompts.\n- Future research should explore the capabilities of individual agents, optimal prompting strategies, and mechanisms inducing LLMs to signal a change of opinion.\n\n## Ethical Considerations\n\nThe deployment of AI agents for persuasion and social interactions necessitates the consideration of potential risks and ethical challenges, such as the use of LLMs for misinformation and the trade-offs between benefits and power consumption.\n\nThe study provides valuable insights into the persuasive capabilities of LLMs and highlights the importance of further research to understand and mitigate the risks associated with using these models for persuasion in online social contexts.", "meta": {"url": "https://browse.arxiv.org/html/2312.15523v1", "title": "The Persuasive Power of Large Language Models", "subtitle": "Large Language Models can generate effective arguments and interact to reproduce human social dynamics in opinion dynamics research.", "categories": ["hci"], "publish_date": "2023-12-24", "model": "gpt-3.5-turbo-1106", "word_count": 12372, "is_truncated": false}}
{"id": "2312.14949v1", "text": "# Introduction\n\nWith the advent of Large Language Models (LLMs) like GPT-3, there is growing interest in their potential to optimize source code. This paper focuses on methodologically stringent case studies applied to well-known open source python libraries pillow and numpy.\n\n## Aims\n- The paper aims to explore the use of LLMs for code optimization in collaboration with humans. This includes assessing the potential improvements and whether human expertise is necessary in the process.\n\n## Why Optimize Source Code?\n- **Energy Consumption**: Code optimization can potentially reduce energy consumption, which is significant given the escalating energy costs of computation.\n- **Sub-Optimal Source Code Quality**: Inefficient code hampers performance and contributes to escalating energy consumption.\n- **Benefits of Code Optimization**: Optimized code can reduce energy consumption, operational costs, and offers better user experience and system reliability.\n\n## Prior Art\n- **LLMs for Code Optimization**: Limited attention has been given to the use of LLMs for code optimization.\n- **Collaborative use of LLMs for coding tasks**: This paper is the first to explore the use of LLMs for code optimization in collaboration with humans.\n- **Evaluation of the Usefulness of LLMs for Coding Tasks**: Previous literature has focused on functional correctness and quantitative evaluation of performance improvement on coding tasks.\n\n## Objectives and Scope of the Paper\nThe specific aims include exploring the use of LLMs for code optimization in collaboration with an expert, evaluating the magnitude of improvement, assessing the necessity of human expert involvement, and deriving generalizable insights for collaborative code optimization.\n\n# Methods\nThe paper details the methodology employed in the collaborative interaction between an expert and a Large Language Model (LLM) for code optimization.\n\n## The Expert and the Machine\n- The expert, Andreas Florath, and ChatGPT-4 were used for all case studies.\n- A custom-configured version of GPT was utilized in November 2023 to assess reproducibility and consistency of results obtained using the standard version of ChatGPT-4.\n\n## Selection of Source Code Locus\n- Open source Python libraries pillow and numpy were chosen as the natural environment due to their wide usage, allowing for direct comparison and easy replication.\n\n## The Collaborative Optimization Process\n- The process involved preparation, starting prompts, iteration, evaluation, termination, and generalization and post-optimization.\n\n## Evaluation of Benefit\nFour types of evaluation were conducted, focusing on quantitative performance improvement, quantitative code size improvement, correctness, and real-world impact through pull request validation.\n\n## Are the Chosen Metrics Good Proxies for Benefit of Collaborative Optimization?\nThe paper acknowledges the need for a larger and more systematic study to evidence the benefits of collaborative code optimization. It also outlines the potential bias and considerations for cost and energy savings.\n\n# Optimization Process\n\n## Original Source Code\nThe original source code under focus is located in the ImageStat module of the python pillow library.\n\n**Note**: Due to the extensive content provided, a more detailed summary with specific quotes from the text to illustrate key points and findings can be further produced upon request.", "meta": {"url": "https://browse.arxiv.org/html/2312.14949v1", "title": "LLM Interactive Optimization of Open Source Python Libraries -- Case Studies and Generalization", "subtitle": "LLMs, like ChatGPT-4, can optimize source code energy efficiency with human help, yielding significant performance improvements.", "categories": ["hci", "programming"], "publish_date": "2023-12-08", "model": "gpt-3.5-turbo-1106", "word_count": 43275, "is_truncated": true}}
{"id": "2312.14345v1", "text": "# Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs\n\n## 1. Introduction\n- Explainable recommender systems are important for transparency and interpretability in AI-driven decision-making processes.\n- Large Language Models (LLMs) have the capability to provide explanations in recommender systems but face challenges in personalization and trustworthiness.\n\n## 2. Characteristics of a Good Explanation\n- Identified five key characteristics for high-quality AI-generated explanations: **Personalization**, **Factuality**, **Robustness**, **Human readability**, and **Proper utterance**.\n- Presented examples of zero-shot and Logic-Scaffolding model explanations to highlight limitations of the zero-shot approach.\n\n## 3. Aspect-Instructed Recommendation Evidence Generation\n- Described the Logic-Scaffolding framework with three main steps: Relevant Item Selection, Aspect Extraction, and Chain-of-Thought Reasoning.\n  - **3.1 Relevant Item Selection**: Utilized a pre-trained sentence transformer model to select influential items related to the recommended item from the user\u2019s history.\n  - **3.2 Aspect Extraction**: Leveraged few-shot learning technique to extract essential aspects associated with each item.\n  - **3.3 Chain-of-Thought Reasoning**: Used chain-of-thought prompting technique to guide the generation of explanations.\n\n## 4. Demonstration of Results\n- Showcased the interactive user interface used in the demonstration to explore the effect of the proposed framework on the quality of explanations.\n- Conducted human evaluation comparing the explanations generated by the zero-shot approach and the Logic-Scaffolding model, highlighting the superior performance of the latter.\n\n### Human Evaluation Results\n- The Logic-Scaffolding model consistently received higher ratings than the zero-shot approach across criteria of relevance, human-readability, factuality, and proper utterance.\n- Effect size test indicated a \"large\" effect in three out of four criteria, with significant improvements in factuality.\n\nOverall, the Logic-Scaffolding framework provides a practical solution to address the limitations of generic LLMs in generating reliable, personalized, and responsible explanations in recommender systems.", "meta": {"url": "https://browse.arxiv.org/html/2312.14345v1", "title": "Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs", "subtitle": "Large Language Models (LLMs) can offer recommendation explanations, but current models struggle. Logic-Scaffolding proposes a solution using intermediate reasoning.", "categories": ["hci", "prompt engineering"], "publish_date": "2023-12-22", "model": "gpt-3.5-turbo-1106", "word_count": 4674, "is_truncated": false}}
{"id": "2312.12924v1", "text": "# Android Dialogue System for Customer Service\n\n## I. Introduction\nThe paper describes a dialogue system developed for the Dialogue Robot Competition 2023, focusing on generative AI and hospitality in customer service.\n\n- Generative AI, such as ChatGPT, has improved the performance of dialogue systems.\n- Hospitality is crucial in customer service, and dialogue systems are expected to respond with **hospitality**.\n- The paper aims to construct a dialogue system with various elements of hospitality service and evaluate users.\n\n## II. Proposed System\nThe proposed system focuses on **topic control**, **compliments generation**, and **user preference extraction**.\n\n- **Controlling Topics with ChatGPT Prompts**: The system achieves topic control for trip planning by inserting text into prompts using ChatGPT.\n- **Dialogue Flow**: The system elicits customer requests, confirms travel plan requirements, and discusses a plan that meets the customer's needs.\n- **Function to Complement a User's Physical Appearance**: The system captures the user's upper body image to recognize appearance characteristics and issue compliments based on estimated attributes.\n- **Control Using User's Past Speech**: The system uses ChatGPT to determine sightseeing spots and create travel plans based on the user's past utterances.\n- **Overall Configuration**: The overall system configuration is explained and shown in Figure 2.\n\n## III. User Evaluation and Preliminary Results\nThe system undergoes user evaluation, which consists of satisfaction evaluation and plan evaluation.\n\n- **Satisfaction Evaluation** includes sufficiency of information, naturalness of dialogue, satisfaction with dialogue, trustworthiness of the robot, among others.\n- **Plan Evaluation** assesses the feasibility of the travel plan created by the system.\n\n## IV. Conclusion\nThe dialogue system, leveraging ChatGPT for topic control and compliment generation, was ranked first in both satisfaction and plan ratings during the preliminary round evaluation, indicating its effectiveness in real customer environments.\n\nThe paper provides insights into the development of an **Android dialogue system** that harnesses the power of **generative AI** and **hospitality** in customer service, with promising preliminary results from real customer evaluations.\n\n---\nThe paper highlights the development of a dialogue system for the Dialogue Robot Competition 2023, emphasizing the utilization of ChatGPT for topic control and compliment generation, as well as the system's effectiveness in real customer evaluations.", "meta": {"url": "https://browse.arxiv.org/html/2312.12924v1", "title": "Android dialogue system for customer service using prompt-based topic control and compliments generation", "subtitle": "Dialogue system for trip planning achieved topic control using ChatGPT-API and user recognition, evaluated positively in preliminary round.", "categories": ["hci", "prompt engineering"], "publish_date": "2023-12-20", "model": "gpt-3.5-turbo-1106", "word_count": 2370, "is_truncated": false}}
{"id": "2312.16018v1", "text": "### Summary of \"RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation\"\n\n#### I. Introduction\nThe paper introduces \"RecRanker\" which uses large language models (LLMs) for top-k recommendations and proposes various strategies for instruction tuning and hybrid ranking to enhance model performance.\n\n#### II. Related Work\nThe section discusses top-k recommendation paradigms, collaborative filtering-based direct recommendations, sequential recommendations, and the integration of LLMs into recommender systems.\n\n#### III. Preliminaries\nIt provides the mathematical definitions for top-k recommendation and the representation of the original LLM in the recommendation methods.\n\n#### IV. Methodology\n- **IV-A Overview**: Describes the training and inference pipeline of RecRanker.\n- **IV-B Adaptive User Sampling**: Discusses the importance-aware sampling, clustering-based sampling, and penalties for repetitive sampling strategies.\n- **IV-C Candidate Items Selection**: Explains negative sampling and retrieval model-based item selection.\n- **IV-D Prompt Construction**: Describes pointwise, pairwise, and listwise ranking, position shifting, and prompt enhancement strategies.\n- **IV-E Optimization via Instruction Tuning**: Details the fine-tuning of LLM using instruction-based approaches.\n- **IV-F Hybrid Ranking**: Introduces a hybrid ranking method by ensembling multiple ranking tasks.\n\n#### V. Experiment\n- **V-A Experimental Setup**: Describes the datasets, evaluation metrics, and research questions being investigated.\n- **V-A1 Dataset**: Introduces the MovieLens-100K, MovieLens-1M, and BookCrossing datasets.\n- **V-A2 Evaluation Metrics**: Specifies the evaluation metrics used for the experiments.\n\nThe paper includes tables presenting dataset descriptions and experimental results, which analyze the performance of RecRanker against baseline models for different recommendation scenarios.\n\nOverall, the paper introduces RecRanker as a framework for instruction-tuning LLMs for top-k recommendations, proposing strategies for user sampling, prompt construction, model tuning, and hybrid ranking, and presents comprehensive experiments to validate the effectiveness of the proposed framework.", "meta": {"url": "https://browse.arxiv.org/html/2312.16018v1", "title": "RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation", "subtitle": "Large language models (LLMs) are enhanced for better recommendation with advanced prompts and instruction tuning methods in the RecRanker system.", "categories": ["prompt engineering"], "publish_date": "2023-12-26", "model": "gpt-3.5-turbo-1106", "word_count": 29329, "is_truncated": true}}
{"id": "2312.14335v1", "text": "# Summary: Context-aware Decoding Reduces Hallucination in Query-focused Summarization\n\n## 1 Introduction\n- Query-focused summarization (QFS) aims to provide a summary of a single or multiple documents to satisfy a given query.\n- The prototypical QFS pipeline consists of a retriever and a generator, where large language models (LLMs) are commonly used as generators.\n- However, using LLMs in QFS may lead to **hallucinations**, which is a growing concern. This has led to an interest in developing new **decoding methods** to improve generation quality and reduce hallucinations.\n\n## 2 Background\n- **Context-aware Decoding** (CAD) leverages the idea of pointwise mutual information to make the generation more conditioned on the input evidence.\n- The computational cost for CAD is assessed for the Transformer architecture.\n\n## 3 Experiments\n- The study involves experiments on QFS and news summarization datasets, employing different language models and evaluation metrics.\n- **Prompting templates** and **hyperparameters** are provided for detailed experiment set-up.\n\n## 4 Results and Analysis\n- Quality of Generation: CAD improves QFS quality and can reduce factual mistakes/hallucinations, albeit with increased computational complexity and reduced speed.\n- Choice of hyperparameter **\u00ce\u00b1**: A trade-off is observed between improved factuality and reduced ROUGE score as \u00ce\u00b1 increases.\n- CAD vs. vanilla decoding: CAD improves ROUGE scores and reduces factuality errors on news summarization datasets, while improving FactKB scores on QFS datasets.\n\n## 5 Related Work\n- Previous work has focused on understanding and addressing the issue of **hallucination** in natural language generation, as well as developing various **decoding methods** to improve generation quality.\n\n## 6 Conclusion and Limitations\n- CAD is shown to improve generation quality, reduce factuality errors, and also slow down decoding speed.\n- The study was limited to language models up to 11B parameters, and larger models may exhibit different performance patterns.\n\n# References, Appendix, and Additional Tables are available in the original text.", "meta": {"url": "https://browse.arxiv.org/html/2312.14335v1", "title": "Context-aware Decoding Reduces Hallucination in Query-focused Summarization", "subtitle": "Query-focused summarization (QFS) is improved with Context-aware Decoding method, reducing hallucinations but affecting speed and complexity.", "categories": ["robustness"], "publish_date": "2023-12-21", "model": "gpt-3.5-turbo-1106", "word_count": 14988, "is_truncated": false}}
{"id": "2312.08189v1", "text": "# Summary of \"GuardRails: Automated Suggestions for Clarifying Ambiguous Purpose Statements\"\n\n## Introduction\nThe paper proposes an automated tool called GuardRails that suggests inputs to clarify ambiguous purpose statements in Python function docstrings. The authors also discuss related work, their heuristic and implementation, a comparison with Copilot Chat, limitations, discussion on usage, and future work. \n\n## Motivating Example\n- Purpose statements in Python functions may be ambiguous and fail to specify the intended behavior for certain inputs.\n- GuardRails proposes a heuristic that suggests inputs using Large Language Models (LLMs) and compares its output with GitHub Copilot\u2019s Chat feature.\n\n## Related Work\n- The authors discuss the need for realistic problem specifications that contain ambiguities and highlight the sensitivity of Large Language Models (LLMs) to the level of detail provided.\n\n## Heuristic and Implementation\n- GuardRails heuristic involves using LLMs to suggest implementations, mutating these suggestions, fuzzing them with various inputs, and comparing pairwise equivalence of suggestions.\n- The implementation is open-sourced and available as a Visual Studio Code extension.\n\n## Comparison with Copilot Chat\n- The study compares the abilities of GuardRails and Copilot Chat to suggest inputs from known ambiguous input classes (AICs) for different function variants.\n- It also evaluates the relative and absolute performance of both tools across different function variants.\n\n## Limitations\n- GuardRails is limited to Python 3 and individual functions, preventing its use for whole programs or functions calling helper functions.\n- The tool relies on LLMs, which may occasionally produce poor results.\n\n## Discussion and Future Work\n- The authors believe that GuardRails can be useful for instructors in creating code-writing tasks and for novice programmers to identify ambiguities in broader contexts.\n- Future work may involve expanding the tool's applicability and addressing its limitations.\n\nThe paper discusses an automated tool, GuardRails, that aims to clarify ambiguous purpose statements in Python functions using a heuristic based on Large Language Models (LLMs). The authors compare the tool's performance with GitHub Copilot\u2019s Chat feature and identify limitations and potential future applications for the tool.", "meta": {"url": "https://browse.arxiv.org/html/2312.08189v1", "title": "GuardRails: Automated Suggestions for Clarifying Ambiguous Purpose Statements", "subtitle": "Programmers should write clear purpose statements and can use a heuristic to identify potential ambiguities and improve code clarity.", "categories": ["prompt engineering", "programming"], "publish_date": "2023-12-13", "model": "gpt-3.5-turbo-1106", "word_count": 7937, "is_truncated": false}}
{"id": "2312.07399v1", "text": "# Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales\n\n## Introduction\n- LLMs demonstrated capacity for reasoning.\n- Most NLP-driven clinical projects lack clinical reasoning for disease diagnosis.\n- Proposed framework rationalizes diagnostic process and clinical CoT reasoning.\n\n## Problem Formulation\n- Clinical reasoning connects patient description and diagnosis.\n- Absence of effective clinical reasoning leads to diagnostic errors.\n\n## Testbed: Alzheimer\u2019s Disease Diagnosis\n- AD diagnosis chosen due to the disease's complexity.\n- Patient description includes MRI scan, demographic info, educational level, MMSE results, and APOE4 allele presence.\n\n## Reasoning-Aware Diagnosis Framework\n- Framework involves clinical rationalization, few-shot reasoning, and knowledge distillation.\n- Empirical evidence shows better performance in disease diagnosis and human-like reasoning.\n\n## Experiments\n- Used ADNI and AIBL datasets.\n- Implemented LLMs, unimodal-student models, and multimodal-student models.\n- Results indicate improved diagnostic performance and data efficiency.\n\n## Results and Discussion\n- Clinically meaningful rationales improve AD diagnosis.\n- Knowledge distillation benefits small models.\n- Framework is helpful in data-scarce scenarios.\n- Ineffective rationales not the cause of misdiagnoses.\n\n## Clinician Study: Quality of Rationales\n- Proposed criteria to evaluate machine-generated rationales for clinical diagnosis.\n- Human evaluation indicates effective replication of clinician reasoning.\n\n## Case Study of Machine-generated Rationales\n- Contextual interpretation of clinical evidence and selective summarization observed.\n\n## Related Work\n- Previous methods focus on classification or reading comprehension without addressing clinical reasoning.\n\n## Conclusion\n- Proposed framework facilitates clinical reasoning for disease diagnosis.\n- Limitations include prompt length and need for real-world implementation.\n\n## Ethical Consideration and Societal Impacts\n- Patient data is de-identified for privacy.\n- Ongoing assessment for societal impacts needed.\n\n## Acknowledgements\n- Supported by various institutions.\n\n## References\n- Detailed in-text references included in the summarized content.", "meta": {"url": "https://browse.arxiv.org/html/2312.07399v1", "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales", "subtitle": "New framework uses prompt-based learning for efficient clinical reasoning in disease diagnosis, demonstrating potential for real-world clinical settings.", "categories": ["prompt engineering"], "publish_date": "2023-12-12", "model": "gpt-3.5-turbo-1106", "word_count": 16964, "is_truncated": true}}
