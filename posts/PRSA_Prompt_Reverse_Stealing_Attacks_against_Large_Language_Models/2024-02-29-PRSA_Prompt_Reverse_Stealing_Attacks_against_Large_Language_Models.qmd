
---
title: "PRSA: Prompt Reverse Stealing Attacks against Large Language Models"
id: "2402.19200v1"
description: "New framework steals prompts from commercial language models, highlighting prompt leakage security concern."
author: Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19200v1/extracted/5440247/figures/prompt_jailbreaking.png"
categories: ['production', 'robustness', 'security', 'prompt-engineering', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19200v1/extracted/5440247/figures/prompt_jailbreaking.png)

### Summary

- Prompt Stealing Attacks against Large Language Models (LLMs) pose a risk to intellectual property rights.
- PRSA (Prompt Reverse Stealing Attacks) is a novel attack framework that infers and steals target prompts by analyzing critical features of input-output pairs.
- PRSA consists of two main phases: prompt mutation and prompt pruning.
- The prompt mutation phase uses a prompt attention algorithm based on differential feedback to capture critical features.
- The prompt pruning phase identifies and masks words dependent on specific inputs, enabling prompts to accommodate diverse inputs.
- PRSA poses a severe threat in real-world scenarios, as demonstrated by its effectiveness and generality across various categories and platforms.

### Major Findings

1. PRSA can effectively steal prompts from various categories in the marketplace with high success rates.
2. PRSA can achieve success rates of 91.5% and 82.3% on popular LLM application platforms OpenGPT and GPTsdex, respectively, at a low cost.
3. PRSA has been reported to prompt service providers, and collaborative efforts are being made to implement protective measures for prompt copyright.

### Analysis and Critique

- The paper focuses on the risks of prompt leakage from revealed input-output pairs in LLMs.
- The study highlights the need for risk awareness among prompt service providers and the implementation of prompt copyright protection measures.
- The paper could benefit from further exploration of possible defenses, such as input obfuscation and prompt watermarking, to provide a more comprehensive analysis.
- The paper could also discuss potential countermeasures that LLM providers could implement to mitigate the risks of PRSA.
- Further research could investigate the effectiveness of PRSA on other cross-modal large models and refine attack scenarios to reveal broader security vulnerabilities.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.19200v1](https://arxiv.org/abs/2402.19200v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19200v1](https://browse.arxiv.org/html/2402.19200v1)       |
| Truncated       | False       |
| Word Count       | 12660       |