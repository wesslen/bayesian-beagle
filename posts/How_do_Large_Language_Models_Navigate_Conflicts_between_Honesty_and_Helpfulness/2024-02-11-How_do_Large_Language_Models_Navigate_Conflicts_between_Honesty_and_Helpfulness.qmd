
---
title: "How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?"
id: "2402.07282v1"
description: "LLMs balance honesty and helpfulness, influenced by human feedback and prompting. GPT-4 Turbo mimics human responses."
author: Ryan Liu, Theodore R. Sumers, Ishita Dasgupta, Thomas L. Griffiths
date: "2024-02-11"
image: "../../img/2402.07282v1/image_1.png"
categories: ['prompt-engineering', 'social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.07282v1/image_1.png)

### Summary:
- The article explores the trade-offs between honesty and helpfulness in Large Language Models (LLMs) in day-to-day communication.
- It introduces the signaling bandits experimental paradigm to measure and trade-off the values of honesty and helpfulness.
- The experiments prompt LLMs with different scenarios and evaluate their responses for honesty and helpfulness.
- The results show that RLHF improves the honesty and helpfulness of LLMs, while Chain of Thought (CoT) prompting increases helpfulness but often at the cost of honesty.
- The prompts used for querying each large language model in each experiment are outlined, providing examples of system prompts, user prompts, and chain-of-thought user prompts for different settings.
- Examples of outputs from various models are provided to demonstrate how LLMs navigate honesty and helpfulness.

### Major Findings:
1. RLHF improves the honesty and helpfulness of LLMs.
2. CoT prompting increases helpfulness but often at the cost of honesty.
3. LLMs can be steered to prioritize either honesty or helpfulness based on the prompts.

### Analysis and Critique:
- The article provides valuable insights into the ethical considerations of using LLMs and how they can be steered to prioritize honesty or helpfulness.
- The prompts used in the experiments are crucial in understanding how LLMs navigate honesty and helpfulness, highlighting the importance of prompt design in studying the capabilities of LLMs.
- The examples of outputs from various models illustrate the complexity of decision-making in balancing honesty and helpfulness, contributing to the broader understanding of how language models navigate ethical considerations in decision-making processes.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.07282v1](https://arxiv.org/abs/2402.07282v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07282v1](https://browse.arxiv.org/html/2402.07282v1)       |
| Truncated       | True       |
| Word Count       | 21874       |