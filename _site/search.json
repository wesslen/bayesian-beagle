[
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The paper introduces novel prompting techniques to enhance the performance of automatic summarization systems for scientific articles. It discusses the challenges in summarizing scientific articles due to their length, complexity, and irregular organizational structures.\n\n\n\nThe related work section discusses prior approaches to automatic summarization, including extractive and abstractive methods, as well as prior techniques such as planning with learned entity prompts and faceted summarization.\n\n\n\nThe paper proposes and evaluates three key dimensions for enhancing summarization: ### Prompting Technique Dimension It compares various approaches for generating prompts, including keywords, MeSH, KeyBERT, TF, and TF-IDF, to provide scientific summarizers with useful contextual information. ### Model Dimension The paper studies the integration of proposed prompting techniques with a range of current state-of-the-art transformer models for scientific summarization. ### Input Text Dimension It investigates different text input conditions to evaluate the impact of prompts when summarizing sections independently.\n\n\n\nThe paper presents the results of experiments evaluating the proposed prompting techniques integrated with various state-of-the-art summarization models and input texts. It provides a detailed description of the dataset used, preprocessing, training methodology, evaluation protocol, and the results obtained.\n\n\n\nThe discussion section analyzes the findings and explores the implications of the results, particularly in terms of the impact of prompting techniques on the performance of different summarization models.\n\n\n\nThe paper proposes opportunities for future research, highlighting potential avenues for further exploration and improvement in the field of scientific article summarization.\n\n\n\nThe paper concludes with a summary of the key contributions and findings, emphasizing the potential of prompting techniques to assist smaller models in overcoming the limitations of less powerful summarization systems.\nIn addition, the authors encounter difficulties with HTML conversions and acknowledge issues with unsupported packages. The paper also provides author information and an abstract outlining the core contributions of the work.\nOverall, the paper introduces novel prompting techniques to improve the summarization of scientific articles and provides valuable insights into the potential impact of these techniques on automatic summarization systems.\n\n\nLink: https://browse.arxiv.org/html/2312.08282v2  Truncated: True  Word Count: 46040"
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#introduction",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#introduction",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The paper introduces novel prompting techniques to enhance the performance of automatic summarization systems for scientific articles. It discusses the challenges in summarizing scientific articles due to their length, complexity, and irregular organizational structures."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#related-work",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#related-work",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The related work section discusses prior approaches to automatic summarization, including extractive and abstractive methods, as well as prior techniques such as planning with learned entity prompts and faceted summarization."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#methods",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#methods",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The paper proposes and evaluates three key dimensions for enhancing summarization: ### Prompting Technique Dimension It compares various approaches for generating prompts, including keywords, MeSH, KeyBERT, TF, and TF-IDF, to provide scientific summarizers with useful contextual information. ### Model Dimension The paper studies the integration of proposed prompting techniques with a range of current state-of-the-art transformer models for scientific summarization. ### Input Text Dimension It investigates different text input conditions to evaluate the impact of prompts when summarizing sections independently."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#results",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#results",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The paper presents the results of experiments evaluating the proposed prompting techniques integrated with various state-of-the-art summarization models and input texts. It provides a detailed description of the dataset used, preprocessing, training methodology, evaluation protocol, and the results obtained."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#discussion",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#discussion",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The discussion section analyzes the findings and explores the implications of the results, particularly in terms of the impact of prompting techniques on the performance of different summarization models."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#future-work",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#future-work",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The paper proposes opportunities for future research, highlighting potential avenues for further exploration and improvement in the field of scientific article summarization."
  },
  {
    "objectID": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#conclusion",
    "href": "posts/Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles/2023-12-13-Prompting_LLMs_with_content_plans_to_enhance_the_summarization_of_scientific_articles.html#conclusion",
    "title": "Prompting LLMs with content plans to enhance the summarization of scientific articles",
    "section": "",
    "text": "The paper concludes with a summary of the key contributions and findings, emphasizing the potential of prompting techniques to assist smaller models in overcoming the limitations of less powerful summarization systems.\nIn addition, the authors encounter difficulties with HTML conversions and acknowledge issues with unsupported packages. The paper also provides author information and an abstract outlining the core contributions of the work.\nOverall, the paper introduces novel prompting techniques to improve the summarization of scientific articles and provides valuable insights into the potential impact of these techniques on automatic summarization systems.\n\n\nLink: https://browse.arxiv.org/html/2312.08282v2  Truncated: True  Word Count: 46040"
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The paper addresses the vulnerability of federated learning to data injection attacks, where malicious agents manipulate the learning process to compromise the model. It proposes a novel technique to detect and mitigate data injection attacks, with a focus on constant-output and label-flip attacks. The proposed method involves comparing updates from participating agents over time and ignoring updates from suspicious agents. Simulations demonstrate the effectiveness of the technique in overcoming attacks.\n\n\n\nThe introduction highlights the increasing need for data privacy in various industries and introduces federated learning as a technique that allows collaborative model training while preserving data privacy. However, it also outlines the vulnerability of federated learning to various security threats, including data injection attacks.\n\n\n\n\n\nThis section describes the process of federated learning, where agents refine their model parameters iteratively using their private datasets and share updates with a coordinating node.\n\n\n\nThe paper discusses the potential threat of data injection attacks in federated learning, particularly focusing on scenarios where some participating agents are malicious. It presents scenarios of attacks such as constant-output and label-flip attacks.\n\n\n\n\nThe proposed method for detecting data injection attacks involves comparing updates from agents over time and ignoring updates from suspicious agents based on a low-complexity metric. The method is designed to overcome false alarms and ensure convergence to a truthful model with a high probability.\n\n\n\nThe paper presents simulated attacks on decentralized learning and evaluates the performance of the detection algorithm. It provides examples of constant-output attacks and label-flip attacks, demonstrating the effectiveness of the proposed detection and mitigation technique.\n\n\n\nThe paper concludes by summarizing the proposed robust federated learning algorithm and its ability to operate in the presence of data injection attacks. It also outlines future work to be conducted including detailed proofs, and bounds on detection probability and false alarm probability.\nThe paper reflects the growing concern and interest in addressing security threats in federated learning systems, and it provides a novel perspective on detecting and mitigating data injection attacks. The simulation results demonstrate the potential effectiveness of the proposed technique in overcoming such attacks.\n\n\nLink: https://browse.arxiv.org/html/2312.02102v2  Truncated: False  Word Count: 13622"
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#abstract",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#abstract",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The paper addresses the vulnerability of federated learning to data injection attacks, where malicious agents manipulate the learning process to compromise the model. It proposes a novel technique to detect and mitigate data injection attacks, with a focus on constant-output and label-flip attacks. The proposed method involves comparing updates from participating agents over time and ignoring updates from suspicious agents. Simulations demonstrate the effectiveness of the technique in overcoming attacks."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#introduction",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#introduction",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The introduction highlights the increasing need for data privacy in various industries and introduces federated learning as a technique that allows collaborative model training while preserving data privacy. However, it also outlines the vulnerability of federated learning to various security threats, including data injection attacks."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#problem-formulation",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#problem-formulation",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "This section describes the process of federated learning, where agents refine their model parameters iteratively using their private datasets and share updates with a coordinating node.\n\n\n\nThe paper discusses the potential threat of data injection attacks in federated learning, particularly focusing on scenarios where some participating agents are malicious. It presents scenarios of attacks such as constant-output and label-flip attacks."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#attacker-detection-and-avoidance",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#attacker-detection-and-avoidance",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The proposed method for detecting data injection attacks involves comparing updates from agents over time and ignoring updates from suspicious agents based on a low-complexity metric. The method is designed to overcome false alarms and ensure convergence to a truthful model with a high probability."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#simulations",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#simulations",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The paper presents simulated attacks on decentralized learning and evaluates the performance of the detection algorithm. It provides examples of constant-output attacks and label-flip attacks, demonstrating the effectiveness of the proposed detection and mitigation technique."
  },
  {
    "objectID": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#conclusions",
    "href": "posts/Mitigating_Data_Injection_Attacks_on_Federated_Learning/2023-12-04-Mitigating_Data_Injection_Attacks_on_Federated_Learning.html#conclusions",
    "title": "Mitigating Data Injection Attacks on Federated Learning",
    "section": "",
    "text": "The paper concludes by summarizing the proposed robust federated learning algorithm and its ability to operate in the presence of data injection attacks. It also outlines future work to be conducted including detailed proofs, and bounds on detection probability and false alarm probability.\nThe paper reflects the growing concern and interest in addressing security threats in federated learning systems, and it provides a novel perspective on detecting and mitigating data injection attacks. The simulation results demonstrate the potential effectiveness of the proposed technique in overcoming such attacks.\n\n\nLink: https://browse.arxiv.org/html/2312.02102v2  Truncated: False  Word Count: 13622"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ryan Wesslen",
    "section": "",
    "text": "I’m a machine learning engineer at Explosion"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Wesslen",
    "section": "",
    "text": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n\n\nsecurity\n\n\nopen-source\n\n\n\n\n\n\n\ngpt-3.5-turbo-1106\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPrompting LLMs with content plans to enhance the summarization of scientific articles\n\n\n\nprompt engineering\n\n\n\n\n\n\n\ngpt-3.5-turbo-1106\n\n\nDec 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMitigating Data Injection Attacks on Federated Learning\n\n\n\nsecurity\n\n\n\n\n\n\n\ngpt-3.5-turbo-1106\n\n\nDec 4, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "The paper investigates the vulnerability of state-of-the-art open-source LLMs to priming attacks, which circumvent safety training and can be used to extract harmful outputs.\n\n\n\nThe researchers use a few-shot prompt approach to generate priming attacks and automate the evaluation process using another LLM. The results show that the proposed priming attack outperforms baselines in terms of attack success rate on harmful behaviors.\n\n\nExperiments were conducted on a server with specific hardware specifications.\n\n\n\nThe paper provides examples of few-shot prompts used for generating priming attacks.\n\n\n\nInstructions given to Llama Guard for evaluating the attacks are provided, along with specific unsafe content categories.\n\n\n\nThe paper clarifies the manual evaluation benchmark for labeling harmful responses, providing examples of harmful and safe responses.\n\n\n\nA comparison between manual evaluation and Llama Guard’s evaluation indicates differences in the assessment of harmful responses.\n\n\n\nA comparison of runtimes for Llama-2 and few-shot prompting is provided, demonstrating the efficiency of the few-shot task.\n\n\n\n\nThe study highlights the fragility of current LLM safety measures and raises concerns about future open-sourcing of LLMs, emphasizing the need for further research into safer open-sourcing methods.\n\n\n\nThe paper includes references to related works for further exploration.\n\n\nLink: https://browse.arxiv.org/html/2312.12321v1  Truncated: False  Word Count: 10319"
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#introduction",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#introduction",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "The paper investigates the vulnerability of state-of-the-art open-source LLMs to priming attacks, which circumvent safety training and can be used to extract harmful outputs."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#methodology-results",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#methodology-results",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "The researchers use a few-shot prompt approach to generate priming attacks and automate the evaluation process using another LLM. The results show that the proposed priming attack outperforms baselines in terms of attack success rate on harmful behaviors.\n\n\nExperiments were conducted on a server with specific hardware specifications.\n\n\n\nThe paper provides examples of few-shot prompts used for generating priming attacks.\n\n\n\nInstructions given to Llama Guard for evaluating the attacks are provided, along with specific unsafe content categories.\n\n\n\nThe paper clarifies the manual evaluation benchmark for labeling harmful responses, providing examples of harmful and safe responses.\n\n\n\nA comparison between manual evaluation and Llama Guard’s evaluation indicates differences in the assessment of harmful responses.\n\n\n\nA comparison of runtimes for Llama-2 and few-shot prompting is provided, demonstrating the efficiency of the few-shot task."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#conclusion",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#conclusion",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "The study highlights the fragility of current LLM safety measures and raises concerns about future open-sourcing of LLMs, emphasizing the need for further research into safer open-sourcing methods."
  },
  {
    "objectID": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#references",
    "href": "posts/Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks/2023-12-19-Bypassing_the_Safety_Training_of_Open-Source_LLMs_with_Priming_Attacks.html#references",
    "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks",
    "section": "",
    "text": "The paper includes references to related works for further exploration.\n\n\nLink: https://browse.arxiv.org/html/2312.12321v1  Truncated: False  Word Count: 10319"
  }
]