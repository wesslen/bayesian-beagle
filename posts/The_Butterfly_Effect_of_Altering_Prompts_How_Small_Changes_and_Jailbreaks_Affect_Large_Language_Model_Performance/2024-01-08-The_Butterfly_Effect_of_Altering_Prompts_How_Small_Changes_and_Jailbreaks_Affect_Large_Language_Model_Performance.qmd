
---
title: "The Butterfly Effect of Altering Prompts: How Small Changes and Jailbreaks Affect Large Language Model Performance"
id: "2401.03729v1"
description: "TL;DR: Small changes in how prompts are constructed can significantly impact the decisions made by Large Language Models (LLMs)."
author: ['Abel Salinas', 'Fred Morstatter']
date: "2024-01-08"
image: "https://browse.arxiv.org/html/2401.03729v1/extracted/5335133/figures/aggregate/aggregate-labels-styles-only.png"
categories: ['hci', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.03729v1/extracted/5335133/figures/aggregate/aggregate-labels-styles-only.png)

### Major Takeaways

1. Prompting variations, including output format, perturbations, jailbreaks, and tipping, significantly impact the predictions and accuracy of Large Language Models (LLMs) across various text classification tasks.

2. Even minor changes to prompts, such as adding a space or using different output formats like JSON or CSV, can cause LLMs to change their answers and impact their accuracy.

3. Jailbreaks, used to bypass LLM content filters for sensitive topics, can lead to substantial changes in predictions and considerable performance losses.

### Summary of Sections

#### Introduction
- Large Language Models (LLMs) have become popular for labeling data, with prompt construction being a crucial process involving decisions on wording, output format, and jailbreaks for sensitive topics.

#### Related Work
- Prompt generation and its impact on LLM behavior has been recognized in related literature, highlighting the importance of variations in prompts and prompt ensembles for robust insights.

#### Methodology
- The study explores prompt variations in output formats, perturbations, jailbreaks, and tipping across 11 text classification tasks, using OpenAIâ€™s ChatGPT.

#### Results
- Prompt variations lead to changes in LLM predictions, with formatting specifications, minor perturbations, and jailbreaks affecting accuracy. The similarity of predictions across different prompt variations is explored, and the correlation between prompt variations and annotator disagreement is studied, revealing the minimal impact of confusion on prediction changes.

#### Conclusion
- Overall, prompt variations, particularly formatting changes and jailbreaks, have a significant impact on LLM predictions and accuracy, with implications for future work on generating LLMs resilient to prompt variations.

### Critique
The study provides a comprehensive analysis of how prompt variations affect LLM performance. However, the findings should be interpreted with caution due to the study's reliance on a specific LLM model (ChatGPT) and prompt variations that may not generalize to all LLMs. Additionally, the impact of these prompt variations on real-world applications and user interactions with LLMs remains to be explored. Further research could involve wider experimentation across different LLMs and application scenarios to validate the generalizability of these findings.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [http://arxiv.org/abs/2401.03729v1](http://arxiv.org/abs/2401.03729v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.03729v1](https://browse.arxiv.org/html/2401.03729v1)       |
| Truncated       | False       |
| Word Count       | 6734       |