
---
title: "Shared Imagination: LLMs Hallucinate Alike"
id: "2407.16604v1"
description: "LLMs share a shared imagination space, answering imaginary questions with success, suggesting model homogeneity and computational creativity."
author: Yilun Zhou, Caiming Xiong, Silvio Savarese, Chien-Sheng Wu
date: "2024-07-23"
image: "https://browse.arxiv.org/html/2407.16604v1/x1.png"
categories: ['robustness', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.16604v1/x1.png)

### Summary:

- The paper proposes a novel setting, Imaginary Question Answering (IQA), to better understand model similarity among large language models (LLMs).
- In IQA, one model generates purely imaginary questions, and another model is prompted to answer.
- Despite the fictionality of these questions, all models can answer each other's questions with remarkable success, suggesting a "shared imagination space" in which these models operate during such hallucinations.
- The paper conducts a series of investigations into this phenomenon and discusses implications on model homogeneity, hallucination, and computational creativity.

### Major Findings:

1. LLMs achieve an average 54% correctness rate on directly generated questions, with higher accuracy when the answer model is the same or in the same model family as the question model.
2. For context-based questions, the correctness rate increases significantly to 86%, with certain (QM, AM) pairs achieving as high as 96%.
3. Models exhibit high degrees of agreement on what they hallucinate, which is called "shared imagination."

### Analysis and Critique:

- The paper's findings suggest that LLMs share fundamental commonalities, despite their highly varying benchmark results.
- The homogeneity of LLMs could have broad implications on model hallucination and its detection, as well as the use of LLMs in computational creativity.
- However, the paper does not address potential limitations or biases in the models, nor does it discuss the methodological issues or conflicting evidence that may arise from the use of LLMs in this context.
- Further research is needed to explore the implications of these findings and to address any potential shortcomings or limitations of the proposed approach.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.16604v1](https://arxiv.org/abs/2407.16604v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.16604v1](https://browse.arxiv.org/html/2407.16604v1)       |
| Truncated       | False       |
| Word Count       | 7135       |