
---
title: "Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection"
id: "2402.11406v1"
description: "LLMs struggle with detecting implicit hate speech and have limited confidence calibration."
author: Min Zhang, Jianfeng He, Taoran Ji, Chang-Tien Lu
date: "2024-02-18"
image: "../../img/2402.11406v1/image_1.png"
categories: ['prompt-engineering', 'security']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11406v1/image_1.png)

### Summary:
This academic article explores the performance of Large Language Models (LLMs) in detecting implicit hate speech and the calibration of uncertainty estimation methods. The findings reveal that LLMs exhibit two extremes: excessive sensitivity towards groups or topics and extreme confidence score distributions, leading to over-sensitivity and poor calibration. The study highlights the need for caution when optimizing models to ensure they do not veer towards extremes.

### Major Findings:
1. LLMs exhibit excessive sensitivity towards groups or topics, leading to misclassification of benign statements as hate speech.
2. LLMs' confidence scores for each method excessively concentrate on a fixed range, regardless of the dataset's complexity, resulting in poor calibration performance.
3. Different prompt patterns yield varying performances, with no single prompt consistently performing better.

### Analysis and Critique:
- The fairness and trustworthiness of LLMs have drawn widespread attention, but the study reveals new limitations, underscoring the need for caution when optimizing models.
- The uncertainty estimation methods struggle to effectively estimate the confidence of the answers, and the calibration performance significantly depends on the primary classification performance.
- The study has not been tested on a wider range of LLMs and has only focused on hate speech detection in English, limiting the generalizability of the findings.

Overall, the article provides valuable insights into the limitations of LLMs in detecting implicit hate speech and the calibration of uncertainty estimation methods, highlighting the need for further research in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-20       |
| Abstract | [https://arxiv.org/abs/2402.11406v1](https://arxiv.org/abs/2402.11406v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11406v1](https://browse.arxiv.org/html/2402.11406v1)       |
| Truncated       | False       |
| Word Count       | 13151       |