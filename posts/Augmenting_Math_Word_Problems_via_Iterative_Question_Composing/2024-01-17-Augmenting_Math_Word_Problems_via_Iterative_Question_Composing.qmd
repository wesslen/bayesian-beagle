
---
title: "Augmenting Math Word Problems via Iterative Question Composing"
id: "2401.09003v1"
description: "A dataset is introduced to improve math reasoning in language models, achieving 5.8% higher accuracy on math problems."
author: ['Haoxiong Liu', 'Andrew Chi-Chih Yao']
date: "2024-01-17"
image: "https://browse.arxiv.org/html/2401.09003v1/x1.png"
categories: ['production', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.09003v1/x1.png)

**Summary:**
The article introduces the MMIQC dataset and the IQC (Iterative Question Composing) method to equip large language models with improved mathematical reasoning skills. It highlights the Mistral-7B-MMIQC model's achievement of 36.0% accuracy on the MATH benchmark, which is 5.8% higher than the previous state-of-the-art (SOTA) model. The paper combines high-quality corpora used in pre-training and synthetic question-response pairs to improve model performance. The IQC method, which iteratively asks language models to compose new questions from seed problems and uses rejection sampling, contributes significantly to this improvement.

### Major Findings:
1. Mistral-7B-MMIQC achieved 36.0% accuracy on MATH, surpassing the previous SOTA model by 5.8%.
2. The novel augmentation method IQC, which iteratively asks language models to compose new questions from seed problems and uses rejection sampling, significantly improves the model's performance.
3. The article demonstrates that using multiple augmentation methods to construct datasets for fine-tuning is an efficient way to boost the performance of large language models.

### Analysis and Critique:
The article makes significant contributions by introducing the MMIQC dataset and the IQC method, resulting in improved mathematical reasoning skills for large language models. However, the evaluation focuses solely on the MATH benchmark, raising questions about the generalizability of the findings to other mathematical problem-solving tasks. Additionally, the potential algorithmic biases or limitations of relying on large language models for mathematical problem-solving should be addressed, especially with regard to diverse representation and interpretability issues. Further research could explore the potential ethical implications and societal impacts of using such models for complex reasoning tasks. Moreover, the article acknowledges the performance gap between open-source models and advanced close-source models, but a deeper exploration of the reasons and implications of this gap would enhance the article's critical analysis.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-31       |
| Abstract | [http://arxiv.org/abs/2401.09003v1](http://arxiv.org/abs/2401.09003v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.09003v1](https://browse.arxiv.org/html/2401.09003v1)       |
| Truncated       | False       |
| Word Count       | 4740       |