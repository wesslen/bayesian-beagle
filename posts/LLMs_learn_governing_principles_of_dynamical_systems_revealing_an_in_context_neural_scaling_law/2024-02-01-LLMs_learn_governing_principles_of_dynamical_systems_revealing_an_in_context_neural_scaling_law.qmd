
---
title: "LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law"
id: "2402.00795v1"
description: "Pretrained LLMs can accurately forecast dynamical systems without fine-tuning."
author: Toni J. B. Liu, Nicolas Boullé, Raphaël Sarfati, Christopher J. Earls
date: "2024-02-01"
image: "https://browse.arxiv.org/html/2402.00795v1/extracted/5383505/figures/auto_completion_plot.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.00795v1/extracted/5383505/figures/auto_completion_plot.png)

### **Summary:**
- Pretrained large language models (LLMs) are effective at performing zero-shot tasks, including time-series forecasting.
- LLMs can accurately predict dynamical system time series without fine-tuning or prompt engineering.
- The accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law.

### Major Findings:
1. LLMs achieve accurate predictions of dynamical system time series without fine-tuning or prompt engineering.
2. A flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs is presented.
3. A neural scaling law is observed for in-context learning abilities of LLMs.

### Analysis and Critique:
- The study reveals an in-context neural scaling law for LLMs, but it is unclear how this learning ability is implemented during inference.
- The early plateauing of loss curves for certain systems suggests that the LLM might ignore earlier data that is "out of distribution."
- The temperature hyperparameter affects the outcome of the LLM softmax layer and the extracted PDF, indicating the need for careful tuning.
- The study demonstrates the potential of LLMs to extract governing principles of numerical sequences observed in-context, but further research is needed to understand the underlying mechanisms.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.00795v1](https://arxiv.org/abs/2402.00795v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00795v1](https://browse.arxiv.org/html/2402.00795v1)       |
| Truncated       | False       |
| Word Count       | 9254       |