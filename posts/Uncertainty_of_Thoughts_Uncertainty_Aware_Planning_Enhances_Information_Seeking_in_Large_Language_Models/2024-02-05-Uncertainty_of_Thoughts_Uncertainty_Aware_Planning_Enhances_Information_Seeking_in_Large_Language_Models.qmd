
---
title: "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models"
id: "2402.03271v1"
description: "Algorithm UoT improves large language models by actively seeking information, achieving 57.8% performance improvement."
author: Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi
date: "2024-02-05"
image: "img/2402.03271v1/image_1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](img/2402.03271v1/image_1.png)

### Summary:
- The Uncertainty of Thoughts (UoT) algorithm enhances large language models (LLMs) by actively seeking information through effective questioning.
- The algorithm combines an uncertainty-aware simulation approach, uncertainty-based rewards, and a reward propagation scheme to select the optimal question to ask.
- Experimental results demonstrate that UoT improves the success rate of multiple LLMs by 57.8% on average compared with direct prompting, achieving top performance on both task success and efficiency.
- The information gain formula, simulation depth, reliability of GPT-4, reward function, limitations, future work, experimental setups, examples, and prompts are crucial components of the methodology and findings.
- Structured prompts are used for troubleshooting, medical diagnosis, and the 20 Questions game, ensuring efficiency and accuracy in information-seeking tasks.

### Major Findings:
1. UoT improves the success rate of LLMs by 57.8% on average compared with direct prompting.
2. The algorithm outperforms other methods across various datasets and LLMs, demonstrating its superiority in enhancing success rates and planning efficacy.
3. Structured prompts designed for different scenarios ensure efficiency and accuracy in troubleshooting, medical diagnosis, and the 20 Questions game.

### Analysis and Critique:
- The UoT algorithm addresses the need for LLMs to actively seek information in uncertain and ambiguous settings, going beyond conventional question-answering scenarios.
- The comparison at equal computational efficiency and the evaluation in the open set setting solidify the significance of UoT in active information-seeking tasks.
- The methodology provides a systematic approach to quantifying and propagating rewards, ultimately leading to the selection of the optimal question, emphasizing the importance of uncertainty reduction and expected rewards.
- The structured prompts ensure efficiency and accuracy in various information-seeking tasks, highlighting the practical application of these prompts in different contexts.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-06       |
| Abstract | [https://arxiv.org/abs/2402.03271v1](https://arxiv.org/abs/2402.03271v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.03271v1](https://browse.arxiv.org/html/2402.03271v1)       |
| Truncated       | True       |
| Word Count       | 21050       |