
---
title: "CaLMQA: Exploring culturally specific long-form question answering across 23 languages"
id: "2406.17761v1"
description: "TL;DR: CaLMQA dataset evaluates multilingual LLMs on complex questions, revealing gaps in low-resource languages and cultural specificity."
author: Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi
date: "2024-06-25"
image: "https://browse.arxiv.org/html/2406.17761v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17761v1/x1.png)

### Summary:

The paper introduces CaLMQA, a dataset of 2.6K complex questions spanning 23 languages, including under-resourced languages such as Fijian and Kirundi. The dataset includes both naturally-occurring questions collected from community web forums and questions written by native speakers. The authors conduct automatic evaluation across a suite of open- and closed-source models using their novel metric CaLMScore, which detects incorrect language and token repetitions in answers. They observe that the quality of LLM-generated answers degrades significantly for some low-resource languages. Human evaluation on a subset of models reveals that model performance is significantly worse for culturally specific questions than for culturally agnostic questions. The findings highlight the need for further research in LLM multilingual capabilities and non-English LFQA evaluation.

### Major Findings:

1. The CaLMQA dataset includes 2.6K complex questions spanning 23 languages, including under-resourced languages such as Fijian and Kirundi.
2. The authors introduce a novel metric, CaLMScore, to evaluate the quality of LLM-generated answers, which detects incorrect language and token repetitions.
3. Automatic evaluation reveals that the quality of LLM-generated answers degrades significantly for some low-resource languages.
4. Human evaluation on a subset of models shows that model performance is significantly worse for culturally specific questions than for culturally agnostic questions.

### Analysis and Critique:

The paper presents a valuable contribution to the field of LFQA by introducing a new dataset, CaLMQA, which includes under-resourced languages and culturally specific questions. The authors' novel metric, CaLMScore, provides a useful tool for evaluating the quality of LLM-generated answers. However, the paper could benefit from a more detailed analysis of the limitations and potential biases of the dataset and the evaluation metrics used. Additionally, the authors could explore the potential of using CaLMQA to evaluate other LLMs and compare their performance to the models evaluated in this study.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17761v1](https://arxiv.org/abs/2406.17761v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17761v1](https://browse.arxiv.org/html/2406.17761v1)       |
| Truncated       | False       |
| Word Count       | 11413       |