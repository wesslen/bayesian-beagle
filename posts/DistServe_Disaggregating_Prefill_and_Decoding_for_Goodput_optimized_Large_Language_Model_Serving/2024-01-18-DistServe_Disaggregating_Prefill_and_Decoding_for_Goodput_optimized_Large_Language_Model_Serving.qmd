
---
title: "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving"
id: "2401.09670v1"
description: "DistServe enhances large language model serving by separating prefill and decoding computation, reducing interference, and improving performance."
author: Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, Hao Zhang
date: "2024-01-18"
image: "https://browse.arxiv.org/html/2401.09670v1/x1.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.09670v1/x1.png)

### Summary of the Article:

The article presents DistServe, an approach that aims to improve the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation. Existing LLM serving systems batch the computation of prefill and decoding across all users and requests, leading to strong prefill-decoding interferences and coupling of resource allocation and parallelism plans for both phases. DistServe assigns prefill and decoding computation to different GPUs, eliminating interferences, and co-optimizes the resource allocation and parallelism strategy tailored for each phase. The approach significantly improves LLM serving performance in terms of the maximum request rate that can be served within both time to first token (TTFT) and time per output token (TPOT) constraints on each GPU.

### Major Findings:
1. **Disaggregation of Prefill and Decoding**: DistServe addresses the interference between prefill and decoding computation in LLM serving systems by assigning them to separate GPUs, thereby eliminating interference and improving system performance.
2. **Co-Optimized Resource Allocation and Parallelism**: The approach co-optimizes the resource allocation and parallelism strategy tailored for each phase, leading to improved performance within TTFT and TPOT constraints on each GPU.
3. **Performance Improvement**: DistServe outperforms state-of-the-art systems, serving up to 4.48 times more requests under latency constraints, while staying within latency constraints for over 90% of requests.

### Analysis and Critique:
The article provides a comprehensive and innovative approach to addressing the challenges in LLM serving systems, particularly in improving the performance and managing latency constraints. By disaggregating the prefill and decoding computation and co-optimizing resource allocation and parallelism, DistServe demonstrates significant performance improvements. The latency breakdown and ablation studies serve to verify the effectiveness of the proposed approach.

However, the article lacks a detailed analysis of potential drawbacks or limitations of the DistServe approach. Further exploration of potential issues, such as scalability, practical implementation complexities, or cost implications, could provide a more balanced view of the effectiveness and applicability of the proposed approach. Additionally, the article could benefit from a more detailed discussion of potential deployment challenges and adaptation to real-world production settings.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-27       |
| Abstract | [http://arxiv.org/abs/2401.09670v1](http://arxiv.org/abs/2401.09670v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.09670v1](https://browse.arxiv.org/html/2401.09670v1)       |
| Truncated       | True       |
| Word Count       | 15168       |