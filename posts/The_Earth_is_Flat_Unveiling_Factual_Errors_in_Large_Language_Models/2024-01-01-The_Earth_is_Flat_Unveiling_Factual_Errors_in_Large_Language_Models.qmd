
---
title: "The Earth is Flat? Unveiling Factual Errors in Large Language Models"
id: "2401.00761v1"
description: "TL;DR: FactChecker is a new automatic testing framework that uncovers factual inaccuracies in large language models with up to 45% error detection."
author: Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu
date: "2024-01-01"
image: "https://browse.arxiv.org/html/2401.00761v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.00761v1/x1.png)

# BiasAsker: Measuring the Bias in Your Chatbot via Asking Questions

## Summary

### Major Takeaways
1. **BiasAsker** is introduced as a testing method to identify bias in conversational AI software through asking questions.
2. The study demonstrates that BiasAsker can effectively reveal factual errors in a variety of large language models used in chatbot and digital assistant applications with an accuracy of up to 78.2% for commercial LLMs and an improvement of 33.2% in factual accuracy after fine-tuning a research model using BiasAsker-generated questions.
3. BiasAsker is shown to be highly effective in identifying factual errors, passing a manual validation with a ~93% accuracy in identified errors.

### Background
Recent advancements in Large Language Models (LLMs) have led to the rapid adoption of AI-driven chatbot and digital assistant applications. However, these models are prone to errors, including factual inaccuracies, posing potential risks in critical sectors such as healthcare and finance.

### Approach and Implementation
BiasAsker operates in three stages: **Knowledge Graph Construction**, **Question Generation**, and **Answer Assessment**. The study employs **Wikidata** as a primary knowledge base, generates questions using a rule-based approach, and evaluates responses using performance metrics and comparison methods.

### Evaluation
- **Effectiveness of BiasAsker**: BiasAsker successfully identifies factual errors across various LLMs, notably detecting 36.9% of the test cases with errors.
- **Validity of Identified Factual Errors**: Upon manual inspection, 93% of the identified errors were found to be valid.
- **Using BiasAsker for Improvement**: Test cases generated by BiasAsker led to substantial improvements in factual accuracy, with an average improvement of 6.5% using in-context learning and 33.2% via fine-tuning of the research models.

## Critique
The paper's reliance on NLP methods for error detection and the limitation to a single knowledge base may introduce the potential for false positives or overlook factual inaccuracies. Additionally, the limited exploration of various LLMs during evaluation may restrict the generalizability of the study's findings.

Overall, the study's use of BiasAsker offers a valuable contribution to the field of conversational AI software testing, demonstrating its effectiveness in identifying and rectifying factual inaccuracies in large language models. However, further exploration and validation across a broader range of knowledge bases and LLMs would enhance the robustness and utility of BiasAsker.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-10       |
| Abstract | [http://arxiv.org/abs/2401.00761v1](http://arxiv.org/abs/2401.00761v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.00761v1](https://browse.arxiv.org/html/2401.00761v1)       |
| Truncated       | False       |
| Word Count       | 11574       |