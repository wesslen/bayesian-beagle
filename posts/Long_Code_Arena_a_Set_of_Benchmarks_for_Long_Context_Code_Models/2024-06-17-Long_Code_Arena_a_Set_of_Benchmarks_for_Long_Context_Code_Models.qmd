
---
title: "Long Code Arena: a Set of Benchmarks for Long-Context Code Models"
id: "2406.11612v1"
description: "Long Code Arena: Benchmarks for Project-wide Code Processing Tasks"
author: Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, Timofey Bryksin
date: "2024-06-17"
image: "../../../bayesian-beagle.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

**Summary:**

The paper introduces Long Code Arena, a suite of six benchmarks for code processing tasks that require project-wide context. These tasks include library-based code generation, CI builds repair, project-level code completion, commit message generation, bug localization, and module summarization. The paper highlights the limitations of existing ML4SE benchmarks, such as short context length and limited resemblance to practical use cases. Long Code Arena aims to address these issues by providing manually verified datasets, evaluation suites, and open-source baseline solutions based on popular LLMs. The benchmark page, leaderboard, and links to datasets are available on HuggingFace Spaces.

**Major Findings:**

1. Long Code Arena provides a suite of six benchmarks for code processing tasks that require project-wide context.
2. The benchmarks address the limitations of existing ML4SE benchmarks, such as short context length and limited re

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.11612v1](https://arxiv.org/abs/2406.11612v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.11612v1](https://browse.arxiv.org/html/2406.11612v1)       |
| Truncated       | True       |
| Word Count       | 31007       |