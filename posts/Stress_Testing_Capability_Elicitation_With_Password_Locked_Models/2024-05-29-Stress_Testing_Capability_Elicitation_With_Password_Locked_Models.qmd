
---
title: "Stress-Testing Capability Elicitation With Password-Locked Models"
id: "2405.19550v1"
description: "Fine-tuning can effectively elicit hidden capabilities of LLMs, but may be unreliable without high-quality demonstrations."
author: Ryan Greenblatt, Fabien Roger, Dmitrii Krasheninnikov, David Krueger
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.19550v1/x1.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19550v1/x1.png)

### Summary:

The paper investigates the conditions under which fine-tuning-based elicitation can reveal the hidden capabilities of large language models (LLMs). The authors introduce password-locked models, which are LLMs fine-tuned to exhibit certain capabilities only when a password is present in the prompt. This approach enables a novel method of evaluating capabilities elicitation methods by testing whether these password-locked capabilities can be elicited without using the password. The study finds that a few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities. Surprisingly, fine-tuning can also elicit other capabilities that have been locked using the same password or even different passwords. Furthermore, when only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities. However, fine-tuning may be unreliable when high-quality demonstrations are not available, such as when models' capabilities exceed those of human demonstrators.

### Major Findings:

1. A few high-quality demonstrations are often sufficient to fully elicit password-locked capabilities.
2. Fine-tuning can elicit other capabilities that have been locked using the same password or even different passwords.
3. When only evaluations, and not demonstrations, are available, approaches like reinforcement learning are still often able to elicit capabilities.

### Analysis and Critique:

The paper presents an innovative approach to evaluating the capabilities of LLMs by introducing password-locked models. This method allows for a more systematic and controlled evaluation of elicitation techniques. However, the study's findings are limited to the specific password-locked models used, and the results may not generalize to other models or scenarios. Additionally, the reliance on high-quality demonstrations for successful elicitation raises concerns about the applicability of this approach in situations where such demonstrations are not available. The study also does not address the potential risks associated with eliciting hidden capabilities, such as the misuse of these capabilities for malicious purposes. Overall, the paper provides valuable insights into the potential of fine-tuning-based elicitation for revealing hidden capabilities in LLMs, but further research is needed to address the limitations and potential risks of this approach.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19550v1](https://arxiv.org/abs/2405.19550v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19550v1](https://browse.arxiv.org/html/2405.19550v1)       |
| Truncated       | False       |
| Word Count       | 13197       |