
---
title: "Vikhr: The Family of Open-Source Instruction-Tuned Large Language Models for Russian"
id: "2405.13929v1"
description: "Vikhr: New open-source LLM for Russian, outperforms some closed-source models, available at Hugging Face."
author: Aleksandr Nikolich, Konstantin Korolev, Artem Shelmanov
date: "2024-05-22"
image: "../../../bayesian-beagle.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

Vikhr is a new state-of-the-art open-source instruction-tuned Large Language Model (LLM) designed specifically for the Russian language. Unlike previous efforts that utilize computationally inexpensive LoRA adapters on top of English-oriented models, Vikhr features an adapted tokenizer vocabulary and undergoes continued pre-training and instruction tuning of all weights. This approach not only enhances the modelâ€™s performance but also significantly improves its computational and contextual efficiency. Vikhr sets the new state of the art among open-source LLMs for Russian and even outperforms some proprietary closed-source models on certain benchmarks.

### Major Findings:

1. **Adapted Tokenizer Vocabulary**: Vikhr addresses the issue of poor generation quality and reduced computational performance in non-English languages by adapting the tokenizer vocabulary and undergoing continued pre-training and instruction tuning of all weights.
2. **Improved Performance**: Vikhr's remarkable performance across various Russian-language benchmarks can be attributed to its expanded instruction datasets and corpora for continued pre-training.
3. **Open-Source and State-of-the-Art**: Vikhr is a state-of-the-art open-source LLM for Russian, outperforming other open-source counterparts across a wide range of benchmarks.

### Analysis and Critique:

While Vikhr represents a significant advancement in the development of LLMs for the Russian language, there are a few potential limitations and areas for further research:

1. **Dependence on English-oriented Models**: Vikhr's performance is partially dependent on the strong logical and common reasoning capabilities, as well as the extensive world knowledge present in English-oriented LLMs. This dependence may limit Vikhr's ability to fully capture the nuances of the Russian language and culture.
2. **Lack of RLHF/DPO Fine-tuning**: The authors do not implement RLHF/DPO fine-tuning of Vikhr due to the lack of resources for human annotation. This could potentially limit Vikhr's performance and alignment.
3. **Limited Instruction-Output Pairs**: The authors do not introduce additional instruction-output pairs to facilitate LLM alignment.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.13929v1](https://arxiv.org/abs/2405.13929v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.13929v1](https://browse.arxiv.org/html/2405.13929v1)       |
| Truncated       | False       |
| Word Count       | 4406       |