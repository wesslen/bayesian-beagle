
---
title: "Overview of the PromptCBLUE Shared Task in CHIP2023"
id: "2312.17522v1"
description: "Overview of PromptCBLUE shared task at CHIP-2023 Conference, featuring reformulated benchmarks for testing Chinese language models in medical domains."
author: Wei Zhu, Xiaoling Wang, Mosha Chen, Buzhou Tang
date: "2023-12-29"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](None)

### Major Takeaways

1. **PromptCBLUE Shared Task**: The paper provides an overview of the PromptCBLUE shared task held in the CHIP-2023 Conference, focusing on the multitask capabilities of Chinese large language models (LLMs) in medical natural language processing. It comprises two tracks: the Parameter-efficient Fine-tuning (PEFT) Track and the In-Context Learning (ICL) Track, with participation from both industry and academia.

2. **PEFT Track**: The PEFT track challenges participants to fine-tune Chinese LLMs with a single PEFT module for 18 sub-tasks, aiming to explore novel PEFT modules and multi-task training methods. Notably, 362 teams participated in the first round, with clear performance differences between 7B and 13B models.

3. **ICL Track**: The ICL track evaluates medium-sized (6B, 7B, or 13B parameters) open-sourced LLMs, with a focus on maximizing in-context learning capabilities without introducing additional parameters. This track attracted 238 teams in the first round, showcasing the significance of demonstration selection techniques and the core role of ICL capabilities in handling emergent tasks.

### Related Work

The paper provides detailed insights into medical natural language processing and parameter-efficient fine-tuning methods. It offers an extensive review of the advancements in large language models and the application of in-context learning to improve LLMs' task-solving and reasoning abilities.

### PromptCBLUE Overview

The overview section explains the extensive multi-task test suite of the PromptCBLUE shared task, encompassing medical information extraction, text classification, natural language inference tasks, symptom status understanding, and medical content generation. The paper also describes the process of prompt collection, response formats, sample formats, and dataset splits for PromptCBLUE.

### Participating Teams and Methods

The paper reviews the participating teams and methods in both the PEFT and ICL tracks, highlighting the employed pre-trained backbones, data processing and augmentation methods, parameter-efficient fine-tuning techniques, and demonstration selection strategies. Furthermore, it emphasizes the challenges and successes of the shared task, demonstrating the impact of the winning teams' approaches.

### Critique and Potential Problems

The paper provides a comprehensive overview of the PromptCBLUE shared task and the methodologies employed by participating teams. However, it lacks explicit details on the specific results achieved by the winning teams and the potential implications of the shared task on the future development of Chinese LLMs in medical natural language processing. Additionally, the paper could benefit from a more in-depth analysis of the limitations or shortcomings of the shared task and the discussed methodologies.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-10       |
| Abstract | [http://arxiv.org/abs/2312.17522v1](http://arxiv.org/abs/2312.17522v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.17522v1](https://browse.arxiv.org/html/2312.17522v1)       |
| Truncated       | False       |
| Word Count       | 7037       |