
---
title: "LSTPrompt: Large Language Models as Zero-Shot Time Series Forecasters by Long-Short-Term Prompting"
id: "2402.16132v1"
description: "LSTPrompt improves time-series forecasting with tailored prompts for better adaptability and performance."
author: Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, B. Aditya Prakash
date: "2024-02-25"
image: "https://browse.arxiv.org/html/2402.16132v1/extracted/5430411/content/Picture/ablation.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16132v1/extracted/5430411/content/Picture/ablation.png)

### **Summary:**
- Large Language Models (LLMs) have shown strong zero-shot time series forecasting (TSF) capabilities while maintaining computational efficiency.
- Existing prompting methods for LLMs oversimplify TSF as language next-token predictions, overlooking the dynamic nature of TSF tasks.
- LSTPrompt is proposed as a novel approach for prompting LLMs in zero-shot TSF tasks, decomposing TSF into short-term and long-term forecasting sub-tasks and guiding LLMs to regularly reassess forecasting mechanisms.

### **Major Findings:**
1. LSTPrompt decomposes TSF into short-term and long-term forecasting sub-tasks, tailoring prompts to each.
2. LSTPrompt guides LLMs to regularly reassess forecasting mechanisms to enhance adaptability.
3. Extensive evaluations demonstrate consistently better performance of LSTPrompt than existing prompting methods, and competitive results compared to foundation TSF models.

### **Analysis and Critique:**
- LSTPrompt may suffer from reduced interpretability due to the limited understanding of LLMs.
- Incorporating additional instructions in the prompts could potentially introduce information leaks that are exploited by the LLMs.
- Further research is needed to investigate the trustworthiness of LLMs and ensure that LSTPrompt can be deployed without concerns regarding information leakage issues.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-27       |
| Abstract | [https://arxiv.org/abs/2402.16132v1](https://arxiv.org/abs/2402.16132v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16132v1](https://browse.arxiv.org/html/2402.16132v1)       |
| Truncated       | False       |
| Word Count       | 5366       |