
---
title: "Learning to Compress Prompt in Natural Language Formats"
id: "2402.18700v1"
description: "Nano-Capsulator compresses long prompts for efficient, transferable LLM usage."
author: Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu
date: "2024-02-28"
image: "https://browse.arxiv.org/html/2402.18700v1/x1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.18700v1/x1.png)

### Summary:

- Large language models (LLMs) have limitations in processing long contexts, slow inference speed, and high computing costs.
- Existing works rely on compressing long prompt contexts into soft prompts, but they encounter limitations in transferability across different LLMs, especially API-based LLMs.
- This work proposes a Natural Language Prompt Encapsulation (Nano-Capsulator) framework to compress original prompts into NL-formatted Capsule Prompt while maintaining prompt utility and transferability.
- The Nano-Capsulator is optimized using a reward function that interacts with a semantics preserving loss and length constraints.
- Experimental results show that the Capsule Prompt reduces 81.4% of the original length, decreases inference latency up to 4.5Ã—, and saves 80.1% of budget overheads while providing transferability

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.18700v1](https://arxiv.org/abs/2402.18700v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.18700v1](https://browse.arxiv.org/html/2402.18700v1)       |
| Truncated       | False       |
| Word Count       | 7229       |