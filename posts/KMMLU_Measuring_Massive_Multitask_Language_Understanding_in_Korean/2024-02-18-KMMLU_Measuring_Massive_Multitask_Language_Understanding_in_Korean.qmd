
---
title: "KMMLU: Measuring Massive Multitask Language Understanding in Korean"
id: "2402.11548v1"
description: "New Korean benchmark KMMLU tests LLMs, showing need for improvement in Korean language models."
author: Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, Stella Biderman
date: "2024-02-18"
image: "../../img/2402.11548v1/image_1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11548v1/image_1.png)

### Summary:
- The article introduces the KMMLU benchmark, a new Korean benchmark with expert-level multiple-choice questions across 45 subjects, sourced from original Korean exams. It evaluates 26 large language models (LLMs) on KMMLU, finding significant room for improvement. The best publicly available model achieves 50.54% accuracy, far below the average human performance of 62.6%. The study also discusses the importance of localized benchmarks and the creation of CoT exemplars to test models' reasoning capabilities.
- The methodology and results of evaluating LLMs on the KMMLU benchmark are discussed, including the use of diverse prompt techniques and the creation of a more manageable subset called KMMLU Hard. The section also presents an analysis of the performance of 26 LLMs on KMMLU, comparing their performance on Korea-specific questions and the effectiveness of different prompting methods.
- The study explores the use of Chain-of-Thought (CoT) prompting to improve performance on the KMMLU benchmark, highlighting the effectiveness of CoT prompting in improving performance on Korea-specific contexts and the need for Korean pre-training for better performance.
- An overview of the individuals involved in the project, their specific contributions, and the evaluation of various language models is provided. The section also includes a comparison of model performance against human accuracy and an analysis of how language models handle negation, as well as details on the prompting format used for evaluation, CoT exemplar creation, an overview of evaluated models, and the licensing of the KMMLU benchmark.
- Tables are presented showing the 5-shot accuracy using the Direct and CoT methods for various language models across different academic categories.

### Major Findings:
1. The best publicly available model achieves 50.54% accuracy on the KMMLU benchmark, highlighting significant room for improvement in Korean LLMs.
2. Chain-of-Thought (CoT) prompting improves performance on Korea-specific contexts, emphasizing the importance of Korean pre-training for LLMs.
3. The performance of different language models varies across academic categories, demonstrating their effectiveness in understanding and generating content in specific domains.

### Analysis and Critique:
- The study emphasizes the need for localized benchmarks like KMMLU for evaluating large language models and highlights the importance of Korean pre-training for improved performance on Korea-specific questions.
- The findings raise questions about the effectiveness of different prompting methods in enhancing model performance and underscore the need for language models tailored to specific linguistic and cultural contexts.
- The comparison of model performance against human accuracy and their handling of negation provides valuable insights into the capabilities and limitations of language models.
- The tables showing the 5-shot accuracy using the Direct and CoT methods for various academic categories offer a comprehensive comparison of model performance, informing the selection of the most suitable model for different applications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.11548v1](https://arxiv.org/abs/2402.11548v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11548v1](https://browse.arxiv.org/html/2402.11548v1)       |
| Truncated       | True       |
| Word Count       | 23405       |