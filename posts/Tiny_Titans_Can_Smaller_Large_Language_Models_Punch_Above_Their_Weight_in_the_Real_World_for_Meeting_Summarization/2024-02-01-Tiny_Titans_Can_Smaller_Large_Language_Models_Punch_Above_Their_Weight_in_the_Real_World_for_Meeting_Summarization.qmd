
---
title: "Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?"
id: "2402.00841v1"
description: "Smaller LLMs like FLAN-T5 are cost-efficient for real-world industrial deployment."
author: Xue-Yong Fu, Md Tahmid Rahman Laskar, Elena Khasanova, Cheng Chen, Shashi Bhushan TN
date: "2024-02-01"
image: "../../img/2402.00841v1/image_1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.00841v1/image_1.png)

### Summary:
- The study investigates whether smaller, compact Large Language Models (LLMs) are a good alternative to larger LLMs for meeting summarization in real-world industrial environments.
- The study compares the performance of fine-tuned compact LLMs with zero-shot larger LLMs and finds that most smaller LLMs fail to outperform larger zero-shot LLMs in meeting summarization datasets.
- However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs, making it a suitable cost-efficient solution for real-world industrial deployment.

### Major Findings:
1. Most smaller LLMs fail to outperform larger zero-shot LLMs in meeting summarization datasets.
2. FLAN-T5 (780M parameters) performs on par or even better than many zero-shot Larger LLMs, making it a suitable cost-efficient solution for real-world industrial deployment.
3. The study highlights the potential of FLAN-T5 as a cost-efficient solution for real-world industrial deployment.

### Analysis and Critique:
- The study provides valuable insights into the performance of smaller LLMs compared to larger LLMs in meeting summarization tasks.
- The limitations of using smaller LLMs in real-world scenarios are highlighted, especially in datasets with longer meeting lengths.
- The study raises the need for further research to improve the performance of smaller LLMs in datasets with longer meeting lengths and to evaluate the effects of the size of the datasets used for fine-tuning smaller LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.00841v1](https://arxiv.org/abs/2402.00841v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00841v1](https://browse.arxiv.org/html/2402.00841v1)       |
| Truncated       | False       |
| Word Count       | 10022       |