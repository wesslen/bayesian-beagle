
---
title: "A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models"
id: "2401.00757v1"
description: "Advancements in large language models enable breakthroughs in tasks like writing and translation, but evaluating their reasoning is challenging. LogicAsker assesses logical reasoning in LLMs."
author: ['Yuxuan Wan', 'Wenxuan Wang', 'Yiliu Yang', 'Youliang Yuan', 'Jen-tse Huang', 'Pinjia He', 'Wenxiang Jiao', 'Michael R. Lyu']
date: "2024-01-01"
image: "https://browse.arxiv.org/html/2401.00757v1/x1.png"
categories: ['social-sciences', 'hci', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.00757v1/x1.png)

## BiasAsker: Measuring the Bias in Your Chatbot via Asking Questions

### Major Findings
1. **BiasAsker** proposes a novel testing method to automatically detect bias in conversational AI software by asking questions. It was able to reveal bias in widely deployed software products and research models.
2. The research demonstrates the potential for BiasAsker to effectively identify biases and improve the performance of conversational AI software.
3. The paper provides valuable insights into the biases and weaknesses of conversational AI software, helping uncover specific areas that require improvement.

### Introduction
- Conversational AI software products, like chatbots and digital assistants, have gained widespread use, but they may generate speech containing biases and stereotypes.
- Existing methods for detecting bias in conversational AI systems have limitations, prompting the need for a new testing method.

### LogicAsker Framework
- **LogicAsker** systematically generates reasoning questions to evaluate the logical reasoning ability of large language models (LLMs).
- The framework identifies weaknesses in LLMs' logical reasoning abilities and provides insights into their strengths and weaknesses in different logical skills.

### Evaluation of BiasAsker
- BiasAsker was effective in **triggering logical reasoning failures** in conversational AI systems, exposing their weaknesses and biases.
- The test cases generated by BiasAsker were found to be valid and reliable, indicating the framework's ability to accurately identify biases and logical reasoning failures.
- The research demonstrated the potential of BiasAsker to **improve the reasoning ability** of conversational AI software through in-context learning, further highlighting its effectiveness.

### Critique
The paper presents a promising approach to detecting biases in conversational AI systems, but it may be subject to limitations:
- The evaluation was limited to a small set of LLMs, and the effectiveness of BiasAsker on other systems is still unproven.
- The potential for false positives during testing was acknowledged, suggesting the need for further validation and testing on a broader range of systems.
- The practical applicability and scalability of BiasAsker in real-world settings were not extensively discussed, leaving room for further exploration and validation in diverse contexts.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-31       |
| Abstract | [http://arxiv.org/abs/2401.00757v1](http://arxiv.org/abs/2401.00757v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.00757v1](https://browse.arxiv.org/html/2401.00757v1)       |
| Truncated       | False       |
| Word Count       | 10017       |