
---
title: "Investigating Data Contamination for Pre-training Language Models"
id: "2401.06059v1"
description: "Pre-trained language models could be artificially boosted by including evaluation data in their training corpus, impacting their performance."
author: Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo
date: "2024-01-11"
image: "https://browse.arxiv.org/html/2401.06059v1/extracted/5332364/figure/factor.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.06059v1/extracted/5332364/figure/factor.png)

# Investigating Data Contamination for Pre-training Language Models

## Major Findings
- The study explores the impact of **data contamination** at the pre-training stage on language models' performance on downstream tasks.
- Both **text contamination** and **ground-truth contamination** from evaluation data are highlighted as influential factors in the study.
- The study suggests that the **n-gram-based contamination definitions** used in recent reports are inadequate in identifying contamination accurately.

## Introduction
- Concerns arise regarding potential **data contamination** in pre-training corpora, impacting the accuracy of language models' capabilities on scientific analyses.
- Prior LLM reports have explored contamination of evaluation data within the pre-training corpora, primarily focusing on n-gram-based definitions.

## Contamination Definitions
- Existing studies have proposed **n-gram-based definitions** for data contamination, often centred on **direct duplications** present in both training and evaluation datasets.
- The paper explores the limitations of these definitions and their focus on the **evaluation level** analysis, rather than pre-training level analysis.

## Experimental Setup
- Pre-trained a series of GPT-2 models from scratch and evaluated various contamination factors, including text and ground-truth contamination.
- Explored the effects of **repeated contamination** on model performance, finding a U-shaped performance trend with increasing contamination factors.
- Critically analyzed the effects of **filtering out contamination** from the pre-training corpus according to existing definitions, revealing the inadequacy of such definitions in identifying effective contamination.

## Scaling Up with a Larger Model
- Expanded the experiment to incorporate GPT-2-large to assess if the effects of data contamination observed in smaller-scale models persist in larger models.

## Assessment of Evaluation-Level Contamination Analysis
- Examined existing categories for evaluation data contamination using Llama 2's definitions, indicating that models may not be immune to contamination based on such categorical evaluations.

## Critique
- The study primarily focuses on GPT-2 models and does not explore a wider range of language models.
- The limitations of existing contamination definitions are acknowledged, but alternative methods for more accurate detection are not proposed.

In conclusion, the paper offers valuable insights into data contamination's effects on language model capabilities and raises concerns about the adequacy of current contamination definitions. However, the approach's practical applicability and potential solutions to improve contamination detection remain as open research questions.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-27       |
| Abstract | [http://arxiv.org/abs/2401.06059v1](http://arxiv.org/abs/2401.06059v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.06059v1](https://browse.arxiv.org/html/2401.06059v1)       |
| Truncated       | False       |
| Word Count       | 9968       |