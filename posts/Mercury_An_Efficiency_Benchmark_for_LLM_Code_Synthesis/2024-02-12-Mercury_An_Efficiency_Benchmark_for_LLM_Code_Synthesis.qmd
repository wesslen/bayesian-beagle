
---
title: "Mercury: An Efficiency Benchmark for LLM Code Synthesis"
id: "2402.07844v1"
description: "Mercury is a new benchmark for evaluating code efficiency of Large Language Models."
author: Mingzhe Du, Anh Tuan Luu, Bin Ji, See-Kiong Ng
date: "2024-02-12"
image: "../../img/2402.07844v1/image_1.png"
categories: ['architectures', 'programming', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.07844v1/image_1.png)

### Summary:
- Mercury is the first benchmark for assessing the code efficiency of Large Language Models (LLMs) for code synthesis tasks.
- The benchmark consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation.
- Mercury integrates a novel metric Beyond@K to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis.

### Major Findings:
1. LLMs demonstrate the remarkable capability to generate functionally correct code.
2. There exists a substantial gap in the efficiency output of LLMs, underscoring a new frontier for LLM research and development.
3. Mercury distinguishes itself by including a dedicated test case generator for each task, empowering the automatic production of unlimited test cases.

### Analysis and Critique:
- Existing NL2Code benchmarks primarily focus on functional correctness but overlook code efficiency, which is critical for real-world applicability.
- The major limitation of existing NL2Code benchmarks is the lack of code efficiency evaluation.
- The distribution of code runtime in the real world is more intricate, which may need more solution samples to support more precise modeling.

The article provides a comprehensive overview of the Mercury benchmark, highlighting its significance in evaluating the code efficiency of LLMs for code synthesis tasks. The findings reveal the importance of code efficiency in addition to functional correctness and emphasize the need for further research and development in this area. However, the article also acknowledges the limitations of the benchmark and the challenges associated with evaluating code efficiency in real-world scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.07844v1](https://arxiv.org/abs/2402.07844v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07844v1](https://browse.arxiv.org/html/2402.07844v1)       |
| Truncated       | False       |
| Word Count       | 14539       |