
---
title: "Why Would You Suggest That? Human Trust in Language Model Responses"
id: "2406.02018v1"
description: "TL;DR: Explanations in LLM responses boost user trust, but only when comparing multiple responses. Trust is equal when responses are shown alone."
author: Manasi Sharma, Ho Chit Siu, Rohan Paleja, Jaime D. Pe√±a
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02018v1/extracted/5642220/figures/likert_results.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02018v1/extracted/5642220/figures/likert_results.png)

### Summary:

The study investigates how the framing and presence of explanations in model responses affect user trust and model performance in the context of human-AI collaboration. The research focuses on the open-ended News Headline Generation task from the LaMP benchmark. The findings suggest that adding an explanation to justify the model's reasoning significantly increases self-reported user trust when users can compare various responses. However, this effect disappears when users are shown responses independently, suggesting that humans trust all model responses, including deceptive ones, when shown in isolation. The position and faithfulness of these explanations are also important factors in user trust. The study urges future research to delve deeper into the nuanced evaluation of trust in human-machine teaming systems.

### Major Findings:

1. The presence and framing of an explanation in a model recommendation significantly impact user trust.
2. The truthfulness of the model explanation of a model recommendation impacts user trust.
3. There are no tradeoffs between types of explanations that maximize user trust and model performance.

### Analysis and Critique:

1. The study relies on self-reported trust, which may not accurately reflect actual trust levels.
2. The study does not consider the impact of user familiarity with AI or the specific task on trust.
3. The study does not explore the potential long-term effects of exposure to explanations on user trust.
4. The study does not consider the potential for users to become over-reliant on explanations, leading to decreased trust when explanations are not provided.
5. The study does not consider the potential for explanations to be used to manipulate user trust.
6. The study does not explore the potential for explanations to be used to justify incorrect or biased model outputs.
7. The study does not consider the potential for explanations to be used to obscure the limitations of the model.
8. The study does not explore the potential for explanations to be used to justify the use of models in high-stakes decision-making contexts.
9. The study does not consider the potential for explanations to be used to justify the use of models in contexts where they may not be appropriate.
10. The study does not explore the potential for explanations to be used to justify the use of models

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02018v1](https://arxiv.org/abs/2406.02018v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02018v1](https://browse.arxiv.org/html/2406.02018v1)       |
| Truncated       | False       |
| Word Count       | 8165       |