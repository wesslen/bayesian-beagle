---
title: "Inter-annotator Agreement (IAA) in Prodigy"
subtitle: "Calculating IAA in binary text classification with two annotators."
author: "Ryan Wesslen"
date: "2022-09-27"
categories: [prodigy, textcat, inter-rater relilability, multi-user sessions]
format:
  html:
    code-overflow: wrap
---

TODO: add preview image

## Multi-user sessions in Prodigy 

Since v1.7.0 was released, Prodigy has offered [multi-user sessions](https://prodi.gy/docs/api-web-app#multi-user-sessions) within the same Prodigy instance. This functionality enables dividing up Prodigy annotations across different annotators when saving annotations to the same dataset and performing an identical task.

TODO: 2-3 sentences on example, link to past TECH issues

```python
# news_headlines.jsonl
{'text': 'Uber’s Lesson: Silicon Valley’s Start-Up Machine Needs Fixing',
 'meta': {'source': 'The New York Times'}
}
```

TODO: add section on data input/formatting

```{.bash filename="Terminal"}
python -m prodigy textcat.manual news_textcat news_headlines.jsonl --label TECHNOLOGY

Using 1 label(s): TECHNOLOGY

✨  Starting the web server at http://localhost:8080 ...
Open the app in your browser and start annotating!
```

<br>

:::{.column-body-outset}
![](img/IAA-Jordan.png) 

To create a custom named session, add `?session=xxx` to the annotation app URL. For example, annotator Jordan may access a running Prodigy project via `http://localhost:8080/?session=jordan`. The example shows running the [`textcat.manual`](https://prodi.gy/docs/recipes/#textcat-manual) Prodigy recipe but this works for any Prodigy recipe. This will enable use to track each annotator so we can calculate inter-annotator agreement.
:::

Internally, this will request and send back annotations with a session identifier consisting of the current dataset name and the session ID – for example, `textcat-jordan`. Every time annotator Jordan labels examples for this dataset, their annotations will be associated with this session identifier.

Let's say that in addition to Jordan, we also asked a second annotator, Alex, do both label 20 records in the dataset to determine whether the news headlines are technology-related or not. We provide both their respective URL and have them complete their annotations.

To pull their annotations, we'll use Prodigy's `get_dataset_examples()` function:

```{python}
from prodigy.components.db import connect
import pprint

db = connect()
examples = db.get_dataset_examples("news_textcat")

pprint.pprint(examples[0])
```

::: callout-hint
TODO: deprecation of `get_dataset`
:::

Since we're interested in text classification annotations, we'll focus on the `"answer"` values comparing those that are `"reject"` versus `"accept"`.

```{python}
import pandas as pd

# keep only the "accept" and "reject" answers
anno = [eg for eg in examples if eg["answer"] in ["accept", "reject"]]

# convert to a dataframe
df = pd.DataFrame(anno)

df_annotations = df.pivot(
    index=['_input_hash'], 
    columns='_session_id', 
    values='answer'
)

df_annotations.head(n=1)
```

## Cohen's Kappa

TODO: 2-3 sentences on Cohen's Kappa.

```{python}
from sklearn.metrics import cohen_kappa_score

kappa = cohen_kappa_score(
    df_annotations['news_textcat-alex'], 
    df_annotations['news_textcat-jordan'], 
    labels=None, 
    weights=None
)

print(kappa)
```

So we've found a Cohen's Kappa of `0.737`, which is fairly high. 

TODO: Discussion on interpreting Cohen's Kappa

::: callout-important
In practice, you may have many more complexities like saving different annotations to different datasets, multi-class classification, [span-based tasks like named entity recognition (NER) or spancat](https://support.prodi.gy/t/proper-way-to-calculate-inter-annotator-agreement-for-spans-ner/5760), or handling for more than two annotators. This use case is the simplest case; however, I hope to create more advanced use cases in the future.
:::