
---
title: "MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding"
id: "2406.14515v1"
description: "MMBench-Video: New Benchmark for Video Understanding with LVLMs."
author: Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, Kai Chen
date: "2024-06-20"
image: "https://browse.arxiv.org/html/2406.14515v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.14515v1/x1.png)

### Summary:
MMBench-Video is a new quantitative benchmark designed to rigorously evaluate Large Vision-Language Models (LVLMs) in video understanding. The benchmark incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. MMBench-Video is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy. The evaluation code of MMBench-Video will be integrated into VLMEvalKit.

### Major Findings:
1. MMBench-Video addresses the limitations of traditional VideoQA benchmarks by incorporating lengthy videos and free-form questions, providing a more comprehensive evaluation of LVLMs' proficiency in video understanding.
2. The benchmark is designed to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy.
3. MMBench-Video employs GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations.

### Analysis and Critique:
1. While MMBench-Video offers a more comprehensive evaluation of LVLMs, it may not encompass every video topic and fine-grained capability, potentially limiting its ability to reflect the video understanding capabilities of VLMs in specific tasks or scenarios.
2. The use of GPT-4 for automated assessment, while demonstrating superior accuracy and robustness, may introduce biases or limitations inherent in the model's design and training data.
3. The benchmark's reliance on YouTube videos may limit its generalizability to other video platforms or types of video content.
4. The benchmark's focus on temporal reasoning skills may overlook other important aspects of video understanding, such as spatial reasoning or object recognition.
5. The benchmark's use of free-form questions may introduce variability in the difficulty and complexity of the questions, potentially affecting the reliability and validity of the evaluation.
6. The benchmark's integration into VLMEvalKit may limit its accessibility to researchers who do not have access to this toolkit.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.14515v1](https://arxiv.org/abs/2406.14515v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.14515v1](https://browse.arxiv.org/html/2406.14515v1)       |
| Truncated       | False       |
| Word Count       | 8501       |