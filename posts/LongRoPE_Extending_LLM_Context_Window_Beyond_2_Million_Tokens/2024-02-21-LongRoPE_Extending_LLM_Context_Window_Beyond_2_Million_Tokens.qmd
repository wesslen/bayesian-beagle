
---
title: "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens"
id: "2402.13753v1"
description: "LongRoPE extends context window of large language models to 2048k tokens with minimal fine-tuning."
author: Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao Yang
date: "2024-02-21"
image: "https://browse.arxiv.org/html/2402.13753v1/extracted/5419364/final_ppl.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13753v1/extracted/5419364/final_ppl.png)

### Summary:
- The paper introduces LongRoPE, a method that extends the context window of pre-trained LLMs to an impressive 2048k tokens, while maintaining performance at the original short context window.
- LongRoPE is achieved through three key innovations: identifying and exploiting two forms of non-uniformities in positional interpolation, introducing a progressive extension strategy, and readjusting LongRoPE on 8k length to recover the short context window performance.
- Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of LongRoPE.

### Major Findings:
1. LongRoPE extends the context window of pre-trained LLMs to an impressive 2048k tokens while maintaining performance at the original short context window.
2. The method effectively identifies and exploits two forms of non-uniformities in positional interpolation, leading to improved performance.
3. LongRoPE introduces a progressive extension strategy and readjusts the context window to recover the short context window performance.

### Analysis and Critique:
- The paper presents a novel method for extending the context window of pre-trained LLMs, addressing the limitations of existing approaches.
- The method demonstrates effectiveness in maintaining low perplexity and delivering comparable accuracy on standard benchmarks designed within the 4096 context window.
- The search algorithm used in LongRoPE is efficient and can find high-quality non-uniform RoPE rescale factors, significantly reducing the validation perplexity.
- The paper provides detailed insights into the fine-tuning process and the search efficiency, contributing to the understanding of the method's practical implementation.

Overall, the paper presents a significant advancement in the field of language models, offering a method that extends the context window of LLMs to an unprecedented length while maintaining performance. However, further research may be needed to explore the scalability and generalizability of the method to other LLMs and tasks. Additionally, the potential societal consequences and broader impacts of the work should be carefully considered.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.13753v1](https://arxiv.org/abs/2402.13753v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13753v1](https://browse.arxiv.org/html/2402.13753v1)       |
| Truncated       | False       |
| Word Count       | 9151       |