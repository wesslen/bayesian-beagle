
---
title: "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive"
id: "2402.13228v1"
description: "DPO improves large language model performance, DPOP outperforms DPO, achieves state-of-the-art open-source performance."
author: Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, Colin White
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13228v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13228v1/x1.png)

### **Summary:**
- Direct Preference Optimization (DPO) is effective at improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarization, and alignment.
- The standard DPO loss can lead to a reduction of the model’s likelihood of the preferred completions, especially in datasets with small edit distances between completions.
- DPO-Positive (DPOP) is introduced as a new loss function and training procedure to avoid this failure mode and significantly outperforms DPO across a wide variety of datasets and downstream tasks.

### Major Findings:
1. The standard DPO loss can lead to a reduction of the model’s likelihood of the preferred completions, especially in datasets with small edit distances between completions.
2. DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks.
3. Smaug-72B achieves state-of-the-art open-source performance and becomes the first open-source LLM to surpass an average accuracy of 80%.

### Analysis and Critique:
- The article provides valuable insights into the failure mode of DPO and introduces an effective solution in the form of DPOP.
- The limitations of the study include the lack of a full ablation study on the 72B model and the need for further verification of DPOP's effectiveness on more datasets, especially non-English datasets.
- The article's impact is expected to be positive, with the potential to advance LLMs' abilities in mathematical reasoning and contribute to AI safety efforts.

Overall, the article presents a significant contribution to the field of preference optimization for language models and opens up new possibilities for improving LLM performance. However, further research is needed to address the identified limitations and expand the application of DPOP to diverse datasets.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.13228v1](https://arxiv.org/abs/2402.13228v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13228v1](https://browse.arxiv.org/html/2402.13228v1)       |
| Truncated       | False       |
| Word Count       | 10240       |