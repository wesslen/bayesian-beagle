
---
title: "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models"
id: "2402.17226v1"
description: "LLMs excel in objective tasks, struggle in subjective tasks. RiC method improves subjective task performance."
author: Xiaolong Wang, Yile Wang, Yuanchi Zhang, Fuwen Luo, Peng Li, Maosong Sun, Yang Liu
date: "2024-02-27"
image: "https://browse.arxiv.org/html/2402.17226v1/x1.png"
categories: ['hci', 'social-sciences', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.17226v1/x1.png)

### Summary:
- Large Language Models (LLMs) excel in objective tasks but struggle with subjective tasks like metaphor recognition and dark humor detection.
- RiC (Reasoning in Conversation) proposes using dialogue simulation to solve subjective tasks, aiming to mine useful contextual information.
- RiC significantly improves LLM performance on subjective tasks compared to baselines.

### Major Findings:
1. LLMs perform well in objective tasks but poorly in subjective tasks like metaphor recognition and dark humor detection.
2. RiC, which uses dialogue simulation, significantly improves LLM performance on subjective tasks.
3. RiC outperforms various baselines, including GPT-4, ChatGPT, and OpenChat, across twelve tasks.

### Analysis and Critique:
- Subjective tasks pose challenges for LLMs due to the need for interpretation and emotional response.
- Chain-of-Thought (CoT) style prompting, effective for objective tasks, is not as effective for subjective tasks.
- RiC's effectiveness is demonstrated through experimental results, but potential biases or limitations in the evaluation methods are not discussed.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.17226v1](https://arxiv.org/abs/2402.17226v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17226v1](https://browse.arxiv.org/html/2402.17226v1)       |
| Truncated       | False       |
| Word Count       | 2142       |