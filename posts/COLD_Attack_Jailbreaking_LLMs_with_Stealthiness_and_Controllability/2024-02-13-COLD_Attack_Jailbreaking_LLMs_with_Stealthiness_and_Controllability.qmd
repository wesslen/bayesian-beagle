
---
title: "COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability"
id: "2402.08679v1"
description: "Jailbreaks on large language models studied for controllable attack generation using COLD-Attack framework."
author: Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu
date: "2024-02-13"
image: "../../../bayesian-beagle.png"
categories: ['security', 'production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
The article introduces the COLD-Attack framework, a novel method for automating the generation of controllable adversarial attacks on Large Language Models (LLMs). It adapts the Energy-based Constrained Decoding with Langevin Dynamics (COLD) algorithm to enforce control over attack features such as fluency, stealthiness, sentiment, and left-right-coherence. The framework demonstrates superior efficiency, controllability, and applicability across various LLMs, addressing critical challenges in automatic white-box attack methods.

### Major Findings:
1. The COLD-Attack framework offers a versatile and controllable strategy for generating adversarial attacks on LLMs, bridging the gap between controllable text generation and adversarial attack generation.
2. Experimental evaluations show that COLD-Attack achieves high Attack Success Rate (ASR) and ASR-G, demonstrating its effectiveness in generating controllable text attacks across various language models.
3. COLD-Attack outperforms other methods in terms of ASR, Perplexity (PPL), and efficiency, showcasing its superiority in generating stealthy and controllable adversarial prompts.

### Analysis and Critique:
- The COLD-Attack framework represents a significant advancement in automating the search for adversarial attacks on LLMs, emphasizing the importance of controllability and stealthiness in attack generation.
- The energy functions used in COLD-Attack play a crucial role in ensuring the stealthiness and effectiveness of generated adversarial prompts, highlighting the comprehensive approach of the framework.
- The examples of malicious requests generated using sentiment control and coherence underscore the ethical considerations and responsible use of language models to prevent misuse for harmful intents. Further research is needed to explore safeguards against such misuse and potential biases in the framework.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.08679v1](https://arxiv.org/abs/2402.08679v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08679v1](https://browse.arxiv.org/html/2402.08679v1)       |
| Truncated       | True       |
| Word Count       | 25467       |