
---
title: "Covert Bias: The Severity of Social Views' Unalignment Towards Implicit and Explicit Opinion"
id: "2408.08212v1"
description: "LLMs struggle with implicit bias, favor explicit opinions, and need uncertainty markers for reliability."
author: Abeer Aldayel, Areej Alokaili, Rehab Alahmadi
date: "2024-08-15"
image: "https://browse.arxiv.org/html/2408.08212v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.08212v1/x1.png)

### Summary:

This study investigates the severity of bias in large language models (LLMs) by examining the discrepancy between implicit and explicit opinions. The authors focus on two downstream tasks: hate speech and stance detection, using datasets related to misogyny and religious bigotry. They evaluate the performance of LLMs in identifying implicit and explicit opinions, finding a general tendency of bias toward explicit opinions of opposing stances. The biased models generate more cautious responses using uncertainty phrases compared to unaligned (zero-shot) base models. The direct, incautious responses of the unaligned models suggest a need for further refinement of decisiveness by incorporating uncertainty markers to enhance their reliability, especially on socially nuanced topics with high subjectivity.

### Major Findings:

1. The study reveals a discrepancy in LLM performance in identifying implicit and explicit opinions, with a general tendency of bias toward explicit opinions of opposing stances.
2. Biased models generate more cautious responses using uncertainty phrases compared to unaligned (zero-shot) base models.
3. The direct, incautious responses of the unaligned models suggest a need for further refinement of decisiveness by incorporating uncertainty markers to enhance their reliability, especially on socially nuanced topics with high subjectivity.

### Analysis and Critique:

* The study focuses on two specific topics, misogyny and religious bigotry, which may not be representative of all forms of bias.
* The use of only two types of open-sourced models, LLMs, in the model selection may limit the generalizability of the findings.
* The subjective nature of the hate speech task may introduce bias in the evaluation process.
* The study does not address the potential impact of other factors, such as the size and diversity of the training data, on the severity of bias in LLMs.
* The authors acknowledge the limitations of their study and suggest that a more diversified set of topics or more bias types would be an area for future study.
* The study highlights the need for further research on the impact of implicit bias on LLMs and the importance of incorporating uncertainty markers to enhance their reliability.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.08212v1](https://arxiv.org/abs/2408.08212v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.08212v1](https://browse.arxiv.org/html/2408.08212v1)       |
| Truncated       | False       |
| Word Count       | 5173       |