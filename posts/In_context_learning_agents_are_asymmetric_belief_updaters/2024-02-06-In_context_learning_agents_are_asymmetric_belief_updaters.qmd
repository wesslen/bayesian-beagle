
---
title: "In-context learning agents are asymmetric belief updaters"
id: "2402.03969v1"
description: "LLMs learn asymmetrically from outcomes, influenced by problem framing."
author: Johannes A. Schubert, Akshay K. Jagadish, Marcel Binz, Eric Schulz
date: "2024-02-06"
image: "../../img/2402.03969v1/image_1.png"
categories: ['architectures', 'production', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.03969v1/image_1.png)

### **Summary:**
- The study investigates the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology.
- LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones.
- The effect reverses when learning about counterfactual feedback and disappears when no agency is implied.

### Major Findings:
1. LLMs exhibit an optimism bias, learning more from positive than from negative prediction errors.
2. The bias reverses when learning about the value of the unchosen option, and disappears when no agency is implied.
3. Idealized in-context learning agents trained specifically to solve 2AFC tasks using meta-reinforcement learning show similar patterns.

### Analysis and Critique:
- The study provides valuable insights into the in-context learning dynamics of LLMs and idealized agents.
- The findings have implications for understanding how learning occurs in both natural and artificial agents.
- The study's methodological approach of fitting simpler computational models to the behavior of LLMs provides a tool for explainable machine learning.

Overall, the study contributes to our understanding of in-context learning and its implications for real-world applications. However, the study's external validity and causal links between observed patterns and behavior need further investigation.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.03969v1](https://arxiv.org/abs/2402.03969v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.03969v1](https://browse.arxiv.org/html/2402.03969v1)       |
| Truncated       | False       |
| Word Count       | 15607       |