
---
title: "Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers"
id: "2401.01974v1"
description: "Visual reasoning with large language models can address current limitations by decomposing tasks and leveraging abstract routines."
author: ['Aleksandar StaniÄ‡', 'Sergi Caelles', 'Michael Tschannen']
date: "2024-01-03"
image: "https://browse.arxiv.org/html/2401.01974v1/extracted/5328832/figures/refcoco_skiers_blur.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.01974v1/extracted/5328832/figures/refcoco_skiers_blur.png)

# Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers

## Major Takeaways
- Large language models (LLMs) demonstrate strong performance on compositional visual question answering, visual grounding, and video temporal reasoning tasks. However, their performance heavily relies on human-engineered in-context examples (ICEs) in the prompt.
- The presented framework introduces spatially and temporally abstract routines and leverages a small number of labeled examples to automatically generate in-context examples, thus avoiding the need for human-created ICEs.
- The framework leads to consistent gains in performance, makes LLMs as controllers setup more robust, and removes the need for human engineering of ICEs across various visual reasoning tasks.

## Introduction
Compositional visual question answering necessitates understanding visual information in images and the structure of the question, posing a challenge for end-to-end neural networks, especially in tasks requiring compositional reasoning, spatial reasoning, and counting. A promising alternative involves LLMs as controllers, orchestrating a set of visual tools to decompose tasks into subtasks and solve them by utilizing abstract routines.

## LLMs as programmers for visual reasoning framework
- **Background:** The ViperGPT approach uses an LLM (Codex) and a tools API to generate scripts to solve visual queries, with the prompt consisting of API functions, docstrings, and query-code examples of their use.
- **Abstract API through visual routines:** The framework introduces spatially and temporally abstract routines to reduce the LLM's burden of strong spatial and temporal reasoning.
- **Automatic generation of in-context examples:** Using a few labeled examples, the framework generates query-code examples in a zero-shot manner, thereby eliminating human engineering of ICEs.
- **Self-correction:** The framework enables LLMs to perform self-debugging and self-tuning to correct generated code when execution fails without any ground truth labels.

## Experiments
- **Tasks:** Evaluation is conducted on datasets such as RefCOCO, RefCOCO+, GQA, and NExT-QA, assessing a diverse set of capabilities including visual grounding, compositional image question answering, and video temporal reasoning.
- **Vision and Language Models:** The framework uses a code instruction-tuned version of PaLM2 for code generation and various vision models for object detection, depth estimation, and image captioning.
- **Self-Correction:** Results show that self-tuning, dynamic object detector thresholds, and generated in-context examples lead to improved performance across tasks.

## Conclusion
The framework showcased consistent performance gains while removing the need for human engineering of ICEs and demonstrating the potential of LLMs as controllers for visual reasoning.

## Critique and Future Work
While the framework shows promise, further research is needed to optimize the Abstract API routines and automate prompt engineering with natural language dataset specifications. Additionally, the creation of better benchmarks for evaluating compositional visual reasoning is needed to maximize the framework's potential. There should also be continued exploration of LLMs' self-correction capabilities.


## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [http://arxiv.org/abs/2401.01974v1](http://arxiv.org/abs/2401.01974v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.01974v1](https://browse.arxiv.org/html/2401.01974v1)       |
| Truncated       | False       |
| Word Count       | 13256       |