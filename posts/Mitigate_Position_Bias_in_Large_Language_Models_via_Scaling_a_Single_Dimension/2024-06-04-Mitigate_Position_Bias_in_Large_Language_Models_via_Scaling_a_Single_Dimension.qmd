
---
title: "Mitigate Position Bias in Large Language Models via Scaling a Single Dimension"
id: "2406.02536v1"
description: "Position bias in LLMs harms accuracy; this paper proposes a method to mitigate it, improving performance by up to 15.2% in various tasks."
author: Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02536v1/x1.png"
categories: ['architectures', 'prompt-engineering', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02536v1/x1.png)

### Summary:

This paper explores the position bias in large language models (LLMs), a phenomenon where the placement of key information in different positions of a prompt significantly affects accuracy. The authors identify that, in addition to position embeddings, causal attention masks also contribute to position bias by creating position-specific hidden states. Based on these insights, they propose a method to mitigate position bias by scaling these positional hidden states. Experiments on various models and tasks demonstrate the effectiveness and generalizability of their approach, improving performance by up to 15.2% by modifying just one dimension of hidden states.

### Major Findings:

1. Position bias in LLMs is not only caused by position embeddings but also by causal attention masks, which create position-specific hidden states.
2. The proposed method for mitigating position bias by scaling positional hidden states effectively improves performance across various models and tasks.
3. The method can improve performance by up to 15.2% by modifying just one dimension of hidden states.

### Analysis and Critique:

The paper provides a novel approach to addressing position bias in LLMs, which is a significant issue in long-context scenarios. The authors' method of scaling positional hidden states is a simple yet effective solution that can be applied to various models and tasks. However, the paper does not discuss the potential impact of this method on the overall performance of LLMs, such as its effect on the models' ability to handle short-context tasks or its compatibility with other techniques for improving LLMs. Additionally, the paper does not provide a detailed analysis of the limitations and potential biases of the proposed method. Further research is needed to address these questions and to evaluate the method's performance in more diverse and complex scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02536v1](https://arxiv.org/abs/2406.02536v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02536v1](https://browse.arxiv.org/html/2406.02536v1)       |
| Truncated       | False       |
| Word Count       | 8059       |