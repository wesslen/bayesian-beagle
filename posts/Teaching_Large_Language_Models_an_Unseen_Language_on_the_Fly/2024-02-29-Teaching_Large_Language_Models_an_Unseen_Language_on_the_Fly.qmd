
---
title: "Teaching Large Language Models an Unseen Language on the Fly"
id: "2402.19167v1"
description: "DiPMT++ enables LLMs to learn new languages via prompting, e.g., 16 BLEU for Chinese-to-Zhuang translation."
author: Chen Zhang, Xiao Liu, Jiuheng Lin, Yansong Feng
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19167v1/x1.png"
categories: ['architectures', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19167v1/x1.png)

### Summary:

- Existing large language models (LLMs) struggle to support numerous low-resource languages, particularly the extremely low-resource ones.
- A new framework, DiPMT++, is introduced for adapting LLMs to unseen languages by in-context learning.
- Using a dictionary and only 5K parallel sentences, DiPMT++ significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation.
- The framework has practical utility in aiding humans to translate completely unseen languages, contributing to the preservation of linguistic diversity.

### Major Findings:
1. DiPMT++ is a framework that adapts LLMs to unseen languages through in-context learning.
2. Using a dictionary and 5K parallel sentences

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19167v1](https://arxiv.org/abs/2402.19167v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19167v1](https://browse.arxiv.org/html/2402.19167v1)       |
| Truncated       | False       |
| Word Count       | 8344       |