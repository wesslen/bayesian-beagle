
---
title: "NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models"
id: "2402.09773v1"
description: "Structured pruning compresses Large Language Models for efficient deployment on resource-constrained hardware. NutePrune method enhances performance."
author: Shengrui Li, Xueting Han, Jing Bai
date: "2024-02-15"
image: "https://browse.arxiv.org/html/2402.09773v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.09773v1/x1.png)

### **Summary:**
- Large Language Models (LLMs) present challenges for deployment on resource-constrained hardware.
- Structured pruning offers an effective means to compress LLMs, reducing storage costs and enhancing inference speed.
- NutePrune is a novel efficient progressive structured pruning method for LLMs, leveraging numerous teachers with varying capacities to guide the pruned model.

### **Major Findings:**
1. NutePrune retains 97.17% of the performance of the original model at 20% sparsity and 95.07% at 25% sparsity.
2. NutePrune achieves higher model sparsity without significant performance decline on limited data through progressive knowledge distillation.
3. NutePrune only loads one intact model and switches it between teacher and student modes by incorporating various masks and LoRA modules, introducing no extra memory cost.

### **Analysis and Critique:**
- NutePrune demonstrates effectiveness in compressing LLMs to higher sparsity levels, but the evaluation is limited to LLaMA-7B and LLaMA-13B models.
- The study acknowledges the limitation of not evaluating other model families due to limited computation resources.
- The authors highlight the need for future work to explore pruning with extensive data and evaluate other model families.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.09773v1](https://arxiv.org/abs/2402.09773v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.09773v1](https://browse.arxiv.org/html/2402.09773v1)       |
| Truncated       | False       |
| Word Count       | 6177       |