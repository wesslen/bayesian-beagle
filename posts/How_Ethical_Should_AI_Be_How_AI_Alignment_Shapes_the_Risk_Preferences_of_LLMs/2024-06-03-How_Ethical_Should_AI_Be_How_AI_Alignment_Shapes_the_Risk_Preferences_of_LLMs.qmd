
---
title: "How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs"
id: "2406.01168v1"
description: "AI alignment shifts LLMs towards risk aversion, potentially causing underinvestment in finance. Balance is key."
author: Shumiao Ouyang, Hayong Yun, Xingjian Zheng
date: "2024-06-03"
image: "../../img/2406.01168v1/image_1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../img/2406.01168v1/image_1.png)

**Summary:**

This study investigates the risk preferences of Large Language Models (LLMs) and the impact of aligning them with human ethical standards on their economic decision-making. The researchers analyzed 30 LLMs and found a range of inherent risk profiles, from risk-averse to risk-seeking. They then examined how different types of AI alignment, focusing on harmlessness, helpfulness, and honesty, alter these base risk preferences. The results show that alignment significantly shifts LLMs towards risk aversion, with models incorporating all three ethical dimensions exhibiting the most conservative investment behavior. However, while some alignment can improve investment forecast accuracy, excessive alignment can lead to overly cautious predictions, potentially causing underinvestment.

**Major Findings:**

1. LLMs exhibit a range of inherent risk profiles, from risk-averse to risk-seeking.
2. Aligning LLMs with human ethical

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.01168v1](https://arxiv.org/abs/2406.01168v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.01168v1](https://browse.arxiv.org/html/2406.01168v1)       |
| Truncated       | True       |
| Word Count       | 29132       |