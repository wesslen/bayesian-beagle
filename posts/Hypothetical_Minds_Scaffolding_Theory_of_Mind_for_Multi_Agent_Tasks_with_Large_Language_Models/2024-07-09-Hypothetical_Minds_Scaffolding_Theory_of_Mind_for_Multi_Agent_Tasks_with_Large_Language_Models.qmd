
---
title: "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models"
id: "2407.07086v1"
description: "Hypothetical Minds agent, leveraging LLMs, improves MARL performance in diverse domains, highlighting the value of hypothesis evaluation and refinement."
author: Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, Nick Haber
date: "2024-07-09"
image: "../../img/2407.07086v1/image_1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2407.07086v1/image_1.png)

**Summary:**

The paper introduces Hypothetical Minds, an autonomous agent that leverages large language models (LLMs) to handle the challenges of multi-agent reinforcement learning (MARL). The agent features a cognitively-inspired architecture with modular components for perception, memory, and hierarchical planning over two levels of abstraction. The Theory of Mind module is a key component that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language, evaluating, and iteratively refining these hypotheses based on their predictive accuracy. The paper demonstrates that Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark.

**Major Findings:**

1. Hypothetical Minds out

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.07086v1](https://arxiv.org/abs/2407.07086v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.07086v1](https://browse.arxiv.org/html/2407.07086v1)       |
| Truncated       | True       |
| Word Count       | 27806       |