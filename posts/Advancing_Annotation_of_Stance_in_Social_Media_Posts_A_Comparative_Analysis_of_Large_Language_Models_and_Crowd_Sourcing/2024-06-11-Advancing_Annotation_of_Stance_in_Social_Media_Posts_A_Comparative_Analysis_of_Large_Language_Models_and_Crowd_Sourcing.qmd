
---
title: "Advancing Annotation of Stance in Social Media Posts: A Comparative Analysis of Large Language Models and Crowd Sourcing"
id: "2406.07483v1"
description: "LLMs' stance annotation accuracy depends on text's explicitness, often mirroring human performance."
author: Mao Li, Frederick Conrad
date: "2024-06-11"
image: "https://browse.arxiv.org/html/2406.07483v1/extracted/5659896/figure/distribution.png"
categories: ['production', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.07483v1/extracted/5659896/figure/distribution.png)

### Summary:

This study investigates the performance of eight open-source and proprietary Large Language Models (LLMs) in annotating the stance expressed in social media posts, benchmarking their performance against human annotators' judgments. The explicitness of text expressing a stance plays a critical role in how faithfully LLMs' stance judgments match humans'. The study argues that LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement. The study concludes with recommendations for a comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions.

### Major Findings:

1. The explicitness of text expressing a stance plays a critical role in how faithfully LLMs' stance judgments match humans'.
2. LLMs perform well when human annotators do, and when LLMs fail, it often corresponds to situations in which human annotators struggle to reach an agreement.
3. A comprehensive approach that combines the precision of human expertise with the scalability of LLM predictions is recommended.

### Analysis and Critique:

- The study does not provide a detailed methodology for the comparison of LLMs and human annotators, making it difficult to assess the validity of the findings.
- The study does not discuss the potential biases of LLMs and human annotators, which could impact the accuracy of stance annotation.
- The study does not provide a clear definition of "explicitness" and how it was measured, making it difficult to understand the relationship between explicitness and LLM performance.
- The study does not discuss the potential limitations of using LLMs for stance annotation, such as the lack of contextual understanding and the potential for overfitting to training data.
- The study does not discuss the potential ethical implications of using LLMs for stance annotation, such as the potential for bias and the impact on privacy.
- The study does not provide a clear recommendation for how to improve the accuracy and comprehensiveness of automated stance detection, beyond combining human expertise and LLM predictions.
- The study does not discuss the potential impact of LLMs on the field of social media analysis and the potential for LLMs to be used for malicious purposes.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.07483v1](https://arxiv.org/abs/2406.07483v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.07483v1](https://browse.arxiv.org/html/2406.07483v1)       |
| Truncated       | False       |
| Word Count       | 7463       |