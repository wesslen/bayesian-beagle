
---
title: "GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks"
id: "2402.07197v1"
description: "LLMs and GMs combined for pre-defined and open-ended tasks in graph domain."
author: Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi
date: "2024-02-11"
image: "https://browse.arxiv.org/html/2402.07197v1/x1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.07197v1/x1.png)

### Summary:
The article proposes a novel framework, GraphTranslator, to align graph models (GMs) with Large Language Models (LLMs) for open-ended tasks. The proposed framework introduces a Translator module to eliminate the modality gap between GM and LLM and a Producer module to generate alignment data. The experimental results demonstrate the effectiveness of GraphTranslator on zero-shot node classification and graph question answering tasks.

### Major Findings:
1. The proposed GraphTranslator effectively improves the results of zero-shot node classification on real-world datasets, outperforming several baselines.
2. GraphTranslator exhibits a notable improvement in recall for positive instances, demonstrating its capability to accurately identify positive instances in zero-shot classification tasks.
3. Preliminary graph question answering experiments reveal the potential of GraphTranslator to extract, explain, and reason the graph information, showcasing its multi-turn dialogue capability.

### Analysis and Critique:
- The Producer module plays a pivotal role in dictating the overall quality of the Translator model. Future work should focus on including more topology information encoded in node embeddings and employing larger-scale LLMs to improve the quality of generated text descriptions.
- The experimental evaluation lacks a complete and quantitative assessment of model capabilities in open-ended tasks. Future work should develop an evaluation dataset and devise corresponding metrics for a comprehensive evaluation.

### Prompt Design:
The prompts for the Taobao and ArXiv datasets are designed to force the model to answer specific questions related to user interests, friends' interests, and friendship analysis. The prompts are carefully crafted to evaluate the model's capabilities in understanding, explaining, and reasoning graph information.

### Further Experiment:
The two-phase training of GraphTranslator is crucial for enabling LLM to comprehend the graph information, ultimately leading to optimal results. The comparison with its variants, "Stage 1 Only" and "Stage 2 Only," validates the effectiveness of the proposed training strategies.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.07197v1](https://arxiv.org/abs/2402.07197v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.07197v1](https://browse.arxiv.org/html/2402.07197v1)       |
| Truncated       | False       |
| Word Count       | 9244       |