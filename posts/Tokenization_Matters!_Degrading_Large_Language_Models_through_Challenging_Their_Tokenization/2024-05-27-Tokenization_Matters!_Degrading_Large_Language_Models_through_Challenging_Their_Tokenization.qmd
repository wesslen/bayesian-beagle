
---
title: "Tokenization Matters! Degrading Large Language Models through Challenging Their Tokenization"
id: "2405.17067v1"
description: "LLMs struggle with accurate responses due to tokenization flaws, as demonstrated by the ADT dataset."
author: Dixuan Wang, Yanda Li, Junyuan Jiang, Zepeng Ding, Guochao Jiang, Jiaqing Liang, Deqing Yang
date: "2024-05-27"
image: "https://browse.arxiv.org/html/2405.17067v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.17067v1/x1.png)

### Summary:

This paper investigates the vulnerability of Large Language Models (LLMs) in terms of challenging their token segmentation, which has not been previously explored. The authors construct an adversarial dataset, ADT (Adversarial Dataset for Tokenizer), to challenge the tokenization of leading LLMs, including GPT-4o, Llama-3, Qwen2.5-max, and others. The dataset consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto. The empirical results reveal that ADT is highly effective in challenging the tokenization of these LLMs, leading to their degraded capabilities. The authors also propose an efficient and robust method for automatic data generation, which can be applied to any open-source LLMs.

### Major Findings:

1. The study is the first to investigate LLMs' vulnerability in terms of challenging their token segmentation, which can shed light on improving LLMs' capabilities through optimizing their tokenization process and algorithms.
2. The authors construct an adversarial dataset, ADT, which consists of two subsets: the manually constructed ADT-Human and the automatically generated ADT-Auto.
3. The empirical results demonstrate that ADT is highly effective in challenging the tokenization of leading LLMs, leading to their degraded capabilities.
4. The authors propose an efficient and robust method for automatic data generation, which can be applied to any open-source LLMs.

### Analysis and Critique:

The paper presents an innovative approach to investigating the vulnerability of LLMs in terms of challenging their token segmentation. The construction of the adversarial dataset, ADT, and the proposed method for automatic data generation are significant contributions to the field. However, the paper does not provide a detailed analysis of the limitations and potential biases of the proposed method. Additionally, the paper does not discuss the potential impact of the proposed method on the performance of LLMs in real-world applications. Further research is needed to evaluate the effectiveness of the proposed method in improving the performance of LLMs in various tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.17067v1](https://arxiv.org/abs/2405.17067v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.17067v1](https://browse.arxiv.org/html/2405.17067v1)       |
| Truncated       | False       |
| Word Count       | 6332       |