
---
title: "Towards Dialogues for Joint Human-AI Reasoning and Value Alignment"
id: "2405.18073v1"
description: "Promote human-AI inquiry dialogues for value-aligned decision making."
author: Elfia Bezou-Vrakatseli, Oana Cocarascu, Sanjay Modgil
date: "2024-05-28"
image: "../../../bayesian-beagle.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The paper argues for the importance of enabling human-AI dialogue to support joint reasoning, ensuring that AI decision-making is aligned with human values and preferences. The authors suggest that logic-based models of argumentation and dialogue, specifically focusing on inquiry dialogues, are suitable for this purpose. They highlight the need for research into inquiry dialogues to support joint human-LLM reasoning tasks that are ethically salient and require value alignment.

### Major Findings:

1. Human-AI dialogue is required to support joint reasoning and accommodate input and reasoning about human values and preferences. This will ensure that individual decisions are value-aligned and that humans can refine their ethical positions.
2. Dialogues generalizing argumentative formalizations of non-monotonic reasoning, such as those based on argumentation schemes (AS) and critical questions (CQs), are the most suitable candidates for enabling joint human-LLM inquiry.
3. Strategic considerations should inform the choice of AS or CQs at any given point in a dialogue, and dialogues should be guided so that declarative locutions can be combined to define complete arguments.

### Analysis and Critique:

* The paper provides a compelling argument for the importance of inquiry dialogues in ensuring value alignment in AI decision-making. However, it does not address the potential challenges and limitations of implementing such dialogues in practice.
* The authors focus on the use of argumentation schemes and critical questions as a means of enabling joint human-LLM inquiry. While this approach has its merits, it may not be sufficient to capture the full complexity of human values and preferences.
* The paper does not discuss the potential biases that may be introduced by the use of large language models in inquiry dialogues. It is important to consider how these biases might impact the alignment of AI decisions with human values and preferences.
* The authors do not provide a clear roadmap for how to integrate inquiry dialogues into existing AI systems. Further research is needed to explore the practical implications of implementing inquiry dialogues in real-world contexts.
* The paper does not address the potential risks and ethical concerns associated with the use of inquiry dialogues in AI decision-making. It is important to consider the potential consequences of relying on AI systems to make decisions that are aligned with human values

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.18073v1](https://arxiv.org/abs/2405.18073v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.18073v1](https://browse.arxiv.org/html/2405.18073v1)       |
| Truncated       | False       |
| Word Count       | 5659       |