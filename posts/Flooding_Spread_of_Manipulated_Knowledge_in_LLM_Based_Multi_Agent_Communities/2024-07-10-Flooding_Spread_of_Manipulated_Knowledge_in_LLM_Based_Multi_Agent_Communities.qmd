
---
title: "Flooding Spread of Manipulated Knowledge in LLM-Based Multi-Agent Communities"
id: "2407.07791v1"
description: "LLM-based multi-agent systems are vulnerable to manipulated knowledge spread, posing security risks."
author: Tianjie Ju, Yiting Wang, Xinbei Ma, Pengzhou Cheng, Haodong Zhao, Yulong Wang, Lifeng Liu, Jian Xie, Zhuosheng Zhang, Gongshen Liu
date: "2024-07-10"
image: "https://browse.arxiv.org/html/2407.07791v1/x1.png"
categories: ['security', 'hci', 'robustness', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.07791v1/x1.png)

### Summary:

The paper investigates the security implications of large language models (LLMs) in multi-agent systems, focusing on the spread of manipulated knowledge. The authors construct a detailed threat model and a comprehensive simulation environment to mirror real-world multi-agent deployments. They propose a novel two-stage attack method involving Persuasiveness Injection and Manipulated Knowledge Injection to explore the potential for manipulated knowledge spread without explicit prompt manipulation. The method leverages the inherent vulnerabilities of LLMs in handling world knowledge, which can be exploited by attackers to unconsciously spread fabricated information. Extensive experiments demonstrate that the attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication. The manipulations can persist through popular retrieval-augmented generation frameworks, where benign agents store and retrieve manipulated chat histories for future interactions. This persistence indicates that even after the interaction has ended, the benign agents may continue to be influenced by manipulated knowledge. The findings reveal significant security risks in LLM-based multi-agent systems, emphasizing the need for robust defenses against manipulated knowledge spread.

### Major Findings:

1. The proposed two-stage attack method can successfully induce LLM-based agents to spread both counterfactual and toxic knowledge without degrading their foundational capabilities during agent communication.
2. The manipulations can persist through popular retrieval-augmented generation frameworks, where benign agents store and retrieve manipulated chat histories for future interactions.
3. Even after the interaction has ended, the benign agents may continue to be influenced by manipulated knowledge.

### Analysis and Critique:

The paper presents a well-structured and coherent summary of the security risks associated with the spread of manipulated knowledge in LLM-based multi-agent systems. The authors provide a detailed threat model and a comprehensive simulation environment to investigate the potential for manipulated knowledge spread. The proposed two-stage attack method effectively demonstrates the vulnerabilities of LLMs in handling world knowledge and the potential for attackers to exploit these vulnerabilities to spread fabricated information.

However, the paper does not discuss potential countermeasures or defenses against the proposed attack method. It would be beneficial to explore possible

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.07791v1](https://arxiv.org/abs/2407.07791v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.07791v1](https://browse.arxiv.org/html/2407.07791v1)       |
| Truncated       | False       |
| Word Count       | 11413       |