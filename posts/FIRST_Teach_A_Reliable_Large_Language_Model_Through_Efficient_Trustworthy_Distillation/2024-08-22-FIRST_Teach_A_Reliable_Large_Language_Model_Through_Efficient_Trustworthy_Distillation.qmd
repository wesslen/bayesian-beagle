
---
title: "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation"
id: "2408.12168v1"
description: "FIRST method improves LLM trustworthiness, offering better accuracy and less mis-calibration by efficiently utilizing concentrated knowledge from the teacher model."
author: KaShun Shum, Minrui Xu, Jianshu Zhang, Zixin Chen, Shizhe Diao, Hanze Dong, Jipeng Zhang, Muhammad Omer Raza
date: "2024-08-22"
image: "https://browse.arxiv.org/html/2408.12168v1/extracted/5806746/Figures/why-top5.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.12168v1/extracted/5806746/Figures/why-top5.png)

# Summary:

The paper "FIRST: Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation" presents a novel method for obtaining a trustworthy language model by addressing the issue of "tuning-induced mis-calibration" in fine-tuned models. The proposed method, eFfIcient tRustworthy disTillation (FIRST), utilizes a small portion of teacher's knowledge to obtain a reliable language model in a cost-efficient way. The method identifies the "concentrated knowledge" phenomenon during distillation, which significantly reduces the computational burden, and applies a "trustworthy maximization" process to optimize the utilization of this small portion of concentrated knowledge before transferring it to the student. Experimental results demonstrate the effectiveness of the method, with better accuracy and less mis-calibration on average across both in-domain and out-of-domain scenarios.

# Major Findings:

1. The paper identifies the "tuning-induced mis-calibration" issue in fine-tuned models, which leads to a mismatch between the model's confidence and true likelihood.
2. The proposed method, FIRST, utilizes a small portion of teacher's knowledge to obtain a reliable language model in a cost-efficient way.
3. The method identifies the "concentrated knowledge" phenomenon during distillation, which significantly reduces the computational burden.
4. The method applies a "trustworthy maximization" process to optimize the utilization of this small portion of concentrated knowledge before transferring it to the student.
5. Experimental results demonstrate the effectiveness of the method, with better accuracy and less mis-calibration on average across both in-domain and out-of-domain scenarios.

# Analysis and Critique:

1. The paper provides a detailed analysis of the "tuning-induced mis-calibration" issue in fine-tuned models and proposes a novel method to address this issue.
2. The proposed method, FIRST, is based on the "concentrated knowledge" phenomenon and the "trustworthy maximization" process, which are well-explained and supported by experimental results.
3. The paper provides a comprehensive evaluation of the proposed method, including both in-domain

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.12168v1](https://arxiv.org/abs/2408.12168v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.12168v1](https://browse.arxiv.org/html/2408.12168v1)       |
| Truncated       | False       |
| Word Count       | 7033       |