
---
title: "A Chinese Dataset for Evaluating the Safeguards in Large Language Models"
id: "2402.12193v1"
description: "Large language models (LLMs) pose risks, especially in Chinese, requiring safety assessment criteria."
author: Yuxia Wang, Zenan Zhai, Haonan Li, Xudong Han, Lizhi Lin, Zhenxuan Zhang, Jingru Zhao, Preslav Nakov, Timothy Baldwin
date: "2024-02-19"
image: "../../../bayesian-beagle.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
- The article introduces a new Chinese dataset for evaluating the safeguards in large language models (LLMs).
- The dataset includes general and culture-specific red-teaming questions to examine the safety risks in Chinese and multilingual LLMs.
- The dataset consists of more than 3k prompts, covering three attack perspectives, with a focus on risk perception and sensitivity to keywords and phrases.
- 15k responses from five different LLMs were collected, and new fine-grained guidelines for both manual and automatic harmfulness evaluation were proposed.

### Major Findings:
1. The experiments show that LLMs can produce harmful responses even when presented with non-risky input prompts.
2. Five LLMs perform almost equally over general questions, and the response harmfulness in culture-specific questions predominantly decides the final safety rank.
3. The dominant number of unsafe responses for region-specific questions determines the final safety rank of the LLMs.

### Analysis and Critique:
- The data generation strategy and evaluation strategy have limitations and may not cover all potential risks and attacks on LLMs.
- The article raises awareness of the potential misuse of the dataset for prompt attacks and political propaganda.
- The study provides valuable insights into the safety evaluation of LLMs and highlights the need for further research to improve the safety and reliability of LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.12193v1](https://arxiv.org/abs/2402.12193v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12193v1](https://browse.arxiv.org/html/2402.12193v1)       |
| Truncated       | False       |
| Word Count       | 11547       |