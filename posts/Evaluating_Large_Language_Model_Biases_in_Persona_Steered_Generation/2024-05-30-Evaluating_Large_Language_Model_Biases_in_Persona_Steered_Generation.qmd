
---
title: "Evaluating Large Language Model Biases in Persona-Steered Generation"
id: "2405.20253v1"
description: "LLMs struggle to generate text reflecting incongruous personas, but RLHF fine-tuning improves steerability, though with less diverse views."
author: Andy Liu, Mona Diab, Daniel Fried
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.20253v1/x1.png"
categories: ['hci', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.20253v1/x1.png)

### Summary:

The study investigates the steerability of large language models (LLMs) towards incongruous personas, which are personas with multiple traits where one trait makes its other traits less likely in human survey data. The authors find that LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance. Models fine-tuned with Reinforcement Learning from Human Feedback (RLHF) are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas. The study also finds variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation. The results highlight the importance of evaluating models in open-ended text generation to surface new LLM opinion biases and shed light on our ability to steer models toward a richer and more diverse range of viewpoints.

### Major Findings:
1. LLMs are 9.7% less steerable towards incongruous personas than congruous ones, sometimes generating the stereotypical stance associated with its demographic rather than the target stance.
2. Models fine-tuned with RLHF are more steerable, especially towards stances associated with political liberals and women, but present significantly less diverse views of personas.
3. There is variance in LLM steerability that cannot be predicted from multiple-choice opinion evaluation.

### Analysis and Critique:
- The study highlights the importance of evaluating LLMs in open-ended text generation to surface new opinion biases and improve steerability towards a diverse range of viewpoints.
- The findings suggest that current LLMs may perpetuate monolithic and insufficiently nuanced views of demographics, as they struggle to represent personas with multiple stances that are not fully aligned with the stereotypical views of a single demographic.
- The study could be improved by exploring additional methods for fine-tuning LLMs to better represent incongruous personas and reduce opinion biases.
- The authors could also investigate the impact of different prompting strategies on LLM steerability and the representation of diverse viewpoints.
- Future research should consider the ethical implications of using LLMs for

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.20253v1](https://arxiv.org/abs/2405.20253v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.20253v1](https://browse.arxiv.org/html/2405.20253v1)       |
| Truncated       | False       |
| Word Count       | 9904       |