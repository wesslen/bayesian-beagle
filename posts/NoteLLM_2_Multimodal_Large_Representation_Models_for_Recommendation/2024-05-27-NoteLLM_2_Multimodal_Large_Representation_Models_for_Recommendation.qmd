
---
title: "NoteLLM-2: Multimodal Large Representation Models for Recommendation"
id: "2405.16789v1"
description: "TL;DR: NoteLLM-2 enhances multimodal representation in I2I recommendations by focusing on visual content and fusing it with textual information."
author: Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Yan Gao, Yao Hu, Enhong Chen
date: "2024-05-27"
image: "https://browse.arxiv.org/html/2405.16789v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.16789v1/x1.png)

### Summary:

The paper explores the use of Large Language Models (LLMs) to improve textual comprehension in multimodal representation tasks under item-to-item (I2I) recommendation scenarios. The authors propose an end-to-end training method that can customize the integration of any existing LLMs and vision encoders, reducing reliance on open-source Multimodal Large Language Models (MLLMs) that require costly multimodal pre-training. To address the problem of ignoring visual information in end-to-end training, the authors propose NoteLLM-2, which contains multimodal In-Context Learning (mICL) and late fusion methods. The effectiveness of this approach is confirmed through extensive experiments.

### Major Findings:

1. The proposed end-to-end training method can customize the integration of any existing LLMs and vision encoders, reducing reliance on open-source MLLMs and their costly multimodal pre-training.
2. The NoteLLM-2 framework, which includes mICL and late fusion methods, effectively addresses the problem of ignoring visual information in end-to-end training.
3. Extensive experiments demonstrate the effectiveness of the proposed approach in enhancing multimodal representation for I2I recommendation scenarios.

### Analysis and Critique:

1. The paper successfully demonstrates the potential of LLMs in improving textual comprehension in multimodal representation tasks, which is a significant contribution to the field.
2. The proposed end-to-end training method and NoteLLM-2 framework effectively address the challenges of integrating LLMs and vision encoders and the problem of ignoring visual information in end-to-end training.
3. The paper could benefit from a more detailed comparison with other existing methods for integrating LLMs and vision encoders, as well as a more comprehensive evaluation of the proposed approach in various I2I recommendation scenarios.
4. The paper does not discuss the potential limitations and challenges of using LLMs in multimodal representation tasks, such as the computational resources required for training and inference, and the need for large-scale, high-quality multimodal datasets.
5. Future work could explore the application of the proposed approach in other multimodal representation

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-14       |
| Abstract | [https://arxiv.org/abs/2405.16789v1](https://arxiv.org/abs/2405.16789v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.16789v1](https://browse.arxiv.org/html/2405.16789v1)       |
| Truncated       | False       |
| Word Count       | 7838       |