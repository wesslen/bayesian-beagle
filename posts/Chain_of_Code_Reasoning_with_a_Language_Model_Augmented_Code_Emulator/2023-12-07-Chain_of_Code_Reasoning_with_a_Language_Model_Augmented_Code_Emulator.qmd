
---
title: "Chain of Code: Reasoning with a Language Model-Augmented Code Emulator"
id: "2312.04474v1"
description: "Code-writing aids language models in Chain of Thought reasoning, improving linguistic and logical tasks. Chain of Code outperforms Chain of Thought."
author: ['Chengshu Li', 'Jacky Liang', 'Andy Zeng', 'Xinyun Chen', 'Karol Hausman', 'Dorsa Sadigh', 'Sergey Levine', 'Li Fei-Fei', 'Fei Xia', 'Brian Ichter']
date: "2023-12-07"
image: "https://browse.arxiv.org/html/2312.04474v1/extracted/5279122/fig/code_prelim_cot.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.04474v1/extracted/5279122/fig/code_prelim_cot.png)

# Chain of Code: Reasoning with a Language Model-Augmented Code Emulator

## Key Findings
- Chain of Code (CoC) proposes to utilize both code and language models (LMs) to improve reasoning performance across various reasoning tasks, achieving significant improvements over other baseline techniques.
- CoC generates reasoning substeps in the form of code or pseudocode and executes the code with a Python interpreter, using an LMulator to simulate execution for non-executable code, which allows it to perform well on tasks that involve both numeric and semantic reasoning.
- The overall performance of CoC outperforms Chain of Thought and other baselines across a variety of benchmarks, achieving 84% accuracy on BIG-Bench Hard, a gain of 12% over Chain of Thought.

## Introduction
- Language models (LMs) have shown to improve reasoning tasks, and using code to prompt LMs has been advantageous due to the structured nature of code and the interface it provides for performing precise algorithmic computations.
- While writing and executing code may improve LM reasoning performance across arithmetic tasks, it struggles with many semantic tasks difficult to express in code.

## Chain of Code: Reasoning with an LMulator
- CoC encourages LMs to format semantic sub-tasks as flexible pseudocode that can be explicitly caught and handed off to an LMulator for simulation at runtime.
- CoC proceeds in two steps: generation, wherein an LM generates code or pseudocode to solve a problem, and execution, with the code being run using a Python interpreter or an LMulator.
- The approach scales well with large and small models alike and outperforms Chain of Thought and other baselines across various tasks, even achieving human-rater level performance on several tasks.

## Experimental Evaluation
- CoC exhibits high performance across varied problems, particularly excelling in algorithmic tasks and performing on par with Chain of Thought for natural language tasks.
- Ablations demonstrate that the interweaving of code and language execution provides significant improvements in performance across tasks.
- CoC's performance increases with model size, and it outperforms other prompting techniques even with instruction-tuned chat models.
- CoC demonstrates promising results for applications involving robotic tasks that require semantic and algorithmic reasoning.

## Critique
- CoC requires additional context length and computation time due to its two-step process and interweaving of code and language execution.
- The approach may not perform well on tasks where code is not beneficial and has limitations in modifying custom Python objects while simulating code execution.

Overall, the paper presents an innovative approach, CoC, that combines the strengths of both code and language models to improve reasoning performance across a variety of tasks. However, the paper would benefit from further discussions on potential limitations and future work for extending the applicability of CoC.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [http://arxiv.org/abs/2312.04474v1](http://arxiv.org/abs/2312.04474v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.04474v1](https://browse.arxiv.org/html/2312.04474v1)       |
| Truncated       | False       |
| Word Count       | 9590       |