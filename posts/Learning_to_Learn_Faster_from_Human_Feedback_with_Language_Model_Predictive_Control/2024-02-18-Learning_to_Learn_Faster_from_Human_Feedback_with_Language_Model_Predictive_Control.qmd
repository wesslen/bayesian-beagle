
---
title: "Learning to Learn Faster from Human Feedback with Language Model Predictive Control"
id: "2402.11450v1"
description: "LLMs improved to remember interactions and adapt efficiently, enhancing robot teachability and success rates."
author: Jacky Liang, Fei Xia, Wenhao Yu, Andy Zeng, Montserrat Gonzalez Arenas, Maria Attarian, Maria Bauza, Matthew Bennice, Alex Bewley, Adil Dostmohamed, Chuyuan Kelly Fu, Nimrod Gileadi, Marissa Giustina, Keerthana Gopalakrishnan, Leonard Hasenclever, Jan Humplik, Jasmine Hsu, Nikhil Joshi, Ben Jyenis, Chase Kew, Sean Kirmani, Tsang-Wei Edward Lee, Kuang-Huei Lee, Assaf Hurwitz Michaely, Joss Moore, Ken Oslund, Dushyant Rao, Allen Ren, Baruch Tabanpour, Quan Vuong, Ayzaan Wahid, Ted Xiao, Ying Xu, Vincent Zhuang, Peng Xu, Erik Frey, Ken Caluwaerts, Tingnan Zhang, Brian Ichter, Jonathan Tompson, Leila Takayama, Vincent Vanhoucke, Izhak Shafran, Maja Mataric, Dorsa Sadigh, Nicolas Heess, Kanishka Rao, Nik Stewart, Jie Tan, Carolina Parada
date: "2024-02-18"
image: "../../img/2402.11450v1/image_1.png"
categories: ['education', 'programming']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11450v1/image_1.png)

### Summary:
- The article discusses the use of large language models (LLMs) to enable non-experts to teach robots new tasks with language. The authors introduce Language Model Predictive Control (LMPC) as a framework to improve the teachability of LLMs on various robot embodiments, leading to significant improvements in non-expert teaching success rates and the responsiveness of LLMs to user feedback. The section also covers the comparison of base and finetuned models, real-world evaluations, and data augmentation for training LMPC models.

### Major Findings:
1. LMPC significantly improves non-expert teaching success rates on unseen tasks and reduces the average number of human corrections.
2. LMPC produces strong meta-learners, improving the success rate of in-context learning new tasks on unseen robot embodiments and APIs.
3. LMPC-Rollouts and LMPC-Skip outperform a RAG baseline across all embodiments, demonstrating the effectiveness of LMPC in improving the teachability of the base model.

### Analysis and Critique:
- The article provides valuable insights into the potential of LLMs and LMPC in human-robot interactions, but further research is needed to address potential biases and methodological issues. The detailed comparison of different models and their performance across various metrics is a strength, but the study's limitations and unanswered questions should be acknowledged. The technical details provided in the appendix are crucial for understanding the experimental setup and results, but the potential implications of the findings for real-world applications should be further explored.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.11450v1](https://arxiv.org/abs/2402.11450v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11450v1](https://browse.arxiv.org/html/2402.11450v1)       |
| Truncated       | True       |
| Word Count       | 37317       |