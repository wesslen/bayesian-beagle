
---
title: "Can Editing LLMs Inject Harm?"
id: "2407.20224v1"
description: "Editing attacks can inject misinformation and bias into LLMs, posing safety threats and impacting overall fairness."
author: Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, Jindong Gu, Huaxiu Yao, Chaowei Xiao, Xifeng Yan, William Yang Wang, Philip Torr, Dawn Song, Kai Shu
date: "2024-07-29"
image: "https://browse.arxiv.org/html/2407.20224v1/x1.png"
categories: ['production', 'robustness', 'architectures', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.20224v1/x1.png)

### Summary:

The paper explores the potential risks of knowledge editing techniques in Large Language Models (LLMs), specifically focusing on the possibility of injecting harm into LLMs. The authors propose a new type of threat, Editing Attack, and investigate its two major risks: Misinformation Injection and Bias Injection. Through extensive experiments, they demonstrate that editing attacks can effectively inject both misinformation and biased information into LLMs, and even increase the bias in LLMs' general outputs via a single biased sentence injection. The paper also highlights the high degree of stealthiness of editing attacks, as they have minimal impact on LLMs' general knowledge or reasoning capacities.

### Major Findings:

1. Editing attacks can effectively inject both misinformation and biased information into LLMs, with high effectiveness.
2. A single biased sentence injection can increase the bias in LLMs' general outputs, demonstrating a catastrophic impact on overall fairness.
3. Editing attacks exhibit a high degree of stealthiness, as they have minimal impact on LLMs' general knowledge or reasoning capacities.

### Analysis and Critique:

The paper provides a comprehensive analysis of the potential risks of knowledge editing techniques in LLMs. However, there are some limitations and areas for future research. The experiments were conducted on LLMs with a relatively small scale of parameters, and the effectiveness of editing attacks on larger models should be further assessed. Additionally, the paper calls for more research on developing defense methods based on the inner mechanisms of editing and enhancing LLMs' intrinsic robustness against editing attacks.

The paper also raises ethical concerns regarding the potential misuse of knowledge editing techniques to inject misinformation or biased information into LLMs. The authors emphasize the need for open discussions from different stakeholders on the governance of open-source LLMs to maximize the benefit and minimize the potential risk.

In conclusion, the paper highlights the critical misuse risks of knowledge editing techniques and the fragility of LLMs' safety alignment under editing attacks. It calls for more research on understanding the inner mechanisms of editing attacks, designing defense techniques, and enhancing LLMs' intrinsic robustness.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.20224v1](https://arxiv.org/abs/2407.20224v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.20224v1](https://browse.arxiv.org/html/2407.20224v1)       |
| Truncated       | False       |
| Word Count       | 9390       |