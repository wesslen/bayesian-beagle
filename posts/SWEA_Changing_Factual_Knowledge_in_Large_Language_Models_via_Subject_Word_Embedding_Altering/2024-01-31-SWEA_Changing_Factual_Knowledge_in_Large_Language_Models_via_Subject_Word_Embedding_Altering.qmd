
---
title: "SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering"
id: "2401.17809v1"
description: "SWEAOS: Precise Model Editing in LLMs without Overhead, Demonstrating SOTA Performance and Reasoning Ability."
author: Xiaopeng Li, Shasha Li, Bin Ji, Shezheng Song, Xi Wang, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Weimin Zhang
date: "2024-01-31"
image: "../../img/2401.17809v1/image_1.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../img/2401.17809v1/image_1.png)

### Summary

- Model editing has gained attention as an alternative to retraining large language models (LLMs) for updating knowledge.
- Current editing methods include adding additional modules, global optimization, and local editing.
- Adding additional modules avoids disrupting original parameters but uses fuzzy vector matching, which can be unreliable.
- Global optimization methods can cause overfitting, while local editing methods can cause irreversible damage to LLMs.
- The proposed SWEA framework modifies subject word embeddings during inference, addressing these issues.

### Major Findings
1. **SWEA framework**: The SWEA framework modifies the representation of subjects to edit knowledge, using precise key matching outside the model and performing reliable subject word embedding altering.
2. **SWEAOS method**: The SWEAOS method, which combines SWEA with an optimizing then suppressing fusion method, demonstrates state-of-the-art performance on the COUNTERFACT and zsRE datasets and shows superior reasoning ability on the RIPPLEEDITS benchmark.
3. **Comparison with existing methods**: SWEAOS outperforms existing methods in editing factual knowledge, achieving higher efficacy, generalization, specificity, and consistency scores.

### Analysis and Critique
- The SWEA framework and SWEAOS method show promising results, but there are some limitations and areas for further research:
  - The SWEA framework is only applicable when the subject appears in the sentence, which may limit its applicability in certain contexts.
  - The SWEAOS method relies on the optimizing then suppressing fusion method, which may introduce undesired knowledge embedding dimensions, causing confusion in the LLMs' knowledge about the subject.
  - The SWEA framework and SWEAOS method should be tested on a more diverse range of datasets and tasks to ensure their generalizability.
- Future work could focus on addressing these limitations, interpreting the specific knowledge represented by each dimension of word embeddings, and exploring adjustments to the output embedding layer to improve the SWEA framework's performance.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.17809v1](https://arxiv.org/abs/2401.17809v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.17809v1](https://browse.arxiv.org/html/2401.17809v1)       |
| Truncated       | False       |
| Word Count       | 11646       |