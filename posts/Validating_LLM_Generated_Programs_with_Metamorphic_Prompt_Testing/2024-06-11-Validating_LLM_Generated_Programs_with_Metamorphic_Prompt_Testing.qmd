
---
title: "Validating LLM-Generated Programs with Metamorphic Prompt Testing"
id: "2406.06864v1"
description: "TL;DR: Metamorphic prompt testing detects 75% of GPT-4's erroneous code, with 8.6% false positives."
author: Xiaoyin Wang, Dakai Zhu
date: "2024-06-11"
image: "https://browse.arxiv.org/html/2406.06864v1/x1.png"
categories: ['robustness', 'security', 'prompt-engineering', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.06864v1/x1.png)

### Summary:

The paper proposes a novel solution called metamorphic prompt testing to address the challenge of validating LLM-generated code without canonical solutions or ground truth output. The approach leverages the intuition that intrinsic consistency always exists among correct code pieces but may not exist among flawed code pieces. The technique involves varying a given prompt to multiple prompts with paraphrasing and asking the LLM to acquire multiple versions of generated code. The semantic relations are then cross-validated to detect inconsistencies and flaws in the code. The evaluation on HumanEval shows that metamorphic prompt testing is able to detect 75% of the erroneous programs generated by GPT-4, with a false positive rate of 8.6%.

### Major Findings:

1. The proposed metamorphic prompt testing technique is able to detect 75% of the erroneous programs generated by GPT-4, with a false positive rate of 8.6%.
2. The approach does not require any canonical solutions or ground truth output, making it a promising solution for validating LLM-generated code.
3. The technique leverages the intuition that intrinsic consistency always exists among correct code pieces but may not exist among flawed code pieces, allowing for the detection of flaws in the code.

### Analysis and Critique:

1. The proposed technique relies on the ability to generate multiple versions of code from a given prompt, which may not always be possible or practical.
2. The technique assumes that the LLM is capable of generating code with intrinsic consistency, which may not always be the case.
3. The evaluation of the technique is limited to the HumanEval dataset, and further evaluation on other datasets and LLMs is needed to establish its generalizability.
4. The technique does not address the issue of generating code that is semantically correct but does not meet the requirements of the prompt, which is a common challenge in LLM-generated code.
5. The technique does not provide a mechanism for correcting the detected flaws in the code, which is an important aspect of code validation.

Overall, the proposed metamorphic prompt testing technique is a promising solution for validating LLM-generated code, but further research is needed to address its limitations and establish its generalizability

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-12       |
| Abstract | [https://arxiv.org/abs/2406.06864v1](https://arxiv.org/abs/2406.06864v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06864v1](https://browse.arxiv.org/html/2406.06864v1)       |
| Truncated       | False       |
| Word Count       | 6738       |