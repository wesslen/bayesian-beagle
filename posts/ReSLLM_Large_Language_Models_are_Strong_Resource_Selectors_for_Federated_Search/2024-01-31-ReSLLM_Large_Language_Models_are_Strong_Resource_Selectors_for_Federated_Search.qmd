
---
title: "ReSLLM: Large Language Models are Strong Resource Selectors for Federated Search"
id: "2401.17645v1"
description: "Federated search with LLMs improves resource selection without extensive labels or features."
author: Shuai Wang, Shengyao Zhuang, Bevan Koopman, Guido Zuccon
date: "2024-01-31"
image: "https://browse.arxiv.org/html/2401.17645v1/x1.png"
categories: ['architectures', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.17645v1/x1.png)

### **Summary:**
The article discusses the use of Large Language Models (LLMs) for resource selection in federated search. It introduces ReSLLM, a zero-shot LLM-based method for resource selection, and the Synthetic Label Augmentation Tuning (SLAT) protocol, a fine-tuning approach that leverages LLM-generated synthetic labels. The study demonstrates that both ReSLLM and SLAT+ReSLLM can effectively select resources in a federated search environment, outperforming several baseline methods and matching the efficacy of supervised models in certain scenarios.

### **Major Findings:**
1. **ReSLLM Method:** ReSLLM is a zero-shot LLM-based method for resource selection that operates without the need for human-labeled data.
2. **SLAT Protocol:** The Synthetic Label Augmentation Tuning (SLAT) protocol is a fine-tuning approach that leverages LLM-generated synthetic labels, providing an effective means of tuning ReSLLM without human intervention.
3. **Effectiveness:** Both ReSLLM and SLAT+ReSLLM can effectively select resources in a federated search environment, outperforming several baseline methods and matching the efficacy of supervised models in certain scenarios.

### **Analysis and Critique:**
The study demonstrates the potential benefits of using LLMs for resource selection in federated search. It highlights the effectiveness of ReSLLM and SLAT+ReSLLM, especially in scenarios where human-annotated data is limited or unavailable. The study also identifies the impact of LLM size, architecture, and resource representation on the effectiveness of resource selection. However, the study acknowledges limitations in fine-tuning for conversational queries and the need for further research in this area. Overall, the article provides valuable insights into the use of LLMs for resource selection and highlights the potential for future research in this domain.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2401.17645v1](https://arxiv.org/abs/2401.17645v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.17645v1](https://browse.arxiv.org/html/2401.17645v1)       |
| Truncated       | False       |
| Word Count       | 10157       |