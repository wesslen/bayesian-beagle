
---
title: "FedMKT: Federated Mutual Knowledge Transfer for Large and Small Language Models"
id: "2406.02224v1"
description: "FedMKT: A framework for mutual knowledge transfer between large and small language models, improving performance in both."
author: Tao Fan, Guoqiang Ma, Yan Kang, Hanlin Gu, Lixin Fan, Qiang Yang
date: "2024-06-04"
image: "https://browse.arxiv.org/html/2406.02224v1/extracted/5642999/imgs/fedmkt_framework.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.02224v1/extracted/5642999/imgs/fedmkt_framework.png)

### Summary:

The paper introduces FedMKT, a parameter-efficient federated mutual knowledge transfer framework for large and small language models. This framework aims to adaptively transfer knowledge from a server's large language model (LLM) to clients' small language models (SLMs) while simultaneously enriching the LLM with clients' unique domain insights. The framework uses minimum edit distance (MinED) for token alignment and selective mutual knowledge transfer to enhance the performance of both LLMs and SLMs.

### Major Findings:

1. FedMKT enables effective knowledge transfer between LLMs and SLMs, addressing the challenges of domain-specific knowledge privacy, constrained computing resources, and mutual knowledge transfer between LLMs and SLMs.
2. The framework uses token alignment with MinED to address model heterogeneity between LLMs and SLMs, ensuring efficient knowledge transfer.
3. FedMKT implements a selective knowledge transfer mechanism that distills knowledge from the most informative SLMs to the server's LLM and vice versa.
4. Extensive experiments on various public LLMs and SLMs demonstrate significant performance enhancements in clients' SLMs with the aid of the server's LLM. The LLM optimized by FedMKT achieves comparable performance to direct fine-tuning on clients' data, highlighting the framework's effectiveness and adaptability.

### Analysis and Critique:

1. The paper provides a comprehensive solution to the challenges of integrating LLMs into real-world applications while maintaining data privacy.
2. The use of token alignment with MinED is a novel approach to addressing model heterogeneity between LLMs and SLMs, which is a significant challenge in federated learning.
3. The selective knowledge transfer mechanism ensures that the most accurate and informative knowledge is distilled between the LLM and SLMs, improving the performance of both models.
4. The paper's empirical evaluation demonstrates the effectiveness of FedMKT in enhancing the performance of both LLMs and SLMs. However, the study is limited by computational and storage resources, preventing the exploration of larger model sizes. Future research should address this limitation.
5. The paper does not discuss potential biases or limitations in the data used for training the LLMs and SLMs, which could impact the performance of the models

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2406.02224v1](https://arxiv.org/abs/2406.02224v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.02224v1](https://browse.arxiv.org/html/2406.02224v1)       |
| Truncated       | False       |
| Word Count       | 7191       |