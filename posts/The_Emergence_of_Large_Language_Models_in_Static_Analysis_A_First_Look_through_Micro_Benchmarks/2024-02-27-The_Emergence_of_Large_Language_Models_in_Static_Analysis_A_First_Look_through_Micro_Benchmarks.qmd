
---
title: "The Emergence of Large Language Models in Static Analysis: A First Look through Micro-Benchmarks"
id: "2402.17679v1"
description: "LLMs improve type inference in Python, but need fine-tuning for callgraph analysis."
author: Ashwin Prasad Shivarpatna Venkatesh, Samkutty Sabu, Amir M. Mir, Sofia Reis, Eric Bodden
date: "2024-02-27"
image: "../../../bayesian-beagle.png"
categories: ['production', 'programming', 'architectures', 'education']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
- The study investigates the role of Large Language Models (LLMs) in improving callgraph analysis and type inference for Python programs.
- 26 LLMs, including OpenAIâ€™s GPT series and open-source models such as LLaMA, were evaluated using PyCG, HeaderGen, and TypeEvalPy micro-benchmarks.
- LLMs show promising results in type inference but exhibit limitations in callgraph analysis, emphasizing the need for specialized fine-tuning.

### Major Findings:
1. LLMs demonstrate higher accuracy in type inference than traditional methods.
2. Traditional methods outperform LLMs in callgraph analysis.
3. Fine-tuning LLMs can lead to significant improvements in specific static analysis tasks.

### Analysis and Critique:
- The study provides valuable insights into the capabilities of LLMs in static analysis tasks, but it also highlights several limitations and challenges.
- The study's focus on Python programs may limit the generalizability of the findings to other programming languages.
- The computational and monetary requirements associated with LLMs raise concerns about practical utility and cost-effectiveness.
- The study acknowledges the need for further research in model compression, explainability methods, and broader evaluation of fine-tuned open-source models to optimize LLMs for static analysis tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.17679v1](https://arxiv.org/abs/2402.17679v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.17679v1](https://browse.arxiv.org/html/2402.17679v1)       |
| Truncated       | False       |
| Word Count       | 4369       |