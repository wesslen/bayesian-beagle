
---
title: "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models"
id: "2406.16838v1"
description: "Survey explores scaling compute during inference in LLMs, focusing on token-level, meta-generation, and efficient generation algorithms."
author: Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, Zaid Harchaoui
date: "2024-06-24"
image: "../../img/2406.16838v1/image_1.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](../../img/2406.16838v1/image_1.png)

**Summary:**

The paper explores the use of large language models (LLMs) in natural language processing, focusing on three main themes: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, such as decoding algorithms, have a rich history in natural language processing and operate by sampling one token at a time or constructing a token-level search space. Recently, there has been growing interest in meta-generation algorithms, which operate on partial or full sequences and treat the LLM as a black box that is called as part of a larger generation program. These algorithms can increase the compute resources devoted to generation by making multiple model calls, augmenting the model with search algorithms, or incorporating external data sources. The paper also discusses the limitations of the Maximum A Posteriori (MAP) decoding objective in neural machine translation (NMT) and the use of reranking and transforming N-

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.16838v1](https://arxiv.org/abs/2406.16838v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.16838v1](https://browse.arxiv.org/html/2406.16838v1)       |
| Truncated       | True       |
| Word Count       | 45988       |