
---
title: "Code Needs Comments: Enhancing Code LLMs with Comment Augmentation"
id: "2402.13013v1"
description: "TL;DR: Pre-training data impacts code-focused LLMs; new method improves performance on programming skill benchmarks."
author: Demin Song, Honglin Guo, Yunhua Zhou, Shuhao Xing, Yudong Wang, Zifan Song, Wenwei Zhang, Qipeng Guo, Hang Yan, Xipeng Qiu, Dahua Lin
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13013v1/x1.png"
categories: ['programming', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13013v1/x1.png)

### **Summary:**
- The article examines the impact of pre-training data on code-focused Large Language Models' (LLMs) performance by assessing comment density as a measure of programming language-natural language alignment.
- Due to the scarcity of code-comment aligned data in pre-training corpora, the authors introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out poorly correlated code data.
- Experiments on three code-focused LLMs show consistent improvements in performance on two widely-used programming skill benchmarks, with the model trained on augmented data outperforming both the model used for generating comments and the model further trained on the data without augmentation.

### Major Findings:
1. Comment density significantly affects the performance of LLM models in downstream tasks, with higher comment density leading to improved outcomes.
2. The proposed data augmentation method, coupled with data filtering, results in substantial improvements on Llama 2, Code Llama, and InternLM2.
3. The model trained on augmented data outperforms both the model used for generating comments and the model further trained on the data without augmentation.

### Analysis and Critique:
- The article provides a comprehensive overview of the proposed data augmentation method and its impact on the performance of code-focused LLMs. However, the reliance on data distillation with a teacher model and the considerable GPU overhead for data augmentation are potential limitations that need further exploration.
- The use of "<|EOT|>" as the modelâ€™s output in the implicit filter stage may not align well with the behavioral patterns typically exhibited by a language model, suggesting the need for alternative approaches.
- The marginal improvements observed during the next iteration of self-augmentation raise questions about the scalability and effectiveness of the proposed method, indicating the need for further investigation.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.13013v1](https://arxiv.org/abs/2402.13013v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13013v1](https://browse.arxiv.org/html/2402.13013v1)       |
| Truncated       | False       |
| Word Count       | 5792       |