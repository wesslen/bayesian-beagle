
---
title: "FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy Labels"
id: "2312.12263v1"
description: "F-LNL aims for optimal server model via collaborative learning, FedDiv introduces global noise filter for stability and performance."
author: ['Jichang Li', 'Guanbin Li', 'Hui Cheng', 'Zicheng Liao', 'Yizhou Yu']
date: "2023-12-19"
image: "https://browse.arxiv.org/html/2312.12263v1/x2.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.12263v1/x2.png)

### Major Findings
1. **Federated Learning with Noisy Labels (F-LNL)** seeks to optimize server model via collaborative distributed learning by aggregating multiple client models trained with local noisy or clean samples.
2. **FedDiv** presents a global noise filter called **Federated Noise Filter** which effectively identifies samples with noisy labels on every client, thus raising stability during local training sessions.
3. **Predictive Consistency based Sampler** is introduced to identify more credible local data for local model training, preventing noise memorization and further boosting training stability.

### Introduction
- **Federated Learning (FL)** facilitates collaborative learning across multiple clients without requiring centralized local data, showing significant real-world success in various areas.
- **Federated Learning with Noisy Labels (F-LNL)** deals with the presence of noisy labels in the private data of local clients, posing a challenge to training stability.

### Related Work
- **Centralized Learning with Noisy Labels (C-LNL)** aims to reduce model overfitting to noisy labels and includes various methods such as JointOpt and DivideMix.
- **Federated Learning with Noisy Labels** is addressed by FedRN, RoFL, and FedCorr while existing methods concentrate on local noise filtering and fail to exploit collective knowledge across clients.

### Methodology
- **Federated Noise Filter** is proposed to model the global distribution of clean and noisy samples across all clients, effectively identifying label noise on each local client.
- **Predictive Consistency based Sampler** is introduced to re-select labeled samples for local training, improving predictions' reliability for local samples.

### Experiments
- Experiments are conducted on CIFAR-10, CIFAR-100, and Clothing1M datasets showcasing the superiority of **FedDiv** over state-of-the-art methods under various label noise settings for both IID and non-IID data partitions.

### Ablation Analysis
- An ablation study demonstrates the effectiveness of **FedDiv** and the importance of each individual component in improving classification performance.

### Critique
The paper provides a thorough analysis of the proposed **FedDiv** framework and its effectiveness in tackling Federated Learning with Noisy Labels. However, there may be a need for further comparison with a wider range of baselines and additional analysis on potential privacy implications and computational complexity of the proposed methods.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [http://arxiv.org/abs/2312.12263v1](http://arxiv.org/abs/2312.12263v1)        |
| HTML     | [https://browse.arxiv.org/html/2312.12263v1](https://browse.arxiv.org/html/2312.12263v1)       |
| Truncated       | False       |
| Word Count       | 10854       |