
---
title: "Every Language Counts: Learn and Unlearn in Multilingual LLMs"
id: "2406.13748v1"
description: "Multilingual LLMs can spread fake info; standard unlearning methods are inadequate. Comprehensive unlearning strategies needed."
author: Taiming Lu, Philipp Koehn
date: "2024-06-19"
image: "https://browse.arxiv.org/html/2406.13748v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.13748v1/x1.png)

### Summary:

This paper investigates the propagation of harmful information in multilingual large language models (LLMs) and evaluates the efficacy of various unlearning methods. The study demonstrates that fake information, once introduced into these models through training data, can spread across different languages, compromising the integrity and reliability of the generated content. The findings reveal that standard unlearning techniques, which typically focus on English data, are insufficient in mitigating the spread of harmful content in multilingual contexts and could inadvertently reinforce harmful content across languages. The study shows that only by addressing harmful responses in both English and the original language of the harmful data can we effectively eliminate generations for all languages. This underscores the critical need for comprehensive unlearning strategies that consider the multilingual nature of modern LLMs to enhance their safety and reliability across diverse linguistic landscapes.

### Major Findings:

1. Fake information from all language sources propagates within multilingual LLMs.
2. Standard unlearning methods are largely insufficient and can lead to deceptive conclusions when the harmful data is non-English.
3. Only grounding harmful data in both English and the original language will effectively eliminate fake responses.

### Analysis and Critique:

* The study focuses on the propagation of harmful information in multilingual LLMs, which is a significant concern in the field of natural language processing.
* The findings highlight the limitations of current unlearning methods, which are primarily focused on English data, and the need for more comprehensive unlearning strategies that consider the multilingual nature of modern LLMs.
* The study's experimental setup and evaluation metrics are well-designed and provide a clear demonstration of the propagation of fake information across languages.
* However, the study does not address the potential impact of different types of harmful information, such as hate speech or misinformation, on the propagation and unlearning of fake information.
* Additionally, the study does not consider the potential impact of different model architectures or training methods on the propagation and unlearning of fake information.
* Future research should explore the impact of different types of harmful information and model architectures on the propagation and unlearning of fake information in multilingual LLMs.
* Overall, the study provides valuable insights into the challenges of unlearning harmful information in mult

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.13748v1](https://arxiv.org/abs/2406.13748v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.13748v1](https://browse.arxiv.org/html/2406.13748v1)       |
| Truncated       | False       |
| Word Count       | 5047       |