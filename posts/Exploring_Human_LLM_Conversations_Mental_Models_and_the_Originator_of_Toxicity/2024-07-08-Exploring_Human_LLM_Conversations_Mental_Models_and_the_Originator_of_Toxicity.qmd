
---
title: "Exploring Human-LLM Conversations: Mental Models and the Originator of Toxicity"
id: "2407.05977v1"
description: "LLMs provide toxic content mainly due to human demand or provocation."
author: Johannes Schneider, Arianna Casanova Flores, Anne-Catherine Kranz
date: "2024-07-08"
image: "https://browse.arxiv.org/html/2407.05977v1/extracted/5717670/figs/doesnotAllow2.png"
categories: ['social-sciences', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.05977v1/extracted/5717670/figs/doesnotAllow2.png)

# Summary:

This study explores real-world human interactions with large language models (LLMs) in diverse, unconstrained settings, focusing on understanding the originator of toxicity. The findings suggest that although LLMs are accused of providing toxic content, it is mostly demanded or provoked by humans. The manual analysis of hundreds of conversations judged as toxic by API commercial vendors also raises questions about current practices of refusing to answer certain user requests. Furthermore, the study conjectures that humans exhibit a change in their mental model, switching from interacting with a machine to interacting with a human.

# Major Findings:

1. LLMs are often accused of providing toxic content, but this is mostly demanded or provoked by humans who actively seek such content.
2. The manual analysis of conversations judged as toxic by API commercial vendors raises questions about current practices of refusing to answer certain user requests.
3. Humans exhibit a change in their mental model, switching from interacting with a machine to interacting with a human.

# Analysis and Critique:

1. The study's focus on real-world human interactions with LLMs in diverse, unconstrained settings is a strength, as it provides a more accurate representation of how humans interact with these models.
2. The conjecture that humans exhibit a change in their mental model is an interesting finding, but it requires further research to confirm its validity.
3. The study raises important questions about the current practices of refusing to answer certain user requests, which could have implications for the development and deployment of LLMs.
4. The study's focus on toxicity is important, but it could also benefit from exploring other aspects of human-LLM interactions, such as the impact of LLMs on human creativity and problem-solving abilities.
5. The study's reliance on a single dataset may limit the generalizability of its findings, and future research could benefit from using multiple datasets to confirm the results.
6. The study's focus on toxicity raises ethical concerns, and it is important for future research to consider the potential negative impacts of LLMs on society.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.05977v1](https://arxiv.org/abs/2407.05977v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.05977v1](https://browse.arxiv.org/html/2407.05977v1)       |
| Truncated       | False       |
| Word Count       | 8762       |