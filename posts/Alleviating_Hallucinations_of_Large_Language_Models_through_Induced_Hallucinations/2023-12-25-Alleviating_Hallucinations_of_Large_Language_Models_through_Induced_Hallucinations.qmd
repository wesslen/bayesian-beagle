
---
title: "Alleviating Hallucinations of Large Language Models through Induced Hallucinations"
description: "ICD strategy reduces LLM hallucinations, improving factuality in generated content across models. Effective on TruthfulQA and \textsc{FActScore} benchmarks."
author: "gpt-3.5-turbo-1106"
date: "2023-12-25"
link: "https://browse.arxiv.org/html/2312.15710v1"
image: "https://browse.arxiv.org/html/2312.15710v1/x1.png"
categories: ['robustness']
file-modified: 2024-01-02
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2312.15710v1/x1.png)

# Summary of "Alleviating Hallucinations of Large Language Models through Induced Hallucinations" 

## Key Findings:
1. **Hallucinations in Large Language Models (LLMs):** The study aims to address the issue of hallucinations in LLMs, where the models generate inaccurate or fabricated information, hindering their practical application.
2. **Induce-then-Contrast Decoding (ICD):** The proposed ICD method involves inducing hallucinations from LLMs and using them as penalty terms during decoding to improve factuality. Experimental results show significant improvement in truthfulness across various model sizes and families, comparable to state-of-the-art LLMs when equipped with ICD.
3. **Effectiveness:** ICD method provides substantial improvements in truthfulness on TruthfulQA and reduces hallucinations in open-ended text generation on FActScore compared to baseline decoding methods.

## Introduction
- Large Language Models (LLMs) have shown impressive capabilities but continue to struggle with hallucinations, generating inaccurate or fabricated information.
- Previous research suggests that training objectives and lack of knowledge may contribute to hallucinations in LLMs.

## Induce-then-Contrast Decoding
### Inducing Hallucinations from LLMs
- Factually weak LLMs are constructed by inducing hallucinations through fine-tuning or zero-shot prompting, which is then used as a penalty term to guide LLMs to generate more factual content.
- The fine-tuning process involves maximizing the log probability of the target output given system prompt and user input with new learned weights.

### Factually Weak LLM as A Penalty
- The decoding process involves amplifying predictions from the original model and downplaying untruthful predictions using a contrasting distribution to improve factuality.
- An adaptive plausibility constraint is introduced to select tokens for penalty, focusing only on those with higher probabilities than a certain proportion of the maximum probability assigned by the original model.

## Experiments
- Experimental evaluation on TruthfulQA and FActScore benchmarks demonstrates the effectiveness of ICD method in improving factuality and reducing hallucinations in LLMs compared to baseline decoding methods.
- Additional analyses include suitability of different task formats for inducing hallucinations, effectiveness across different LLM model sizes, and impact of data size and data source for inducing hallucinations.

## Critique
- The additional computational costs and latency introduced by contrastive decoding could be a limitation in practical application.
- The study primarily focuses on evaluating the effectiveness of ICD on TruthfulQA and FActScore, but the universal applicability of the method across different domains and tasks is yet to be determined.

Overall, the study introduces an innovative method, ICD, for alleviating hallucinations in LLMs, providing empirical evidence of its effectiveness. The potential of the method in addressing hallucinations in open domains and general tasks, as well as its scalability and practical application, warrant further investigation.

## Appendix

|          |          |
|----------|----------|
| Date Generated     | 2024-01-02       |
| HTML     | [https://browse.arxiv.org/html/2312.15710v1](https://browse.arxiv.org/html/2312.15710v1)       |
| Truncated       | False       |
| Word Count       | 4999       |