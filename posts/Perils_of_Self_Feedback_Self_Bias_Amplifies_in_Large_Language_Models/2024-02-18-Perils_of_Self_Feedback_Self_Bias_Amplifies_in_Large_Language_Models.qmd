
---
title: "Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models"
id: "2402.11436v1"
description: "Self-feedback improves some tasks, worsens others due to large language model bias."
author: Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Yang Wang
date: "2024-02-18"
image: "../../img/2402.11436v1/image_1.png"
categories: ['education']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11436v1/image_1.png)

### Summary:
- The article explores the concept of self-bias in large language models (LLMs) and its impact on the self-refinement pipeline. It presents findings from experiments on open-source LLMs, GPT-4, GPT-3.5-Turbo, and Gemini, showing that self-bias amplifies over iterations, leading to biased ensembles in reasoning paths and translation outputs. The study also discusses the impact of parameter size on self-bias in LLMs during self-refinement steps, highlighting the prevalence of self-bias in self-refine or self-rewarding pipelines and the potential for larger LLMs to be more resilient to self-bias. Additionally, the section outlines a method for aligning machine-generated scores with human scores, which is essential for evaluating the performance of machine translation models. Furthermore, it discusses the concept-to-text and concept-to-text refinement prompts used to test and refine large language models (LLMs) at Commongen Hard, providing examples of the prompts used, the generated sentences, and the feedback received for each prompt.

### Major Findings:
1. Self-bias amplifies over iterations in large language models, leading to biased ensembles in reasoning paths and translation outputs.
2. Larger LLMs exhibit less self-bias throughout self-refinement, with the 70B model showing a plateau in self-bias after the 5th iteration.
3. The methodology for aligning machine-generated scores with human scores is crucial for evaluating the quality of machine translation models.

### Analysis and Critique:
- The findings have significant implications for improving the performance of LLMs in various tasks by addressing self-bias through model size adjustments and external feedback incorporation.
- The study raises questions for future research on bias measurement between different LLMs and knowledge-distilled counterparts.
- The process described in the article provides a systematic approach to mapping the BLEURT score distribution to the human score distribution, ensuring that the relative ordering of scores is preserved, which is crucial for accurately assessing the quality of machine translations and can have implications for the development and improvement of translation models.
- The examples provided in the section on concept-to-text and concept-to-text refinement prompts illustrate the effectiveness of these prompts in refining LLMs and highlight the importance of providing accurate and comprehensive feedback to ensure that the generated sentences cover all specified concepts.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.11436v1](https://arxiv.org/abs/2402.11436v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11436v1](https://browse.arxiv.org/html/2402.11436v1)       |
| Truncated       | True       |
| Word Count       | 19127       |