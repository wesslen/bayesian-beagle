
---
title: "Ocassionally Secure: A Comparative Analysis of Code Generation Assistants"
id: "2402.00689v1"
description: "TL;DR: Study evaluates LLMs for secure code generation in real-world scenarios."
author: Ran Elgedawy, John Sadik, Senjuti Dutta, Anuj Gautam, Konstantinos Georgiou, Farzin Gholamrezae, Fujiao Ji, Kyungchan Lim, Qian Liu, Scott Ruoti
date: "2024-02-01"
image: "https://browse.arxiv.org/html/2402.00689v1/extracted/5375936/images/flowchart.png"
categories: ['production', 'programming', 'robustness', 'security', 'hci', 'architectures', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.00689v1/extracted/5375936/images/flowchart.png)

The summary of the academic article "Occasionally Secure: A Comparative Analysis of Code Generation Assistants" is as follows:

### Summary:
- The article focuses on identifying and understanding the conditions and contexts in which Large Language Models (LLMs) can be effectively and safely deployed in real-world scenarios to generate quality code.
- A comparative analysis of four advanced LLMs—GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google—was conducted using 9 separate tasks to assess each model’s code generation capabilities.
- The study contextualized the typical use cases of a real-life developer employing LLMs for everyday tasks as work and placed an emphasis on security awareness represented through the use of two distinct versions of a developer persona.
- In total, 61 code outputs were collected and analyzed across several aspects: functionality, security, performance, complexity, and reliability.

### Major Findings:
1. Notable differences in the type of code generated by each LLM platform were uncovered, with Bard being less likely to use external libraries, limiting its exposure to supply chain-related vulnerabilities.
2. Code generated by LLMs exhibited variable levels of security, with significant issues in areas crucial for maintaining code integrity, such as input validation, sanitization, and secret key management.
3. Expressing security consciousness to the LLM platform produced different results for different users, with varying effects on the security and functionality of the code generated.

### Analysis and Critique:
- The study provides valuable insights into the capabilities and limitations of LLMs for practical applications in automated code generation.
- However, the study is limited by the subjective nature of the evaluation process and the lack of control over the LLMs, which may have been updated during the data collection period.
- Future research should explore a wider range of models, personas, tasks, and programming languages to gain a more comprehensive understanding of LLM-generated code quality and security.

Overall, the article provides valuable insights into the performance and security considerations of LLMs in code generation, but further research is needed to address the limitations and expand the scope of the study.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.00689v1](https://arxiv.org/abs/2402.00689v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.00689v1](https://browse.arxiv.org/html/2402.00689v1)       |
| Truncated       | False       |
| Word Count       | 15088       |