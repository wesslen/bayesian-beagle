
---
title: "DracoGPT: Extracting Visualization Design Preferences from Large Language Models"
id: "2408.06845v1"
description: "DracoGPT assesses visualization design preferences in LLMs, finding moderate agreement between ranking and recommendation pipelines, but substantial divergence from human-based guidelines."
author: Huichen Will Wang, Mitchell Gordon, Leilani Battle, Jeffrey Heer
date: "2024-08-13"
image: "https://browse.arxiv.org/html/2408.06845v1/x2.png"
categories: ['hci', 'recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.06845v1/x2.png)

### Summary:

The paper introduces DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from Large Language Models (LLMs). The authors use Draco as a shared knowledge base to represent LLM design preferences and compare them to best practices from empirical research. The study demonstrates that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. The authors find that DracoGPT-Rank and -Recommend moderately agree with each other but both substantially diverge from guidelines drawn from human subjects experiments.

### Major Findings:

1. DracoGPT is a method for extracting, modeling, and assessing visualization design preferences from LLMs, which can be applied across various LLMs, prompts, and tasks.
2. DracoGPT-Rank and -Recommend can learn knowledge base configurations whose preferences accurately match LLM judgments, allowing for the analysis of LLM preferences by comparing fitted Draco knowledge bases.
3. Draco chart costs derived from GPT4-Turbo rank and recommend pipelines moderately correlate with each other, while both correlate weakly with costs learned from human performance data, thus diverging from empirical performance data.

### Analysis and Critique:

1. The study focuses on a limited set of LLMs and tasks, which may not be representative of the broader landscape of LLMs and visualization tasks.
2. The authors acknowledge that subtle changes to textual prompts can sometimes lead to different LLM responses, which may introduce variability in the results.
3. The study does not address the potential impact of LLM-generated visualization design preferences on real-world applications, such as data analysis and decision-making.
4. The authors do not discuss the potential implications of LLMs' divergence from empirical best practices, such as the need for further research to understand the factors contributing to this divergence.
5. The study does not explore the potential for LLMs to learn and adapt to new visualization design best practices over time, which could be an important area for future research.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-20       |
| Abstract | [https://arxiv.org/abs/2408.06845v1](https://arxiv.org/abs/2408.06845v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.06845v1](https://browse.arxiv.org/html/2408.06845v1)       |
| Truncated       | False       |
| Word Count       | 5592       |