
---
title: "Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification"
id: "2407.20859v1"
description: "TL;DR: New attack method can mislead LLM agents, causing up to 80% failure rates, posing significant risks that are hard to detect."
author: Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, Yang Zhang
date: "2024-07-30"
image: "https://browse.arxiv.org/html/2407.20859v1/extracted/5764182/images/system.png"
categories: ['security', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.20859v1/extracted/5764182/images/system.png)

### Summary:

This paper introduces a new type of attack on autonomous agents built on large language models (LLMs) that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions. The authors conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility. The experiments reveal that these attacks can induce failure rates exceeding 80% in multiple scenarios. The study also highlights the realistic risks associated with these vulnerabilities in multi-agent scenarios. The authors propose self-examination detection methods, but their findings indicate these attacks are difficult to detect effectively using LLMs alone, emphasizing the substantial risks associated with this vulnerability.

### Major Findings:
1. The proposed attack can induce malfunctions in LLM agents, causing failure rates exceeding 80% in multiple scenarios.
2. The attack is more difficult to detect compared to prior approaches that sought overtly harmful actions.
3. The self-examination defense's limited effectiveness against the proposed attack further underscores the severity of the vulnerability.

### Analysis and Critique:

The paper presents a novel approach to attacking autonomous agents built on LLMs, highlighting the potential risks associated with their deployment. The authors' comprehensive evaluations and the use of implemented and deployable agents in multi-agent scenarios provide a realistic assessment of the vulnerabilities. However, the study's focus on a single type of attack and the limited effectiveness of the proposed self-examination defense method may not fully capture the complexity of the problem. Future research should explore additional attack vectors and more robust defense mechanisms to ensure the safe and secure deployment of LLM-based agents.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.20859v1](https://arxiv.org/abs/2407.20859v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.20859v1](https://browse.arxiv.org/html/2407.20859v1)       |
| Truncated       | False       |
| Word Count       | 11241       |