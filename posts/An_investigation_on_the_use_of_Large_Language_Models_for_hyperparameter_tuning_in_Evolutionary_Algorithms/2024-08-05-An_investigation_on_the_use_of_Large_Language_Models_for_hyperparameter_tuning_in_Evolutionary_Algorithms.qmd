
---
title: "An investigation on the use of Large Language Models for hyperparameter tuning in Evolutionary Algorithms"
id: "2408.02451v1"
description: "LLMs can optimize hyperparameters in Evolution Strategies, as shown in a preliminary study on step-size adaptation for (1+1)11(1+1)( 1 + 1 )-ES."
author: Leonardo Lucio Custode, Fabio Caraffini, Anil Yaman, Giovanni Iacca
date: "2024-08-05"
image: "https://browse.arxiv.org/html/2408.02451v1/x1.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.02451v1/x1.png)

### Summary:
- The article explores the use of Large Language Models (LLMs) for hyperparameter tuning in Evolutionary Algorithms (EAs), specifically focusing on step-size adaptation for -ES.
- LLMs have been successfully applied to various domains, including chemistry, protein design, robotics, and urban delivery route optimization.
- The authors conduct a preliminary investigation on the possibility of automating "empirical" step-size adaptation in Evolution Strategies (ES) using LLMs, specifically Llama2-70b and Mixtral.
- The results show that LLMs can compete with well-known traditional mechanisms for step-size adaptation.

### Major Findings:
1. LLMs can be an effective method for optimizing hyperparameters in Evolution Strategies, encouraging further research in this direction.
2. The study focuses on the case of tuning the step-size for -ES using state-of-the-art LLMs, namely Llama2-70b and Mixtral.
3. The results suggest that LLMs can compete with well-known traditional mechanisms for step-size adaptation.

### Analysis and Critique:
- The article provides a promising direction for using LLMs in hyperparameter optimization for EAs.
- However, the study is preliminary and focuses on a specific case of step-size adaptation for -ES. Further research is needed to explore the potential of LLMs in other hyperparameter optimization scenarios.
- The study does not discuss the computational cost of using LLMs for hyperparameter optimization, which could be a significant limitation.
- The authors acknowledge that current LLMs are not well-suited for low-level (i.e., numerical) continuous optimization tasks due to the inference cost of querying LLMs and their limited mathematical reasoning capabilities. This could be a potential problem when applying LLMs to other hyperparameter optimization tasks.
- The study does not compare the performance of LLMs with other hyperparameter optimization methods, which could provide a more comprehensive evaluation of the proposed approach.
- The authors do not discuss the potential biases or limitations of the LLMs used in the study, which could impact the generalizability of the findings.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2408.02451v1](https://arxiv.org/abs/2408.02451v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.02451v1](https://browse.arxiv.org/html/2408.02451v1)       |
| Truncated       | False       |
| Word Count       | 5174       |