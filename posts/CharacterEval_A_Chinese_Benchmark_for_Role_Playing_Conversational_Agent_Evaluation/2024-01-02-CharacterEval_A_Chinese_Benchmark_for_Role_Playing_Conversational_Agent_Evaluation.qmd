
---
title: "CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation"
id: "2401.01275v1"
description: "An introduction of CharacterEval, a Chinese benchmark for Role-Playing Conversational Agents' assessment with a tailored dataset."
author: ['Quan Tu', 'Shilong Fan', 'Zihang Tian', 'Rui Yan']
date: "2024-01-02"
image: "https://browse.arxiv.org/html/2401.01275v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.01275v1/x1.png)

## Major Findings

1. **CharacterEval** presents a novel Chinese benchmark for evaluating Role-Playing Conversational Agents (RPCAs), addressing the absence of comprehensive benchmarks in the field of emotionally engaging conversational agents.
   
2. The benchmark introduces a dataset of 1,785 multi-turn role-playing dialogues, featuring 77 characters derived from Chinese novels and scripts, carefully constructed and rigorously controlled for quality.

3. The evaluation approach includes thirteen specific metrics on four dimensions and introduces a role-playing reward model, **CharacterRM**, based on human annotations, which outperforms GPT-4 in correlation with human judgment.

## Introduction
- Large language models (LLMs) have revolutionized generative agents and opened up new possibilities in various applications, including in *Role-Playing Conversational Agents* (RPCAs), which engage users in dynamic scenarios as specific characters or roles from existing compositions (e.g., novels, films).
- There is considerable interest in the *multifaceted capabilities* of RPCAs, but the absence of a comprehensive benchmark impedes the systematic assessment and comparison of RPCA capabilities.

## Data Collection
- The construction of a dataset for role-playing conversation is complex and requires careful consideration of fidelity to source material, diversity in distribution, multi-turn features, and human-in-the-loop involvement to ensure quality and authenticity.
- The dataset comprises 1,785 multi-turn role-playing dialogues and 77 leading characters drawn from diverse Chinese novels and scripts, carefully constructed through a process involving GPT-4 extraction, human filtering, and detailed character profiles from Baidu Baike.

## Evaluation Metric
- **CharacterEval** employs a multifaceted evaluation approach, encompassing thirteen specific metrics on four dimensions: conversational ability, character consistency, role-playing attractiveness, and personality back-testing. These metrics are designed to comprehensively assess RPCA capabilities in role-playing conversation.

## Experiment
- Comprehensive evaluations of existing LLMs on **CharacterEval** demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in Chinese role-playing conversation.
- The results indicate that specialized models designed for role-playing dialogues, such as BC-Character-Turbo and MiniMax, outperform general-purpose LLMs like GPT-4 and GPT-3.5 in specific dimensions such as character consistency and role-playing attractiveness.

## Critique
The paper presents a comprehensive and rigorous approach to evaluating RPCAs. However, potential limitations and problems to consider include:
- The reliance on human annotations for training CharacterRM and evaluating RPCAs may introduce subjectivity and bias.
- The research focuses largely on the specific context of Chinese role-playing conversation, which may limit the generalizability of the findings to other languages or cultural contexts.
- The complexity in constructing a high-quality dataset may limit scalability and accessibility for researchers in the field.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [http://arxiv.org/abs/2401.01275v1](http://arxiv.org/abs/2401.01275v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.01275v1](https://browse.arxiv.org/html/2401.01275v1)       |
| Truncated       | False       |
| Word Count       | 7604       |