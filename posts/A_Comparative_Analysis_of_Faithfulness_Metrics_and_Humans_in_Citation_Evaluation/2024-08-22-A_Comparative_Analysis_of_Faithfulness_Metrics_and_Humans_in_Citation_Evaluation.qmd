
---
title: "A Comparative Analysis of Faithfulness Metrics and Humans in Citation Evaluation"
id: "2408.12398v1"
description: "LLMs struggle to verify citations; current faithfulness metrics aren't consistently effective for fine-grained citation support. No single metric excels in all evaluations."
author: Weijia Zhang, Mohammad Aliannejadi, Jiahuan Pei, Yifei Yuan, Jia-Hong Huang, Evangelos Kanoulas
date: "2024-08-22"
image: "https://browse.arxiv.org/html/2408.12398v1/x2.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.12398v1/x2.png)

### Summary:

- The paper explores the effectiveness of faithfulness metrics in evaluating the support provided by citations to statements generated by large language models (LLMs).
- The study focuses on fine-grained citation support scenarios, including full, partial, and no support, which are not adequately addressed by existing research.
- A comparative evaluation framework is proposed to assess the alignment between metric scores and human judgments across three evaluation protocols: correlation analysis, classification evaluation, and retrieval evaluation.

### Major Findings:

1. **No Single Metric Consistently Excels**: Experimental results reveal that no single faithfulness metric consistently outperforms others across all evaluation protocols, indicating the complexity of automated citation evaluation and the limitations of existing metrics.
2. **Challenges in Identifying Partial Support**: The best-performing metrics, such as the entailment-based AutoAIS, struggle to identify cases of partial support, highlighting the inherent complexities of automated citation evaluation.
3. **Retrieval Evaluation**: In terms of retrieval evaluation, similarity-based metrics, such as BERTScore, consistently surpass best-performing entailment-based metrics. This indicates that entailment-based metrics exhibit higher sensitivity to noisy data, which is introduced by a considerable number of irrelevant documents in such scenarios.

### Analysis and Critique:

- The paper provides a comprehensive evaluation of faithfulness metrics in fine-grained citation support scenarios, which is a significant contribution to the field.
- The proposed comparative evaluation framework offers a systematic approach to assess the alignment between metric scores and human judgments, providing valuable insights into the effectiveness of different metrics.
- However, the study does not provide a clear solution to the limitations of existing metrics in identifying partial support scenarios. Further research is needed to develop more effective metrics that can accurately distinguish between full, partial, and no support.
- The paper also highlights the need for improving the robustness of entailment-based metrics against irrelevant documents in retrieval scenarios. This could be addressed by introducing contrastive learning frameworks for fine-tuning metrics.
- The study could benefit from a more detailed analysis of the strengths and weaknesses of each metric in different evaluation protocols. This would provide a more nuanced understanding of the capabilities and limitations

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.12398v1](https://arxiv.org/abs/2408.12398v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.12398v1](https://browse.arxiv.org/html/2408.12398v1)       |
| Truncated       | False       |
| Word Count       | 6443       |