{"id": "2312.12321v1", "text": "# Bypassing the Safety Training of Open-Source LLMs with Priming Attacks\n\n## 1 Introduction\nThe paper investigates the vulnerability of state-of-the-art (SOTA) open-source Large Language Models (LLMs) to \"priming attacks,\" which bypass safety training and exploit the autoregressive nature of LLMs to fulfill harmful requests.\n\n## 2 Methodology & Results\n- **Few-shot Priming Attacks:** The authors use a few-shot prompting method to generate priming attacks with a helper LLM, demonstrating improved attack success rates compared to baselines.\n- **Experimental Setup:** The study uses the Llama-2 model as the helper LLM and Harmful Behaviors prompts for attack evaluation. The results show that the priming attack outperforms baselines in all cases.\n\n## 3 Conclusion\nThe research highlights the fragility of current LLM safety measures under practical assumptions and emphasizes the need for further research into safer open-sourcing of LLMs.\n\n### A Details on experimental setup\nThe experiments were conducted on a server and utilized the Llama-2 model for few-shot prompting.\n\n### B Few-Shot Prompt for Generating Priming Attacks\nThe format for few-shot prompts used to generate priming attacks is illustrated.\n\n### C Llama Guard Task Instructions\nThe task instructions provided to Llama Guard for evaluating the attacks are detailed.\n\n### D Manual Evaluation Benchmark\nExamples of LLM responses evaluated as harmful or safe, with an emphasis on response content and backtracking.\n\n### E Manual Evaluation vs. Llama Guard\nA comparison of manual evaluation with Llama Guard results, showing discrepancies and the conservative nature of Llama Guard's classifications.\n\n### F Runtime Comparison\nThe runtime comparison between few-shot prompting and optimization-based techniques is provided.\n\nThe findings suggest a need for improved safety measures for open-source LLMs and encourage further investigation into novel methods for safer open-sourcing.\n\n---\n\nThe summary includes the main sections of the paper, along with important details and comparisons. If you need further details on specific sections or aspects of the paper, feel free to ask!", "meta": {"url": "https://browse.arxiv.org/html/2312.12321v1", "title": "Bypassing the Safety Training of Open-Source LLMs with Priming Attacks", "subtitle": "LLMs vulnerable to \"priming attacks\", bypass safety training, up to 3.3x more successful harmful behaviors. Source code available.", "categories": ["security", "open-source"], "publish_date": "2023-12-19"}}
{"id": "2312.02102v2", "text": "# Mitigating Data Injection Attacks on Federated Learning\n\n## 1 Introduction\nThe paper addresses the vulnerability of federated learning to data injection attacks, where malicious entities manipulate the learning process by injecting false data. It proposes a novel detection and mitigation method to address these attacks.\n\n## 2 Problem Formulation\n### 2.1 Federated Learning\nThe paper defines the federated learning problem, where multiple independent agents train local models using their private datasets and share model parameters with a coordinating node.\n\n### 2.2 Data Injection Attacks\nIt describes the nature of data injection attacks, including \"constant-output attack\" and \"label-flip attack,\" and the challenges in detecting them in a decentralized setting.\n\n## 3 Attacker Detection and Avoidance\nThe paper proposes a method for detecting attackers by comparing the updates received from edge agents over time. A low-complexity metric is used to identify and ignore updates from suspected attackers.\n\n## 4 Simulations\nThe paper presents simulations of two types of attacks: constant-output attack and label-flip attack. It demonstrates the effectiveness of the proposed detection and mitigation scheme in identifying and overcoming these attacks.\n\n## 5 Conclusions\nThe paper concludes by outlining the robustness of the proposed federated learning algorithm in the presence of data injection attacks. It also mentions plans for future work, including detailed proofs and bounds. \n\nFinally, the paper cites several references covering related work in federated learning and data security.", "meta": {"url": "https://browse.arxiv.org/html/2312.02102v2", "title": "Mitigating Data Injection Attacks on Federated Learning", "subtitle": "Federated learning faces data injection attacks, but a new method detects and mitigates attacks during training.", "categories": ["security"], "publish_date": "2023-12-04"}}
