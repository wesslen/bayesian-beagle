
---
title: "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset"
id: "2402.10176v1"
description: "Synthetic datasets improve math instruction tuning for large language models. OpenMathInstruct-1 dataset and model released."
author: Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman
date: "2024-02-15"
image: "../../../bayesian-beagle.png"
categories: ['architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:
- Recent work has shown the potential of synthetically generated datasets for training large language models (LLMs) for acquiring targeted skills.
- OpenMathInstruct-1 is a math instruction tuning dataset with 1.8M problem-solution pairs constructed using the Mixtral model.
- The dataset achieves competitive scores with gpt-distilled models and is released under a commercially permissive license.

### Major Findings:
1. OpenMathInstruct-1 is constructed using the Mixtral model and achieves competitive scores with gpt-distilled models.
2. The dataset has a training set coverage of 93% for MATH and 99.9% for GSM8K.
3. The dataset is at least four times bigger than previous mathematical reasoning fine-tuning datasets and is permissively licensed.

### Analysis and Critique:
- The article provides a comprehensive overview of the dataset construction process and the performance of the fine-tuned models.
- The use of masked text solutions and code-preferential data selection strategies are highlighted as effective techniques.
- The article acknowledges the limitations of the dataset, such as the presence of semantically noisy solutions and the need for further research on semantic filters.
- The potential impact of the dataset on the development of open-source LLMs for mathematical reasoning is emphasized.

Overall, the article provides valuable insights into the construction and performance of the OpenMathInstruct-1 dataset, while also acknowledging the need for further research and development in this area.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.10176v1](https://arxiv.org/abs/2402.10176v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.10176v1](https://browse.arxiv.org/html/2402.10176v1)       |
| Truncated       | False       |
| Word Count       | 8009       |