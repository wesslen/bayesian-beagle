
---
title: "DHP Benchmark: Are LLMs Good NLG Evaluators?"
id: "2408.13704v1"
description: "LLMs' NLG evaluation discernment is benchmarked using DHP framework, revealing strengths and limitations across six datasets and four tasks."
author: Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu, Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, Xia Hu
date: "2024-08-25"
image: "https://browse.arxiv.org/html/2408.13704v1/x1.png"
categories: ['social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.13704v1/x1.png)

### Summary:

The paper introduces the DHP (Discernment of Hierarchical Perturbation) benchmarking framework, which aims to quantitatively measure the evaluation capabilities of LLMs (Large Language Models) as NLG (Natural Language Generation) evaluators. The framework utilizes hierarchically perturbed text data and statistical tests to provide discernment scores for LLMs, eliminating the need for human annotations. The study re-establishes six evaluation datasets for four NLG tasks: Summarization, Story Completion, Question Answering, and Translation. The benchmarking of five major LLM series provides insights into their strengths and limitations as NLG evaluators.

### Major Findings:

1. The DHP benchmarking framework provides a quantitative approach to assessing LLMs' NLG evaluation capabilities, addressing the lack of clear and unbiased measurement in existing methods.
2. The framework employs hierarchical perturbation and statistical tests to overcome the challenges of biased response styles and multiple evaluation metrics in assessing LLMs as NLG evaluators.
3. The study re-establishes six evaluation datasets for four NLG tasks, covering a range of text perturbations from minor character problems to significant sentence alterations, to test the potential discernment limits of LLMs.

### Analysis and Critique:

1. The DHP benchmarking framework offers a more rigorous and comprehensive evaluation of LLM performance, independent of the response styles of the tested models.
2. The framework's reliance on hierarchical perturbation and statistical tests allows for a more accurate and fair assessment of LLM capabilities, focusing on the relative quality assessment rather than absolute values.
3. The study's comprehensive benchmarking of five major LLM series provides critical insights into their capabilities as NLG evaluators, highlighting areas where they excel and where they may fall short.
4. The paper's focus on quantitative discernment scores for LLMs as NLG evaluators emphasizes the necessity of considering multiple metrics for accurate and reliable evaluations.
5. A potential limitation of the DHP benchmark is its specificity to each NLG dataset, which may not fully capture the general evaluation capabilities of LLMs across all NLG tasks.
6. The benchmark'

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.13704v1](https://arxiv.org/abs/2408.13704v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.13704v1](https://browse.arxiv.org/html/2408.13704v1)       |
| Truncated       | False       |
| Word Count       | 7759       |