
---
title: "RelayAttention for Efficient Large Language Model Serving with Long System Prompts"
id: "2402.14808v1"
description: "Improving efficiency of large language models with long prompts using RelayAttention algorithm."
author: Lei Zhu, Xinjiang Wang, Wayne Zhang, Rynson W. H. Lau
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14808v1/x1.png"
categories: ['prompt-engineering', 'robustness', 'architectures', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14808v1/x1.png)

### **Summary:**
- The paper addresses the issue of long system prompts causing throughput/latency bottlenecks in large language model (LLM) services.
- The authors propose RelayAttention, an attention algorithm that reduces redundant memory accesses and improves the efficiency of LLM services.
- RelayAttention is shown to maintain generation quality without requiring model retraining.

### **Major Findings:**
1. Long system prompts in LLM services cause substantial throughput and latency degradation, increasing the per-request cost.
2. RelayAttention, by eliminating redundant memory access, provides up to 2.2x sustainable request rate and 2.0x throughput improvement for LLMs in a chatbot workload.
3. The proposed approach is applicable to various popular LLMs and consistently improves efficiency across different data center GPUs.

### **Analysis and Critique:**
- The proposed RelayAttention algorithm is effective in improving the efficiency of LLM services, particularly in batched inference scenarios.
- However, the limitations of RelayAttention, such as its suitability for cloud-serving scenarios and diminishing efficiency gains with longer request-specific contexts, should be considered.
- The paper provides a comprehensive analysis and empirical validation of the proposed approach, demonstrating its potential for practical implementation in LLM serving systems.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.14808v1](https://arxiv.org/abs/2402.14808v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14808v1](https://browse.arxiv.org/html/2402.14808v1)       |
| Truncated       | False       |
| Word Count       | 6523       |