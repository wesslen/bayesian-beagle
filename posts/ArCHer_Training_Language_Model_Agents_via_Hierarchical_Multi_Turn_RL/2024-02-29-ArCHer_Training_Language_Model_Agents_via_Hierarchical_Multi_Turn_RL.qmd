
---
title: "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL"
id: "2402.19446v1"
description: "ArCHer: Hierarchical RL for Efficient Multi-turn Decision-making in LLMs

This paper proposes a new algorithmic framework, ArCHer, for developing multi-turn reinforcement learning (RL) algorithms for large language models (LLMs) used in goal-directed decision-making tasks. ArCHer uses a hierarchical RL approach, running two RL algorithms in parallel: a high-level off-policy RL algorithm that trains a value function to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token-by-token policy within each utterance. The paper finds that ArCHer significantly improves efficiency and performance on multi-turn tasks, with a sample efficiency of about 100x over existing on-policy methods. The project page and code can be found at <https://yifeizhou02."
author: Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, Aviral Kumar
date: "2024-02-29"
image: "https://browse.arxiv.org/html/2402.19446v1/x1.png"
categories: ['hci', 'production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.19446v1/x1.png)

### Summary:

- Large language models (LLMs) have the potential to address decision-making or "agent" problems that can be expressed in text or natural language.
- Current RL methods for LLMs largely focus on single-turn reward maximization, which cannot effectively train LLMs to seek and incorporate information over multiple turns, perform credit assignment, or reason about their past actions.
- The paper proposes an algorithmic framework, ArCHer, for developing multi-turn RL algorithms for fine-tuning LLMs, preserving the flexibility of existing single-turn RL methods while accommodating multiple turns, long horizons, and delayed rewards effectively.
- ArCHer adopts a hierarchical RL approach, running two RL algorithms in parallel: a high-level off-policy RL algorithm that trains a value function to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x7b-instruct       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.19446v1](https://arxiv.org/abs/2402.19446v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.19446v1](https://browse.arxiv.org/html/2402.19446v1)       |
| Truncated       | False       |
| Word Count       | 16897       |