
---
title: "PrefCLM: Enhancing Preference-based Reinforcement Learning with Crowdsourced Large Language Models"
id: "2407.08213v1"
description: "PrefCLM uses crowdsourced LLMs for preference-based robot learning, improving user satisfaction in HRI scenarios."
author: Ruiqi Wang, Dezhong Zhao, Ziqin Yuan, Ike Obi, Byung-Cheol Min
date: "2024-07-11"
image: "../../img/2407.08213v1/image_1.png"
categories: ['social-sciences', 'hci', 'education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2407.08213v1/image_1.png)

Summary:

The article introduces PrefCLM, a novel framework that utilizes crowdsourced large language models (LLMs) as simulated teachers in preference-based reinforcement learning (PbRL). PrefCLM aims to address the challenges of existing PbRL methods, which often require a large volume of feedback and rely on synthetic feedback generated by scripted teachers. The framework employs Dempster-Shafer Theory to fuse individual preferences from multiple LLM agents at the score level, efficiently leveraging their diversity and collective intelligence. Additionally, PrefCLM includes a human-in-the-loop pipeline that facilitates collective refinements based on user interactive feedback.

Major Findings:

1. PrefCLM achieves competitive performance compared to traditional scripted teachers and excels in facilitating more natural and efficient behaviors.
2. A real-world user study (N=10) demonstrates that PrefCLM significantly enhances user satisfaction in human-robot interaction (HRI) scenarios by tailoring robot behaviors to individual user preferences.

Analysis and Critique:

While PrefCLM shows promising results, there are potential limitations and areas for further research. The reliance on LLMs for generating synthetic feedback may introduce biases or inaccuracies, as LLMs may not fully capture the nuances of human preferences. Additionally, the scalability and generalizability of PrefCLM to more complex tasks and environments remain to be explored. Further research is needed to address these challenges and validate the effectiveness of PrefCLM in diverse HRI scenarios.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.08213v1](https://arxiv.org/abs/2407.08213v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.08213v1](https://browse.arxiv.org/html/2407.08213v1)       |
| Truncated       | False       |
| Word Count       | 19231       |