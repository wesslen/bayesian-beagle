
---
title: "REPOEXEC: Evaluate Code Generation with a Repository-Level Executable Benchmark"
id: "2406.11927v1"
description: "RepoExec benchmark evaluates code generation at repository-level, focusing on executability, correctness, and dependency integration."
author: Nam Le Hai, Dung Manh Nguyen, Nghi D. Q. Bui
date: "2024-06-17"
image: "https://browse.arxiv.org/html/2406.11927v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.11927v1/x1.png)

### Summary:

- The paper introduces RepoExec, a novel benchmark for evaluating code generation at the repository level, emphasizing executability and correctness.
- RepoExec provides an automated system that verifies requirements and incorporates a mechanism for dynamically generating high-coverage test cases to assess the functionality of generated code.
- The benchmark focuses on a controlled scenario where developers specify necessary code dependencies, challenging the model to integrate these accurately.
- Experiments show that pretrained LLMs outperform instruction-tuning models in correctness, while the latter excel in utilizing provided dependencies and demonstrating debugging capabilities.
- RepoExec aims to provide a comprehensive evaluation of code functionality and alignment with developer intent, paving the way for more reliable and applicable CodeLLMs in real-world scenarios.

### Major Findings:

1. Pretrained LLMs outperform instruction-tuning models in correctness.
2. Instruction-tuning models excel in utilizing provided dependencies and demonstrating debugging capabilities.
3. RepoExec provides a comprehensive evaluation of code functionality and alignment with developer intent.

### Analysis and Critique:

- The paper does not provide a detailed comparison of RepoExec with existing benchmarks, making it difficult to assess its novelty and advantages.
- The paper does not discuss the potential limitations or biases of the proposed benchmark, which could impact its generalizability and applicability.
- The paper does not provide a clear definition of "executability" and "correctness," which are crucial for understanding the benchmark's evaluation criteria.
- The paper does not discuss the potential impact of the benchmark on the development of CodeLLMs or the broader implications for software engineering research.
- The paper does not provide a clear roadmap for future research or potential applications of the benchmark.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.11927v1](https://arxiv.org/abs/2406.11927v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.11927v1](https://browse.arxiv.org/html/2406.11927v1)       |
| Truncated       | False       |
| Word Count       | 7146       |