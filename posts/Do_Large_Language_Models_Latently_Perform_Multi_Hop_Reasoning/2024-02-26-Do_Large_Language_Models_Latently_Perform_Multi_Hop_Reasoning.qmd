
---
title: "Do Large Language Models Latently Perform Multi-Hop Reasoning?"
id: "2402.16837v1"
description: "Large Language Models (LLMs) show evidence of latent multi-hop reasoning in complex prompts."
author: Sohee Yang, Elena Gribovskaya, Nora Kassner, Mor Geva, Sebastian Riedel
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16837v1/x1.png"
categories: ['prompt-engineering', 'robustness', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16837v1/x1.png)

### **Summary:**
- The study investigates whether Large Language Models (LLMs) perform latent multi-hop reasoning with complex prompts.
- Evidence of latent multi-hop reasoning is found for certain relation types, but the utilization is highly contextual and varies across different types of prompts.
- The evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop.

### Major Findings:
1. Strong evidence of latent multi-hop reasoning for certain relation types, with the reasoning pathway used in more than 80% of the prompts.
2. The evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop.
3. A clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop.

### Analysis and Critique:
- The study provides valuable insights into the latent multi-hop reasoning abilities of LLMs, but it has some limitations and potential biases.
- The study focuses on one pathway for latent multi-hop reasoning, and other pathways might exist.
- The dataset construction process involves manual selection and may introduce noise from Wikidata.
- The proposed metrics, such as internal entity recall score and consistency score, are approximations and may have limitations.
- The study provides a foundation for future research aiming to promote and strengthen latent multi-hop reasoning in LLMs.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.16837v1](https://arxiv.org/abs/2402.16837v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16837v1](https://browse.arxiv.org/html/2402.16837v1)       |
| Truncated       | False       |
| Word Count       | 11483       |