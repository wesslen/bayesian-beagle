
---
title: "SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words"
id: "2406.13340v1"
description: "TL;DR: SD-Eval benchmark assesses spoken dialogue understanding & generation, focusing on paralinguistic & environmental info, with models conditioned on this data outperforming others."
author: Junyi Ao, Yuancheng Wang, Xiaohai Tian, Dekun Chen, Jun Zhang, Lu Lu, Yuxuan Wang, Haizhou Li, Zhizheng Wu
date: "2024-06-19"
image: "https://browse.arxiv.org/html/2406.13340v1/x1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.13340v1/x1.png)

### Summary:

The paper introduces a novel benchmark dataset, SD-Eval, for multidimensional evaluation of spoken dialogue understanding and generation. The dataset focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound. The paper also presents three different models implemented to assess the SD-Eval benchmark dataset and conducts a comprehensive evaluation using objective evaluation methods, subjective evaluations, and LLM-based metrics for the generated responses. The results show that models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures. Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics.

### Major Findings:

1. The SD-Eval benchmark dataset is a novel dataset for multidimensional evaluation of spoken dialogue understanding and generation, focusing on paralinguistic and environmental information.
2. The dataset includes 7,303 utterances, amounting to 8.76 hours of speech data, aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound.
3. Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures.
4. LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics.

### Analysis and Critique:

The paper presents a well-structured and comprehensive evaluation of the SD-Eval benchmark dataset. The dataset is a valuable contribution to the field of spoken dialogue understanding and generation, as it focuses on paralinguistic and environmental information, which is often overlooked in other datasets. The use of LLM-based metrics for evaluation is also a significant contribution, as it shows a higher correlation with human evaluation compared to traditional metrics.

However, the paper does not discuss the limitations of the dataset or the evaluation methods used. It would be beneficial to include a discussion of the potential biases or shortcomings of the dataset and the evaluation methods. Additionally, the paper does not provide any information on the generalizability of the results to other datasets or domains.

Overall, the

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.13340v1](https://arxiv.org/abs/2406.13340v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.13340v1](https://browse.arxiv.org/html/2406.13340v1)       |
| Truncated       | False       |
| Word Count       | 5962       |