
---
title: "Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency"
id: "2402.15930v1"
description: "Analysis of GEC prompting strategies with LLMs based on language proficiency to reduce overcorrection."
author: Min Zeng, Jiexin Kuang, Mengyang Qiu, Jayoung Song, Jungyeul Park
date: "2024-02-24"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### **Summary:**
- The paper analyzes prompting strategies for grammatical error correction (GEC) with large language models (LLM) based on language proficiency.
- The study aims to reduce overcorrection by examining the interaction between LLM’s performance and L2 language proficiency.
- The results indicate that overcorrection happens primarily in advanced language learners’ writing (proficiency C) rather than proficiency A and B.

### Major Findings:
1. GEC using generative LLMs has a tendency to overcorrect, leading to higher recall but lower precision measures.
2. Overcorrection happens primarily in advanced language learners’ writing (proficiency C) rather than proficiency A and B.
3. Few-shot prompting exhibits better performance compared to zero-shot prompting in GEC using GPT-2 and GPT-3.5.

### Analysis and Critique:
- The study provides valuable insights into the performance of LLMs in GEC based on language proficiency levels.
- The findings suggest that few-shot prompting may not have a significant impact on fine-tuned GPT-2 models.
- The study raises questions about the effectiveness of prompting GPT for the GEC task and highlights the need for further investigation and exploration of potential improvements.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-27       |
| Abstract | [https://arxiv.org/abs/2402.15930v1](https://arxiv.org/abs/2402.15930v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.15930v1](https://browse.arxiv.org/html/2402.15930v1)       |
| Truncated       | False       |
| Word Count       | 3379       |