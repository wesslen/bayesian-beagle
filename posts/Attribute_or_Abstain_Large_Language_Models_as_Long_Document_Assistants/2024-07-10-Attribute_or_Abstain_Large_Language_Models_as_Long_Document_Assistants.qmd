
---
title: "Attribute or Abstain: Large Language Models as Long Document Assistants"
id: "2407.07799v1"
description: "LLMs can improve long document work, but hallucinate. Attribution boosts trust; new benchmark LAB evaluates attribution in long documents, finding citation-based approach most effective. Evidence quality predicts response quality for simple, not complex, responses."
author: Jan Buchmann, Xiao Liu, Iryna Gurevych
date: "2024-07-10"
image: "https://browse.arxiv.org/html/2407.07799v1/x1.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.07799v1/x1.png)

### Summary:

This paper presents a benchmark called LAB (Long-document Attribution Benchmark) for evaluating the attribution capabilities of large language models (LLMs) in long document tasks. The benchmark consists of 6 diverse long document tasks with attribution, and the authors experiment with different approaches to attribution on 4 LLMs of different sizes, both prompted and fine-tuned. The main findings of the paper are:

1. Citation, i.e., response generation and evidence extraction in one step, mostly performs best in terms of evidence quality.
2. The "Lost in the Middle" phenomenon, where LLMs struggle with information in the middle of long documents, does not exist for attribution.
3. Evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims.

### Major Findings:

1. Citation performs best: The authors find that citation, i.e., response generation and evidence extraction in one step, mostly performs best in terms of evidence quality. This is in line with recent work showing LLM capabilities for retrieval.
2. No "Lost in the Middle" phenomenon: The authors investigate whether the "Lost in the Middle" phenomenon, where LLMs struggle with information in the middle of long documents, exists for attribution. They do not find this to be the case.
3. Evidence quality predicts response quality for simple responses: The authors find that evidence quality can predict response quality on datasets with simple responses, but not so for complex responses, as models struggle with providing evidence for complex claims.

### Analysis and Critique:

* The paper provides a comprehensive evaluation of LLMs for attribution in long document tasks, which is a valuable contribution to the field.
* The authors use a diverse set of tasks and datasets, which helps to ensure the generalizability of their findings.
* The paper could benefit from a more detailed analysis of the limitations of the study, such as the use of a limited number of LLMs and the potential impact of the choice of attribution approach on the results.
* The paper could also benefit from a more detailed discussion of the implications of the findings for the development of LLMs for long document tasks.
* The paper could also benefit from a

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.07799v1](https://arxiv.org/abs/2407.07799v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.07799v1](https://browse.arxiv.org/html/2407.07799v1)       |
| Truncated       | False       |
| Word Count       | 9325       |