
---
title: "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing"
id: "2402.16192v1"
description: "TL;DR: SemanticSmooth defends against jailbreaking attacks on large language models with strong performance. Codes available."
author: Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang
date: "2024-02-25"
image: "https://browse.arxiv.org/html/2402.16192v1/x1.png"
categories: ['robustness', 'prompt-engineering', 'security', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16192v1/x1.png)

### Summary:
- Large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass safeguards and generate objectionable content.
- SemanticSmooth is proposed as a defense, using semantic transformations and a learnable policy network to aggregate predictions.
- Experimental results show that SemanticSmooth achieves state-of-the-art robustness against attacks while maintaining strong nominal performance.

### Major Findings:
1. SemanticSmooth achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction-following benchmarks.
2. The defense algorithm uses semantic-preserving transformations and a learnable policy network to adaptively select transformations for different inputs.
3. The defense algorithm provides a favorable trade-off between robustness and nominal performance compared to other state-of-the-art baselines.

### Analysis and Critique:
- The defense algorithm improves robustness with minimal nominal performance degradation.
- The learned policy distribution shows that different transformations are optimal for different inputs, indicating the adaptability of the defense.
- The defense algorithm also provides a quantitative analysis examining the interpretability of nonsensical adversarial suffixes generated by GCG attacks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.16192v1](https://arxiv.org/abs/2402.16192v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16192v1](https://browse.arxiv.org/html/2402.16192v1)       |
| Truncated       | False       |
| Word Count       | 9788       |