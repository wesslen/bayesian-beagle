
---
title: "LLMs for User Interest Exploration: A Hybrid Approach"
id: "2405.16363v1"
description: "Hybrid framework using LLMs and classic models improves novel interest discovery, boosting user enjoyment."
author: Jianling Wang, Haokai Lu, Yifan Liu, He Ma, Yueqi Wang, Yang Gu, Shuzhou Zhang, Ningren, Han, Shuchao Bi, Lexi Baugher, Ed Chi, Minmin Chen
date: "2024-05-25"
image: "https://browse.arxiv.org/html/2405.16363v1/x1.png"
categories: ['recommender']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.16363v1/x1.png)

### Summary:

The paper introduces a hybrid hierarchical framework that combines Large Language Models (LLMs) and classic recommendation models for user interest exploration. The framework controls the interfacing between LLMs and classic recommendation models through "interest clusters," the granularity of which can be explicitly determined by algorithm designers. The approach recommends novel interests by first representing "interest clusters" using language and employing a fine-tuned LLM to generate novel interest descriptions that are strictly within these predefined clusters. At the low level, it grounds these generated interests to an item-level policy by restricting classic recommendation models to return items that fall within the novel clusters generated at the high level. The efficacy of this approach is showcased on an industrial-scale commercial platform serving billions of users, with live experiments showing a significant increase in both exploration of novel interests and overall user enjoyment of the platform.

### Major Findings:

1. The hybrid hierarchical framework effectively combines LLMs and classic recommendation models for user interest exploration, leveraging LLMs' reasoning and generalization capabilities and classic recommendation models' strong personalization and grounded item corpus knowledge.
2. The framework uses supervised fine-tuning with real-world novel consumption behaviors for in-domain user alignment and enables LLMs to perform controlled generation, producing novel interest descriptions that directly match one of the pre-defined clusters.
3. The use of topically coherent interest clusters with cluster-level descriptions to represent recommendation objects allows for the pre-computation of novel interest transitions offline with LLM bulk inference, which can then be served online with simple table lookup operations.

### Analysis and Critique:

The paper presents an innovative approach to user interest exploration by combining LLMs and classic recommendation models in a hybrid hierarchical framework. The use of supervised fine-tuning and controlled generation enables the alignment of LLMs with actual user behaviors and the generation of novel interests that match predefined interest clusters. The use of topically coherent interest clusters also addresses the challenge of LLM inference by allowing for the pre-computation of novel interest transitions offline.

However, the paper does not discuss potential limitations or biases in the approach, such as the potential for LLMs to reinforce existing biases in the data or the need for ongoing updates to the LLMs to ensure their

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.16363v1](https://arxiv.org/abs/2405.16363v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.16363v1](https://browse.arxiv.org/html/2405.16363v1)       |
| Truncated       | False       |
| Word Count       | 5005       |