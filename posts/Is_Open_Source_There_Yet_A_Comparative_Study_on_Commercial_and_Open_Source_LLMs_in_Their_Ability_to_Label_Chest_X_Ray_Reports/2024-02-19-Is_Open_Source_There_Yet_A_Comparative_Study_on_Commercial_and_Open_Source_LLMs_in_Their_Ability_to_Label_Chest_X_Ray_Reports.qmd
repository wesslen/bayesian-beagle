
---
title: "Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports"
id: "2402.12298v1"
description: "TL;DR: GPT-4 outperforms open-source models in zero-shot labeling, but few-shot prompting brings them on par."
author: Felix J. Dorfner, Liv JÃ¼rgensen, Leonhard Donle, Fares Al Mohamad, Tobias R. Bodenmann, Mason C. Cleveland, Felix Busch, Lisa C. Adams, James Sato, Thomas Schultz, Albert E. Kim, Jameson Merkow, Keno K. Bressem, Christopher P. Bridge
date: "2024-02-19"
image: "../../img/2402.12298v1/image_1.png"
categories: ['production', 'prompt-engineering', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.12298v1/image_1.png)

### **Summary:**
- The study compared the performance of commercial and open-source large language models (LLMs) in labeling chest x-ray reports.
- Two datasets were used, one from Massachusetts General Hospital and the other from the ImaGenome dataset.
- The study found that open-source LLMs performed comparably to GPT-4 in labeling chest x-ray reports, especially with few-shot prompting.

### **Major Findings:**
1. **Comparison of Zero-Shot Prompting:**
   - GPT-4 outperformed open-source models on the ImaGenome dataset, achieving a micro F1-score of 0.975.
   - Llama2-70B was the best performing open-source model with a micro F1-score of 0.972.
   - On the institutional dataset, GPT-4 achieved a micro F1-score of 0.975, while QWEN1.5-72B and Llama2-70B achieved micro F1-scores of 0.952 and 0.950, respectively.

2. **Comparison of Few-Shot Prompting:**
   - GPT-4 achieved a micro F1-score of 0.984 on the ImaGenome dataset, while Llama2-70B achieved a micro F1-score of 0.970.
   - On the institutional dataset, GPT-4 achieved a micro F1-score of 0.973, while QWEN1.5-72B, Llama2-70B, and Mixtral-8x7B achieved micro F1-scores of 0.965, 0.965, and 0.963, respectively.

3. **Ensemble Model Performance:**
   - An ensemble model of Mixtral-8x7B, Llama2-70B, and QWEN1.5-72B closely matched the performance of GPT-4 on the institutional dataset with few-shot prompts, achieving a micro F1-score of 0.971.

### **Analysis and Critique:**
- The study demonstrated that open-source LLMs are a viable alternative to proprietary models for medical tasks such as radiology report classification.
- Open-source LLMs offer cost advantages, privacy, and reproducibility over proprietary models.
- The study had limitations related to prompt design and class imbalances in the datasets, which could impact model performance.

Overall, the study provides valuable insights into the performance of open-source LLMs in medical tasks and highlights their potential as an alternative to proprietary models. Further research is needed to optimize prompt design and address class imbalances in datasets.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.12298v1](https://arxiv.org/abs/2402.12298v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.12298v1](https://browse.arxiv.org/html/2402.12298v1)       |
| Truncated       | False       |
| Word Count       | 14656       |