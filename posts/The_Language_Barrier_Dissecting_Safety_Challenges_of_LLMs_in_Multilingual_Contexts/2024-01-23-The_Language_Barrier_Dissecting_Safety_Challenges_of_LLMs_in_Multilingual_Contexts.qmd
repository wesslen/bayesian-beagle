
---
title: "The Language Barrier: Dissecting Safety Challenges of LLMs in Multilingual Contexts"
id: "2401.13136v1"
description: "Study explores large language model safety challenges across languages, finding disparities in unsafe and irrelevant responses. Training impacts alignment."
author: Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp Koehn, Daniel Khashabi
date: "2024-01-23"
image: "https://browse.arxiv.org/html/2401.13136v1/x1.png"
categories: ['security', 'robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.13136v1/x1.png)

**Summary of the Article:**

The article examines the safety challenges faced by large language models (LLMs) in multilingual settings, specifically focusing on the variations in safety challenges across different languages. The study highlights two main safety-related findings: LLMs tend to generate unsafe or irrelevant content more often when prompted with lower-resource languages compared to higher-resource ones. The authors also investigate the effect of aligning LLMs with instruction-tuning datasets in different languages and find little to no improvement in safety with training on lower-resource languages, suggesting that the bottleneck of cross-lingual alignment is rooted in the pretraining stage.

**Major Findings:**

1. **Safety Challenges across Languages:**
   - LLMs generate unsafe responses more often when prompted with lower-resource languages compared to higher-resource ones.
   - LLMs tend to generate less relevant responses in lower-resource languages, indicating limited instruction-following ability.

2. **Effect of Alignment Training:**
   - Training with high-resource languages improves model alignment, while training in lower-resource languages yields minimal improvement, suggesting challenges in cross-lingual LLM safety.
   - Two safety-related curses are identified when jailbreaking GPT-4: harmful rate and following rate.

3. **Effectiveness of Common Alignment Techniques:**
   - Alignment methods such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) exhibit limited effectiveness in reducing harmful rate and increasing following rate for low-resource languages.

**Analysis and Critique:**

The article provides valuable insights into the safety challenges of LLMs in multilingual contexts, identifying the heightened vulnerability of LLMs to generate unsafe and irrelevant content in lower-resource languages. The findings also raise concerns about the limited effectiveness of common alignment techniques in addressing these safety challenges. One potential shortcoming of the study is the lack of high-quality human evaluation for harmful rate and following rate due to a limited budget, which may introduce noise into the evaluation process. Additionally, the article does not address the potential biases inherent in the translation process, which could impact the evaluation of harmful rate and following rate. Moreover, the study highlights the difficulty of resolving safety challenges in LLMs through alignment methods, calling for future research to enhance the cross-lingual abilities of LLMs.

Overall, the study provides valuable insights into the safety challenges of LLMs in multilingual settings and the limitations of current alignment techniques in addressing these challenges. However, it also presents areas for further investigation, such as the impact of biases in the translation process and the need for high-quality human evaluation to ensure the accuracy of the findings.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-25       |
| Abstract | [http://arxiv.org/abs/2401.13136v1](http://arxiv.org/abs/2401.13136v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.13136v1](https://browse.arxiv.org/html/2401.13136v1)       |
| Truncated       | False       |
| Word Count       | 7861       |