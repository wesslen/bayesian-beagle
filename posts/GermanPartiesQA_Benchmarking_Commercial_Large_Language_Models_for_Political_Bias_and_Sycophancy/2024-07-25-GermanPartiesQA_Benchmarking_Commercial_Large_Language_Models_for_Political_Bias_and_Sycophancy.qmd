
---
title: "GermanPartiesQA: Benchmarking Commercial Large Language Models for Political Bias and Sycophancy"
id: "2407.18008v1"
description: "LLMs show left-green bias, can be ideologically steered with political personas, but changes in output are more like personalization than sycophancy."
author: Jan Batzner, Volker Stocker, Stefan Schmid, Gjergji Kasneci
date: "2024-07-25"
image: "https://browse.arxiv.org/html/2407.18008v1/extracted/5753394/wom.graph.modelcomp.png"
categories: ['prompt-engineering', 'social-sciences', 'education', 'hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.18008v1/extracted/5753394/wom.graph.modelcomp.png)

### Summary:

The paper "GermanPartiesQA: Benchmarking Commercial Large Language Models for Political Bias and Sycophancy" evaluates and compares the alignment of six LLMs by OpenAI, Anthropic, and Cohere with German party positions and evaluates sycophancy based on a prompt experiment. The authors develop the benchmark dataset GermanPartiesQA based on the Voting Advice Application Wahl-o-Mat covering 10 state and 1 national elections between 2021 and 2023. The study finds a left-green tendency across all examined LLMs. The prompt experiment uses the benchmark and sociodemographic data of leading German parliamentarians to evaluate changes in LLMs responses. The authors use "I am [politician X], …" and "You are [politician X], …" prompts to differentiate between sycophancy and steerability. However, they do not observe notable differences between prompting "I am" and "You are". The findings suggest that LLM responses can be ideologically steered with political personas, but observed changes in LLM outputs could be better described as personalization to the given context rather than sycophancy.

### Major Findings:

1. The study finds a left-green tendency across all examined LLMs.
2. The prompt experiment using the benchmark and sociodemographic data of leading German parliamentarians evaluates changes in LLMs responses.
3. The authors use "I am [politician X], …" and "You are [politician X], …" prompts to differentiate between sycophancy and steerability.
4. The study does not observe notable differences between prompting "I am" and "You are".
5. The findings suggest that LLM responses can be ideologically steered with political personas, but observed changes in LLM outputs could be better described as personalization to the given context rather than sycophancy.

### Analysis and Critique:

The paper provides a valuable contribution to the evaluation of political bias and sycophancy in multi-party systems across major commercial LLMs. The development of the benchmark dataset GermanPartiesQA based on the Voting Advice Application Wahl-o-Mat is a significant contribution to the field. However, the study

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.18008v1](https://arxiv.org/abs/2407.18008v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.18008v1](https://browse.arxiv.org/html/2407.18008v1)       |
| Truncated       | False       |
| Word Count       | 6995       |