
---
title: "LLM Targeted Underperformance Disproportionately Impacts Vulnerable Users"
id: "2406.17737v1"
description: "LLMs' reliability varies with user traits; lower proficiency, education, and non-US users receive less accurate, truthful, and more refused responses."
author: Elinor Poole-Dayan, Deb Roy, Jad Kabbara
date: "2024-06-25"
image: "https://browse.arxiv.org/html/2406.17737v1/extracted/5676886/images/grouped_tqa_plot.png"
categories: ['robustness']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.17737v1/extracted/5676886/images/grouped_tqa_plot.png)

### Summary:

The study investigates the performance of three state-of-the-art Large Language Models (LLMs) – GPT-4, Claude Opus, and Llama 3-8B – in providing accurate, truthful, and appropriate information to users with varying English proficiency, education level, and country of origin. The models were evaluated on two datasets: TruthfulQA and SciQ. The findings suggest that undesirable behaviors, such as reduced information accuracy, truthfulness, and increased refusals, occur disproportionately more for users with lower English proficiency, less formal education, and those originating from outside the US. This raises concerns about the reliability of these models as sources of information for their most vulnerable users.

### Major Findings:

1. LLMs exhibit reduced information accuracy and truthfulness for users with lower English proficiency, less formal education, and those originating from outside the US.
2. LLMs generate more misconceptions and have a higher rate of withholding information for users with lower English proficiency, less formal education, and those originating from outside the US.
3. LLMs display a tendency to patronize and produce condescending responses to users with lower English proficiency, less formal education, and those originating from outside the US.

### Analysis and Critique:

The study provides valuable insights into the performance of LLMs across different user demographics. However, there are several limitations and areas for improvement:

1. The experimental setup is not conventional and may not reflect real-world usage of LLMs.
2. The generated user bios may exaggerate and caricature users, potentially reinforcing negative stereotypes.
3. The study only tests English language queries and does not explore the phenomenon in other languages.
4. The study does not measure the effects of targeted underperformance on actual users.
5. The study does not explore the implications of LLMs' condescending behavior towards marginalized groups.

In conclusion, the study highlights the need for further research into the limitations and shortcomings of LLMs, particularly in relation to their performance for different user demographics. This is crucial for ensuring that LLMs perform equitably across all users and do not exacerbate existing inequities and discrepancies in education.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.17737v1](https://arxiv.org/abs/2406.17737v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.17737v1](https://browse.arxiv.org/html/2406.17737v1)       |
| Truncated       | False       |
| Word Count       | 6850       |