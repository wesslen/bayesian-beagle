
---
title: "Over-Reasoning and Redundant Calculation of Large Language Models"
id: "2401.11467v1"
description: "Large language models generate redundant calculations in solving math problems, despite unnecessary, according to a study on GSM8K-Zero."
author: ['Cheng-Han Chiang', 'Hung-yi Lee']
date: "2024-01-21"
image: "https://browse.arxiv.org/html/2401.11467v1/x1.png"
categories: ['education', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.11467v1/x1.png)

**Summary:**
The article investigates the behaviors of Large Language Models (LLMs) in generating redundant calculations and reasoning. It focuses on a math Question-Answer (QA) dataset called GSM8K-Zero where the questions are designed to be answerable without any calculations. The study finds that LLMs, including popular models like GPT-4 and ChatGPT, tend to produce unnecessary calculations and reasoning on this dataset, leading to longer and sometimes incorrect answers. The research also explores the influence of reinforcement learning with human feedback (RLHF) on LLMs' tendency to generate redundant outputs and provides insights into the preference of reward models for verbose responses. The authors propose that LLMs might lack the ability to differentiate between questions requiring step-by-step reasoning and those that can be answered directly.

### Major Findings:
1. LLMs, including GPT-4 and ChatGPT, generate redundant calculations and reasoning when answering questions that can be handled without any calculations.
2. The study shows that these redundant outputs can sometimes result in incorrect answers, impacting the performance of LLMs.
3. Proxy reward models (RMs) like GPT-4 and ChatGPT exhibit a preference for longer answers containing redundant calculations, even if the answers are incorrect.

### Analysis and Critique:
The article provides valuable insights into the over-reasoning and redundant calculation behaviors of LLMs, shedding light on potential issues in their performance and revealing the influence of training techniques such as RLHF on their outputs. However, the study is limited to a manually constructed dataset, and the reliance on proxy RMs to understand the preference for verbose outputs raises questions about the generalizability of the findings. Additionally, the potential biases and noises in the dataset could affect the interpretation of the results. Further research on a broader range of datasets and LLM training methods is needed to validate the observed behaviors and their implications.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-02       |
| Abstract | [http://arxiv.org/abs/2401.11467v1](http://arxiv.org/abs/2401.11467v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.11467v1](https://browse.arxiv.org/html/2401.11467v1)       |
| Truncated       | False       |
| Word Count       | 6674       |