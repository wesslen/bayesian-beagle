
---
title: "A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models"
id: "2405.20183v1"
description: "LLMs can generate useful programming exercises, but challenges like LLMs solving their own exercises exist."
author: Eduard Frankford, Ingo HÃ¶hn, Clemens Sauerwein, Ruth Breu
date: "2024-05-30"
image: "../../../bayesian-beagle.png"
categories: ['prompt-engineering', 'architectures', 'education', 'programming']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

This paper presents a survey study on the state of the art of programming exercise generation using Large Language Models (LLMs). The study aims to define the current state, extract strengths and weaknesses, and propose an evaluation matrix to help researchers and educators choose the best LLM for programming exercise generation. The results show that multiple LLMs are capable of producing useful programming exercises, but challenges exist, such as the ease with which LLMs might solve exercises generated by LLMs.

### Major Findings:

1. **Current State of Programming Exercise Generation**: Working implementations for programming exercise generation exist and produce sensible results. LLMs like Codex, Google T5, and CodeT5 have been used for generating programming exercises.
2. **Strengths and Weaknesses of Existing Solutions**: The main advantage of automated exercise generation is its ability to create learning material in a time-efficient way. However, the quality of the generated tests or test suites needs improvement. Another significant challenge is that exercises generated using LLMs seem to be easily solvable by LLMs as well.
3. **Evaluating LLMs Suitability for Exercise Generation**: Most benchmarks for LLMs evaluate their performance based on a set of problems that the models are expected to solve. In programming, the HumanEval metric is a well-established benchmark. However, a more complete picture of LLM performance for programming exercise generation can be formed when evaluating a sample of generated exercises using the evaluation matrix presented in the study.

### Analysis and Critique:

- The study provides a comprehensive overview of the current state of programming exercise generation using LLMs. However, the comprehensiveness might be limited by the search strategy and terms used to gather relevant publications.
- The study highlights the potential of LLMs to transform programming education, but also raises concerns about the risks of over-reliance on LLMs, particularly as it seems like LLM-generated exercises are easily solvable by other LLMs.
- The proposed evaluation matrix provides a structured approach for assessing LLMs regarding their exercise generation capabilities. However, it remains to be seen how effectively this matrix can be used to identify the most effective models for specific needs.
- The study does not address the potential ethical implications of using LLMs for programming exercise generation, such as the impact on student learning outcomes and

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.20183v1](https://arxiv.org/abs/2405.20183v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.20183v1](https://browse.arxiv.org/html/2405.20183v1)       |
| Truncated       | False       |
| Word Count       | 3639       |