
---
title: "How well can large language models explain business processes?"
id: "2401.12846v1"
description: "LLMs used in AI-augmented business systems improve explanations but can reduce interpretability."
author: ['Dirk Fahland', 'Fabian Fournier', 'Lior Limonad', 'Inna Skarbovsky', 'Ava J. E. Swevels']
date: "2024-01-23"
image: "https://browse.arxiv.org/html/2401.12846v1/extracted/5364307/images/general-approach.png"
categories: ['architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2401.12846v1/extracted/5364307/images/general-approach.png)

**Summary of the Article:**

Large Language Models (LLMs) are becoming increasingly important in automating various aspects of business operations. This article presents the SAX4BPM framework, designed to generate Situation-Aware eXplainability (SAX) explanations for business process management systems (ABPMSs). The framework combines LLMs with a set of services and a central knowledge repository to improve the quality of SAX explanations. The article also discusses the potential challenges associated with using LLMs for SAX, such as hallucination and lack of inherent capacity to reason.

The study aims to guardrail the functionality of LLMs by injecting different knowledge articulations as input to alter and improve the perceived quality of the generated explanations. Through a rigorous user study, the findings demonstrate that inputting knowledge ingredients to LLMs improved the fidelity of SAX explanations, moderated by the perception of trust and curiosity.

### Major Findings:
1. LLMs can be leveraged to provide improved SAX explanations by injecting various knowledge ingredients as input.
2. Knowledge ingredients aided in guardrailing the performance of LLMs, resulting in better-perceived fidelity of SAX explanations.
3. The improvement in fidelity was moderated by the perception of trust and curiosity.

### Analysis and Critique:
The article provides significant insights into the utilization of LLMs for generating business process explanations. However, it has certain limitations and potential biases that need to be addressed:

1. **Methodological Limitations:** The methodological evaluation and user study have their own set of limitations. The study's sample size and the composition of the population may not be fully representative. Moreover, the measure used to gauge the improvements in fidelity and perceived trust may lack robustness.

2. **Unanswered Questions:** The article does not address the potential ethical implications and biases introduced by leveraging LLMs, especially in the context of automation and decision-making within business processes.

3. **Causal Understanding:** While the article discusses the importance of causal relationships in explanations, it fails to provide a detailed discussion on how LLMs handle causal understanding and reasoning.

4. **Lack of Comparative Analysis:** The article does not compare the effectiveness of LLM-generated explanations with explanations produced by other models or traditional methods, which could provide a more comprehensive assessment of their utility.

In conclusion, while the article contributes valuable insights into leveraging LLMs for business process explanations, it is crucial to address the aforementioned limitations. Future research should focus on conducting more extensive and diverse user studies, considering the ethical implications, and comparing LLM-generated explanations with alternative methods for a more comprehensive understanding.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-01-29       |
| Abstract | [http://arxiv.org/abs/2401.12846v1](http://arxiv.org/abs/2401.12846v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.12846v1](https://browse.arxiv.org/html/2401.12846v1)       |
| Truncated       | True       |
| Word Count       | 19518       |