
---
title: "Enhancing Computer Programming Education with LLMs: A Study on Effective Prompt Engineering for Python Code Generation"
id: "2407.05437v1"
description: "LLMs, like GPT-4o, excel in programming education with tailored prompt strategies, offering personalized instruction and improved learning outcomes."
author: Tianyu Wang, Nianjun Zhou, Zhixiong Chen
date: "2024-07-07"
image: "https://browse.arxiv.org/html/2407.05437v1/extracted/5670732/images/system.png"
categories: ['education', 'prompt-engineering', 'programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.05437v1/extracted/5670732/images/system.png)

### Summary:

This paper explores the potential of prompt engineering in large language models (LLMs) to enhance educational outcomes in computer programming instruction. The research focuses on three key questions: the systematic categorization of prompt engineering strategies tailored to educational requirements, the empowerment of LLMs to solve complex problems, and the establishment of a robust framework for testing and implementing these strategies.

The study finds that the GPT-4 and GPT-4o models outperform other LLMs such as Llama3-8b and Mixtral-8x7b in terms of pass rates, execution times, and adherence to coding standards. The GPT-4o model, in particular, demonstrated a successfully pass rate with the "multi" prompt strategy, highlighting its superior adaptability and efficiency. These results lead to the recommendation of GPT-4o as the preferred model for educational purposes in computer programming.

The paper proposes tailored prompt strategies based on educational requirements. For foundational learning and skill-building, such as LeetCode, asking questions directly without prompt engineering is sufficient, providing structured guidance that helps students grasp essential concepts and techniques. For competition preparation, such as USACO, the "Multi-Step Conversational Prompt" strategy proves beneficial, facilitating dynamic interaction and iterative refinement that enhance contextual understanding and problem-solving skills. For advanced problem-solving, the "Specific Prompt Engineering" strategy is ideal, offering detailed instructions that address complex topics like algorithm design and optimization.

The study also highlights the significant role of prompt engineering in maximizing the potential of LLMs in educational contexts. By categorizing and testing various strategies, the paper establishes a robust framework for their implementation, providing educators with comprehensive guidelines to optimize LLM-based learning experiences. Despite the advancements, certain complex problems remain challenging for current LLMs, suggesting the need for further research to enhance context retention, logical reasoning, and handling of numerical and combinatorial complexities.

### Major Findings:

1. GPT-4 and GPT-4o models outperform other LLMs in terms of pass rates, execution times, and adherence to coding standards.
2. GPT-4o model demonstrated a successfully pass rate with the "multi" prompt strategy, highlighting its superior adaptability and efficiency.
3

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.05437v1](https://arxiv.org/abs/2407.05437v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.05437v1](https://browse.arxiv.org/html/2407.05437v1)       |
| Truncated       | False       |
| Word Count       | 9547       |