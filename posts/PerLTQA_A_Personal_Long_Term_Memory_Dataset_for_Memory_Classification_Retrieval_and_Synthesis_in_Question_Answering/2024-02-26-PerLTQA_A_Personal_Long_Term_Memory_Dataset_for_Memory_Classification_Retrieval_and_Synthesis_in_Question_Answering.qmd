
---
title: "PerLTQA: A Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Synthesis in Question Answering"
id: "2402.16288v1"
description: "PerLTQA dataset combines semantic and episodic memories for personalized QA tasks, outperforming LLMs."
author: Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, Kam-Fai Wong
date: "2024-02-26"
image: "https://browse.arxiv.org/html/2402.16288v1/x1.png"
categories: ['hci', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.16288v1/x1.png)

### **Summary:**
- The PerLTQA dataset is introduced, featuring a memory database and memory-based QA pairs.
- The dataset includes personal long-term memory, divided into semantic and episodic memory categories.
- Three subtasks are outlined: memory classification, retrieval, and synthesis, with baseline experiments using five LLMs and three retrievers.

### **Major Findings:**
1. BERT-based memory classification outperforms other LLMs in categorizing memory types.
2. Different retrieval models show variable Recall@K and time performance, with DPR notably improving Recall@K as k increases.
3. Memory synthesis capabilities of LLMs are significantly improved with memory classification and retrieval.

### **Analysis and Critique:**
- **Limitations:** The process of generating memory data could be varied, and the dataset is fictional, potentially leading to inaccuracies. The evaluations are limited to specific LLMs, and the evaluation metric may have uncertainties in accurately measuring response quality.
- **Ethics Statement:** The dataset is generated from ChatGPT and does not violate any licenses or policies. Annotators were paid above average local rates for their work.
- **Conclusion:** The study significantly deepens the understanding and evaluation of LLMs in the context of personal long-term memory. However, the limitations and potential biases in the dataset should be considered in future research.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-27       |
| Abstract | [https://arxiv.org/abs/2402.16288v1](https://arxiv.org/abs/2402.16288v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.16288v1](https://browse.arxiv.org/html/2402.16288v1)       |
| Truncated       | False       |
| Word Count       | 6901       |