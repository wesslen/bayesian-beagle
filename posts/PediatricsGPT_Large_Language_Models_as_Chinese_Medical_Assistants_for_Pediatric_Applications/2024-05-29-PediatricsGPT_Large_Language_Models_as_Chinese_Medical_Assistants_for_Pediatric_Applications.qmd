
---
title: "PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications"
id: "2405.19266v1"
description: "This paper introduces PediatricsGPT, a Chinese pediatric LLM assistant, and PedCorpus, a high-quality dataset, to improve diagnostic efficiency in pediatrics."
author: Dingkang Yang, Jinjie Wei, Dongling Xiao, Shunli Wang, Tong Wu, Gang Li, Mingcheng Li, Shuaibing Wang, Jiawei Chen, Yue Jiang, Qingyao Xu, Ke Li, Peng Zhai, Lihua Zhang
date: "2024-05-29"
image: "https://browse.arxiv.org/html/2405.19266v1/x1.png"
categories: ['education', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.19266v1/x1.png)

### Summary:

The paper presents PediatricsGPT, a Chinese medical LLM assistant with medical generalist and pediatric expertise capabilities. The model is developed based on the well-designed PedCorpus dataset and undergoes a systematic and robust procedure, including continuous pre-training, supervised fine-tuning, and human preference optimization. The model demonstrates competence in different pediatric and general healthcare service scenarios, outperforming currently available Chinese medical LLMs.

### Major Findings:

1. PediatricsGPT is the first Chinese pediatric LLM assistant with pediatric expertise and medical generalist capabilities, developed on a systematic training pipeline.
2. The model is built on the Baichuan2-Base series, which has comprehensive potential among similar contenders.
3. The PedCorpus dataset is constructed through multi-dimensional corpus across three application-oriented medical tasks, ensuring medical knowledge accuracy.
4. The model introduces a hybrid instruction pre-training mechanism in CPT to bridge the capability weakening due to corpus format discrepancies between the internal and injected medical knowledge of foundation models.
5. PediatricsGPT outperforms open-source Chinese medical LLMs and baselines, yielding competitive performance compared to GPT-3.5-turbo.

### Analysis and Critique:

The paper presents a promising approach to developing a Chinese pediatric LLM assistant with pediatric expertise and medical generalist capabilities. The use of the PedCorpus dataset and the systematic training pipeline, including continuous pre-training, supervised fine-tuning, and human preference optimization, are well-designed and contribute to the model's performance.

However, the paper does not discuss the potential limitations and biases of the model, such as the risk of generating incorrect or misleading information, the need for continuous updates to keep up with the latest medical knowledge, and the potential for the model to be used inappropriately. Additionally, the paper does not provide a detailed comparison with other Chinese medical LLMs, which could help to better understand the model's strengths and weaknesses.

Overall, the paper presents a valuable contribution to the development of Chinese pediatric LLM assistants, but further research is needed to address the potential limitations and biases of the model.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.19266v1](https://arxiv.org/abs/2405.19266v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.19266v1](https://browse.arxiv.org/html/2405.19266v1)       |
| Truncated       | False       |
| Word Count       | 8171       |