
---
title: "Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions"
id: "2405.20267v1"
description: "Auto-Arena: Automated LLM Evaluation Framework Outperforms Human Voting Platforms."
author: Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, Lidong Bing
date: "2024-05-30"
image: "https://browse.arxiv.org/html/2405.20267v1/extracted/5632916/images/new_diagram_overall.png"
categories: ['architectures', 'production', 'social-sciences', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2405.20267v1/extracted/5632916/images/new_diagram_overall.png)

### Summary:

The paper introduces Auto-Arena of LLMs, an automatic, reliable, and human-like evaluation framework for large language models (LLMs). The framework automates the entire LLM evaluation process with LLM agents, consisting of three stages: question generation, peer battles, and committee discussions. The question generation stage involves an LLM examiner agent generating diverse and difficult questions. In the peer battle stage, two candidate LLMs engage in a multi-round debate around a given question, pointing out each other's weaknesses and raising follow-up questions. The committee discussion stage involves a committee of LLM judge agents collectively discussing and evaluating the ability of the two candidates. The paper presents an extensive experiment with 17 models, showing that Auto-Arena increases the Spearman correlation with human preferences by 4.5%, resulting in state-of-the-art alignment.

### Major Findings:

1. Auto-Arena of LLMs is an automatic, reliable, and human-like evaluation framework that alleviates data contamination concerns, reduces single-model bias, and avoids long wait times for new models entering human voting platforms.
2. The extensive experiment with 17 models shows that Auto-Arena increases the Spearman correlation with human preferences by 4.5%, resulting in state-of-the-art alignment.
3. Before and after peer battles, the Spearman correlation with human preferences increases by 46.4%, verifying the hypothesis that peer battles can better display performance gaps.
4. Before and after committee discussions, committee agreement increases by 20%, showcasing the effectiveness of the committee discussion mechanism.
5. The study of peer battles reveals intriguing LLM agent behaviors such as self-improvement and competitive actions.
6. The automated framework is easily extendable to non-mainstream languages and domains, as demonstrated by the Chinese language example.

### Analysis and Critique:

1. The paper presents a novel and promising approach to LLM evaluation, addressing the limitations of existing methods such as static benchmarks and human annotations.
2. The extensive experiment with 17 models demonstrates the effectiveness of Auto-Arena in providing reliable and human-like evaluations.
3. The paper could benefit from a more detailed discussion of the potential limitations and

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2405.20267v1](https://arxiv.org/abs/2405.20267v1)        |
| HTML     | [https://browse.arxiv.org/html/2405.20267v1](https://browse.arxiv.org/html/2405.20267v1)       |
| Truncated       | False       |
| Word Count       | 7988       |