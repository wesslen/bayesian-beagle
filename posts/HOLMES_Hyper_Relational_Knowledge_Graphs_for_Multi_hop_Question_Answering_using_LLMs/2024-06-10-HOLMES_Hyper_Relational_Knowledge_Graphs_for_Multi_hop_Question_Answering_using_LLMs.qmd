
---
title: "HOLMES: Hyper-Relational Knowledge Graphs for Multi-hop Question Answering using LLMs"
id: "2406.06027v1"
description: "TL;DR: Our method uses context-aware, query-relevant knowledge graphs to improve LLM performance on complex questions, reducing token usage by up to 67%."
author: Pranoy Panda, Ankush Agarwal, Chaitanya Devaguptapu, Manohar Kaul, Prathosh A P
date: "2024-06-10"
image: "../../img/2406.06027v1/image_1.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](../../img/2406.06027v1/image_1.png)

**Summary:**

The paper introduces a new method called HOLMES for multi-hop question answering (MHQA) using large language models (LLMs). The method involves transforming unstructured text into a hyper-relational knowledge graph (KG) using a query-derived schema, which is then used as input to the LLM. The proposed method significantly improves upon the state-of-the-art (SoTA) multi-hop QA method, achieving 18.7% and 20% improvements in exact match (EM) scores on the Hotpot dataset and 26% and 14.3% on the MuSiQue dataset for GPT-3.5 and GPT-4, respectively. Additionally, the method uses up to 67% fewer tokens to represent query-relevant information than the current SoTA method and up to 60% fewer tokens compared to the original supporting documents.

**Major Findings:**

1. The proposed method, HOLMES, significantly improves upon the SoTA multi-hop QA method, achieving 18.7% and 20% improvements in EM scores on the Hotpot dataset and 26% and 14.3% on the MuSiQue dataset for GPT-3.5 and GPT-4, respectively.
2. The method uses up to 67% fewer tokens to represent query-relevant information than the current SoTA method and up to 60% fewer tokens compared to the original supporting documents.
3. The method involves transforming unstructured text into a hyper-relational KG using a query-derived schema, which is then used as input to the LLM.

**Analysis and Critique:**

The proposed method, HOLMES, presents a significant improvement over the SoTA multi-hop QA method. The use of a hyper-relational KG as input to the LLM allows for a more efficient and effective representation of query-relevant information. The method's ability to use fewer tokens to represent this information is particularly noteworthy, as it can lead to reduced computational costs and improved performance.

However, there are some potential limitations and areas for further research. For example, the method's reliance on a query-

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-06-23       |
| Abstract | [https://arxiv.org/abs/2406.06027v1](https://arxiv.org/abs/2406.06027v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.06027v1](https://browse.arxiv.org/html/2406.06027v1)       |
| Truncated       | False       |
| Word Count       | 20470       |