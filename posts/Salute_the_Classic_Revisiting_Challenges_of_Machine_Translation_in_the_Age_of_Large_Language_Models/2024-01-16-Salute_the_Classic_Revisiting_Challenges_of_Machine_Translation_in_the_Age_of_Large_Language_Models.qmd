
---
title: "Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models"
id: "2401.08350v1"
description: "Evolution of Neural Machine Translation influenced by 6 core challenges, LLMs address some but new challenges arise."
author: Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F. Wong, Shuming Shi, Zhaopeng Tu
date: "2024-01-16"
image: "../../../bayesian-beagle.png"
categories: ['production', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

The article examines the challenges and advancements in Neural Machine Translation (NMT) using Large Language Models (LLMs). It identifies six core challenges of NMT and highlights the effectiveness of LLMs in addressing traditional challenges while also identifying new challenges specific to LLMs. The study evaluates the translation performance of LLM and Enc2Dec models for long sentences and document-level translation tasks, finding that LLMs outperform the Enc2Dec model. Additionally, it discusses the evaluation of bi-directional translation results using Llama2-7b based translation models across multiple language pairs, emphasizing the importance of combining both automatic and human evaluation methods. The article also presents detailed experimental settings and results of training LLMs for machine translation, providing insights into word precision, deletion rates, and word alignment outcomes.

### Major Findings:
1. LLMs effectively address traditional NMT challenges and demonstrate proficiency in translating long sentences and entire documents.
2. Human and automatic evaluations provide complementary insights into document-level translation quality, highlighting the need for human-centered evaluation methods in LLMs.
3. Detailed experimental results shed light on the performance and behavior of LLMs in translation tasks, particularly in handling different types of words and achieving accurate word alignment.

### Analysis and Critique:
The study's findings have significant implications for the development and optimization of LLM-based translation systems. However, the article could benefit from further discussion on potential limitations and areas that require additional research, such as the challenges of domain mismatch and prediction of rare words. Additionally, the human-centered approach to evaluation in LLMs is crucial for ensuring that translation models are not only technically proficient but also practically useful and acceptable to end users. Further research is needed to refine evaluation methods that better align with human preferences and expectations in translation. The detailed experimental results provide valuable insights into the performance and behavior of LLMs, contributing to the broader understanding of their capabilities and limitations in machine translation tasks.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2401.08350v1](https://arxiv.org/abs/2401.08350v1)        |
| HTML     | [https://browse.arxiv.org/html/2401.08350v1](https://browse.arxiv.org/html/2401.08350v1)       |
| Truncated       | True       |
| Word Count       | 17629       |