
---
title: "STBench: Assessing the Ability of Large Language Models in Spatio-Temporal Analysis"
id: "2406.19065v1"
description: "STBench evaluates LLMs' spatio-temporal understanding across 13 tasks, revealing strengths and areas for improvement."
author: Wenbin Li, Di Yao, Ruibo Zhao, Wenjie Chen, Zijie Xu, Chengxue Luo, Chang Gong, Quanliang Jing, Haining Tan, Jingping Bi
date: "2024-06-27"
image: "https://browse.arxiv.org/html/2406.19065v1/extracted/5683029/figs/overview.png"
categories: ['architectures', 'education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2406.19065v1/extracted/5683029/figs/overview.png)

### Summary:

The paper introduces STBench, a benchmark dataset for evaluating the spatio-temporal analysis capabilities of large language models (LLMs). The dataset consists of 13 distinct tasks and over 60,000 question-answer pairs, covering four dimensions: knowledge comprehension, spatio-temporal reasoning, accurate computation, and downstream applications. The authors evaluate 13 LLMs, including GPT-4o and ChatGPT, and find that existing models show remarkable performance on knowledge comprehension and spatio-temporal reasoning tasks. However, there is potential for improvement in accurate computation and downstream applications through in-context learning, chain-of-thought prompting, and fine-tuning.

### Major Findings:

1. STBench is a comprehensive benchmark dataset for evaluating the spatio-temporal analysis capabilities of LLMs, consisting of 13 tasks and over 60,000 question-answer pairs.
2. Existing LLMs show remarkable performance on knowledge comprehension and spatio-temporal reasoning tasks, with closed-source models like GPT-4o and ChatGPT outperforming other models in many instances.
3. Performance across all models is generally low for accurate computation tasks, but in-context learning and chain-of-thought prompting have been shown to enhance performance.

### Analysis and Critique:

The paper provides a valuable contribution to the field by introducing a comprehensive benchmark dataset for evaluating the spatio-temporal analysis capabilities of LLMs. The authors' evaluation of 13 LLMs on the STBench dataset highlights the strengths and limitations of these models in spatio-temporal analysis. However, the rapid evolution of large language models and their enormous computational costs make it difficult to cover the latest models in the assessment. Additionally, the lack of training on relevant corpora may limit the performance of some models on certain tasks. The authors acknowledge these limitations and plan to maintain the project and benchmark more LLMs in the future.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-07       |
| Abstract | [https://arxiv.org/abs/2406.19065v1](https://arxiv.org/abs/2406.19065v1)        |
| HTML     | [https://browse.arxiv.org/html/2406.19065v1](https://browse.arxiv.org/html/2406.19065v1)       |
| Truncated       | False       |
| Word Count       | 11715       |