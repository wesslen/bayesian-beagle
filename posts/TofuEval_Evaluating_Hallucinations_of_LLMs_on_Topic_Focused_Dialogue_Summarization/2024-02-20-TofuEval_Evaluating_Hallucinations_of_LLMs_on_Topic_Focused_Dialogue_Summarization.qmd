
---
title: "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization"
id: "2402.13249v1"
description: "Advances in news summarization don't carry over to dialogue summarization. LLMs generate factual errors. Benchmark dataset released."
author: Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W. Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, Kathleen McKeown
date: "2024-02-20"
image: "https://browse.arxiv.org/html/2402.13249v1/x3.png"
categories: ['robustness', 'production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.13249v1/x3.png)

### Summary:
The article "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization" presents a new evaluation benchmark for topic-focused dialogue summarization, focusing on the factual consistency of summaries generated by Large Language Models (LLMs). The study finds that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model’s size. The authors also evaluate LLMs as summarizers across relevance, completeness, and factual consistency, and show that LLMs perform poorly on factual consistency in the dialogue domain. The study also compares LLM-based evaluators and non-LLM-based factuality metrics, finding that non-LLM-based metrics demonstrate superior performance compared to most LLMs tested. The authors release the benchmark dataset with human-annotated data to enable further research into improved automated evaluation of summary factuality.

### Major Findings:
1. Existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model’s size.
2. LLMs perform poorly on factual consistency in the dialogue domain.
3. Non-LLM-based factuality metrics demonstrate superior performance compared to most LLMs tested.

### Analysis and Critique:
- The study provides valuable insights into the limitations of LLMs in generating factually consistent summaries in the dialogue domain.
- The comparison between LLM-based evaluators and non-LLM-based factuality metrics highlights the need for improved automated evaluation of summary factuality.
- The study's findings raise caution against unquestioning admiration for using cutting-edge LLMs as evaluators and emphasize the need for further research to enhance automated evaluation metrics and the development of summarizers with better factual consistency performance.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.13249v1](https://arxiv.org/abs/2402.13249v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.13249v1](https://browse.arxiv.org/html/2402.13249v1)       |
| Truncated       | False       |
| Word Count       | 13546       |