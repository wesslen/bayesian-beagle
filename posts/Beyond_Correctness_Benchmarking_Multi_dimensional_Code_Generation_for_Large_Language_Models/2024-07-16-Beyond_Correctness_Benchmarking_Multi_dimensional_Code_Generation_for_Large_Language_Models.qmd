
---
title: "Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models"
id: "2407.11470v1"
description: "TL;DR: RACE benchmark evaluates LLMs' code quality across 4 dimensions: Readability, Maintainability, Correctness, and Efficiency. Current LLMs fall short in generating high-quality code on demand."
author: Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun
date: "2024-07-16"
image: "https://browse.arxiv.org/html/2407.11470v1/x1.png"
categories: ['programming']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.11470v1/x1.png)

# Summary:

The paper proposes the RACE benchmark, a comprehensive evaluation framework for code generated by large language models (LLMs) across four dimensions: Readability, mAintainability, Correctness, and Efficiency. The authors evaluate 18 representative LLMs on RACE and find that current models struggle to generate high-quality code on demand, with readability serving as a critical indicator of overall code quality. The findings reveal the limitations of current Code LLMs and shed light on future optimization directions.

## Major Findings:

1. Current LLMs' ability to generate high-quality code on demand does not yet meet the requirements of software development.
2. Readability serves as a critical indicator of the overall quality of generated code.
3. Most LLMs exhibit an inherent preference for specific coding styles, making it difficult for them to follow user instructions that are inconsistent with their preference.

## Analysis and Critique:

The RACE benchmark provides a valuable contribution to the evaluation of code generated by LLMs, addressing the limitations of existing benchmarks that primarily focus on code correctness. However, the benchmark could be expanded to include additional dimensions, such as security, testability, and dynamic behavior. Additionally, the experiments have only been conducted on Python code data, and future work should consider expanding to multilingual code to explore differences in model preferences across languages.

The findings highlight the need for further improvement in the ability of LLMs to generate high-quality code across multiple dimensions based on user demands. The inherent preference bias of LLMs for specific coding styles can lead to the ossification of code style and hinder their ability to meet specific real-world project requirements. Future efforts should focus on improving the ability of LLMs to meet real-world requirements and explore deeper factors influencing generated code quality.

In conclusion, the RACE benchmark provides a valuable tool for evaluating the quality of code generated by LLMs and highlights the need for further improvement in this area. The findings of this study can help researchers gain a deeper understanding of the coding capabilities of current LLMs and guide future directions for model improvement.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-30       |
| Abstract | [https://arxiv.org/abs/2407.11470v1](https://arxiv.org/abs/2407.11470v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.11470v1](https://browse.arxiv.org/html/2407.11470v1)       |
| Truncated       | False       |
| Word Count       | 6582       |