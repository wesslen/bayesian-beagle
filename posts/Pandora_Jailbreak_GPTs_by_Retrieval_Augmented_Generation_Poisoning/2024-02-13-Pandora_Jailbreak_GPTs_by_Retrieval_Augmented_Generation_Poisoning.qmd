
---
title: "Pandora: Jailbreak GPTs by Retrieval Augmented Generation Poisoning"
id: "2402.08416v1"
description: "LLMs face security risks, including indirect jailbreak attacks like Pandora, which manipulates RAG to generate malicious content."
author: Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu
date: "2024-02-13"
image: "../../img/2402.08416v1/image_1.png"
categories: ['robustness', 'production', 'security', 'architectures']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.08416v1/image_1.png)

### **Summary:**
- Large Language Models (LLMs) are increasingly being used in various applications, making their security a critical concern.
- Jailbreak attacks, which manipulate LLMs to generate malicious content, are a significant vulnerability.
- The integration of Retrieval Augmented Generation (RAG) into LLMs introduces new avenues for indirect jailbreak attacks.

### **Major Findings:**
1. **Novel Attack Vector:** The study introduces a novel attack vector named Retrieval Augmented Generation Poisoning (PANDORA) that exploits the synergy between LLMs and RAG through prompt manipulation to generate unexpected responses.
2. **High Success Rates:** PANDORA successfully conducts jailbreak attacks in four different scenarios, achieving higher success rates than direct attacks, with 64.3% for GPT-3.5 and 34.8% for GPT-4.
3. **Vulnerability of GPT Models:** The study reveals the vulnerability of GPT models to sophisticated attack strategies, highlighting the need for improvements in model resilience and security measures.

### **Analysis and Critique:**
- The study provides valuable insights into the vulnerability of GPT models to indirect jailbreak attacks, shedding light on the need for enhanced security measures.
- However, the study primarily focuses on the development and evaluation of the PANDORA attack method, with limited discussion on potential mitigation strategies or ethical implications.
- Future research should aim to develop automated RAG Poisoning methods, enhance interpretability, and devise effective mitigation strategies to protect GPT models from malicious attacks.

Overall, the study provides a comprehensive analysis of indirect jailbreak attacks on LLMs, emphasizing the need for improved security measures and further research in this area. However, it would benefit from a more in-depth discussion of potential mitigation strategies and ethical considerations.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-14       |
| Abstract | [https://arxiv.org/abs/2402.08416v1](https://arxiv.org/abs/2402.08416v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.08416v1](https://browse.arxiv.org/html/2402.08416v1)       |
| Truncated       | False       |
| Word Count       | 7685       |