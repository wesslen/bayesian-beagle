
---
title: "Do Machines and Humans Focus on Similar Code? Exploring Explainability of Large Language Models in Code Summarization"
id: "2402.14182v1"
description: "Language models lack explainability in code summarization, with no alignment between human and model focus."
author: Jiliang Li, Yifan Zhang, Zachary Karas, Collin McMillan, Kevin Leach, Yu Huang
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14182v1/x1.png"
categories: ['programming', 'social-sciences']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14182v1/x1.png)

### **Summary:**
- Recent language models have shown proficiency in summarizing source code, but lack explainability.
- The study aimed to investigate the explainability of language models in code summarization through the lens of human comprehension.
- The study found no statistically significant relationship between language models' focus and human programmers' attention.

### Major Findings:
1. Lack of statistically significant relationship between language models' focus and human programmers' attention.
2. Alignment between model and human foci does not dictate the quality of the LLM-generated summaries.
3. Inability to align human focus with SHAP-based model focus measures.

### Analysis and Critique:
- Lack of correlation between human and machine foci across the Java methods studied.
- Lack of correlation between the quality of summaries generated by language models and how well their focus on code aligns with humans'.
- SHAP measure of feature attribution did not correlate with human eye attention in the measures or models studied.

The study highlights the need for alternative methods to assess feature influence in black-box language models for code summarization, aiming for better alignment with human attention. The findings also suggest that machines may reason about code differently from humans when tasked to summarize source code.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.14182v1](https://arxiv.org/abs/2402.14182v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14182v1](https://browse.arxiv.org/html/2402.14182v1)       |
| Truncated       | False       |
| Word Count       | 4726       |