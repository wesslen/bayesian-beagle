
---
title: "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts"
id: "2408.12226v1"
description: "Automated models (EvalYaks) effectively evaluate CEFR B2 English speaking assessments with 96% accuracy, offering scalable, automated language proficiency evaluation."
author: Nicy Scaria, Silvester John Joseph Kennedy, Thomas Latinovich, Deepak Subramani
date: "2024-08-22"
image: "https://browse.arxiv.org/html/2408.12226v1/extracted/5807011/Json.png"
categories: ['prompt-engineering', 'education']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.12226v1/extracted/5807011/Json.png)

**Summary:**

The paper presents a study on automating the evaluation of CEFR B2 English speaking assessments in e-learning environments using conversation transcripts. The authors evaluate the capability of leading open-source and commercial Large Language Models (LLMs) to score candidates' performance across various criteria in the CEFR B2 speaking exam. They also create a new expert-validated, CEFR-aligned synthetic conversational dataset with transcripts rated at different assessment scores. The authors then perform parameter-efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a family of models called EvalYaks. Four models in this family are for assessing the four sections of the CEFR B2 speaking exam, one for identifying the CEFR level of vocabulary and generating level-specific vocabulary, and another for detecting the CEFR level of text and generating level-specific text. EvalYaks achieved an average acceptable accuracy of 96%, a degree of variation of 0.35 levels, and performed 3 times better than the next best model. The study demonstrates that a 7B parameter LLM instruction-tuned with high-quality CEFR-aligned assessment data can effectively evaluate and score CEFR B2 English speaking assessments.

**Major Findings:**

1. EvalYaks, a family of models developed through parameter-efficient instruction tuning of Mistral Instruct 7B v0.2, achieved an average acceptable accuracy of 96% in evaluating CEFR B2 English speaking assessments.
2. EvalYaks performed 3 times better than the next best model in evaluating CEFR B2 English speaking assessments.
3. A 7B parameter LLM instruction-tuned with high-quality CEFR-aligned assessment data can effectively evaluate and score CEFR B2 English speaking assessments.

**Analysis and Critique:**

The paper presents a promising approach to automating the evaluation of CEFR B2 English speaking assessments using LLMs. The authors' creation of a new expert-validated, CEFR-aligned synthetic conversational dataset and the development of EvalYaks through parameter-efficient instruction tuning are significant contributions to the field. However, the study has some limitations. The authors do not discuss the potential biases in the synthetic dataset or the impact of cultural and

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.12226v1](https://arxiv.org/abs/2408.12226v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.12226v1](https://browse.arxiv.org/html/2408.12226v1)       |
| Truncated       | False       |
| Word Count       | 11370       |