
---
title: "ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding"
id: "2402.11889v1"
description: "ROSE method boosts safety of large language models without additional training, improving output."
author: Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, Dacheng Tao
date: "2024-02-19"
image: "../../img/2402.11889v1/image_1.png"
categories: ['robustness', 'security']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.11889v1/image_1.png)

### Summary:
- The Reverse prOmpt contraStive dEcoding (ROSE) method aims to improve the safety of existing instruction-tuned large language models (LLMs) without additional training by suppressing undesired outputs induced by reverse prompts to boost the probability of safe output. Experiments show consistent and significant safety improvements across various LLMs, with potential for combination with other safety-tuned methods for better performance.
- The ROSE method consistently outperforms regular decoding and other inference-time counterparts, with in-depth analyses on the impact of different prompts and the parameter Î±. It demonstrates robustness and complementary benefits to safety-tuned methods, enhancing the safety and general-purpose ability of LLMs.
- ROSE is effective in enhancing the safety performance of existing safety-tuned LLMs, defending against attacks and encouraging safer outputs, with potential for broader applicability in improving LLM safety.

### Major Findings:
1. The ROSE method significantly improves safety across various LLMs without additional training.
2. ROSE consistently outperforms regular decoding and other inference-time counterparts, demonstrating robustness and complementary benefits to safety-tuned methods.
3. ROSE enhances the safety performance of existing safety-tuned LLMs, defending against attacks and encouraging safer outputs.

### Analysis and Critique:
- The study demonstrates the potential of the ROSE method to address safety concerns associated with LLMs, offering consistent and significant safety improvements across various tasks and LLMs.
- The method's flexibility and potential for combination with other safety-tuned methods make it a valuable contribution to the field of LLM research.
- The comparison between automatic evaluators and human evaluation adds credibility to the study's findings, contributing to the comprehensive understanding of the evaluation process and the reliability of the results.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-06-05       |
| Abstract | [https://arxiv.org/abs/2402.11889v1](https://arxiv.org/abs/2402.11889v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.11889v1](https://browse.arxiv.org/html/2402.11889v1)       |
| Truncated       | True       |
| Word Count       | 17552       |