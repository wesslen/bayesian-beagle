
---
title: "Comparison of Large Language Models for Generating Contextually Relevant Questions"
id: "2407.20578v1"
description: "LLMs, like GPT-3.5 and Llama 2-Chat 13B, excel at generating clear, relevant, and aligned questions for educational use."
author: Ivo Lodovico Molina, Valdemar Švábenský, Tsubasa Minematsu, Li Chen, Fumiya Okubo, Atsushi Shimada
date: "2024-07-30"
image: "../../../bayesian-beagle.png"
categories: ['education', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../../bayesian-beagle.png)

### Summary:

This study compares the effectiveness of three Large Language Models (LLMs) - GPT-3.5 Turbo, Flan T5 XXL, and Llama 2-Chat 13B - in generating contextually relevant educational questions using university slide text without fine-tuning. The two-step pipeline involves extracting answer phrases from slides using Llama 2-Chat 13B and then generating questions for each answer using the three models. A survey was conducted with students to evaluate the generated questions across five metrics: clarity, relevance, difficulty, slide relation, and question-answer alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan T5 XXL, particularly in terms of clarity and question-answer alignment. GPT-3.5 excels at tailoring questions to match input answers.

### Major Findings:

1. GPT-3.5 and Llama 2-Chat 13B outperform Flan T5 XXL in generating contextually relevant educational questions from university slide text without fine-tuning.
2. GPT-3.5 excels at tailoring questions to match input answers, demonstrating superior question-answer alignment.
3. All three models score high in clarity, relevance, and slide relation, making them immediately applicable for educational applications.

### Analysis and Critique:

1. The study does not explore the impact of fine-tuning the models on their performance, which could potentially improve the quality of AI-generated questions.
2. The evaluation of the generated questions is based on a survey with students, which may introduce subjectivity and bias in the results.
3. The study does not compare the performance of the LLMs with traditional rule-based or supervised learning approaches for question generation.
4. The study focuses on slide-based teaching materials, and the findings may not generalize to other types of educational content.
5. The study does not address the ethical implications of using LLMs for question generation, such as potential biases in the generated questions or the impact on human educators.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-06       |
| Abstract | [https://arxiv.org/abs/2407.20578v1](https://arxiv.org/abs/2407.20578v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.20578v1](https://browse.arxiv.org/html/2407.20578v1)       |
| Truncated       | False       |
| Word Count       | 2839       |