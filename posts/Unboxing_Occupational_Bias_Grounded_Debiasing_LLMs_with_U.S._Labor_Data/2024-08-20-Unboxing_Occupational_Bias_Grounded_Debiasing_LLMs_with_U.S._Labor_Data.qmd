
---
title: "Unboxing Occupational Bias: Grounded Debiasing LLMs with U.S. Labor Data"
id: "2408.11247v1"
description: "LLMs can amplify societal biases; this study proposes a debiasing mechanism using NBLS data, reducing bias in seven LLMs."
author: Atmika Gorti, Manas Gaur, Aman Chadha
date: "2024-08-20"
image: "https://browse.arxiv.org/html/2408.11247v1/extracted/5803852/architecture_jair_2.png"
categories: ['hci']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2408.11247v1/extracted/5803852/architecture_jair_2.png)

### Summary:

The article discusses the issue of inherent biases in Large Language Models (LLMs) and their potential to reinforce harmful stereotypes related to gender, occupation, and other sensitive categories. The authors propose a comprehensive bias analysis framework to evaluate LLMs, including Falcon, GPT-Neo, Gemini 1.5, and GPT-4o, using robust statistical measures. They also introduce a simple yet effective prompting method using contextual examples from U.S. NBLS data to address these biases. The study reveals significant levels of bias in LLMs that are often overlooked by existing bias detection techniques. The proposed debiasing method demonstrates a substantial reduction in bias scores, highlighting its efficacy in creating fairer and more reliable LLMs.

### Major Findings:

1. The study reveals significant levels of bias in LLMs, which are often overlooked by existing bias detection techniques.
2. A comprehensive bias analysis framework was developed to evaluate LLMs using robust statistical measures, including the Kolmogorov-Smirnov (KS) test for normality and the ANOVA test.
3. A simple yet effective prompting method using contextual examples from U.S. NBLS data was introduced to address these biases, achieving, on average, a 65% reduction in bias.

### Analysis and Critique:

The article provides a valuable contribution to the field by highlighting the issue of biases in LLMs and proposing a debiasing method. However, the study has some limitations. The authors only evaluate seven LLMs, which may not be representative of the entire field. Additionally, the proposed debiasing method relies on external datasets, which may not always be available or applicable. The study also does not address the potential impact of biases in LLMs on downstream tasks, such as natural language understanding or generation. Further research is needed to explore these aspects and develop more robust debiasing techniques.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-08-27       |
| Abstract | [https://arxiv.org/abs/2408.11247v1](https://arxiv.org/abs/2408.11247v1)        |
| HTML     | [https://browse.arxiv.org/html/2408.11247v1](https://browse.arxiv.org/html/2408.11247v1)       |
| Truncated       | False       |
| Word Count       | 5208       |