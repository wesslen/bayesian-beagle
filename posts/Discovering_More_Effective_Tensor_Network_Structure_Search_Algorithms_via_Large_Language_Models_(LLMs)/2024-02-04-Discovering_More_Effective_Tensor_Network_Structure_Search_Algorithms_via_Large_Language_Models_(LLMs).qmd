
---
title: "Discovering More Effective Tensor Network Structure Search Algorithms via Large Language Models (LLMs)"
id: "2402.02456v1"
description: "GPTN-SS uses large language models to develop effective tensor network structure search algorithms."
author: Junhua Zeng, Guoxu Zhou, Chao Li, Zhun Sun, Qibin Zhao
date: "2024-02-04"
image: "../../img/2402.02456v1/image_1.png"
categories: ['prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](../../img/2402.02456v1/image_1.png)

### **Summary:**
- Tensor network structure search (TN-SS) aims to find suitable tensor network (TN) structures for high-dimensional problems.
- Existing algorithms for TN-SS are challenging to use and require a labor-intensive development process.
- The GPTN-SS approach leverages large language models (LLMs) to automatically design TN-SS algorithms.
- GPTN-SS effectively leverages insights from existing methods to develop novel TN-SS algorithms that achieve a better balance between exploration and exploitation.

### Major Findings:
1. Tensor networks are powerful methods for analyzing and computing high-dimensional problems.
2. Existing TN-SS algorithms, such as evolutionary algorithms and local sampling, have limitations and require a lot of evaluation.
3. GPTN-SS leverages LLMs to automatically design TN-SS algorithms that demonstrate superior performance in searching for high-quality TN structures.

### Analysis and Critique:
- The GPTN-SS approach shows promise in automating the design of TN-SS algorithms and achieving better performance. However, the final performance of the designed algorithms is subject to variation due to changes in LLMs.
- The method's effectiveness is demonstrated through real-world applications such as image compression and model parameters compression.
- The GPTN-SS approach addresses the limitations of existing TN-SS algorithms and provides a more efficient and automated solution. However, the reliance on LLMs introduces variability in the performance of the designed algorithms. Further research is needed to address this limitation and improve the robustness of the approach.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-02-26       |
| Abstract | [https://arxiv.org/abs/2402.02456v1](https://arxiv.org/abs/2402.02456v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.02456v1](https://browse.arxiv.org/html/2402.02456v1)       |
| Truncated       | False       |
| Word Count       | 12564       |