
---
title: "Arabic Automatic Story Generation with Large Language Models"
id: "2407.07551v1"
description: "This work generates Arabic stories from LLMs, using MT and GPT-4 data, achieving coherent results in MSA and Arabic dialects."
author: Ahmed Oumar El-Shangiti, Fakhraddin Alwajih, Muhammad Abdul-Mageed
date: "2024-07-10"
image: "https://browse.arxiv.org/html/2407.07551v1/x1.png"
categories: ['programming', 'prompt-engineering']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.07551v1/x1.png)

### Summary:

This study focuses on the task of generating stories from large language models (LLMs) in Arabic, a task that has been under-explored in the Arabic NLP community. The authors introduce a novel approach to automatic story generation using the Arabic LLM, AraLLaMa, which is fine-tuned with both translated and synthetic datasets to optimize its story-generating capabilities. The study presents two fine-tuning strategies: one involving direct application of a synthetic dataset produced by GPT-4, and another beginning with an analogous synthetic dataset translated from English. The efficacy of the model is assessed through human evaluation, which confirmed its ability to produce coherent and fluent narratives as per specified instructions.

### Major Findings:

1. The authors introduce powerful models capable of generating coherent and fluent stories in Modern Standard Arabic (MSA) and two Arabic dialects (Egyptian and Moroccan).
2. A new framework for Arabic automatic story evaluation based on LLMs is offered.
3. Two novel datasets for automatic story generation are developed: one consisting of translated narratives from the TinyStories dataset, and another comprising a synthetic dataset created using GPT-4, featuring narratives in MSA and two dialects.
4. The fine-tuned models are compared against AceGPT-7B, GPT-3.5, and Command-R222 using extensive automatic and human evaluations.

### Analysis and Critique:

* The study presents a significant contribution to the Arabic NLP community by addressing the scarcity of Arabic short story data and the minimal focus from the research community on automatic story generation in Arabic.
* The use of both translated and synthetic datasets for fine-tuning AraLLaMa is a novel approach that could be further explored and refined in future research.
* The human evaluation conducted in this study is a valuable method for assessing the model's ability to generate coherent and fluent narratives, but it would be beneficial to include more diverse evaluators to ensure a broader perspective on the model's performance.
* The study could be improved by providing more detailed information on the methodology used for fine-tuning the models, as well as the specific criteria used for human evaluation.

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-16       |
| Abstract | [https://arxiv.org/abs/2407.07551v1](https://arxiv.org/abs/2407.07551v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.07551v1](https://browse.arxiv.org/html/2407.07551v1)       |
| Truncated       | False       |
| Word Count       | 6596       |