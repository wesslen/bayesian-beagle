
---
title: "Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models"
id: "2402.14714v1"
description: "EEVE-Korean-v1.0 is a leading Korean pre-trained model for text understanding."
author: Seungduk Kim, Seungtaek Choi, Myeongho Jeong
date: "2024-02-22"
image: "https://browse.arxiv.org/html/2402.14714v1/extracted/5424172/figure_latex/figures/figure-stages-0.png"
categories: ['production']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2402.14714v1/extracted/5424172/figure_latex/figures/figure-stages-0.png)

### **Summary:**
- EEVE-Korean-v1.0 is a Korean adaptation of large language models that demonstrates remarkable capabilities across English and Korean text understanding.
- The report introduces an efficient and effective vocabulary expansion (EEVE) method, which significantly boosts non-English proficiency within just 2 billion tokens.
- The model ranks as the leading Korean pre-trained model in the open-source community, surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard.

### **Major Findings:**
1. EEVE-Korean-v1.0 ranks as the leading Korean pre-trained model in the open-source community, surpassing most instruction-tuned LLMs on the Open Ko-LLM Leaderboard.
2. The EEVE method significantly boosts non-English proficiency within just 2 billion tokens.
3. The model demonstrates remarkable capabilities across English and Korean text understanding.

### **Analysis and Critique:**
- The report does not address potential biases or limitations in the methodology used for the vocabulary expansion.
- The evaluation of the models is limited to Korean and English tasks, and there is no discussion of the model's performance in other languages.
- The report does not provide a detailed analysis of the computational efficiency and cost-effectiveness of the EEVE-Korean models compared to other models.

## Appendix

|          |          |
|----------|----------|
| Model     | gpt-3.5-turbo-1106       |
| Date Generated     | 2024-03-13       |
| Abstract | [https://arxiv.org/abs/2402.14714v1](https://arxiv.org/abs/2402.14714v1)        |
| HTML     | [https://browse.arxiv.org/html/2402.14714v1](https://browse.arxiv.org/html/2402.14714v1)       |
| Truncated       | False       |
| Word Count       | 4913       |