
---
title: "Is Your AI-Generated Code Really Secure? Evaluating Large Language Models on Secure Code Generation with CodeSecEval"
id: "2407.02395v1"
description: "LLMs for code generation/repair risk security vulnerabilities. This study evaluates and enhances their security, introducing CodeSecEval dataset and strategies to mitigate vulnerabilities."
author: Jiexin Wang, Xitong Luo, Liuwen Cao, Hongkui He, Hailin Huang, Jiayuan Xie, Adam Jatowt, Yi Cai
date: "2024-07-02"
image: "https://browse.arxiv.org/html/2407.02395v1/extracted/5706036/Figures/dataset1.png"
categories: ['robustness', 'programming', 'security']
format:
  html:
    code-overflow: wrap
---

![](https://browse.arxiv.org/html/2407.02395v1/extracted/5706036/Figures/dataset1.png)

### Summary:

The study titled "Is Your AI-Generated Code Really Secure? Evaluating Large Language Models on Secure Code Generation with CodeSecEval" aims to present a comprehensive study on the security aspects of code LLMs. The authors introduce CodeSecEval, a meticulously curated dataset designed to address 44 critical vulnerability types with 180 distinct samples. The dataset serves as the foundation for the automatic evaluation of code models in two crucial tasks: code generation and code repair, with a strong emphasis on security. The experimental results reveal that current models frequently overlook security issues during both code generation and repair processes, resulting in the creation of vulnerable code. In response, the authors propose different strategies that leverage vulnerability-aware information and insecure code explanations to mitigate these security vulnerabilities. The study highlights that certain vulnerability types particularly challenge model performance, influencing their effectiveness in real-world applications. The authors believe their study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing LLMs, thereby leading to safer and more trustworthy model deployment.

### Major Findings:

1. Current models frequently overlook security issues during both code generation and repair processes, resulting in the creation of vulnerable code.
2. The study proposes different strategies that leverage vulnerability-aware information and insecure code explanations to mitigate security vulnerabilities.
3. Certain vulnerability types particularly challenge model performance, influencing their effectiveness in real-world applications.

### Analysis and Critique:

The study provides a comprehensive evaluation of the security aspects of code LLMs, which is a crucial aspect of software engineering. The introduction of the CodeSecEval dataset is a significant contribution, as it allows for the automatic evaluation of code models in two crucial tasks: code generation and code repair. The experimental results highlight the limitations of current models in addressing security issues, which is an important finding for the software engineering community.

However, the study does not provide a detailed analysis of the specific vulnerability types that challenge model performance. It would be beneficial to have a more in-depth analysis of these vulnerability types to better understand their impact on model performance. Additionally, the study does not discuss the potential implications of these findings for the development of new models or the improvement of existing ones.

Furthermore, the study does not discuss the potential

## Appendix

|          |          |
|----------|----------|
| Model     | accounts/fireworks/models/mixtral-8x22b-instruct       |
| Date Generated     | 2024-07-09       |
| Abstract | [https://arxiv.org/abs/2407.02395v1](https://arxiv.org/abs/2407.02395v1)        |
| HTML     | [https://browse.arxiv.org/html/2407.02395v1](https://browse.arxiv.org/html/2407.02395v1)       |
| Truncated       | False       |
| Word Count       | 9474       |